[
  {
    "id": "arXiv:2502.14880",
    "title": "KKA: Improving Vision Anomaly Detection through Anomaly-related Knowledge from Large Language Models",
    "abstract": "           Vision anomaly detection, particularly in unsupervised settings, often struggles to distinguish between normal samples and anomalies due to the wide variability in anomalies. Recently, an increasing number of studies have focused on generating anomalies to help detectors learn more effective boundaries between normal samples and anomalies. However, as the generated anomalies are often derived from random factors, they frequently lack realism. Additionally, randomly generated anomalies typically offer limited support in constructing effective boundaries, as most differ substantially from normal samples and lie far from the boundary. To address these challenges, we propose Key Knowledge Augmentation (KKA), a method that extracts anomaly-related knowledge from large language models (LLMs). More specifically, KKA leverages the extensive prior knowledge of LLMs to generate meaningful anomalies based on normal samples. Then, KKA classifies the generated anomalies as easy anomalies and hard anomalies according to their similarity to normal samples. Easy anomalies exhibit significant differences from normal samples, whereas hard anomalies closely resemble normal samples. KKA iteratively updates the generated anomalies, and gradually increasing the proportion of hard anomalies to enable the detector to learn a more effective boundary. Experimental results show that the proposed method significantly improves the performance of various vision anomaly detectors while maintaining low generation costs. The code for CMG can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.14880",
    "authors": [
      "Dong Chen",
      "Zhengqing Hu",
      "Peiguang Fan",
      "Yueting Zhuang",
      "Yafei Li",
      "Qidong Liu",
      "Xiaoheng Jiang",
      "Mingliang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14884",
    "title": "SEM-CLIP: Precise Few-Shot Learning for Nanoscale Defect Detection in Scanning Electron Microscope Image",
    "abstract": "           In the field of integrated circuit manufacturing, the detection and classification of nanoscale wafer defects are critical for subsequent root cause analysis and yield enhancement. The complex background patterns observed in scanning electron microscope (SEM) images and the diverse textures of the defects pose significant challenges. Traditional methods usually suffer from insufficient data, labels, and poor transferability. In this paper, we propose a novel few-shot learning approach, SEM-CLIP, for accurate defect classification and segmentation. SEM-CLIP customizes the Contrastive Language-Image Pretraining (CLIP) model to better focus on defect areas and minimize background distractions, thereby enhancing segmentation accuracy. We employ text prompts enriched with domain knowledge as prior information to assist in precise analysis. Additionally, our approach incorporates feature engineering with textual guidance to categorize defects more effectively. SEM-CLIP requires little annotated data, substantially reducing labor demands in the semiconductor industry. Extensive experimental validation demonstrates that our model achieves impressive classification and segmentation results under few-shot learning scenarios.         ",
    "url": "https://arxiv.org/abs/2502.14884",
    "authors": [
      "Qian Jin",
      "Yuqi Jiang",
      "Xudong Lu",
      "Yumeng Liu",
      "Yining Chen",
      "Dawei Gao",
      "Qi Sun",
      "Cheng Zhuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14891",
    "title": "CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection",
    "abstract": "           Collaborative 3D object detection holds significant importance in the field of autonomous driving, as it greatly enhances the perception capabilities of each individual agent by facilitating information exchange among multiple agents. However, in practice, due to pose estimation errors and time delays, the fusion of information across agents often results in feature representations with spatial and temporal noise, leading to detection errors. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to explore the use of diffusion models to address the noise problem between multi-agent systems. In this work, we propose CoDiff, a novel robust collaborative perception framework that leverages the potential of diffusion models to generate more comprehensive and clearer feature representations. To the best of our knowledge, this is the first work to apply diffusion models to multi-agent collaborative perception. Specifically, we project high-dimensional feature map into the latent space of a powerful pre-trained autoencoder. Within this space, individual agent information serves as a condition to guide the diffusion model's sampling. This process denoises coarse feature maps and progressively refines the fused features. Experimental study on both simulated and real-world datasets demonstrates that the proposed framework CoDiff consistently outperforms existing relevant methods in terms of the collaborative object detection performance, and exhibits highly desired robustness when the pose and delay information of agents is with high-level noise.         ",
    "url": "https://arxiv.org/abs/2502.14891",
    "authors": [
      "Zhe Huang",
      "Shuo Wang",
      "Yongcai Wang",
      "Lei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14894",
    "title": "FOCUS on Contamination: A Geospatial Deep Learning Framework with a Noise-Aware Loss for Surface Water PFAS Prediction",
    "abstract": "           Per and polyfluoroalkyl substances (PFAS), chemicals found in products like non-stick cookware, are unfortunately persistent environmental pollutants with severe health risks. Accurately mapping PFAS contamination is crucial for guiding targeted remediation efforts and protecting public and environmental health, yet detection across large regions remains challenging due to the cost of testing and the difficulty of simulating their spread. In this work, we introduce FOCUS, a geospatial deep learning framework with a label noise-aware loss function, to predict PFAS contamination in surface water over large regions. By integrating hydrological flow data, land cover information, and proximity to known PFAS sources, our approach leverages both spatial and environmental context to improve prediction accuracy. We evaluate the performance of our approach through extensive ablation studies and comparative analyses against baselines like sparse segmentation, as well as existing scientific methods, including Kriging and pollutant transport simulations. Results highlight our framework's potential for scalable PFAS monitoring.         ",
    "url": "https://arxiv.org/abs/2502.14894",
    "authors": [
      "Jowaria Khan",
      "Alexa Friedman",
      "Sydney Evans",
      "Runzi Wang",
      "Kaley Beins",
      "David Andrews",
      "Elizabeth Bondi-Kelly"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14895",
    "title": "High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation",
    "abstract": "           Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored. To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting. Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks. With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals. The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over $16\\times$ higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions.         ",
    "url": "https://arxiv.org/abs/2502.14895",
    "authors": [
      "Ziye Wang",
      "Yiran Qin",
      "Lin Zeng",
      "Ruimao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.14908",
    "title": "KOALA: Knowledge Conflict Augmentations for Robustness in Vision Language Models",
    "abstract": "           The robustness of large language models (LLMs) against knowledge conflicts in unimodal question answering systems has been well studied. However, the effect of conflicts in information sources on vision language models (VLMs) in multimodal settings has not yet been explored. In this work, we propose \\segsub, a framework that applies targeted perturbations to image sources to study and improve the robustness of VLMs against three different types of knowledge conflicts, namely parametric, source, and counterfactual conflicts. Contrary to prior findings that showed that LLMs are sensitive to parametric conflicts arising from textual perturbations, we find VLMs are largely robust to image perturbation. On the other hand, VLMs perform poorly on counterfactual examples (<30% accuracy) and fail to reason over source conflicts (<1% accuracy). We also find a link between hallucinations and image context, with GPT-4o prone to hallucination when presented with highly contextualized counterfactual examples. While challenges persist with source conflicts, finetuning models significantly improves reasoning over counterfactual samples. Our findings highlight the need for VLM training methodologies that enhance their reasoning capabilities, particularly in addressing complex knowledge conflicts between multimodal sources.         ",
    "url": "https://arxiv.org/abs/2502.14908",
    "authors": [
      "Peter Carragher",
      "Nikitha Rao",
      "Abhinand Jha",
      "R Raghav",
      "Kathleen M. Carley"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14910",
    "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
    "abstract": "           Large Language Models (LLMs) have achieved remarkable success in natural language processing tasks, but their massive size and computational demands hinder their deployment in resource-constrained environments. Existing structured pruning methods address this issue by removing redundant structures (e.g., elements, channels, layers) from the model. However, these methods employ a heuristic pruning strategy, which leads to suboptimal performance. Besides, they also ignore the data characteristics when pruning the model. To overcome these limitations, we propose EvoP, an evolutionary pruning framework for robust LLM inference. EvoP first presents a cluster-based calibration dataset sampling (CCDS) strategy for creating a more diverse calibration dataset. EvoP then introduces an evolutionary pruning pattern searching (EPPS) method to find the optimal pruning pattern. Compared to existing structured pruning techniques, EvoP achieves the best performance while maintaining the best efficiency. Experiments across different LLMs and different downstream tasks validate the effectiveness of the proposed EvoP, making it a practical and scalable solution for deploying LLMs in real-world applications.         ",
    "url": "https://arxiv.org/abs/2502.14910",
    "authors": [
      "Shangyu Wu",
      "Hongchao Du",
      "Ying Xiong",
      "Shuai Chen",
      "Tei-wei Kuo",
      "Nan Guan",
      "Chun Jason Xue"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14921",
    "title": "The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
    "abstract": "           How much information about training samples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we design membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned model but only to the synthetic data. We show that such data-based MIAs do significantly better than a random guess, meaning that synthetic data leaks information about the training data. Further, we find that canaries crafted to maximize vulnerability to model-based MIAs are sub-optimal for privacy auditing when only synthetic data is released. Such out-of-distribution canaries have limited influence on the model's output when prompted to generate useful, in-distribution synthetic data, which drastically reduces their vulnerability. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries with an in-distribution prefix and a high-perplexity suffix that leave detectable traces in synthetic data. This enhances the power of data-based MIAs and provides a better assessment of the privacy risks of releasing synthetic data generated by LLMs.         ",
    "url": "https://arxiv.org/abs/2502.14921",
    "authors": [
      "Matthieu Meeus",
      "Lukas Wutschitz",
      "Santiago Zanella-B\u00e9guelin",
      "Shruti Tople",
      "Reza Shokri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14923",
    "title": "AI Thinking as a Meaning-Centered Framework: Reimagining Language Technologies Through Community Agency",
    "abstract": "           While language technologies have advanced significantly, current approaches fail to address the complex sociocultural dimensions of linguistic preservation. AI Thinking proposes a meaning-centered framework that would transform technological development from creating tools FOR communities to co-creating solutions WITH them. This approach recognizes that meaningful solutions emerge through the interplay of cultural understanding, community agency, and technological innovation. The proposal articulates a holistic methodology and a five-layer technological ecosystem where communities maintain control over their linguistic and cultural knowledge representation. This systematic integration of community needs, cultural preservation, and advanced capabilities could revolutionize how we approach linguistic diversity preservation in the digital age.         ",
    "url": "https://arxiv.org/abs/2502.14923",
    "authors": [
      "Jose F Quesada"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14926",
    "title": "DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for LoRaWAN-related engineering task",
    "abstract": "           This paper investigates the performance of 16 Large Language Models (LLMs) in automating LoRaWAN-related engineering tasks involving optimal placement of drones and received power calculation under progressively complex zero-shot, natural language prompts. The primary research question is whether lightweight, locally executed LLMs can generate correct Python code for these tasks. To assess this, we compared locally run models against state-of-the-art alternatives, such as GPT-4 and DeepSeek-V3, which served as reference points. By extracting and executing the Python functions generated by each model, we evaluated their outputs on a zero-to-five scale. Results show that while DeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller models-particularly Phi-4 and LLaMA-3.3-also demonstrated strong performance, underscoring the viability of lightweight alternatives. Other models exhibited errors stemming from incomplete understanding or syntactic issues. These findings illustrate the potential of LLM-based approaches for specialized engineering applications while highlighting the need for careful model selection, rigorous prompt design, and targeted domain fine-tuning to achieve reliable outcomes.         ",
    "url": "https://arxiv.org/abs/2502.14926",
    "authors": [
      "Daniel Fernandes",
      "Jo\u00e3o P. Matos-Carvalho",
      "Carlos M. Fernandes",
      "Nuno Fachada"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.14932",
    "title": "Learning to Retrieve and Reason on Knowledge Graph through Active Self-Reflection",
    "abstract": "           Extensive research has investigated the integration of large language models (LLMs) with knowledge graphs to enhance the reasoning process. However, understanding how models perform reasoning utilizing structured graph knowledge remains underexplored. Most existing approaches rely on LLMs or retrievers to make binary judgments regarding the utilization of knowledge, which is too coarse. Meanwhile, there is still a lack of feedback mechanisms for reflection and correction throughout the entire reasoning path. This paper proposes an Active self-Reflection framework for knowledge Graph reasoning ARG, introducing for the first time an end-to-end training approach to achieve iterative reasoning grounded on structured graphs. Within the framework, the model leverages special tokens to \\textit{actively} determine whether knowledge retrieval is necessary, performs \\textit{reflective} critique based on the retrieved knowledge, and iteratively reasons over the knowledge graph. The reasoning paths generated by the model exhibit high interpretability, enabling deeper exploration of the model's understanding of structured knowledge. Ultimately, the proposed model achieves outstanding results compared to existing baselines in knowledge graph reasoning tasks.         ",
    "url": "https://arxiv.org/abs/2502.14932",
    "authors": [
      "Han Zhang",
      "Langshi Zhou",
      "Hanfang Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.14939",
    "title": "Online hand gesture recognition using Continual Graph Transformers",
    "abstract": "           Online continuous action recognition has emerged as a critical research area due to its practical implications in real-world applications, such as human-computer interaction, healthcare, and robotics. Among various modalities, skeleton-based approaches have gained significant popularity, demonstrating their effectiveness in capturing 3D temporal data while ensuring robustness to environmental variations. However, most existing works focus on segment-based recognition, making them unsuitable for real-time, continuous recognition scenarios. In this paper, we propose a novel online recognition system designed for real-time skeleton sequence streaming. Our approach leverages a hybrid architecture combining Spatial Graph Convolutional Networks (S-GCN) for spatial feature extraction and a Transformer-based Graph Encoder (TGE) for capturing temporal dependencies across frames. Additionally, we introduce a continual learning mechanism to enhance model adaptability to evolving data distributions, ensuring robust recognition in dynamic environments. We evaluate our method on the SHREC'21 benchmark dataset, demonstrating its superior performance in online hand gesture recognition. Our approach not only achieves state-of-the-art accuracy but also significantly reduces false positive rates, making it a compelling solution for real-time applications. The proposed system can be seamlessly integrated into various domains, including human-robot collaboration and assistive technologies, where natural and intuitive interaction is crucial.         ",
    "url": "https://arxiv.org/abs/2502.14939",
    "authors": [
      "Rim Slama",
      "Wael Rabah",
      "Hazem Wannous"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14948",
    "title": "Learning to Solve and Verify: A Self-Play Framework for Code and Test Generation",
    "abstract": "           Recent advances in large language models (LLMs) have improved their performance on coding benchmarks. However, improvement is plateauing due to the exhaustion of readily available high-quality data. Prior work has shown the potential of synthetic self-instruct data, but naively training on a model's own outputs can cause error accumulation, especially in coding tasks, where generalization may collapse due to overly simple or erroneous training data, highlighting the need for rigorous quality checks on synthetic data. In this work, we explore an effective approach whereby the model itself verifies the correctness of its own data. We thus propose Sol-Ver, a self-play solver-verifier framework that jointly improves a single model's code and test generation capacity. By iteratively refining code (LLM-as-a-solver) and tests (LLM-as-a-verifier) together, we boost both capabilities without relying on human annotations or larger teacher models. Experiments with the Llama 3.1 8B model demonstrate substantial performance enhancements, achieving average relative improvements of 19.63% in code generation and 17.49% in test generation on MBPP and LiveCodeBench.         ",
    "url": "https://arxiv.org/abs/2502.14948",
    "authors": [
      "Zi Lin",
      "Sheng Shen",
      "Jingbo Shang",
      "Jason Weston",
      "Yixin Nie"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.14966",
    "title": "CyberSentinel: An Emergent Threat Detection System for AI Security",
    "abstract": "           The rapid advancement of artificial intelligence (AI) has significantly expanded the attack surface for AI-driven cybersecurity threats, necessitating adaptive defense strategies. This paper introduces CyberSentinel, a unified, single-agent system for emergent threat detection, designed to identify and mitigate novel security risks in real time. CyberSentinel integrates: (1) Brute-force attack detection through SSH log analysis, (2) Phishing threat assessment using domain blacklists and heuristic URL scoring, and (3) Emergent threat detection via machine learning-based anomaly detection. By continuously adapting to evolving adversarial tactics, CyberSentinel strengthens proactive cybersecurity defense, addressing critical vulnerabilities in AI security.         ",
    "url": "https://arxiv.org/abs/2502.14966",
    "authors": [
      "Krti Tallam"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14976",
    "title": "EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models",
    "abstract": "           Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large Language Models (LLMs), which are further exacerbated by their multimodal nature. Existing defenses, including adversarial training, input transformations, and heuristic detection, are computationally expensive, architecture-dependent, and fragile against adaptive attacks. We introduce EigenShield, an inference-time defense leveraging Random Matrix Theory to quantify adversarial disruptions in high-dimensional VLM representations. Unlike prior methods that rely on empirical heuristics, EigenShield employs the spiked covariance model to detect structured spectral deviations. Using a Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it separates causal eigenvectors, which encode semantic information, from correlational eigenvectors that are susceptible to adversarial artifacts. By projecting embeddings onto the causal subspace, EigenShield filters adversarial noise without modifying model parameters or requiring adversarial training. This architecture-independent, attack-agnostic approach significantly reduces the attack success rate, establishing spectral analysis as a principled alternative to conventional defenses. Our results demonstrate that EigenShield consistently outperforms all existing defenses, including adversarial training, UNIGUARD, and CIDER.         ",
    "url": "https://arxiv.org/abs/2502.14976",
    "authors": [
      "Nastaran Darabi",
      "Devashri Naik",
      "Sina Tayebati",
      "Dinithi Jayasuriya",
      "Ranganath Krishnan",
      "Amit Ranjan Trivedi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.14987",
    "title": "Taming and Controlling Performance and Energy Trade-offs Automatically in Network Applications",
    "abstract": "           In this paper, we demonstrate that a server running a single latency-sensitive application can be treated as a black box to reduce energy consumption while meeting an SLA target. We find that when the mean offered load is stable, one can find the \"sweet spot\" settings in packet batching (via interrupt coalescing) and controlling the processing rate (DVFS) that represents optimal trade-offs in the interactions of the software stack and hardware with the arrival rate and composition of requests currently being served. Trying a few combinations of settings on the live system, an example Bayesian optimizer can find settings that reduce the energy consumption to meet a desired tail latency for the current load. This research demonstrates that: 1) without software changes, dramatic energy savings (up to 60%) can be achieved across diverse hardware systems if one controls batching and processing rate, 2) specialized research OSes that have been developed for performance can achieve more than 2x better energy efficiency than general-purpose OSes, and 3) a controller, agnostic to the application and system, can easily find energy-efficient settings for the offered load that meets SLA objectives.         ",
    "url": "https://arxiv.org/abs/2502.14987",
    "authors": [
      "Han Dong",
      "Yara Awad",
      "Sanjay Arora",
      "Orran Krieger",
      "Jonathan Appavoo"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)"
    ]
  },
  {
    "id": "arXiv:2502.14994",
    "title": "LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection",
    "abstract": "           The impressive achievements of generative models in creating high-quality videos have raised concerns about digital integrity and privacy vulnerabilities. Recent works of AI-generated content detection have been widely studied in the image field (e.g., deepfake), yet the video field has been unexplored. Large Vision Language Model (LVLM) has become an emerging tool for AI-generated content detection for its strong reasoning and multimodal capabilities. It breaks the limitations of traditional deep learning based methods faced with like lack of transparency and inability to recognize new artifacts. Motivated by this, we propose LAVID, a novel LVLMs-based ai-generated video detection with explicit knowledge enhancement. Our insight list as follows: (1) The leading LVLMs can call external tools to extract useful information to facilitate its own video detection task; (2) Structuring the prompt can affect LVLM's reasoning ability to interpret information in video content. Our proposed pipeline automatically selects a set of explicit knowledge tools for detection, and then adaptively adjusts the structure prompt by self-rewriting. Different from prior SOTA that trains additional detectors, our method is fully training-free and only requires inference of the LVLM for detection. To facilitate our research, we also create a new benchmark \\vidfor with high-quality videos generated from multiple sources of video generation tools. Evaluation results show that LAVID improves F1 scores by 6.2 to 30.2% over the top baselines on our datasets across four SOTA LVLMs.         ",
    "url": "https://arxiv.org/abs/2502.14994",
    "authors": [
      "Qingyuan Liu",
      "Yun-Yun Tsai",
      "Ruijian Zha",
      "Victoria Li",
      "Pengyuan Shi",
      "Chengzhi Mao",
      "Junfeng Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15006",
    "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions",
    "abstract": "           A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to ''black-box'' dynamics by learning an approximate discrete-time control barrier function and incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments.         ",
    "url": "https://arxiv.org/abs/2502.15006",
    "authors": [
      "Ji Yin",
      "Oswin So",
      "Eric Yang Yu",
      "Chuchu Fan",
      "Panagiotis Tsiotras"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.15008",
    "title": "Understanding the Design Principles of Link Prediction in Directed Settings",
    "abstract": "           Link prediction is a widely studied task in Graph Representation Learning (GRL) for modeling relational data. The early theories in GRL were based on the assumption of a symmetric adjacency matrix, reflecting an undirected setting. As a result, much of the following state-of-the-art research has continued to operate under this symmetry assumption, even though real-world data often involve crucial information conveyed through the direction of relationships. This oversight limits the ability of these models to fully capture the complexity of directed interactions. In this paper, we focus on the challenge of directed link prediction by evaluating key heuristics that have been successful in undirected settings. We propose simple but effective adaptations of these heuristics to the directed link prediction task and demonstrate that these modifications produce competitive performance compared to the leading Graph Neural Networks (GNNs) originally designed for undirected graphs. Through an extensive set of experiments, we derive insights that inform the development of a novel framework for directed link prediction, which not only surpasses baseline methods but also outperforms state-of-the-art GNNs on multiple benchmarks.         ",
    "url": "https://arxiv.org/abs/2502.15008",
    "authors": [
      "Jun Zhai",
      "Muberra Ozmen",
      "Thomas Markovich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15012",
    "title": "Graph in the Vault: Protecting Edge GNN Inference with Trusted Execution Environment",
    "abstract": "           Wide deployment of machine learning models on edge devices has rendered the model intellectual property (IP) and data privacy vulnerable. We propose GNNVault, the first secure Graph Neural Network (GNN) deployment strategy based on Trusted Execution Environment (TEE). GNNVault follows the design of 'partition-before-training' and includes a private GNN rectifier to complement with a public backbone model. This way, both critical GNN model parameters and the private graph used during inference are protected within secure TEE compartments. Real-world implementations with Intel SGX demonstrate that GNNVault safeguards GNN inference against state-of-the-art link stealing attacks with negligible accuracy degradation (<2%).         ",
    "url": "https://arxiv.org/abs/2502.15012",
    "authors": [
      "Ruyi Ding",
      "Tianhong Xu",
      "Aidong Adam Ding",
      "Yunsi Fei"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15015",
    "title": "Accelerating Neural Network Training: An Analysis of the AlgoPerf Competition",
    "abstract": "           The goal of the AlgoPerf: Training Algorithms competition is to evaluate practical speed-ups in neural network training achieved solely by improving the underlying training algorithms. In the external tuning ruleset, submissions must provide workload-agnostic hyperparameter search spaces, while in the self-tuning ruleset they must be completely hyperparameter-free. In both rulesets, submissions are compared on time-to-result across multiple deep learning workloads, training on fixed hardware. This paper presents the inaugural AlgoPerf competition's results, which drew 18 diverse submissions from 10 teams. Our investigation reveals several key findings: (1) The winning submission in the external tuning ruleset, using Distributed Shampoo, demonstrates the effectiveness of non-diagonal preconditioning over popular methods like Adam, even when compared on wall-clock runtime. (2) The winning submission in the self-tuning ruleset, based on the Schedule Free AdamW algorithm, demonstrates a new level of effectiveness for completely hyperparameter-free training algorithms. (3) The top-scoring submissions were surprisingly robust to workload changes. We also discuss the engineering challenges encountered in ensuring a fair comparison between different training algorithms. These results highlight both the significant progress so far, and the considerable room for further improvements.         ",
    "url": "https://arxiv.org/abs/2502.15015",
    "authors": [
      "Priya Kasimbeg",
      "Frank Schneider",
      "Runa Eschenhagen",
      "Juhan Bae",
      "Chandramouli Shama Sastry",
      "Mark Saroufim",
      "Boyuan Feng",
      "Less Wright",
      "Edward Z. Yang",
      "Zachary Nado",
      "Sourabh Medapati",
      "Philipp Hennig",
      "Michael Rabbat",
      "George E. Dahl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.15017",
    "title": "Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability",
    "abstract": "           Adversarial attacks in deep learning represent a significant threat to the integrity and reliability of machine learning models. Adversarial training has been a popular defence technique against these adversarial attacks. In this work, we capitalize on a network architecture, namely Deep Linearly Gated Networks (DLGN), which has better interpretation capabilities than regular deep network architectures. Using this architecture, we interpret robust models trained using PGD adversarial training and compare them with standard training. Feature networks in DLGN act as feature extractors, making them the only medium through which an adversary can attack the model. We analyze the feature network of DLGN with fully connected layers with respect to properties like alignment of the hyperplanes, hyperplane relation with PCA, and sub-network overlap among classes and compare these properties between robust and standard models. We also consider this architecture having CNN layers wherein we qualitatively (using visualizations) and quantitatively contrast gating patterns between robust and standard models. We uncover insights into hyperplanes resembling principal components in PGD-AT and STD-TR models, with PGD-AT hyperplanes aligned farther from the data points. We use path activity analysis to show that PGD-AT models create diverse, non-overlapping active subnetworks across classes, preventing attack-induced gating overlaps. Our visualization ideas show the nature of representations learnt by PGD-AT and STD-TR models.         ",
    "url": "https://arxiv.org/abs/2502.15017",
    "authors": [
      "Akshay G Rao",
      "Chandrashekhar Lakshminarayanan",
      "Arun Rajkumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.15029",
    "title": "Voter Model Meets Rumour Spreading: A Study of Consensus Protocols on Graphs with Agnostic Nodes [Extended Version]",
    "abstract": "           Problems of consensus in multi-agent systems are often viewed as a series of independent, simultaneous local decisions made between a limited set of options, all aimed at reaching a global agreement. Key challenges in these protocols include estimating the likelihood of various outcomes and finding bounds for how long it may take to achieve consensus, if it occurs at all. To date, little attention has been given to the case where some agents have no initial opinion. In this paper, we introduce a variant of the consensus problem which includes what we call `agnostic' nodes and frame it as a combination of two known and well-studied processes: voter model and rumour spreading. We show (1) a martingale that describes the probability of consensus for a given colour, (2) bounds on the number of steps for the process to end using results from rumour spreading and voter models, (3) closed formulas for the probability of consensus in a few special cases, and (4) that the computational complexity of estimating the probability with a Markov chain Monte Carlo process is $O(n^2 \\log n)$ for general graphs and $O(n\\log n)$ for Erd\u0151s-R\u00e9nyi graphs, which makes it an efficient method for estimating probabilities of consensus. Furthermore, we present experimental results suggesting that the number of runs needed for a given standard error decreases when the number of nodes increases.         ",
    "url": "https://arxiv.org/abs/2502.15029",
    "authors": [
      "Marcelo Matheus Gauy",
      "Anna Abramishvili",
      "Eduardo Colli",
      "Tiago Madeira",
      "Frederik Mallmann-Trenn",
      "Vin\u00edcius Franco Vasconcelos",
      "David Kohan Marzag\u00e3o"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2502.15051",
    "title": "Approximating Latent Manifolds in Neural Networks via Vanishing Ideals",
    "abstract": "           Deep neural networks have reshaped modern machine learning by learning powerful latent representations that often align with the manifold hypothesis: high-dimensional data lie on lower-dimensional manifolds. In this paper, we establish a connection between manifold learning and computational algebra by demonstrating how vanishing ideals can characterize the latent manifolds of deep networks. To that end, we propose a new neural architecture that (i) truncates a pretrained network at an intermediate layer, (ii) approximates each class manifold via polynomial generators of the vanishing ideal, and (iii) transforms the resulting latent space into linearly separable features through a single polynomial layer. The resulting models have significantly fewer layers than their pretrained baselines, while maintaining comparable accuracy, achieving higher throughput, and utilizing fewer parameters. Furthermore, drawing on spectral complexity analysis, we derive sharper theoretical guarantees for generalization, showing that our approach can in principle offer tighter bounds than standard deep networks. Numerical experiments confirm the effectiveness and efficiency of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2502.15051",
    "authors": [
      "Nico Pelleriti",
      "Max Zimmer",
      "Elias Wirth",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15054",
    "title": "GiGL: Large-Scale Graph Neural Networks at Snapchat",
    "abstract": "           Recent advances in graph machine learning (ML) with the introduction of Graph Neural Networks (GNNs) have led to a widespread interest in applying these approaches to business applications at scale. GNNs enable differentiable end-to-end (E2E) learning of model parameters given graph structure which enables optimization towards popular node, edge (link) and graph-level tasks. While the research innovation in new GNN layers and training strategies has been rapid, industrial adoption and utility of GNNs has lagged considerably due to the unique scale challenges that large-scale graph ML problems create. In this work, we share our approach to training, inference, and utilization of GNNs at Snapchat. To this end, we present GiGL (Gigantic Graph Learning), an open-source library to enable large-scale distributed graph ML to the benefit of researchers, ML engineers, and practitioners. We use GiGL internally at Snapchat to manage the heavy lifting of GNN workflows, including graph data preprocessing from relational DBs, subgraph sampling, distributed training, inference, and orchestration. GiGL is designed to interface cleanly with open-source GNN modeling libraries prominent in academia like PyTorch Geometric (PyG), while handling scaling and productionization challenges that make it easier for internal practitioners to focus on modeling. GiGL is used in multiple production settings, and has powered over 35 launches across multiple business domains in the last 2 years in the contexts of friend recommendation, content recommendation and advertising. This work details high-level design and tools the library provides, scaling properties, case studies in diverse business settings with industry-scale graphs, and several key lessons learned in employing graph ML at scale on large social data. GiGL is open-sourced at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15054",
    "authors": [
      "Tong Zhao",
      "Yozen Liu",
      "Matthew Kolodner",
      "Kyle Montemayor",
      "Elham Ghazizadeh",
      "Ankit Batra",
      "Zihao Fan",
      "Xiaobin Gao",
      "Xuan Guo",
      "Jiwen Ren",
      "Serim Park",
      "Peicheng Yu",
      "Jun Yu",
      "Shubham Vij",
      "Neil Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15058",
    "title": "FIP: Endowing Robust Motion Capture on Daily Garment by Fusing Flex and Inertial Sensors",
    "abstract": "           What if our clothes could capture our body motion accurately? This paper introduces Flexible Inertial Poser (FIP), a novel motion-capturing system using daily garments with two elbow-attached flex sensors and four Inertial Measurement Units (IMUs). To address the inevitable sensor displacements in loose wearables which degrade joint tracking accuracy significantly, we identify the distinct characteristics of the flex and inertial sensor displacements and develop a Displacement Latent Diffusion Model and a Physics-informed Calibrator to compensate for sensor displacements based on such observations, resulting in a substantial improvement in motion capture accuracy. We also introduce a Pose Fusion Predictor to enhance multimodal sensor fusion. Extensive experiments demonstrate that our method achieves robust performance across varying body shapes and motions, significantly outperforming SOTA IMU approaches with a 19.5% improvement in angular error, a 26.4% improvement in elbow angular error, and a 30.1% improvement in positional error. FIP opens up opportunities for ubiquitous human-computer interactions and diverse interactive applications such as Metaverse, rehabilitation, and fitness analysis.         ",
    "url": "https://arxiv.org/abs/2502.15058",
    "authors": [
      "Jiawei Fang",
      "Ruonan Zheng",
      "Yuanyao",
      "Xiaoxia Gao",
      "Chengxu Zuo",
      "Shihui Guo",
      "Yiyue Luo"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.15076",
    "title": "Synth It Like KITTI: Synthetic Data Generation for Object Detection in Driving Scenarios",
    "abstract": "           An important factor in advancing autonomous driving systems is simulation. Yet, there is rather small progress for transferability between the virtual and real world. We revisit this problem for 3D object detection on LiDAR point clouds and propose a dataset generation pipeline based on the CARLA simulator. Utilizing domain randomization strategies and careful modeling, we are able to train an object detector on the synthetic data and demonstrate strong generalization capabilities to the KITTI dataset. Furthermore, we compare different virtual sensor variants to gather insights, which sensor attributes can be responsible for the prevalent domain gap. Finally, fine-tuning with a small portion of real data almost matches the baseline and with the full training set slightly surpasses it.         ",
    "url": "https://arxiv.org/abs/2502.15076",
    "authors": [
      "Richard Marcus",
      "Christian Vogel",
      "Inga Jatzkowski",
      "Niklas Knoop",
      "Marc Stamminger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.15078",
    "title": "Breaking Symmetries in Quantified Graph Search: A Comparative Study",
    "abstract": "           Graph generation and enumeration problems often require handling equivalent graphs -- those that differ only in vertex labeling. We study how to extend SAT Modulo Symmetries (SMS), a framework for eliminating such redundant graphs, to handle more complex constraints. While SMS was originally designed for constraints in propositional logic (in NP), we now extend it to handle quantified Boolean formulas (QBF), allowing for more expressive specifications like non-3-colorability (a coNP-complete property). We develop two approaches: a static QBF encoding and a dynamic method integrating SMS into QBF solvers. Our analysis reveals that while specialized approaches can be faster, QBF-based methods offer easier implementation and formal verification capabilities.         ",
    "url": "https://arxiv.org/abs/2502.15078",
    "authors": [
      "Mikol\u00e1\u0161 Janota",
      "Markus Kirchweger",
      "Tom\u00e1\u0161 Peitl",
      "Stefan Szeider"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2502.15098",
    "title": "MADEA: A Malware Detection Architecture for IoT blending Network Monitoring and Device Attestation",
    "abstract": "           Internet-of-Things (IoT) devices are vulnerable to malware and require new mitigation techniques due to their limited resources. To that end, previous research has used periodic Remote Attestation (RA) or Traffic Analysis (TA) to detect malware in IoT devices. However, RA is expensive, and TA only raises suspicion without confirming malware presence. To solve this, we design MADEA, the first system that blends RA and TA to offer a comprehensive approach to malware detection for the IoT ecosystem. TA builds profiles of expected packet traces during benign operations of each device and then uses them to detect malware from network traffic in real-time. RA confirms the presence or absence of malware on the device. MADEA achieves 100% true positive rate. It also outperforms other approaches with 160x faster detection time. Finally, without MADEA, effective periodic RA can consume at least ~14x the amount of energy that a device needs in one hour.         ",
    "url": "https://arxiv.org/abs/2502.15098",
    "authors": [
      "Renascence Tarafder Prapty",
      "Rahmadi Trimananda",
      "Sashidhar Jakkamsetti",
      "Gene Tsudik",
      "Athina Markopoulou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.15102",
    "title": "Leveraging ChatGPT for Sponsored Ad Detection and Keyword Extraction in YouTube Videos",
    "abstract": "           This work-in-progress paper presents a novel approach to detecting sponsored advertisement segments in YouTube videos and comparing the advertisement with the main content. Our methodology involves the collection of 421 auto-generated and manual transcripts which are then fed into a prompt-engineered GPT-4o for ad detection, a KeyBERT for keyword extraction, and another iteration of ChatGPT for category identification. The results revealed a significant prevalence of product-related ads across various educational topics, with ad categories refined using GPT-4o into succinct 9 content and 4 advertisement categories. This approach provides a scalable and efficient alternative to traditional ad detection methods while offering new insights into the types and relevance of ads embedded within educational content. This study highlights the potential of LLMs in transforming ad detection processes and improving our understanding of advertisement strategies in digital media.         ",
    "url": "https://arxiv.org/abs/2502.15102",
    "authors": [
      "Brice Valentin Kok-Shun",
      "Johnny Chan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15109",
    "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models",
    "abstract": "           Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce Social Genome, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. Social Genome contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). Social Genome is also the first modeling challenge to study external knowledge in social reasoning. Social Genome computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of Social Genome through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.         ",
    "url": "https://arxiv.org/abs/2502.15109",
    "authors": [
      "Leena Mathur",
      "Marian Qian",
      "Paul Pu Liang",
      "Louis-Philippe Morency"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15128",
    "title": "DAM-Seg: Anatomically accurate cardiac segmentation using Dense Associative Networks",
    "abstract": "           Deep learning-based cardiac segmentation has seen significant advancements over the years. Many studies have tackled the challenge of anatomically incorrect segmentation predictions by introducing auxiliary modules. These modules either post-process segmentation outputs or enforce consistency between specific points to ensure anatomical correctness. However, such approaches often increase network complexity, require separate training for these modules, and may lack robustness in scenarios with poor visibility. To address these limitations, we propose a novel transformer-based architecture that leverages dense associative networks to learn and retain specific patterns inherent to cardiac inputs. Unlike traditional methods, our approach restricts the network to memorize a limited set of patterns. During forward propagation, a weighted sum of these patterns is used to enforce anatomical correctness in the output. Since these patterns are input-independent, the model demonstrates enhanced robustness, even in cases with poor visibility. The proposed pipeline was evaluated on two publicly available datasets, CAMUS and CardiacNet. Experimental results indicate that our model consistently outperforms baseline approaches across all metrics, highlighting its effectiveness and reliability for cardiac segmentation tasks.         ",
    "url": "https://arxiv.org/abs/2502.15128",
    "authors": [
      "Zahid Ullah",
      "Jihie Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15153",
    "title": "Investigating the Adaptive Robustness with Knowledge Conflicts in LLM-based Multi-Agent Systems",
    "abstract": "           Recent advances in Large Language Models (LLMs) have upgraded them from sophisticated text generators to autonomous agents capable of corporation and tool use in multi-agent systems (MASs). However, the robustness of these LLM-based MASs, especially under knowledge conflicts, remains unclear. In this paper, we design four comprehensive metrics to investigate the robustness of MASs when facing mild or task-critical knowledge conflicts. We first analyze mild knowledge conflicts introduced by heterogeneous agents and find that they do not harm system robustness but instead improve collaborative decision-making. Next, we investigate task-critical knowledge conflicts by synthesizing knowledge conflicts and embedding them into one of the agents. Our results show that these conflicts have surprisingly little to no impact on MAS robustness. Furthermore, we observe that MASs demonstrate certain self-repairing capabilities by reducing their reliance on knowledge conflicts and adopting alternative solution paths to maintain stability. Finally, we conduct ablation studies on the knowledge conflict number, agent number, and interaction rounds, finding that the self-repairing capability of MASs has intrinsic limits, and all findings hold consistently across various factors. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15153",
    "authors": [
      "Tianjie Ju",
      "Bowen Wang",
      "Hao Fei",
      "Mong-Li Lee",
      "Wynne Hsu",
      "Yun Li",
      "Qianren Wang",
      "Pengzhou Cheng",
      "Zongru Wu",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15160",
    "title": "GraphFuzz: Automated Testing of Graph Algorithm Implementations with Differential Fuzzing and Lightweight Feedback",
    "abstract": "           Graph algorithms, such as shortest path finding, play a crucial role in enabling essential applications and services like infrastructure planning and navigation, making their correctness important. However, thoroughly testing graph algorithm implementations poses several challenges, including their vast input space (i.e., arbitrary graphs). Moreover, through our preliminary study, we find that just a few automatically generated graphs (less than 10) could be enough to cover the code of many graph algorithm implementations, rendering the code coverage-guided fuzzing approach -- one of the state-of-the-art search algorithms -- less efficient than expected. To tackle these challenges, we introduce GraphFuzz, the first automated feedback-guided fuzzing framework for graph algorithm implementations. Our key innovation lies in identifying lightweight and algorithm-specific feedback signals to combine with or completely replace the code coverage feedback to enhance the diversity of the test corpus, thereby speeding up the bug-finding process. This novel idea also allows GraphFuzz to effectively work in both black-box (i.e., no code coverage instrumentation/collection is required) and grey-box setups. GraphFuzz applies differential testing to detect both crash-triggering bugs and logic bugs. Our evaluation demonstrates the effectiveness of GraphFuzz. The tool has successfully discovered 12 previously unknown bugs, including 6 logic bugs, in 9 graph algorithm implementations in two popular graph libraries, NetworkX and iGraph. All of them have been confirmed and and 11 bugs have been rectified by the libraries' maintainers.         ",
    "url": "https://arxiv.org/abs/2502.15160",
    "authors": [
      "Wenqi Yan",
      "Manuel Rigger",
      "Anthony Wirth",
      "Van-Thuan Pham"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.15172",
    "title": "BP-GPT: Auditory Neural Decoding Using fMRI-prompted LLM",
    "abstract": "           Decoding language information from brain signals represents a vital research area within brain-computer interfaces, particularly in the context of deciphering the semantic information from the fMRI signal. Although existing work uses LLM to achieve this goal, their method does not use an end-to-end approach and avoids the LLM in the mapping of fMRI-to-text, leaving space for the exploration of the LLM in auditory decoding. In this paper, we introduce a novel method, the Brain Prompt GPT (BP-GPT). By using the brain representation that is extracted from the fMRI as a prompt, our method can utilize GPT-2 to decode fMRI signals into stimulus text. Further, we introduce the text prompt and align the fMRI prompt to it. By introducing the text prompt, our BP-GPT can extract a more robust brain prompt and promote the decoding of pre-trained LLM. We evaluate our BP-GPT on the open-source auditory semantic decoding dataset and achieve a significant improvement up to 4.61 on METEOR and 2.43 on BERTScore across all the subjects compared to the state-of-the-art method. The experimental results demonstrate that using brain representation as a prompt to further drive LLM for auditory neural decoding is feasible and effective. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15172",
    "authors": [
      "Xiaoyu Chen",
      "Changde Du",
      "Che Liu",
      "Yizhe Wang",
      "Huiguang He"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15181",
    "title": "Debunking the Myth of Join Ordering: Toward Robust SQL Analytics",
    "abstract": "           Join order optimization is critical in achieving good query performance. Despite decades of research and practice, modern query optimizers could still generate inferior join plans that are orders of magnitude slower than optimal. Existing research on robust query processing often lacks theoretical guarantees on join-order robustness while sacrificing query performance. In this paper, we rediscover the recent Predicate Transfer technique from a robustness point of view. We introduce two new algorithms, LargestRoot and SafeSubjoin, and then propose Robust Predicate Transfer (RPT) that is provably robust against arbitrary join orders of an acyclic query. We integrated Robust Predicate Transfer with DuckDB, a state-of-the-art analytical database, and evaluated against all the queries in TPC-H, JOB, and TPC-DS benchmarks. Our experimental results show that RPT improves join-order robustness by orders of magnitude compared to the baseline. With RPT, the largest ratio between the maximum and minimum execution time out of random join orders for a single acyclic query is only 1.6x (the ratio is close to 1 for most evaluated queries). Meanwhile, applying RPT also improves the end-to-end query performance by 1.5x (per-query geometric mean). We hope that this work sheds light on solving the practical join ordering problem.         ",
    "url": "https://arxiv.org/abs/2502.15181",
    "authors": [
      "Junyi Zhao",
      "Kai Su",
      "Yifei Yang",
      "Xiangyao Yu",
      "Paraschos Koutris",
      "Huanchen Zhang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2502.15202",
    "title": "GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer",
    "abstract": "           Code retrieval is a crucial component in modern software development, particularly in large-scale projects. However, existing approaches relying on sequence-based models often fail to fully exploit the structural dependencies inherent in code, leading to suboptimal retrieval performance, particularly with structurally complex code fragments. In this paper, we introduce GNN-Coder, a novel framework based on Graph Neural Network (GNN) to utilize Abstract Syntax Tree (AST). We make the first attempt to study how GNN-integrated Transformer can promote the development of semantic retrieval tasks by capturing the structural and semantic features of code. We further propose an innovative graph pooling method tailored for AST, utilizing the number of child nodes as a key feature to highlight the intrinsic topological relationships within the AST. This design effectively integrates both sequential and hierarchical representations, enhancing the model's ability to capture code structure and semantics. Additionally, we introduce the Mean Angular Margin (MAM), a novel metric for quantifying the uniformity of code embedding distributions, providing a standardized measure of feature separability. The proposed method achieves a lower MAM, indicating a more discriminative feature representation. This underscores GNN-Coder's superior ability to distinguish between code snippets, thereby enhancing retrieval accuracy. Experimental results show that GNN-Coder significantly boosts retrieval performance, with a 1\\%-10\\% improvement in MRR on the CSN dataset, and a notable 20\\% gain in zero-shot performance on the CosQA dataset.         ",
    "url": "https://arxiv.org/abs/2502.15202",
    "authors": [
      "Yufan Ye",
      "Pu Pang",
      "Ting Zhang",
      "Hua Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.15212",
    "title": "Measuring AI agent autonomy: Towards a scalable approach with code inspection",
    "abstract": "           AI agents are AI systems that can achieve complex goals autonomously. Assessing the level of agent autonomy is crucial for understanding both their potential benefits and risks. Current assessments of autonomy often focus on specific risks and rely on run-time evaluations -- observations of agent actions during operation. We introduce a code-based assessment of autonomy that eliminates the need to run an AI agent to perform specific tasks, thereby reducing the costs and risks associated with run-time evaluations. Using this code-based framework, the orchestration code used to run an AI agent can be scored according to a taxonomy that assesses attributes of autonomy: impact and oversight. We demonstrate this approach with the AutoGen framework and select applications.         ",
    "url": "https://arxiv.org/abs/2502.15212",
    "authors": [
      "Peter Cihon",
      "Merlin Stein",
      "Gagan Bansal",
      "Sam Manning",
      "Kevin Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15233",
    "title": "A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation",
    "abstract": "           An increasing number of companies have begun providing services that leverage cloud-based large language models (LLMs), such as ChatGPT. However, this development raises substantial privacy concerns, as users' prompts are transmitted to and processed by the model providers. Among the various privacy protection methods for LLMs, those implemented during the pre-training and fine-tuning phrases fail to mitigate the privacy risks associated with the remote use of cloud-based LLMs by users. On the other hand, methods applied during the inference phrase are primarily effective in scenarios where the LLM's inference does not rely on privacy-sensitive information. In this paper, we outline the process of remote user interaction with LLMs and, for the first time, propose a detailed definition of a general pseudonymization framework applicable to cloud-based LLMs. The experimental results demonstrate that the proposed framework strikes an optimal balance between privacy protection and utility. The code for our method is available to the public at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15233",
    "authors": [
      "Shilong Hou",
      "Ruilin Shang",
      "Zi Long",
      "Xianghua Fu",
      "Yin Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15236",
    "title": "Applicability of the Minimal Dominating Set for Influence Maximisation in Multilayer Networks",
    "abstract": "           The minimal dominating set (MDS) is a well-established concept in network controllability and has been successfully applied in various domains, including sensor placement, network resilience, and epidemic containment. In this study, we adapt the local-improvement MDS routine and explore its potential for enhancing seed selection for influence maximisation in multilayer networks (MLN). We employ the Linear Threshold Model (LTM), which offers an intuitive representation of influence spread or opinion dynamics by accounting for peer influence accumulation. To ensure interpretability, we utilise rank-refining seed selection methods, with the results further filtered with MDS. Our findings reveal that incorporating MDS into the seed selection process improves spread only within a specific range of situations. Notably, the improvement is observed for larger seed set budgets, lower activation thresholds, and when an \"AND\" strategy is used to aggregate influence across network layers. This scenario reflects situations where an individual does not require the majority of their acquaintances to hold a target opinion, but must be influenced across all social circles.         ",
    "url": "https://arxiv.org/abs/2502.15236",
    "authors": [
      "Micha\u0142 Czuba",
      "Mingshan Jia",
      "Piotr Br\u00f3dka",
      "Katarzyna Musial"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2502.15243",
    "title": "Comparative Analysis of Large Language Models for Context-Aware Code Completion using SAFIM Framework",
    "abstract": "           The advent of Large Language Models (LLMs) has revolutionized code completion, transforming it into a more intelligent and context-aware feature in modern integrated development environments. These advancements have significantly enhanced developers' ability to write efficient and error-free code. This study evaluates the performance of several chat-based LLMs, including Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, GPT-4o-mini, and GPT-4 Turbo, using the Syntax-Aware Fill-in-the-Middle (SAFIM) dataset. This benchmark is specifically designed to assess models' capabilities in syntax-sensitive code generation. Performance metrics, such as cosine similarity with ground-truth completions and latency, were employed to measure both accuracy and efficiency. The findings reveal substantial differences in the models' code completion abilities, offering valuable insights into their respective strengths and weaknesses. This work provides a comparative analysis that underscores the trade-offs between accuracy and speed, establishing a benchmark for future advancements in LLM-based code completion.         ",
    "url": "https://arxiv.org/abs/2502.15243",
    "authors": [
      "Hang Zhang",
      "Yanxin Shen",
      "Lun Wang",
      "Chuanqi Shi",
      "Shaoshuai Du",
      "Yiyi Tao",
      "Yixian Shen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15245",
    "title": "Steganographic Embeddings as an Effective Data Augmentation",
    "abstract": "           Image Steganography is a cryptographic technique that embeds secret information into an image, ensuring the hidden data remains undetectable to the human eye while preserving the image's original visual integrity. Least Significant Bit (LSB) Steganography achieves this by replacing the k least significant bits of an image with the k most significant bits of a secret image, maintaining the appearance of the original image while simultaneously encoding the essential elements of the hidden data. In this work, we shift away from conventional applications of steganography in deep learning and explore its potential from a new angle. We present experimental results on CIFAR-10 showing that LSB Steganography, when used as a data augmentation strategy for downstream computer vision tasks such as image classification, can significantly improve the training efficiency of deep neural networks. It can also act as an implicit, uniformly discretized piecewise linear approximation of color augmentations such as (brightness, contrast, hue, and saturation), without introducing additional training overhead through a new joint image training regime that disregards the need for tuning sensitive augmentation hyperparameters.         ",
    "url": "https://arxiv.org/abs/2502.15245",
    "authors": [
      "Nicholas DiSalvo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2502.15250",
    "title": "An ocean front detection and tracking algorithm",
    "abstract": "           Ocean front is defined as the interface between different water masses and plays a vital role in the evolution of many physical phenomena. Previous detection methods are based on histogram, Lyapunov exponent, gradient and machine learning. These algorithms, however, introduce discontinuity, inaccuracy, use less information or just approaching traditional results. Moreover, automatic front tracking algrorithm is not open source in preceding studies. This paper foucuses on large-scale ocean fronts and proposes an automatic front detection and tracking algorithm based on Bayesian decision and metric space. In this, front merging, filling and ring deletion are put forward to enhance continuity. The distance between fronts in different days is firstly defined and is well-defined in metric space for functional analysis. These technologies can be migrated to other areas of computer vision such as edge detection and tracking.         ",
    "url": "https://arxiv.org/abs/2502.15250",
    "authors": [
      "Yishuo Wang",
      "Feng Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15251",
    "title": "SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training",
    "abstract": "           We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SimHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. Specifically, we collect over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. We then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method (PeCLR) in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15251",
    "authors": [
      "Nie Lin",
      "Takehiko Ohkawa",
      "Yifei Huang",
      "Mingfang Zhang",
      "Minjie Cai",
      "Ming Li",
      "Ryosuke Furuta",
      "Yoichi Sato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15252",
    "title": "Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models",
    "abstract": "           Understanding collective pedestrian movement is crucial for applications in crowd management, autonomous navigation, and human-robot interaction. This paper investigates the use of sequential deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers, for real-time flock detection in multi-pedestrian trajectories. Our proposed approach consists of a two-stage process: first, a pre-trained binary classification model is used for pairwise trajectory classification, and second, the learned representations are applied to identify multi-agent flocks dynamically. We validate our method using real-world group movement datasets, demonstrating its robustness across varying sequence lengths and diverse movement patterns. Experimental results indicate that our model consistently detects pedestrian flocks with high accuracy and stability, even in dynamic and noisy environments. Furthermore, we extend our approach to identify other forms of collective motion, such as convoys and swarms, paving the way for more comprehensive multi-agent behavior analysis.         ",
    "url": "https://arxiv.org/abs/2502.15252",
    "authors": [
      "Amartaivan Sanjjamts",
      "Hiroshi Morita",
      "Togootogtokh Enkhtogtokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15286",
    "title": "Soybean pod and seed counting in both outdoor fields and indoor laboratories using unions of deep neural networks",
    "abstract": "           Automatic counting soybean pods and seeds in outdoor fields allows for rapid yield estimation before harvesting, while indoor laboratory counting offers greater accuracy. Both methods can significantly accelerate the breeding process. However, it remains challenging for accurately counting pods and seeds in outdoor fields, and there are still no accurate enough tools for counting pods and seeds in laboratories. In this study, we developed efficient deep learning models for counting soybean pods and seeds in both outdoor fields and indoor laboratories. For outdoor fields, annotating not only visible seeds but also occluded seeds makes YOLO have the ability to estimate the number of soybean seeds that are occluded. Moreover, we enhanced YOLO architecture by integrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques (YOLO-DA), to improve model robustness and generalization across soybean images taken in outdoor fields. Testing on soybean images from the outdoor field, we achieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for seed counting. For the indoor setting, we utilized Mask-RCNN supplemented with a Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on synthetic training images generated from a small set of labeled data. This approach resulted in near-perfect accuracy, with an MAE of 1.07 for pod counting and 1.33 for seed counting across actual laboratory images from two distinct studies.         ",
    "url": "https://arxiv.org/abs/2502.15286",
    "authors": [
      "Tianyou Jiang",
      "Mingshun Shao",
      "Tianyi Zhang",
      "Xiaoyu Liu",
      "Qun Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15290",
    "title": "Multimodal Graph-Based Variational Mixture of Experts Network for Zero-Shot Multimodal Information Extraction",
    "abstract": "           Multimodal information extraction on social media is a series of fundamental tasks to construct the multimodal knowledge graph. The tasks aim to extract the structural information in free texts with the incorporate images, including: multimodal named entity typing and multimodal relation extraction. However, the growing number of multimodal data implies a growing category set and the newly emerged entity types or relations should be recognized without additional training. To address the aforementioned challenges, we focus on the zero-shot multimodal information extraction tasks which require using textual and visual modalities for recognizing unseen categories. Compared with text-based zero-shot information extraction models, the existing multimodal ones make the textual and visual modalities aligned directly and exploit various fusion strategies to improve their performances. But the existing methods ignore the fine-grained semantic correlation of text-image pairs and samples. Therefore, we propose the multimodal graph-based variational mixture of experts network (MG-VMoE) which takes the MoE network as the backbone and exploits it for aligning multimodal representations in a fine-grained way. Considering to learn informative representations of multimodal data, we design each expert network as a variational information bottleneck to process two modalities in a uni-backbone. Moreover, we also propose the multimodal graph-based virtual adversarial training to learn the semantic correlation between the samples. The experimental results on the two benchmark datasets demonstrate the superiority of MG-VMoE over the baselines.         ",
    "url": "https://arxiv.org/abs/2502.15290",
    "authors": [
      "Baohang Zhou",
      "Ying Zhang",
      "Yu Zhao",
      "Xuhui Sui",
      "Xiaojie Yuan"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2502.15302",
    "title": "A Novel Riemannian Sparse Representation Learning Network for Polarimetric SAR Image Classification",
    "abstract": "           Deep learning is an effective end-to-end method for Polarimetric Synthetic Aperture Radar(PolSAR) image classification, but it lacks the guidance of related mathematical principle and is essentially a black-box model. In addition, existing deep models learn features in Euclidean space, where PolSAR complex matrix is commonly converted into a complex-valued vector as the network input, distorting matrix structure and channel relationship. However, the complex covariance matrix is Hermitian positive definite (HPD), and resides on a Riemannian manifold instead of a Euclidean one. Existing methods cannot measure the geometric distance of HPD matrices and easily cause some misclassifications due to inappropriate Euclidean measures. To address these issues, we propose a novel Riemannian Sparse Representation Learning Network (SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian Sparse Representation (SRSR) model is designed to learn the sparse features with Riemannian metric. Then, the optimization procedure of the SRSR model is inferred and further unfolded into an SRSRnet, which can automatically learn the sparse coefficients and dictionary atoms. Furthermore, to learn contextual high-level features, a CNN-enhanced module is added to improve classification performance. The proposed network is a Sparse Representation (SR) guided deep learning model, which can directly utilize the covariance matrix as the network input, and utilize Riemannian metric to learn geometric structure and sparse features of complex matrices in Riemannian space. Experiments on three real PolSAR datasets demonstrate that the proposed method surpasses state-of-the-art techniques in ensuring accurate edge details and correct region homogeneity for classification.         ",
    "url": "https://arxiv.org/abs/2502.15302",
    "authors": [
      "Junfei Shi",
      "Mengmeng Nie",
      "Weisi Lin",
      "Haiyan Jin",
      "Junhuai Li",
      "Rui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15307",
    "title": "Road Traffic Sign Recognition method using Siamese network Combining Efficient-CNN based Encoder",
    "abstract": "           Traffic signs recognition (TSR) plays an essential role in assistant driving and intelligent transportation system. However, the noise of complex environment may lead to motion-blur or occlusion problems, which raise the tough challenge to real-time recognition with high accuracy and robust. In this article, we propose IECES-network which with improved encoders and Siamese net. The three-stage approach of our method includes Efficient-CNN based encoders, Siamese backbone and the fully-connected layers. We firstly use convolutional encoders to extract and encode the traffic sign features of augmented training samples and standard images. Then, we design the Siamese neural network with Efficient-CNN based encoder and contrastive loss function, which can be trained to improve the robustness of TSR problem when facing the samples of motion-blur and occlusion by computing the distance between inputs and templates. Additionally, the template branch of the proposed network can be stopped when executing the recognition tasks after training to raise the process speed of our real-time model, and alleviate the computational resource and parameter scale. Finally, we recombined the feature code and a fully-connected layer with SoftMax function to classify the codes of samples and recognize the category of traffic signs. The results of experiments on the Tsinghua-Tencent 100K dataset and the German Traffic Sign Recognition Benchmark dataset demonstrate the performance of the proposed IECESnetwork. Compared with other state-of-the-art methods, in the case of motion-blur and occluded environment, the proposed method achieves competitive performance precision-recall and accuracy metric average is 88.1%, 86.43% and 86.1% with a 2.9M lightweight scale, respectively. Moreover, processing time of our model is 0.1s per frame, of which the speed is increased by 1.5 times compared with existing methods.         ",
    "url": "https://arxiv.org/abs/2502.15307",
    "authors": [
      "Zhenghao Xi",
      "Yuchao Shao",
      "Yang Zheng",
      "Xiang Liu",
      "Yaqi Liu",
      "Yitong Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15309",
    "title": "DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation",
    "abstract": "           In real-world scenarios, the environment changes caused by agents or human activities make it extremely challenging for robots to perform various long-term tasks. To effectively understand and adapt to dynamic environments, the perception system of a robot needs to extract instance-level semantic information, reconstruct the environment in a fine-grained manner, and update its environment representation in memory according to environment changes. To address these challenges, We propose \\textbf{DynamicGSG}, a dynamic, high-fidelity, open-vocabulary scene graph generation system leveraging Gaussian splatting. Our system comprises three key components: (1) constructing hierarchical scene graphs using advanced vision foundation models to represent the spatial and semantic relationships of objects in the environment, (2) designing a joint feature loss to optimize the Gaussian map for incremental high-fidelity reconstruction, and (3) updating the Gaussian map and scene graph according to real environment changes for long-term environment adaptation. Experiments and ablation studies demonstrate the performance and efficacy of the proposed method in terms of semantic segmentation, language-guided object retrieval, and reconstruction quality. Furthermore, we have validated the dynamic updating capabilities of our system in real laboratory environments. The source code will be released at:~\\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2502.15309",
    "authors": [
      "Luzhou Ge",
      "Xiangyu Zhu",
      "Zhuo Yang",
      "Xuesong Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.15331",
    "title": "Lightweight yet Efficient: An External Attentive Graph Convolutional Network with Positional Prompts for Sequential Recommendation",
    "abstract": "           Graph-based Sequential Recommender systems (GSRs) have gained significant research attention due to their ability to simultaneously handle user-item interactions and sequential relationships between items. Current GSRs often utilize composite or in-depth structures for graph encoding (e.g., the Graph Transformer). Nevertheless, they have high computational complexity, hindering the deployment on resource-constrained edge devices. Moreover, the relative position encoding in Graph Transformer has difficulty in considering the complicated positional dependencies within sequence. To this end, we propose an External Attentive Graph convolutional network with Positional prompts for Sequential recommendation, namely EA-GPS. Specifically, we first introduce an external attentive graph convolutional network that linearly measures the global associations among nodes via two external memory units. Then, we present a positional prompt-based decoder that explicitly treats the absolute item positions as external prompts. By introducing length-adaptive sequential masking and a soft attention network, such a decoder facilitates the model to capture the long-term positional dependencies and contextual relationships within sequences. Extensive experimental results on five real-world datasets demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods. Remarkably, it achieves the superior performance while maintaining a smaller parameter size and lower training overhead. The implementation of this work is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15331",
    "authors": [
      "Jinyu Zhang",
      "Chao Li",
      "Zhongying Zhao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15355",
    "title": "A Universal Framework for Compressing Embeddings in CTR Prediction",
    "abstract": "           Accurate click-through rate (CTR) prediction is vital for online advertising and recommendation systems. Recent deep learning advancements have improved the ability to capture feature interactions and understand user interests. However, optimizing the embedding layer often remains overlooked. Embedding tables, which represent categorical and sequential features, can become excessively large, surpassing GPU memory limits and necessitating storage in CPU memory. This results in high memory consumption and increased latency due to frequent GPU-CPU data transfers. To tackle these challenges, we introduce a Model-agnostic Embedding Compression (MEC) framework that compresses embedding tables by quantizing pre-trained embeddings, without sacrificing recommendation quality. Our approach consists of two stages: first, we apply popularity-weighted regularization to balance code distribution between high- and low-frequency features. Then, we integrate a contrastive learning mechanism to ensure a uniform distribution of quantized codes, enhancing the distinctiveness of embeddings. Experiments on three datasets reveal that our method reduces memory usage by over 50x while maintaining or improving recommendation performance compared to existing models. The implementation code is accessible in our project repository this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15355",
    "authors": [
      "Kefan Wang",
      "Hao Wang",
      "Kenan Song",
      "Wei Guo",
      "Kai Cheng",
      "Zhi Li",
      "Yong Liu",
      "Defu Lian",
      "Enhong Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.15361",
    "title": "Evaluating Social Biases in LLM Reasoning",
    "abstract": "           In the recent development of AI reasoning, large language models (LLMs) are trained to automatically generate chain-of-thought reasoning steps, which have demonstrated compelling performance on math and coding tasks. However, when bias is mixed within the reasoning process to form strong logical arguments, it could cause even more harmful results and further induce hallucinations. In this paper, we have evaluated the 8B and 32B variants of DeepSeek-R1 against their instruction tuned counterparts on the BBQ dataset, and investigated the bias that is elicited out and being amplified through reasoning steps. To the best of our knowledge, this empirical study is the first to assess bias issues in LLM reasoning.         ",
    "url": "https://arxiv.org/abs/2502.15361",
    "authors": [
      "Xuyang Wu",
      "Jinming Nian",
      "Zhiqiang Tao",
      "Yi Fang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15370",
    "title": "Weakly Supervised Video Scene Graph Generation via Natural Language Supervision",
    "abstract": "           Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that only utilizes the readily available video captions for training a VidSGG model. NL-VSGG consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by NL-VSGG is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.         ",
    "url": "https://arxiv.org/abs/2502.15370",
    "authors": [
      "Kibum Kim",
      "Kanghoon Yoon",
      "Yeonjun In",
      "Jaehyeong Jeon",
      "Jinyoung Moon",
      "Donghyun Kim",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15376",
    "title": "Learning Chern Numbers of Topological Insulators with Gauge Equivariant Neural Networks",
    "abstract": "           Equivariant network architectures are a well-established tool for predicting invariant or equivariant quantities. However, almost all learning problems considered in this context feature a global symmetry, i.e. each point of the underlying space is transformed with the same group element, as opposed to a local ``gauge'' symmetry, where each point is transformed with a different group element, exponentially enlarging the size of the symmetry group. Gauge equivariant networks have so far mainly been applied to problems in quantum chromodynamics. Here, we introduce a novel application domain for gauge-equivariant networks in the theory of topological condensed matter physics. We use gauge equivariant networks to predict topological invariants (Chern numbers) of multiband topological insulators. The gauge symmetry of the network guarantees that the predicted quantity is a topological invariant. We introduce a novel gauge equivariant normalization layer to stabilize the training and prove a universal approximation theorem for our setup. We train on samples with trivial Chern number only but show that our models generalize to samples with non-trivial Chern number. We provide various ablations of our setup. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15376",
    "authors": [
      "Longde Huang",
      "Oleksandr Balabanov",
      "Hampus Linander",
      "Mats Granath",
      "Daniel Persson",
      "Jan E. Gerken"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Mesoscale and Nanoscale Physics (cond-mat.mes-hall)"
    ]
  },
  {
    "id": "arXiv:2502.15391",
    "title": "Counting Abstraction for the Verification of Structured Parameterized Networks",
    "abstract": "           We consider the verification of parameterized networks of replicated processes whose architecture is described by hyperedge-replacement graph grammars. Due to the undecidability of verification problems such as reachability or coverability of a given configuration, in which we count the number of replicas in each local state, we develop two orthogonal verification techniques. We present a counting abstraction able to produce, from a graph grammar describing a parameterized system, a finite set of Petri nets that over-approximate the behaviors of the original system. The counting abstraction is implemented in a prototype tool, evalutated on a non-trivial set of test cases. Moreover, we identify a decidable fragment, for which the coverability problem is in 2EXPTIME and PSPACE-hard.         ",
    "url": "https://arxiv.org/abs/2502.15391",
    "authors": [
      "Marius Bozga",
      "Radu Iosif",
      "Arnaud Sangnier",
      "Neven Villani"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2502.15401",
    "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning",
    "abstract": "           In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be publicly available subsequently.         ",
    "url": "https://arxiv.org/abs/2502.15401",
    "authors": [
      "Xuetao Ma",
      "Wenbin Jiang",
      "Hua Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15427",
    "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
    "abstract": "           As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15427",
    "authors": [
      "Giulio Zizzo",
      "Giandomenico Cornacchia",
      "Kieran Fraser",
      "Muhammad Zaid Hameed",
      "Ambrish Rawat",
      "Beat Buesser",
      "Mark Purcell",
      "Pin-Yu Chen",
      "Prasanna Sattigeri",
      "Kush Varshney"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15435",
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "abstract": "           Defending aligned Large Language Models (LLMs) against jailbreaking attacks is a challenging problem, with existing approaches requiring multiple requests or even queries to auxiliary LLMs, making them computationally heavy. Instead, we focus on detecting jailbreaking input in a single forward pass. Our method, called Single Pass Detection SPD, leverages the information carried by the logits to predict whether the output sentence will be harmful. This allows us to defend in just one forward pass. SPD can not only detect attacks effectively on open-source models, but also minimizes the misclassification of harmless inputs. Furthermore, we show that SPD remains effective even without complete logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a promising approach to efficiently safeguard LLMs against adversarial attacks.         ",
    "url": "https://arxiv.org/abs/2502.15435",
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios G. Chrysos",
      "Volkan Cevher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15438",
    "title": "LEAP: Enhancing Vision-Based Occupancy Networks with Lightweight Spatio-Temporal Correlation",
    "abstract": "           Vision-based occupancy networks provide an end-to-end solution for reconstructing the surrounding environment using semantic occupied voxels derived from multi-view images. This technique relies on effectively learning the correlation between pixel-level visual information and voxels. Despite recent advancements, occupancy results still suffer from limited accuracy due to occlusions and sparse visual cues. To address this, we propose a Lightweight Spatio-Temporal Correlation (LEAP)} method, which significantly enhances the performance of existing occupancy networks with minimal computational overhead. LEAP can be seamlessly integrated into various baseline networks, enabling a plug-and-play application. LEAP operates in three stages: 1) it tokenizes information from recent baseline and motion features into a shared, compact latent space; 2) it establishes full correlation through a tri-stream fusion architecture; 3) it generates occupancy results that strengthen the baseline's output. Extensive experiments demonstrate the efficiency and effectiveness of our method, outperforming the latest baseline models. The source code and several demos are available in the supplementary material.         ",
    "url": "https://arxiv.org/abs/2502.15438",
    "authors": [
      "Fengcheng Yu",
      "Haoran Xu",
      "Canming Xia",
      "Guang Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15452",
    "title": "Robust 4D Radar-aided Inertial Navigation for Aerial Vehicles",
    "abstract": "           While LiDAR and cameras are becoming ubiquitous for unmanned aerial vehicles (UAVs) but can be ineffective in challenging environments, 4D millimeter-wave (MMW) radars that can provide robust 3D ranging and Doppler velocity measurements are less exploited for aerial navigation. In this paper, we develop an efficient and robust error-state Kalman filter (ESKF)-based radar-inertial navigation for UAVs. The key idea of the proposed approach is the point-to-distribution radar scan matching to provide motion constraints with proper uncertainty qualification, which are used to update the navigation states in a tightly coupled manner, along with the Doppler velocity measurements. Moreover, we propose a robust keyframe-based matching scheme against the prior map (if available) to bound the accumulated navigation errors and thus provide a radar-based global localization solution with high accuracy. Extensive real-world experimental validations have demonstrated that the proposed radar-aided inertial navigation outperforms state-of-the-art methods in both accuracy and robustness.         ",
    "url": "https://arxiv.org/abs/2502.15452",
    "authors": [
      "Jinwen Zhu",
      "Jun Hu",
      "Xudong Zhao",
      "Xiaoming Lang",
      "Yinian Mao",
      "Guoquan Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.15462",
    "title": "Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks",
    "abstract": "           Soccer analytics rely on two data sources: the player positions on the pitch and the sequences of events they perform. With around 2000 ball events per game, their precise and exhaustive annotation based on a monocular video stream remains a tedious and costly manual task. While state-of-the-art spatio-temporal action detection methods show promise for automating this task, they lack contextual understanding of the game. Assuming professional players' behaviors are interdependent, we hypothesize that incorporating surrounding players' information such as positions, velocity and team membership can enhance purely visual predictions. We propose a spatio-temporal action detection approach that combines visual and game state information via Graph Neural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating improved metrics through game state integration.         ",
    "url": "https://arxiv.org/abs/2502.15462",
    "authors": [
      "Jeremie Ochin",
      "Guillaume Devineau",
      "Bogdan Stanciulescu",
      "Sotiris Manitsaris"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15475",
    "title": "Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning Solution for Protocols Compliance",
    "abstract": "           Neural network-based decoding methods have shown promise in enhancing error correction performance, but traditional approaches struggle with the challenges posed by punctured codes. In particular, these methods fail to address the complexities of variable code rates and the need for protocol compatibility. This paper presents a unified Long Short-Term Memory (LSTM)-based decoding architecture specifically designed to overcome these challenges. The proposed method unifies punctured convolutional and Turbo codes. A puncture embedding mechanism integrates puncturing patterns directly into the network, enabling seamless adaptation to varying code rates, while balanced bit error rate training ensures robustness across different code lengths, rates, and channels, maintaining protocol flexibility. Extensive simulations in Additive White Gaussian Noise and Rayleigh fading channels demonstrate that the proposed approach outperforms conventional decoding techniques, providing significant improvements in decoding accuracy and robustness. These results underscore the potential of LSTM-based decoding as a promising solution for next-generation artificial intelligence powered communication systems.         ",
    "url": "https://arxiv.org/abs/2502.15475",
    "authors": [
      "Yongli Yan",
      "Linglong Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2502.15480",
    "title": "On Neural BRDFs: A Thorough Comparison of State-of-the-Art Approaches",
    "abstract": "           The bidirectional reflectance distribution function (BRDF) is an essential tool to capture the complex interaction of light and matter. Recently, several works have employed neural methods for BRDF modeling, following various strategies, ranging from utilizing existing parametric models to purely neural parametrizations. While all methods yield impressive results, a comprehensive comparison of the different approaches is missing in the literature. In this work, we present a thorough evaluation of several approaches, including results for qualitative and quantitative reconstruction quality and an analysis of reciprocity and energy conservation. Moreover, we propose two extensions that can be added to existing approaches: A novel additive combination strategy for neural BRDFs that split the reflectance into a diffuse and a specular part, and an input mapping that ensures reciprocity exactly by construction, while previous approaches only ensure it by soft constraints.         ",
    "url": "https://arxiv.org/abs/2502.15480",
    "authors": [
      "Florian Hofherr",
      "Bjoern Haefner",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15482",
    "title": "A modular risk concept for complex systems",
    "abstract": "           Our ways of managing risk have in the past been adapted to changes in technology and society. Amidst the ongoing digital transformation, the ur-gency of adapting risk management to changing needs seems higher than ever. This paper starts with a brief historic overview of the development of risk management in the past. The paper motivates the views that for com-plex systems, risk should be controlled by enforcing constrains in a modular way at different system levels, that the constraints can be expressed as assur-ance contracts and that acceptable risk mitigation can be demonstrated in as-surance case modules. Based on extensive industry experience of the authors, a major contribution is to explain how already existing methodologies have been combined to cre-ate a concept for modular risk assessment. Examples from assurance of au-tonomous sea navigation and autonomous driving are used to illustrate the concept. Beyond the existing methodologies this paper generalizes risk con-straints to assurance contracts as an enabler of modular risk assessment spanning all relevant system levels and stakeholder perspectives while main-taining the dependencies between the system parts and accounting for emer-gent system behavior. Furthermore, the use of safety integrity levels (SIL) and similar concepts for assigning assurance rigor have been avoided in favor of direct assessment of assurance case argument rigor, because technology and applications change too fast to justify using past experience as evidence of validity of such prescriptive schemes. This paper aims to help practitioners making efficient and timely risk-informed decisions about complex integrated systems.         ",
    "url": "https://arxiv.org/abs/2502.15482",
    "authors": [
      "Dag McGeorge",
      "Jon Arne Glomsrud"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2502.15483",
    "title": "MoMa: A Modular Deep Learning Framework for Material Property Prediction",
    "abstract": "           Deep learning methods for material property prediction have been widely explored to advance materials discovery. However, the prevailing pre-train then fine-tune paradigm often fails to address the inherent diversity and disparity of material tasks. To overcome these challenges, we introduce MoMa, a Modular framework for Materials that first trains specialized modules across a wide range of tasks and then adaptively composes synergistic modules tailored to each downstream scenario. Evaluation across 17 datasets demonstrates the superiority of MoMa, with a substantial 14% average improvement over the strongest baseline. Few-shot and continual learning experiments further highlight MoMa's potential for real-world applications. Pioneering a new paradigm of modular material learning, MoMa will be open-sourced to foster broader community collaboration.         ",
    "url": "https://arxiv.org/abs/2502.15483",
    "authors": [
      "Botian Wang",
      "Yawen Ouyang",
      "Yaohui Li",
      "Yiqun Wang",
      "Haorui Cui",
      "Jianbing Zhang",
      "Xiaonan Wang",
      "Wei-Ying Ma",
      "Hao Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2502.15487",
    "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
    "abstract": "           Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.         ",
    "url": "https://arxiv.org/abs/2502.15487",
    "authors": [
      "Martina Miliani",
      "Serenna Auriemma",
      "Alessandro Bondielli",
      "Emmanuele Chersoni",
      "Lucia Passaro",
      "Irene Sucameli",
      "Alessandro Lenci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15488",
    "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection",
    "abstract": "           PETR-based methods have dominated benchmarks in 3D perception and are increasingly becoming a key component in modern autonomous driving systems. However, their quantization performance significantly degrades when INT8 inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on the NuScenes dataset. To address this issue, we propose a quantization-aware position embedding transformation for multi-view 3D object detection, termed Q-PETR. Q-PETR offers a quantizationfriendly and deployment-friendly architecture while preserving the original performance of PETR. It substantially narrows the accuracy gap between INT8 and FP32 inference for PETR-series methods. Without bells and whistles, our approach reduces the mAP and NDS drop to within 1% under standard 8-bit per-tensor post-training quantization. Furthermore, our method exceeds the performance of the original PETR in terms of floating-point precision. Extensive experiments across a variety of PETR-series models demonstrate its broad generalization.         ",
    "url": "https://arxiv.org/abs/2502.15488",
    "authors": [
      "Jiangyong Yu",
      "Changyong Shu",
      "Dawei Yang",
      "Zichen Yu",
      "Xing Hu",
      "Yan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15491",
    "title": "Network Resource Optimization for ML-Based UAV Condition Monitoring with Vibration Analysis",
    "abstract": "           As smart cities begin to materialize, the role of Unmanned Aerial Vehicles (UAVs) and their reliability becomes increasingly important. One aspect of reliability relates to Condition Monitoring (CM), where Machine Learning (ML) models are leveraged to identify abnormal and adverse conditions. Given the resource-constrained nature of next-generation edge networks, the utilization of precious network resources must be minimized. This work explores the optimization of network resources for ML-based UAV CM frameworks. The developed framework uses experimental data and varies the feature extraction aggregation interval to optimize ML model selection. Additionally, by leveraging dimensionality reduction techniques, there is a 99.9% reduction in network resource consumption.         ",
    "url": "https://arxiv.org/abs/2502.15491",
    "authors": [
      "Alexandre Gemayel",
      "Dimitrios Michael Manias",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.15493",
    "title": "Programmers Aren't Obsolete Yet: A Syllabus for Teaching CS Students to Responsibly Use Large Language Models for Code Generation",
    "abstract": "           Large Language Models (LLMs) have emerged as powerful tools for automating code generation, offering immense potential to enhance programmer productivity. However, their non-deterministic nature and reliance on user input necessitate a robust understanding of programming fundamentals to ensure their responsible and effective use. In this paper, we argue that foundational computing skills remain crucial in the age of LLMs. We propose a syllabus focused on equipping computer science students to responsibly embrace LLMs as performance enhancement tools. This work contributes to the discussion on the why, when, and how of integrating LLMs into computing education, aiming to better prepare programmers to leverage these tools without compromising foundational software development principles.         ",
    "url": "https://arxiv.org/abs/2502.15493",
    "authors": [
      "Bruno Pereira Cipriano",
      "L\u00facio Studer Ferreira"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2502.15507",
    "title": "Activation Steering in Neural Theorem Provers",
    "abstract": "           Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.         ",
    "url": "https://arxiv.org/abs/2502.15507",
    "authors": [
      "Shashank Kirtania"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15508",
    "title": "Energy Efficient Network Path Reconfiguration for Industrial Field Data",
    "abstract": "           Energy efficiency and reliability are vital design requirements of recent industrial networking solutions. Increased energy consumption, poor data access rates and unpredictable end-to-end data access latencies are catastrophic when transferring high volumes of critical industrial data in strict temporal deadlines. These requirements might become impossible to meet later on, due to node failures, or excessive degradation of the performance of wireless links. In this paper, we focus on maintaining the network functionality required by the industrial, best effort, low-latency applications after such events, by sacrificing latency guarantees to improve energy consumption and reliability. We avoid continuously recomputing the network configuration centrally, by designing an energy efficient, local and distributed path reconfiguration method. Specifically, given the operational parameters required by the applications, our method locally reconfigures the data distribution paths, when a network node fails. Additionally, our method also regulates the return to an operational state of nodes that have been offline in the past. We compare the performance of our method through simulations to the performance of other state of the art protocols and we demonstrate performance gains in terms of energy consumption, data delivery success rate, and in some cases, end-to-end data access latency. We conclude by providing some emerging key insights which can lead to further performance improvements.         ",
    "url": "https://arxiv.org/abs/2502.15508",
    "authors": [
      "Theofanis P. Raptis",
      "Andrea Passarella",
      "Marco Conti"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2502.15516",
    "title": "Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection",
    "abstract": "           Safety and reliability are crucial for the public acceptance of autonomous driving. To ensure accurate and reliable environmental perception, intelligent vehicles must exhibit accuracy and robustness in various environments. Millimeter-wave radar, known for its high penetration capability, can operate effectively in adverse weather conditions such as rain, snow, and fog. Traditional 3D millimeter-wave radars can only provide range, Doppler, and azimuth information for objects. Although the recent emergence of 4D millimeter-wave radars has added elevation resolution, the radar point clouds remain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast, cameras offer rich semantic details but are sensitive to lighting and weather conditions. Hence, this paper leverages these two highly complementary and cost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4D radar spectra with depth-aware camera images and employing attention mechanisms, we fuse texture-rich images with depth-rich radar data in the Bird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally, we propose using GAN-based networks to generate depth images from radar spectra in the absence of depth sensors, further improving detection accuracy.         ",
    "url": "https://arxiv.org/abs/2502.15516",
    "authors": [
      "Yue Sun",
      "Yeqiang Qian",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.15522",
    "title": "Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay",
    "abstract": "           Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution mappings. A standard remedy (e.g., in compressed sensing) establishing uniqueness of the solution mapping is to assume knowledge of latent low-dimensional structure in the source signal. We ask the following question: do deep neural networks adapt to this low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution that accurately solves the inverse problem while implicitly encoding latent subspace structure. To our knowledge, this is the first result to rigorously show that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.         ",
    "url": "https://arxiv.org/abs/2502.15522",
    "authors": [
      "Hannah Laus",
      "Suzanna Parkinson",
      "Vasileios Charisopoulos",
      "Felix Krahmer",
      "Rebecca Willett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2502.15525",
    "title": "Enhanced Probabilistic Collision Detection for Motion Planning Under Sensing Uncertainty",
    "abstract": "           Probabilistic collision detection (PCD) is essential in motion planning for robots operating in unstructured environments, where considering sensing uncertainty helps prevent damage. Existing PCD methods mainly used simplified geometric models and addressed only position estimation errors. This paper presents an enhanced PCD method with two key advancements: (a) using superquadrics for more accurate shape approximation and (b) accounting for both position and orientation estimation errors to improve robustness under sensing uncertainty. Our method first computes an enlarged surface for each object that encapsulates its observed rotated copies, thereby addressing the orientation estimation errors. Then, the collision probability under the position estimation errors is formulated as a chance-constraint problem that is solved with a tight upper bound. Both the two steps leverage the recently developed normal parameterization of superquadric surfaces. Results show that our PCD method is twice as close to the Monte-Carlo sampled baseline as the best existing PCD method and reduces path length by 30% and planning time by 37%, respectively. A Real2Sim pipeline further validates the importance of considering orientation estimation errors, showing that the collision probability of executing the planned path in simulation is only 2%, compared to 9% and 29% when considering only position estimation errors or none at all.         ",
    "url": "https://arxiv.org/abs/2502.15525",
    "authors": [
      "Xiaoli Wang",
      "Sipu Ruan",
      "Xin Meng",
      "Gregory Chirikjian"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.15538",
    "title": "SOTOPIA-\u03a9: Dynamic Strategy Injection Learning and Social Instrucion Following Evaluation for Social Agents",
    "abstract": "           Despite the abundance of prior social strategies possessed by humans, there remains a paucity of research dedicated to their transfer and integration into social agents. Our proposed SOTOPIA-{\\Omega} framework aims to address and bridge this gap, with a particular focus on enhancing the social capabilities of language agents. This framework dynamically injects multi-step reasoning strategies inspired by negotiation theory, along with two simple direct strategies, into expert agents, thereby automating the construction of high-quality social dialogue training corpus. Additionally, we introduce the concept of Social Instruction Following (S-IF) and propose two new S-IF evaluation metrics that are complementary to social capability. We demonstrate that several 7B models trained on high-quality corpus not only significantly surpass the expert agent (GPT-4) in achieving social goals but also enhance S-IF performance. Analysis and variant experiments validate the advantages of dynamic construction, which can especially break the agent's prolonged deadlock.         ",
    "url": "https://arxiv.org/abs/2502.15538",
    "authors": [
      "Wenyuan Zhang",
      "Tianyun Liu",
      "Mengxiao Song",
      "Xiaodong Li",
      "Tingwen Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.15561",
    "title": "A Defensive Framework Against Adversarial Attacks on Machine Learning-Based Network Intrusion Detection Systems",
    "abstract": "           As cyberattacks become increasingly sophisticated, advanced Network Intrusion Detection Systems (NIDS) are critical for modern network security. Traditional signature-based NIDS are inadequate against zero-day and evolving attacks. In response, machine learning (ML)-based NIDS have emerged as promising solutions; however, they are vulnerable to adversarial evasion attacks that subtly manipulate network traffic to bypass detection. To address this vulnerability, we propose a novel defensive framework that enhances the robustness of ML-based NIDS by simultaneously integrating adversarial training, dataset balancing techniques, advanced feature engineering, ensemble learning, and extensive model fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15 datasets. Experimental results show, on average, a 35% increase in detection accuracy and a 12.5% reduction in false positives compared to baseline models, particularly under adversarial conditions. The proposed defense against adversarial attacks significantly advances the practical deployment of robust ML-based NIDS in real-world networks.         ",
    "url": "https://arxiv.org/abs/2502.15561",
    "authors": [
      "Benyamin Tafreshian",
      "Shengzhi Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15567",
    "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
    "abstract": "           The use of machine learning (ML) has become increasingly prevalent in various domains, highlighting the importance of understanding and ensuring its safety. One pressing concern is the vulnerability of ML applications to model stealing attacks. These attacks involve adversaries attempting to recover a learned model through limited query-response interactions, such as those found in cloud-based services or on-chip artificial intelligence interfaces. While existing literature proposes various attack and defense strategies, these often lack a theoretical foundation and standardized evaluation criteria. In response, this work presents a framework called ``Model Privacy'', providing a foundation for comprehensively analyzing model stealing attacks and defenses. We establish a rigorous formulation for the threat model and objectives, propose methods to quantify the goodness of attack and defense strategies, and analyze the fundamental tradeoffs between utility and privacy in ML models. Our developed theory offers valuable insights into enhancing the security of ML models, especially highlighting the importance of the attack-specific structure of perturbations for effective defenses. We demonstrate the application of model privacy from the defender's perspective through various learning scenarios. Extensive experiments corroborate the insights and the effectiveness of defense mechanisms developed under the proposed framework.         ",
    "url": "https://arxiv.org/abs/2502.15567",
    "authors": [
      "Ganghua Wang",
      "Yuhong Yang",
      "Jie Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.15578",
    "title": "FLARE: Fault Attack Leveraging Address Reconfiguration Exploits in Multi-Tenant FPGAs",
    "abstract": "           Modern FPGAs are increasingly supporting multi-tenancy to enable dynamic reconfiguration of user modules. While multi-tenant FPGAs improve utilization and flexibility, this paradigm introduces critical security threats. In this paper, we present FLARE, a fault attack that exploits vulnerabilities in the partial reconfiguration process, specifically while a user bitstream is being uploaded to the FPGA by a reconfiguration manager. Unlike traditional fault attacks that operate during module runtime, FLARE injects faults in the bitstream during its reconfiguration, altering the configuration address and redirecting it to unintended partial reconfigurable regions (PRRs). This enables the overwriting of pre-configured co-tenant modules, disrupting their functionality. FLARE leverages power-wasters that activate briefly during the reconfiguration process, making the attack stealthy and more challenging to detect with existing countermeasures. Experimental results on a Xilinx Pynq FPGA demonstrate the effectiveness of FLARE in compromising multiple user bitstreams during the reconfiguration process.         ",
    "url": "https://arxiv.org/abs/2502.15578",
    "authors": [
      "Jayeeta Chaudhuri",
      "Hassan Nassar",
      "Dennis R.E. Gnad",
      "Jorg Henkel",
      "Mehdi B. Tahoori",
      "Krishnendu Chakrabarty"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.15585",
    "title": "A Comprehensive Survey of Linear, Integer, and Mixed-Integer Programming Approaches for Optimizing Resource Allocation in 5G and Beyond Networks",
    "abstract": "           The introduction of 5G networks has significantly advanced communication technology, offering faster speeds, lower latency, and greater capacity. This progress sets the stage for Beyond 5G (B5G) networks, which present new complexity and performance requirements challenges. Linear Programming (LP), Integer Linear Programming (ILP), and Mixed-Integer Linear Programming (MILP) models have been widely used to model the optimization of resource allocation problems in networks. This paper reviews 103 studies on resource allocation strategies in 5G and B5G, focusing specifically on optimization problems modelled as LP, ILP, and MILP. The selected studies are categorized based on network architectures, types of resource allocation problems, and specific objective functions and constraints. The review also discusses solution methods for NP-hard ILP and MILP problems by categorizing the solution methods into different categories. Additionally, emerging trends, such as integrating AI and machine learning with optimization models, are explored, suggesting promising future research directions in network optimization. The paper concludes that LP, ILP, and MILP models have been widely adopted across various network architectures, resource types, objective functions, and constraints and remain critical to optimizing next-generation networks.         ",
    "url": "https://arxiv.org/abs/2502.15585",
    "authors": [
      "Naveed Ejaz",
      "Salimur Choudhury"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2502.15594",
    "title": "SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention",
    "abstract": "           With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation while ensuring both effectiveness and efficiency, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. SafeInt is built on our analysis of the representations of jailbreak samples. It adjusts representation distributions of jailbreak samples through intervention to align them with the representations of unsafe samples while minimizing unnecessary perturbations to jailbreak-irrelevant representations. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks.         ",
    "url": "https://arxiv.org/abs/2502.15594",
    "authors": [
      "Jiaqi Wu",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15600",
    "title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings",
    "abstract": "           There has been significant prior work using templates to study bias against demographic attributes in MLMs. However, these have limitations: they overlook random variability of templates and target concepts analyzed, assume equality amongst templates, and overlook bias quantification. Addressing these, we propose a systematic statistical approach to assess bias in MLMs, using mixed models to account for random effects, pseudo-perplexity weights for sentences derived from templates and quantify bias using statistical effect sizes. Replicating prior studies, we match on bias scores in magnitude and direction with small to medium effect sizes. Next, we explore the novel problem of gender bias in the context of $\\textit{personality}$ and $\\textit{character}$ traits, across seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased for binary gender but the most biased for non-binary $\\textit{neo}$, while RoBERTa-large is the most biased for binary gender but shows small to no bias for $\\textit{neo}$. There is some alignment of MLM bias and findings in psychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large and $\\textit{emotional stability}$ with BERT-large. There is general agreement for the remaining 3 personality dimensions: both sides observe at most small differences across gender. For character traits, human studies on gender bias are limited thus comparisons are not feasible.         ",
    "url": "https://arxiv.org/abs/2502.15600",
    "authors": [
      "Ingroj Shrestha",
      "Louis Tay",
      "Padmini Srinivasan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.15609",
    "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification",
    "abstract": "           Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels. Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.         ",
    "url": "https://arxiv.org/abs/2502.15609",
    "authors": [
      "Tianle Li",
      "Chenyang Zhang",
      "Xingwu Chen",
      "Yuan Cao",
      "Difan Zou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.15623",
    "title": "Dynamic Knowledge Selector and Evaluator for recommendation with Knowledge Graph",
    "abstract": "           In recent years recommendation systems typically employ the edge information provided by knowledge graphs combined with the advantages of high-order connectivity of graph networks in the recommendation field. However, this method is limited by the sparsity of labels, cannot learn the graph structure well, and a large number of noisy entities in the knowledge graph will affect the accuracy of the recommendation results. In order to alleviate the above problems, we propose a dynamic knowledge-selecting and evaluating method guided by collaborative signals to distill information in the knowledge graph. Specifically, we use a Chain Route Evaluator to evaluate the contributions of different neighborhoods for the recommendation task and employ a Knowledge Selector strategy to filter the less informative knowledge before evaluating. We conduct baseline model comparison and experimental ablation evaluations on three public datasets. The experiments demonstrate that our proposed model outperforms current state-of-the-art baseline models, and each modules effectiveness in our model is demonstrated through ablation experiments.         ",
    "url": "https://arxiv.org/abs/2502.15623",
    "authors": [
      "Feng Xia",
      "Zhifei Hu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15632",
    "title": "Continual Person Identification using Footstep-Induced Floor Vibrations on Heterogeneous Floor Structures",
    "abstract": "           Person identification is important for smart buildings to provide personalized services such as health monitoring, activity tracking, and personnel management. However, previous person identification relies on pre-collected data from everyone, which is impractical in many buildings and public facilities in which visitors are typically expected. This calls for a continual person identification system that gradually learns people's identities on the fly. Existing studies use cameras to achieve this goal, but they require direct line-of-sight and also have raised privacy concerns in public. Other modalities such as wearables and pressure mats are limited by the requirement of device-carrying or dense deployment. Thus, prior studies introduced footstep-induced structural vibration sensing, which is non-intrusive and perceived as more privacy-friendly. However, this approach has a significant challenge: the high variability of vibration data due to structural heterogeneity and human gait variations, which makes online person identification algorithms perform poorly. In this paper, we characterize the variability in footstep-induced structural vibration data for accurate online person identification. To achieve this, we quantify and decompose different sources of variability and then design a feature transformation function to reduce the variability within each person's data to make different people's data more separable. We evaluate our approach through field experiments with 20 people. The results show a 70% variability reduction and a 90% accuracy for online person identification.         ",
    "url": "https://arxiv.org/abs/2502.15632",
    "authors": [
      "Yiwen Dong",
      "Hae Young Noh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)",
      "Applied Physics (physics.app-ph)"
    ]
  },
  {
    "id": "arXiv:2502.15639",
    "title": "Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models",
    "abstract": "           Aligned representations across languages is a desired property in multilingual large language models (mLLMs), as alignment can improve performance in cross-lingual tasks. Typically alignment requires fine-tuning a model, which is computationally expensive, and sizable language data, which often may not be available. A data-efficient alternative to fine-tuning is model interventions -- a method for manipulating model activations to steer generation into the desired direction. We analyze the effect of a popular intervention (finding experts) on the alignment of cross-lingual representations in mLLMs. We identify the neurons to manipulate for a given language and introspect the embedding space of mLLMs pre- and post-manipulation. We show that modifying the mLLM's activations changes its embedding space such that cross-lingual alignment is enhanced. Further, we show that the changes to the embedding space translate into improved downstream performance on retrieval tasks, with up to 2x improvements in top-1 accuracy on cross-lingual retrieval.         ",
    "url": "https://arxiv.org/abs/2502.15639",
    "authors": [
      "Anirudh Sundar",
      "Sinead Williamson",
      "Katherine Metcalf",
      "Barry-John Theobald",
      "Skyler Seto",
      "Masha Fedzechkina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15641",
    "title": "A Deep Neural Network-based Frequency Predictor for Frequency-Constrained Optimal Power Flow",
    "abstract": "           Rate of change of frequency (RoCoF) and frequency nadir should be considered in real-time frequency-constrained optimal power flow (FCOPF) to ensure frequency stability of the modern power systems. Since calculating the frequency response is complex, deep neural network (DNN) could be adopted to capture the nonlinearities and estimate those two metrics accurately. Therefore, in this paper, a DNN-based frequency predictor is developed with the training data obtained from time-domain simulations using PSCAD/EMTDC. Subsequently, it is reformulated using a set of mixed-integer linear programming formulations and then embedded into the FCOPF framework as constraints to ensure grid frequency stability, creating the proposed DNN-FCOPF model. Two benchmark models, a traditional OPF without any frequency constraints and a linear system-wide RoCoF-constrained FCOPF, are also implemented to gauge the proposed DNN-FCOPF. Finally, the solutions obtained with these three models are compared and evaluated with time-domain simulations using PSCAD under various load profiles, demonstrating the effectiveness of the proposed DNN-FCOPF.         ",
    "url": "https://arxiv.org/abs/2502.15641",
    "authors": [
      "Fan Jiang",
      "Xingpeng Li",
      "Pascal Van Hentenryck"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.15642",
    "title": "Training Neural ODEs Using Fully Discretized Simultaneous Optimization",
    "abstract": "           Neural Ordinary Differential Equations (Neural ODEs) represent continuous-time dynamics with neural networks, offering advancements for modeling and control tasks. However, training Neural ODEs requires solving differential equations at each epoch, leading to high computational costs. This work investigates simultaneous optimization methods as a faster training alternative. In particular, we employ a collocation-based, fully discretized formulation and use IPOPT--a solver for large-scale nonlinear optimization--to simultaneously optimize collocation coefficients and neural network parameters. Using the Van der Pol Oscillator as a case study, we demonstrate faster convergence compared to traditional training methods. Furthermore, we introduce a decomposition framework utilizing Alternating Direction Method of Multipliers (ADMM) to effectively coordinate sub-models among data batches. Our results show significant potential for (collocation-based) simultaneous Neural ODE training pipelines.         ",
    "url": "https://arxiv.org/abs/2502.15642",
    "authors": [
      "Mariia Shapovalova",
      "Calvin Tsay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2502.15643",
    "title": "AutoTandemML: Active Learning Enhanced Tandem Neural Networks for Inverse Design Problems",
    "abstract": "           Inverse design in science and engineering involves determining optimal design parameters that achieve desired performance outcomes, a process often hindered by the complexity and high dimensionality of design spaces, leading to significant computational costs. To tackle this challenge, we propose a novel hybrid approach that combines active learning with Tandem Neural Networks to enhance the efficiency and effectiveness of solving inverse design problems. Active learning allows to selectively sample the most informative data points, reducing the required dataset size without compromising accuracy. We investigate this approach using three benchmark problems: airfoil inverse design, photonic surface inverse design, and scalar boundary condition reconstruction in diffusion partial differential equations. We demonstrate that integrating active learning with Tandem Neural Networks outperforms standard approaches across the benchmark suite, achieving better accuracy with fewer training samples.         ",
    "url": "https://arxiv.org/abs/2502.15643",
    "authors": [
      "Luka Grbcic",
      "Juliane M\u00fcller",
      "Wibe Albert de Jong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2502.15648",
    "title": "Logit Disagreement: OoD Detection with Bayesian Neural Networks",
    "abstract": "           Bayesian neural networks (BNNs), which estimate the full posterior distribution over model parameters, are well-known for their role in uncertainty quantification and its promising application in out-of-distribution detection (OoD). Amongst other uncertainty measures, BNNs provide a state-of-the art estimation of predictive entropy (total uncertainty) which can be decomposed as the sum of mutual information and expected entropy. In the context of OoD detection the estimation of predictive uncertainty in the form of the predictive entropy score confounds aleatoric and epistemic uncertainty, the latter being hypothesized to be high for OoD points. Despite these justifications, the mutual information score has been shown to perform worse than predictive entropy. Taking inspiration from Bayesian variational autoencoder (BVAE) literature, this work proposes to measure the disagreement between a corrected version of the pre-softmax quantities, otherwise known as logits, as an estimate of epistemic uncertainty for Bayesian NNs under mean field variational inference. The three proposed epistemic uncertainty scores demonstrate marked improvements over mutual information on a range of OoD experiments, with equal performance otherwise. Moreover, the epistemic uncertainty scores perform on par with the Bayesian benchmark predictive entropy on a range of MNIST and CIFAR10 experiments.         ",
    "url": "https://arxiv.org/abs/2502.15648",
    "authors": [
      "Kevin Raina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.15653",
    "title": "Blockchain-based Trust Management in Security Credential Management System for Vehicular Network",
    "abstract": "           Cellular networking is advancing as a wireless technology to support diverse applications in vehicular communication, enabling vehicles to interact with various applications to enhance the driving experience, even when managed by different authorities. Security Credential Management System (SCMS) is the Public Key Infrastructure (PKI) for vehicular networking and the state-of-the-art distributed PKI to protect the privacy-preserving vehicular networking against an honest-but-curious authority using multiple authorities and to decentralize the trust management. We build a Blockchain-Based Trust Management (BBTM) to provide even greater decentralization and security. Specifically, BBTM uses the blockchain to 1) replace the existing Policy Generator (PG), 2) manage the policy of each authority in SCMS, 3) aggregate the Global Certificate Chain File (GCCF), and 4) provide greater accountability and transparency on the aforementioned functionalities. We implement BBTM on Hyperledger Fabric using a smart contract for experimentation and analyses. Our experiments show that BBTM is lightweight in processing, efficient management in the certificate chain and ledger size, supports a bandwidth of multiple transactions per second, and provides validated end-entities.         ",
    "url": "https://arxiv.org/abs/2502.15653",
    "authors": [
      "SangHyun Byun",
      "Arijet Sarker",
      "Sang-Yoon Chang",
      "Jugal Kalita"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2502.15654",
    "title": "Machine-generated text detection prevents language model collapse",
    "abstract": "           As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since web data is the primary resource for LLM pretraining, future models will be trained on an unknown portion of synthetic data. This will lead to model collapse, a degenerative process which causes models to reinforce their own errors and experience a drop in model performance. In this study, we investigate the impact of decoding strategy on model collapse, where we analyse the characteristics of the generated data during recursive training, its similarity to human references and the resulting model performance. Using the decoding strategies that lead to the most significant model degradation, we tackle the question: how to avoid model collapse when the origin (human or synthetic) of the training data is unknown. We design a novel methodology based on resampling the data distribution using importance weights from our machine-generated text detector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task, demonstrating that we can successfully prevent model collapse and when there is enough human-authored data in the training dataset, our method improves model performance.         ",
    "url": "https://arxiv.org/abs/2502.15654",
    "authors": [
      "George Drayson",
      "Vasileios Lampos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15662",
    "title": "Automating Curriculum Learning for Reinforcement Learning using a Skill-Based Bayesian Network",
    "abstract": "           A major challenge for reinforcement learning is automatically generating curricula to reduce training time or improve performance in some target task. We introduce SEBNs (Skill-Environment Bayesian Networks) which model a probabilistic relationship between a set of skills, a set of goals that relate to the reward structure, and a set of environment features to predict policy performance on (possibly unseen) tasks. We develop an algorithm that uses the inferred estimates of agent success from SEBN to weigh the possible next tasks by expected improvement. We evaluate the benefit of the resulting curriculum on three environments: a discrete gridworld, continuous control, and simulated robotics. The results show that curricula constructed using SEBN frequently outperform other baselines.         ",
    "url": "https://arxiv.org/abs/2502.15662",
    "authors": [
      "Vincent Hsiao",
      "Mark Roberts",
      "Laura M. Hiatt",
      "George Konidaris",
      "Dana Nau"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15675",
    "title": "Exploring subgraph complementation to bounded degree graphs",
    "abstract": "           Graph modification problems are computational tasks where the goal is to change an input graph $G$ using operations from a fixed set, in order to make the resulting graph satisfy a target property, which usually entails membership to a desired graph class $\\mathcal{C}$. Some well-known examples of operations include vertex-deletion, edge-deletion, edge-addition and edge-contraction. In this paper we address an operation known as subgraph complement. Given a graph $G$ and a subset $S$ of its vertices, the subgraph complement $G \\oplus S$ is the graph resulting of complementing the edge set of the subgraph induced by $S$ in $G$. We say that a graph $H$ is a subgraph complement of $G$ if there is an $S$ such that $H$ is isomorphic to $G \\oplus S$. For a graph class $\\mathcal{C}$, subgraph complementation to $\\mathcal{C}$ is the problem of deciding, for a given graph $G$, whether $G$ has a subgraph complement in $\\mathcal{C}$. This problem has been studied and its complexity has been settled for many classes $\\mathcal{C}$ such as $\\mathcal{H}$-free graphs, for various families $\\mathcal{H}$, and for classes of bounded degeneracy. In this work, we focus on classes graphs of minimum/maximum degree upper/lower bounded by some value $k$. In particular, we answer an open question of Antony et al. [Information Processing Letters 188, 106530 (2025)], by showing that subgraph complementation to $\\mathcal{C}$ is NP-complete when $\\mathcal{C}$ is the class of graphs of minimum degree at least $k$, if $k$ is part of the input. We also show that subgraph complementation to $k$-regular parameterized by $k$ is fixed-parameter tractable.         ",
    "url": "https://arxiv.org/abs/2502.15675",
    "authors": [
      "Ivo Koch",
      "Nina Pardal",
      "Vinicius F. dos Santos"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2502.15680",
    "title": "Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training",
    "abstract": "           Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII significantly (in our settings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to other PII being memorized. Model creators should consider these first- and second-order privacy risks when training models to avoid the risk of new PII regurgitation.         ",
    "url": "https://arxiv.org/abs/2502.15680",
    "authors": [
      "Jaydeep Borkar",
      "Matthew Jagielski",
      "Katherine Lee",
      "Niloofar Mireshghallah",
      "David A. Smith",
      "Christopher A. Choquette-Choo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.14904",
    "title": "Towards an automated workflow in materials science for combining multi-modal simulative and experimental information using data mining and large language models",
    "abstract": "           To retrieve and compare scientific data of simulations and experiments in materials science, data needs to be easily accessible and machine readable to qualify and quantify various materials science phenomena. The recent progress in open science leverages the accessibility to data. However, a majority of information is encoded within scientific documents limiting the capability of finding suitable literature as well as material properties. This manuscript showcases an automated workflow, which unravels the encoded information from scientific literature to a machine readable data structure of texts, figures, tables, equations and meta-data, using natural language processing and language as well as vision transformer models to generate a machine-readable database. The machine-readable database can be enriched with local data, as e.g. unpublished or private material data, leading to knowledge synthesis. The study shows that such an automated workflow accelerates information retrieval, proximate context detection and material property extraction from multi-modal input data exemplarily shown for the research field of microstructural analyses of face-centered cubic single crystals. Ultimately, a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) enables a fast and efficient question answering chat bot.         ",
    "url": "https://arxiv.org/abs/2502.14904",
    "authors": [
      "Balduin Katzer",
      "Steffen Klinder",
      "Katrin Schulz"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14919",
    "title": "Optimizing Gene-Based Testing for Antibiotic Resistance Prediction",
    "abstract": "           Antibiotic Resistance (AR) is a critical global health challenge that necessitates the development of cost-effective, efficient, and accurate diagnostic tools. Given the genetic basis of AR, techniques such as Polymerase Chain Reaction (PCR) that target specific resistance genes offer a promising approach for predictive diagnostics using a limited set of key genes. This study introduces GenoARM, a novel framework that integrates reinforcement learning (RL) with transformer-based models to optimize the selection of PCR gene tests and improve AR predictions, leveraging observed metadata for improved accuracy. In our evaluation, we developed several high-performing baselines and compared them using publicly available datasets derived from real-world bacterial samples representing multiple clinically relevant pathogens. The results show that all evaluated methods achieve strong and reliable performance when metadata is not utilized. When metadata is introduced and the number of selected genes increases, GenoARM demonstrates superior performance due to its capacity to approximate rewards for unseen and sparse combinations. Overall, our framework represents a major advancement in optimizing diagnostic tools for AR in clinical settings.         ",
    "url": "https://arxiv.org/abs/2502.14919",
    "authors": [
      "David Hagerman",
      "Anna Johnning",
      "Roman Naeem",
      "Fredrik Kahl",
      "Erik Kristiansson",
      "Lennart Svensson"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14920",
    "title": "Display Field-Of-View Agnostic Robust CT Kernel Synthesis Using Model-Based Deep Learning",
    "abstract": "           In X-ray computed tomography (CT) imaging, the choice of reconstruction kernel is crucial as it significantly impacts the quality of clinical images. Different kernels influence spatial resolution, image noise, and contrast in various ways. Clinical applications involving lung imaging often require images reconstructed with both soft and sharp kernels. The reconstruction of images with different kernels requires raw sinogram data and storing images for all kernels increases processing time and storage requirements. The Display Field-of-View (DFOV) adds complexity to kernel synthesis, as data acquired at different DFOVs exhibit varying levels of sharpness and details. This work introduces an efficient, DFOV-agnostic solution for image-based kernel synthesis using model-based deep learning. The proposed method explicitly integrates CT kernel and DFOV characteristics into the forward model. Experimental results on clinical data, along with quantitative analysis of the estimated modulation transfer function using wire phantom data, clearly demonstrate the utility of the proposed method in real-time. Additionally, a comparative study with a direct learning network, that lacks forward model information, shows that the proposed method is more robust to DFOV variations.         ",
    "url": "https://arxiv.org/abs/2502.14920",
    "authors": [
      "Hemant Kumar Aggarwal",
      "Antony Jerald",
      "Phaneendra K. Yalavarthy",
      "Rajesh Langoju",
      "Bipul Das"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.14928",
    "title": "Distributed U-net model and Image Segmentation for Lung Cancer Detection",
    "abstract": "           Until now, in the wake of the COVID-19 pandemic in 2019, lung diseases, especially diseases such as lung cancer and chronic obstructive pulmonary disease (COPD), have become an urgent global health issue. In order to mitigate the goal problem, early detection and accurate diagnosis of these conditions are critical for effective treatment and improved patient outcomes. To further research and reduce the error rate of hospital diagnoses, this comprehensive study explored the potential of computer-aided design (CAD) systems, especially utilizing advanced deep learning models such as U-Net. And compared with the literature content of other authors, this study explores the capabilities of U-Net in detail, and enhances the ability to simulate CAD systems through the VGG16 algorithm. An extensive dataset consisting of lung CT images and corresponding segmentation masks, curated collaboratively by multiple academic institutions, serves as the basis for empirical validation. In this paper, the efficiency of U-Net model is evaluated rigorously and precisely under multiple hardware configurations, such as single CPU, single GPU, distributed GPU and federated learning, and the effectiveness and development of the method in the segmentation task of lung disease are demonstrated. Empirical results clearly affirm the robust performance of the U-Net model, most effectively utilizing four GPUs for distributed learning, and these results highlight the potential of U-Net-based CAD systems for accurate and timely lung disease detection and diagnosis huge potential.         ",
    "url": "https://arxiv.org/abs/2502.14928",
    "authors": [
      "Tianzuo Hu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2502.14936",
    "title": "Reducing false positives in strong lens detection through effective augmentation and ensemble learning",
    "abstract": "           This research studies the impact of high-quality training datasets on the performance of Convolutional Neural Networks (CNNs) in detecting strong gravitational lenses. We stress the importance of data diversity and representativeness, demonstrating how variations in sample populations influence CNN performance. In addition to the quality of training data, our results highlight the effectiveness of various techniques, such as data augmentation and ensemble learning, in reducing false positives while maintaining model completeness at an acceptable level. This enhances the robustness of gravitational lens detection models and advancing capabilities in this field. Our experiments, employing variations of DenseNet and EfficientNet, achieved a best false positive rate (FP rate) of $10^{-4}$, while successfully identifying over 88 per cent of genuine gravitational lenses in the test dataset. This represents an 11-fold reduction in the FP rate compared to the original training dataset. Notably, this substantial enhancement in the FP rate is accompanied by only a 2.3 per cent decrease in the number of true positive samples. Validated on the KiDS dataset, our findings offer insights applicable to ongoing missions, like Euclid.         ",
    "url": "https://arxiv.org/abs/2502.14936",
    "authors": [
      "Samira Rezaei",
      "Amirmohammad Chegeni",
      "Bharath Chowdhary Nagam",
      "J. P. McKean",
      "Mitra Baratchi",
      "Koen Kuijken",
      "L\u00e9on V. E. Koopmans"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.14937",
    "title": "Compact Latent Representation for Image Compression (CLRIC)",
    "abstract": "           Current image compression models often require separate models for each quality level, making them resource-intensive in terms of both training and storage. To address these limitations, we propose an innovative approach that utilizes latent variables from pre-existing trained models (such as the Stable Diffusion Variational Autoencoder) for perceptual image compression. Our method eliminates the need for distinct models dedicated to different quality levels. We employ overfitted learnable functions to compress the latent representation from the target model at any desired quality level. These overfitted functions operate in the latent space, ensuring low computational complexity, around $25.5$ MAC/pixel for a forward pass on images with dimensions $(1363 \\times 2048)$ pixels. This approach efficiently utilizes resources during both training and decoding. Our method achieves comparable perceptual quality to state-of-the-art learned image compression models while being both model-agnostic and resolution-agnostic. This opens up new possibilities for the development of innovative image compression methods.         ",
    "url": "https://arxiv.org/abs/2502.14937",
    "authors": [
      "Ayman A. Ameen",
      "Thomas Richter",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.14950",
    "title": "Symmetric observations without symmetric causal explanations",
    "abstract": "           Inferring causal models from observed correlations is a challenging task, crucial to many areas of science. In order to alleviate the effort, it is important to know whether symmetries in the observations correspond to symmetries in the underlying realization. Via an explicit example, we answer this question in the negative. We use a tripartite probability distribution over binary events that is realized by using three (different) independent sources of classical randomness. We prove that even removing the condition that the sources distribute systems described by classical physics, the requirements that i) the sources distribute the same physical systems, ii) these physical systems respect relativistic causality, and iii) the correlations are the observed ones, are incompatible.         ",
    "url": "https://arxiv.org/abs/2502.14950",
    "authors": [
      "Christian William",
      "Patrick Remy",
      "Jean-Daniel Bancal",
      "Yu Cai",
      "Nicolas Brunner",
      "Alejandro Pozas-Kerstjens"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.14988",
    "title": "Partial and Exact Recovery of a Random Hypergraph from its Graph Projection",
    "abstract": "           Consider a $d$-uniform random hypergraph on $n$ vertices in which hyperedges are included iid so that the average degree is $n^\\delta$. The projection of a hypergraph is a graph on the same $n$ vertices where an edge connects two vertices if and only if they belong to some hyperedge. The goal is to reconstruct the hypergraph given its projection. An earlier work of Bresler, Guo, and Polyanskiy (COLT 2024) showed that exact recovery for $d=3$ is possible if and only if $\\delta < 2/5$. This work completely resolves the question for all values of $d$ for both exact and partial recovery and for both cases of whether multiplicity information about each edge is available or not. In addition, we show that the reconstruction fidelity undergoes an all-or-nothing transition at a threshold. In particular, this resolves all conjectures from Bresler, Guo, and Polyanskiy (COLT 2024).         ",
    "url": "https://arxiv.org/abs/2502.14988",
    "authors": [
      "Guy Bresler",
      "Chenghao Guo",
      "Yury Polyanskiy",
      "Andrew Yao"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2502.15023",
    "title": "A Quantum Signature Validation Algorithm for Efficient Detection of Tampered Transactions in Blockchain",
    "abstract": "           The Quantum Signature Validation Algorithm (QSVA) is introduced as a novel quantum-based approach designed to enhance the detection of tampered transactions in blockchain systems. Leveraging the powerful capabilities of quantum computing, especially within the framework of transaction-based blockchains, the QSVA aims to surpass classical methods in both speed and efficiency. By utilizing a quantum walk approach integrated with PageRank-based search algorithms, QSVA provides a robust mechanism for identifying fraudulent transactions. Our adaptation of the transaction graph representation efficiently verifies transactions by maintaining a current set of unspent transaction outputs (UTXOs) characteristic of models like Bitcoin. The QSVA not only amplifies detection efficacy through a quadratic speedup but also incorporates two competing quantum search algorithms$-$Quantum SearchRank and Randomized SearchRank$-$to explore their effectiveness as foundational components. Our results indicate that Randomized SearchRank, in particular, outperforms its counterpart in aligning with transaction rankings based on the Classical PageRank algorithm, ensuring more consistent detection probabilities. These findings highlight the potential for quantum algorithms to revolutionize blockchain security by improving detection times to $O(\\sqrt{N})$. Progress in Distributed Ledger Technologies (DLTs) could facilitate future integration of quantum solutions into more general distributed systems. As quantum technology continues to evolve, the QSVA stands as a promising strategy offering significant advancements in blockchain efficiency and security.         ",
    "url": "https://arxiv.org/abs/2502.15023",
    "authors": [
      "Jaime Torres",
      "Sergio A. Ortega",
      "Miguel A. Martin-Delgado"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.15186",
    "title": "LUMINA-Net: Low-light Upgrade through Multi-stage Illumination and Noise Adaptation Network for Image Enhancement",
    "abstract": "           Low-light image enhancement (LLIE) is a crucial task in computer vision aimed to enhance the visual fidelity of images captured under low-illumination conditions. Conventional methods frequently struggle to mitigate pervasive shortcomings such as noise, over-exposure, and color distortion thereby precipitating a pronounced degradation in image quality. To address these challenges, we propose LUMINA-Net an advanced deep learning framework designed specifically by integrating multi-stage illumination and reflectance modules. First, the illumination module intelligently adjusts brightness and contrast levels while meticulously preserving intricate textural details. Second, the reflectance module incorporates a noise reduction mechanism that leverages spatial attention and channel-wise feature refinement to mitigate noise contamination. Through a comprehensive suite of experiments conducted on LOL and SICE datasets using PSNR, SSIM and LPIPS metrics, surpassing state-of-the-art methodologies and showcasing its efficacy in low-light image enhancement.         ",
    "url": "https://arxiv.org/abs/2502.15186",
    "authors": [
      "Namrah Siddiqua",
      "Kim Suneung"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.15215",
    "title": "Tensor Product Neural Networks for Functional ANOVA Model",
    "abstract": "           Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions so called components, is one of the most popular tools for interpretable AI, and recently, various neural network models have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating components since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel interpretable model which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably. We call our proposed model ANOVA-NODE since it is a modification of Neural Oblivious Decision Ensembles (NODE) for the functional ANOVA model. Theoretically, we prove that ANOVA-NODE can approximate a smooth function well. Additionally, we experimentally show that ANOVA-NODE provides much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural network models do.         ",
    "url": "https://arxiv.org/abs/2502.15215",
    "authors": [
      "Seokhun Park",
      "Insung Kong",
      "Yongchan Choi",
      "Chanmoo Park",
      "Yongdai Kim"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2502.15503",
    "title": "BAN: Neuroanatomical Aligning in Auditory Recognition between Artificial Neural Network and Human Cortex",
    "abstract": "           Drawing inspiration from neurosciences, artificial neural networks (ANNs) have evolved from shallow architectures to highly complex, deep structures, yielding exceptional performance in auditory recognition tasks. However, traditional ANNs often struggle to align with brain regions due to their excessive depth and lack of biologically realistic features, like recurrent connection. To address this, a brain-like auditory network (BAN) is introduced, which incorporates four neuroanatomically mapped areas and recurrent connection, guided by a novel metric called the brain-like auditory score (BAS). BAS serves as a benchmark for evaluating the similarity between BAN and human auditory recognition pathway. We further propose that specific areas in the cerebral cortex, mainly the middle and medial superior temporal (T2/T3) areas, correspond to the designed network structure, drawing parallels with the brain's auditory perception pathway. Our findings suggest that the neuroanatomical similarity in the cortex and auditory classification abilities of the ANN are well-aligned. In addition to delivering excellent performance on a music genre classification task, the BAN demonstrates a high BAS score. In conclusion, this study presents BAN as a recurrent, brain-inspired ANN, representing the first model that mirrors the cortical pathway of auditory recognition.         ",
    "url": "https://arxiv.org/abs/2502.15503",
    "authors": [
      "Haidong Wang",
      "Pengfei Xiao",
      "Ao Liu",
      "Jianhua Zhang",
      "Qia Shan"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15540",
    "title": "Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors",
    "abstract": "           We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and \"test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the \"structure\" and \"simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a data-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB).         ",
    "url": "https://arxiv.org/abs/2502.15540",
    "authors": [
      "Milad Sefidgaran",
      "Abdellatif Zaidi",
      "Piotr Krasnowski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15611",
    "title": "Network topology of the Euro Area interbank market",
    "abstract": "           The rapidly increasing availability of large amounts of granular financial data, paired with the advances of big data related technologies induces the need of suitable analytics that can represent and extract meaningful information from such data. In this paper we propose a multi-layer network approach to distill the Euro Area (EA) banking system in different distinct layers. Each layer of the network represents a specific type of financial relationship between banks, based on various sources of EA granular data collections. The resulting multi-layer network allows one to describe, analyze and compare the topology and structure of EA banks from different perspectives, eventually yielding a more complete picture of the financial market. This granular information representation has the potential to enable researchers and practitioners to better apprehend financial system dynamics as well as to support financial policies to manage and monitor financial risk from a more holistic point of view.         ",
    "url": "https://arxiv.org/abs/2502.15611",
    "authors": [
      "Ilias Aarab",
      "Thomas Gottron"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2105.11866",
    "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling",
    "abstract": "           Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion. On the other hand, taking into account interactions between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach, Graph Factorization Machine (GraphFM), by naturally representing features in the graph structure. In particular, we design a mechanism to select the beneficial feature interactions and formulate them as edges between features. Then the proposed model, which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experimental results on several real-world datasets have demonstrated the rationality and effectiveness of our proposed approach. The code and data are available at this https URL}{this https URL ",
    "url": "https://arxiv.org/abs/2105.11866",
    "authors": [
      "Shu Wu",
      "Zekun Li",
      "Yunyue Su",
      "Zeyu Cui",
      "Xiaoyu Zhang",
      "Liang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2201.09698",
    "title": "Graph Neural Diffusion Networks for Semi-supervised Learning",
    "abstract": "           Graph Convolutional Networks (GCN) is a pioneering model for graph-based semi-supervised learning. However, GCN does not perform well on sparsely-labeled graphs. Its two-layer version cannot effectively propagate the label information to the whole graph structure (i.e., the under-smoothing problem) while its deep version over-smoothens and is hard to train (i.e., the over-smoothing problem). To solve these two issues, we propose a new graph neural network called GND-Nets (for Graph Neural Diffusion Networks) that exploits the local and global neighborhood information of a vertex in a single layer. Exploiting the shallow network mitigates the over-smoothing problem while exploiting the local and global neighborhood information mitigates the under-smoothing problem. The utilization of the local and global neighborhood information of a vertex is achieved by a new graph diffusion method called neural diffusions, which integrate neural networks into the conventional linear and nonlinear graph diffusions. The adoption of neural networks makes neural diffusions adaptable to different datasets. Extensive experiments on various sparsely-labeled graphs verify the effectiveness and efficiency of GND-Nets compared to state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2201.09698",
    "authors": [
      "Wei Ye",
      "Zexi Huang",
      "Yunqi Hong",
      "Ambuj Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2206.05860",
    "title": "IGN : Implicit Generative Networks",
    "abstract": "           In this work, we build recent advances in distributional reinforcement learning to give a state-of-art distributional variant of the model based on the IQN. We achieve this by using the GAN model's generator and discriminator function with the quantile regression to approximate the full quantile value for the state-action return distribution. We demonstrate improved performance on our baseline dataset - 57 Atari 2600 games in the ALE. Also, we use our algorithm to show the state-of-art training performance of risk-sensitive policies in Atari games with the policy optimization and evaluation.         ",
    "url": "https://arxiv.org/abs/2206.05860",
    "authors": [
      "Haozheng Luo",
      "Tianyi Wu",
      "Colin Feiyu Han",
      "Zhijun Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2209.06418",
    "title": "Graph Perceiver IO: A General Architecture for Graph Structured Data",
    "abstract": "           Multimodal machine learning has been widely studied for the development of general intelligence. Recently, the Perceiver and Perceiver IO, show competitive results for diverse dataset domains and tasks. However, recent works, Perceiver and Perceiver IO, have focused on heterogeneous modalities, including image, text, and there are few research works for graph structured datasets. A graph has an adjacency matrix different from other datasets such as text and image, and it is not trivial to handle the topological information. In this study, we provide a Graph Perceiver IO (GPIO), the Perceiver IO for the graph structured dataset. We keep the main structure of the GPIO as the Perceiver IO because the Perceiver IO already handles the diverse dataset well, except for the graph structured dataset. The GPIO is a general method that handles diverse datasets, such as graph-structured data, text, and images, by leveraging positional encoding and output query smoothing. Compared to graph neural networks (GNNs), GPIO requires lower complexity and can efficiently incorporate global and local information, which is also empirically validated through experiments. Furthermore, we propose GPIO+ for the multimodal few-shot classification that incorporates both images and graphs simultaneously. GPIO achieves higher benchmark accuracy than GNNs across multiple tasks, including graph classification, node classification, and multimodal text classification, while also attaining superior AP and AUC in link prediction. Additionally, GPIO+ outperforms GNNs in multimodal few-shot classification. Our GPIO(+) can serve as a general architecture for handling various modalities and tasks.         ",
    "url": "https://arxiv.org/abs/2209.06418",
    "authors": [
      "Seyun Bae",
      "Hoyoon Byun",
      "Changdae Oh",
      "Yoon-Sik Cho",
      "Kyungwoo Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.12270",
    "title": "SCALES: Boost Binary Neural Network for Image Super-Resolution with Efficient Scalings",
    "abstract": "           Deep neural networks for image super-resolution (SR) have demonstrated superior performance. However, the large memory and computation consumption hinders their deployment on resource-constrained devices. Binary neural networks (BNNs), which quantize the floating point weights and activations to 1-bit can significantly reduce the cost. Although BNNs for image classification have made great progress these days, existing BNNs for SR still suffer from a large performance gap between the FP SR networks. To this end, we observe the activation distribution in SR networks and find much larger pixel-to-pixel, channel-to-channel, layer-to-layer, and image-to-image variation in the activation distribution than image classification networks. However, existing BNNs for SR fail to capture these variations that contain rich information for image reconstruction, leading to inferior performance. To address this problem, we propose SCALES, a binarization method for SR networks that consists of the layer-wise scaling factor, the spatial re-scaling method, and the channel-wise re-scaling method, capturing the layer-wise, pixel-wise, and channel-wise variations efficiently in an input-dependent manner. We evaluate our method across different network architectures and datasets. For CNN-based SR networks, our binarization method SCALES outperforms the prior art method by 0.2dB with fewer parameters and operations. With SCALES, we achieve the first accurate binary Transformer-based SR network, improving PSNR by more than 1dB compared to the baseline method.         ",
    "url": "https://arxiv.org/abs/2303.12270",
    "authors": [
      "Renjie Wei",
      "Zechun Liu",
      "Yuchen Fan",
      "Runsheng Wang",
      "Ru Huang",
      "Meng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2304.03828",
    "title": "ZigzagNetVis: Suggesting temporal resolutions for graph visualization using zigzag persistence",
    "abstract": "           Temporal graphs are commonly used to represent complex systems and track the evolution of their constituents over time. Visualizing these graphs is crucial as it allows one to quickly identify anomalies, trends, patterns, and other properties that facilitate better decision-making. In this context, selecting an appropriate temporal resolution is essential for constructing and visually analyzing the layout. The choice of resolution is particularly important, especially when dealing with temporally sparse graphs. In such cases, changing the temporal resolution by grouping events (i.e., edges) from consecutive timestamps -- a technique known as timeslicing -- can aid in the analysis and reveal patterns that might not be discernible otherwise. However, selecting an appropriate temporal resolution is a challenging task. In this paper, we propose ZigzagNetVis, a methodology that suggests temporal resolutions potentially relevant for analyzing a given graph, i.e., resolutions that lead to substantial topological changes in the graph structure. ZigzagNetVis achieves this by leveraging zigzag persistent homology, a well-established technique from Topological Data Analysis (TDA). To improve visual graph analysis, ZigzagNetVis incorporates the colored barcode, a novel timeline-based visualization inspired by persistence barcodes commonly used in TDA. We also contribute with a web-based system prototype that implements suggestion methodology and visualization tools. Finally, we demonstrate the usefulness and effectiveness of ZigzagNetVis through a usage scenario, a user study with 27 participants, and a detailed quantitative evaluation.         ",
    "url": "https://arxiv.org/abs/2304.03828",
    "authors": [
      "Rapha\u00ebl Tinarrage",
      "Jean R. Ponciano",
      "Claudio D. G. Linhares",
      "Agma J. M. Traina",
      "Jorge Poco"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2305.10465",
    "title": "Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution",
    "abstract": "           Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. As a popular approach, probabilistic rotation modeling additionally carries prediction uncertainty information, compared to single-prediction rotation regression. For modeling probabilistic distribution over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix Fisher, however they are shown to be sensitive to outlier predictions, e.g. $180^\\circ$ error and thus are unlikely to converge with optimal performance. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region that it can improve. In addition, we show that our method also exhibits robustness to small noises and thus tolerates imperfect annotations. With this benefit, we demonstrate its advantages in semi-supervised rotation regression, where the pseudo labels are noisy. To further capture the multi-modal rotation solution space for symmetric objects, we extend our distribution to rotation Laplace mixture model and demonstrate its effectiveness. Our extensive experiments show that our proposed distribution and the mixture model achieve state-of-the-art performance in all the rotation regression experiments over both probabilistic and non-probabilistic baselines.         ",
    "url": "https://arxiv.org/abs/2305.10465",
    "authors": [
      "Yingda Yin",
      "Jiangran Lyu",
      "Yang Wang",
      "Haoran Liu",
      "He Wang",
      "Baoquan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.14024",
    "title": "Balanced Representation Learning for Long-tailed Skeleton-based Action Recognition",
    "abstract": "           Skeleton-based action recognition has recently made significant progress. However, data imbalance is still a great challenge in real-world scenarios. The performance of current action recognition algorithms declines sharply when training data suffers from heavy class imbalance. The imbalanced data actually degrades the representations learned by these methods and becomes the bottleneck for action recognition. How to learn unbiased representations from imbalanced action data is the key to long-tailed action recognition. In this paper, we propose a novel balanced representation learning method to address the long-tailed problem in action recognition. Firstly, a spatial-temporal action exploration strategy is presented to expand the sample space effectively, generating more valuable samples in a rebalanced manner. Secondly, we design a detached action-aware learning schedule to further mitigate the bias in the representation space. The schedule detaches the representation learning of tail classes from training and proposes an action-aware loss to impose more effective constraints. Additionally, a skip-modal representation is proposed to provide complementary structural information. The proposed method is validated on four skeleton datasets, NTU RGB+D 60, NTU RGB+D 120, NW-UCLA, and Kinetics. It not only achieves consistently large improvement compared to the state-of-the-art (SOTA) methods, but also demonstrates a superior generalization capacity through extensive experiments. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2308.14024",
    "authors": [
      "Hongda Liu",
      "Yunlong Wang",
      "Min Ren",
      "Junxing Hu",
      "Zhengquan Luo",
      "Guangqi Hou",
      "Zhenan Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.04979",
    "title": "DualDynamics: Synergizing Implicit and Explicit Methods for Robust Irregular Time Series Analysis",
    "abstract": "           Real-world time series analysis faces significant challenges when dealing with irregular and incomplete data. While Neural Differential Equation (NDE) based methods have shown promise, they struggle with limited expressiveness, scalability issues, and stability concerns. Conversely, Neural Flows offer stability but falter with irregular data. We introduce 'DualDynamics', a novel framework that synergistically combines NDE-based method and Neural Flow-based method. This approach enhances expressive power while balancing computational demands, addressing critical limitations of existing techniques. We demonstrate DualDynamics' effectiveness across diverse tasks: classification of robustness to dataset shift, irregularly-sampled series analysis, interpolation of missing data, and forecasting with partial observations. Our results show consistent outperformance over state-of-the-art methods, indicating DualDynamics' potential to advance irregular time series analysis significantly.         ",
    "url": "https://arxiv.org/abs/2401.04979",
    "authors": [
      "YongKyung Oh",
      "Dong-Young Lim",
      "Sungil Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.06824",
    "title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective",
    "abstract": "           The recent surge in jailbreaking attacks has revealed significant vulnerabilities in Large Language Models (LLMs) when exposed to malicious inputs. While various defense strategies have been proposed to mitigate these threats, there has been limited research into the underlying mechanisms that make LLMs vulnerable to such attacks. In this study, we suggest that the self-safeguarding capability of LLMs is linked to specific activity patterns within their representation space. Although these patterns have little impact on the semantic content of the generated text, they play a crucial role in shaping LLM behavior under jailbreaking attacks. Our findings demonstrate that these patterns can be detected with just a few pairs of contrastive queries. Extensive experimentation shows that the robustness of LLMs against jailbreaking can be manipulated by weakening or strengthening these patterns. Further visual analysis provides additional evidence for our conclusions, providing new insights into the jailbreaking phenomenon. These findings highlight the importance of addressing the potential misuse of open-source LLMs within the community.         ",
    "url": "https://arxiv.org/abs/2401.06824",
    "authors": [
      "Tianlong Li",
      "Zhenghua Wang",
      "Wenhao Liu",
      "Muling Wu",
      "Shihan Dou",
      "Changze Lv",
      "Xiaohua Wang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.07085",
    "title": "Three Mechanisms of Feature Learning in a Linear Network",
    "abstract": "           Understanding the dynamics of neural networks in different width regimes is crucial for improving their training and performance. We present an exact solution for the learning dynamics of a one-hidden-layer linear network, with one-dimensional data, across any finite width, uniquely exhibiting both kernel and feature learning phases. This study marks a technical advancement by enabling the analysis of the training trajectory from any initialization and a detailed phase diagram under varying common hyperparameters such as width, layer-wise learning rates, and scales of output and initialization. We identify three novel prototype mechanisms specific to the feature learning regime: (1) learning by alignment, (2) learning by disalignment, and (3) learning by rescaling, which contrast starkly with the dynamics observed in the kernel regime. Our theoretical findings are substantiated with empirical evidence showing that these mechanisms also manifest in deep nonlinear networks handling real-world tasks, enhancing our understanding of neural network training dynamics and guiding the design of more effective learning strategies.         ",
    "url": "https://arxiv.org/abs/2401.07085",
    "authors": [
      "Yizhou Xu",
      "Liu Ziyin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.02112",
    "title": "S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation",
    "abstract": "           Autonomous driving simulation system plays a crucial role in enhancing self-driving data and simulating complex and rare traffic scenarios, ensuring navigation safety. However, traditional simulation systems, which often heavily rely on manual modeling and 2D image editing, struggled with scaling to extensive scenes and generating realistic simulation data. In this study, we present S-NeRF++, an innovative autonomous driving simulation system based on neural reconstruction. Trained on widely-used self-driving datasets such as nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street scenes and foreground objects with high rendering quality as well as offering considerable flexibility in manipulation and simulation. Specifically, S-NeRF++ is an enhanced neural radiance field for synthesizing large-scale scenes and moving vehicles, with improved scene parameterization and camera pose learning. The system effectively utilizes noisy and sparse LiDAR data to refine training and address depth outliers, ensuring high-quality reconstruction and novel-view rendering. It also provides a diverse foreground asset bank by reconstructing and generating different foreground vehicles to support comprehensive scenario this http URL, we have developed an advanced foreground-background fusion pipeline that skillfully integrates illumination and shadow effects, further enhancing the realism of our simulations. With the high-quality simulated data provided by our S-NeRF++, we found the perception methods enjoy performance boosts on several autonomous driving downstream tasks, further demonstrating our proposed simulator's effectiveness.         ",
    "url": "https://arxiv.org/abs/2402.02112",
    "authors": [
      "Yurui Chen",
      "Junge Zhang",
      "Ziyang Xie",
      "Wenye Li",
      "Feihu Zhang",
      "Jiachen Lu",
      "Li Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.17465",
    "title": "LaRE$^2$: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection",
    "abstract": "           The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times. Code is available.         ",
    "url": "https://arxiv.org/abs/2403.17465",
    "authors": [
      "Yunpeng Luo",
      "Junlong Du",
      "Ke Yan",
      "Shouhong Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.19532",
    "title": "Dataless Quadratic Neural Networks for the Maximum Independent Set Problem",
    "abstract": "           Combinatorial Optimization (CO) addresses many important problems, including the challenging Maximum Independent Set (MIS) problem. Alongside exact and heuristic solvers, differentiable approaches have emerged, often using continuous relaxations of ReLU-based or quadratic objectives. Noting that an MIS in a graph is a Maximum Clique (MC) in its complement, we propose a new quadratic formulation for MIS by incorporating an MC term, improving convergence and exploration. We show that every maximal independent set corresponds to a local minimizer, derive conditions for the MIS size, and characterize stationary points. To solve our non-convex objective, we propose solving parallel multiple initializations using momentum-based gradient descent, complemented by an efficient MIS checking criterion derived from our theory. Therefore, we dub our method as parallelized Clique-Informed Quadratic Optimization for MIS (pCQO-MIS). Our experimental results demonstrate the effectiveness of the proposed method compared to exact, heuristic, sampling, and data-centric approaches. Notably, our method avoids the out-of-distribution tuning and reliance on (un)labeled data required by data-centric methods, while achieving superior MIS sizes and competitive runtime relative to their inference time. Additionally, a key advantage of pCQO-MIS is that, unlike exact and heuristic solvers, the runtime scales only with the number of nodes in the graph, not the number of edges.         ",
    "url": "https://arxiv.org/abs/2406.19532",
    "authors": [
      "Ismail Alkhouri",
      "Cedric Le Denmat",
      "Yingjie Li",
      "Cunxi Yu",
      "Jia Liu",
      "Rongrong Wang",
      "Alvaro Velasquez"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.21121",
    "title": "Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks",
    "abstract": "           Sinusoidal neural networks have been shown effective as implicit neural representations (INRs) of low-dimensional signals, due to their smoothness and high representation capacity. However, initializing and training them remain empirical tasks which lack on deeper understanding to guide the learning process. To fill this gap, our work introduces a theoretical framework that explains the capacity property of sinusoidal networks and offers robust control mechanisms for initialization and training. Our analysis is based on a novel amplitude-phase expansion of the sinusoidal multilayer perceptron, showing how its layer compositions produce a large number of new frequencies expressed as integer combinations of the input frequencies. This relationship can be directly used to initialize the input neurons, as a form of spectral sampling, and to bound the network's spectrum while training. Our method, referred to as TUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and convergence of sinusoidal INR training, leading to detailed reconstructions, while preventing overfitting.         ",
    "url": "https://arxiv.org/abs/2407.21121",
    "authors": [
      "Tiago Novello",
      "Diana Aldana",
      "Andre Araujo",
      "Luiz Velho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06395",
    "title": "Fast John Ellipsoid Computation with Differential Privacy Optimization",
    "abstract": "           Determining the John ellipsoid - the largest volume ellipsoid contained within a convex polytope - is a fundamental problem with applications in machine learning, optimization, and data analytics. Recent work has developed fast algorithms for approximating the John ellipsoid using sketching and leverage score sampling techniques. However, these algorithms do not provide privacy guarantees for sensitive input data. In this paper, we present the first differentially private algorithm for fast John ellipsoid computation. Our method integrates noise perturbation with sketching and leverages score sampling to achieve both efficiency and privacy. We prove that (1) our algorithm provides $(\\epsilon,\\delta)$-differential privacy and the privacy guarantee holds for neighboring datasets that are $\\epsilon_0$-close, allowing flexibility in the privacy definition; (2) our algorithm still converges to a $(1+\\xi)$-approximation of the optimal John ellipsoid in $\\Theta(\\xi^{-2}(\\log(n/\\delta_0) + (L\\epsilon_0)^{-2}))$ iterations where $n$ is the number of data point, $L$ is the Lipschitz constant, $\\delta_0$ is the failure probability, and $\\epsilon_0$ is the closeness of neighboring input datasets. Our theoretical analysis demonstrates the algorithm's convergence and privacy properties, providing a robust approach for balancing utility and privacy in John ellipsoid computation. This is the first differentially private algorithm for fast John ellipsoid computation, opening avenues for future research in privacy-preserving optimization techniques.         ",
    "url": "https://arxiv.org/abs/2408.06395",
    "authors": [
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Junwei Yu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.09121",
    "title": "Selective Prompt Anchoring for Code Generation",
    "abstract": "           Recent advances in large language models (LLMs) have transformed software development by automatically generating code from natural language. Yet challenges remain in generating fully correct code that aligns with user intent. Our study reveals that LLMs tend to pay less attention to user prompts as more code tokens are generated. We hypothesize that this attention dilution issue is an important reason for code generation errors. To mitigate this issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay more attention to user intent when generating code. We evaluate SPA using six base LLMs across six benchmarks. Our results demonstrate that SPA enhances Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods in all settings. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.09121",
    "authors": [
      "Yuan Tian",
      "Tianyi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.04679",
    "title": "Neural Augmentation Based Panoramic High Dynamic Range Stitching",
    "abstract": "           Due to saturated regions of inputting low dynamic range (LDR) images and large intensity changes among the LDR images caused by different exposures, it is challenging to produce an information enriched panoramic LDR image without visual artifacts for a high dynamic range (HDR) scene through stitching multiple geometrically synchronized LDR images with different exposures and pairwise overlapping fields of views (OFOVs). Fortunately, the stitching of such images is innately a perfect scenario for the fusion of a physics-driven approach and a data-driven approach due to their OFOVs. Based on this new insight, a novel neural augmentation based panoramic HDR stitching algorithm is proposed in this paper. The physics-driven approach is built up using the OFOVs. Different exposed images of each view are initially generated by using the physics-driven approach, are then refined by a data-driven approach, and are finally used to produce panoramic LDR images with different exposures. All the panoramic LDR images with different exposures are combined together via a multi-scale exposure fusion algorithm to produce the final panoramic LDR image. Experimental results demonstrate the proposed algorithm outperforms existing panoramic stitching algorithms.         ",
    "url": "https://arxiv.org/abs/2409.04679",
    "authors": [
      "Chaobing Zheng",
      "Yilun Xu",
      "Weihai Chen",
      "Shiqian Wu",
      "Sen Zhang",
      "Zhengguo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.14783",
    "title": "Disjoint covering of bipartite graphs with $s$-clubs",
    "abstract": "           For a positive integer $s$, an $s$-club in a graph $G$ is a set of vertices inducing a subgraph with diameter at most $s$. As generalizations of cliques, $s$-clubs offer a flexible model for real-world networks. This paper addresses the problems of partitioning and disjoint covering of vertices with $s$-clubs on bipartite graphs. First we prove that for any fixed $s \\geq 6$ and fixed $k \\geq 5$, determining whether the vertices of $G$ can be partitioned into at most $k$ disjoint $s$-clubs is NP-complete even for bipartite graphs. Note that our NP-completeness result is stronger than the one in Abbas and Stewart (1999), as we assume that both $s$ and $k$ are constants and not part of the input. Additionally, we study the Maximum Disjoint $(t,s)$-Club Covering problem (MAX-DCC($t,s$)), which aims to find a collection of vertex-disjoint $(t,s)$-clubs (i.e. $s$-clubs with at least $t$ vertices) that covers the maximum number of vertices in $G$. We prove that it is NP-hard to achieve an approximation factor of $\\frac{95}{94} $ for MAX-DCC($t,3$) for any fixed $t\\geq 8$ and for MAX-DCC($t,2$) for any fixed $t\\geq 5$ even for bipartite graphs. Previously, results were known only for MAX-DCC($3,2$). Finally, we provide a polynomial-time algorithm for MAX-DCC($2,2$).         ",
    "url": "https://arxiv.org/abs/2409.14783",
    "authors": [
      "Angelo Monti",
      "Blerina Sinaimeri"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2409.16467",
    "title": "Learning Dynamics of a Ball with Differentiable Factor Graph and Roto-Translational Invariant Representations",
    "abstract": "           Robots in dynamic environments need fast, accurate models of how objects move in their environments to support agile planning. In sports such as ping pong, analytical models often struggle to accurately predict ball trajectories with spins due to complex aerodynamics, elastic behaviors, and the challenges of modeling sliding and rolling friction. On the other hand, despite the promise of data-driven methods, machine learning struggles to make accurate, consistent predictions without precise input. In this paper, we propose an end-to-end learning framework that can jointly train a dynamics model and a factor graph estimator. Our approach leverages a Gram-Schmidt (GS) process to extract roto-translational invariant representations to improve the model performance, which can further reduce the validation error compared to data augmentation method. Additionally, we propose a network architecture that enhances nonlinearity by using self-multiplicative bypasses in the layer connections. By leveraging these novel methods, our proposed approach predicts the ball's position with an RMSE of 37.2 mm of the paddle radius at the apex after the first bounce, and 71.5 mm after the second bounce.         ",
    "url": "https://arxiv.org/abs/2409.16467",
    "authors": [
      "Qingyu Xiao",
      "Zixuan Wu",
      "Matthew Gombolay"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.18219",
    "title": "Packet Inspection Transformer: A Self-Supervised Journey to Unseen Malware Detection with Few Samples",
    "abstract": "           As networks continue to expand and become more interconnected, the need for novel malware detection methods becomes more pronounced. Traditional security measures are increasingly inadequate against the sophistication of modern cyber attacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network security, offering an in-depth analysis of network traffic that surpasses conventional monitoring techniques. DPI not only examines the metadata of network packets, but also dives into the actual content being carried within the packet payloads, providing a comprehensive view of the data flowing through networks. While the integration of advanced deep learning techniques with DPI has introduced modern methodologies into malware detection and network traffic classification, state-of-the-art supervised learning approaches are limited by their reliance on large amounts of annotated data and their inability to generalize to novel, unseen malware threats. To address these limitations, this paper leverages the recent advancements in self-supervised learning (SSL) and few-shot learning (FSL). Our proposed self-supervised approach trains a transformer via SSL to learn the embedding of packet content, including payload, from vast amounts of unlabeled data by masking portions of packets, leading to a learned representation that generalizes to various downstream tasks. Once the representation is extracted from the packets, they are used to train a malware detection algorithm. The representation obtained from the transformer is then used to adapt the malware detector to novel types of attacks using few-shot learning approaches. Our experimental results demonstrate that our method achieves classification accuracies of up to 94.76% on the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.         ",
    "url": "https://arxiv.org/abs/2409.18219",
    "authors": [
      "Kyle Stein",
      "Arash Mahyari",
      "Guillermo Francia III",
      "Eman El-Sheikh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.19976",
    "title": "Deep Parallel Spectral Neural Operators for Solving Partial Differential Equations with Enhanced Low-Frequency Learning Capability",
    "abstract": "           Designing universal artificial intelligence (AI) solver for partial differential equations (PDEs) is an open-ended problem and a significant challenge in science and engineering. Currently, data-driven solvers have achieved great success, such as neural operators. However, the ability of various neural operator solvers to learn low-frequency information still needs improvement. In this study, we propose a Deep Parallel Spectral Neural Operator (DPNO) to enhance the ability to learn low-frequency information. Our method enhances the neural operator's ability to learn low-frequency information through parallel modules. In addition, due to the presence of truncation coefficients, some high-frequency information is lost during the nonlinear learning process. We smooth this information through convolutional mappings, thereby reducing high-frequency errors. We selected several challenging partial differential equation datasets for experimentation, and DPNO performed exceptionally well. As a neural operator, DPNO also possesses the capability of resolution invariance.         ",
    "url": "https://arxiv.org/abs/2409.19976",
    "authors": [
      "Qinglong Ma",
      "Peizhi Zhao",
      "Sen Wang",
      "Tao Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2410.01504",
    "title": "PersonaMath: Boosting Mathematical Reasoning via Persona-Driven Data Augmentation",
    "abstract": "           While closed-source Large Language Models (LLMs) demonstrate strong mathematical problem-solving abilities, open-source models still face challenges with such tasks. To bridge this gap, we propose a data augmentation approach and introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we train the PersonaMath models. Our approach consists of two stages: the first stage focuses on learning from Persona Diversification, and the second stage emphasizes learning from Reflection. In the first stage, we regenerate detailed chain-of-thought (CoT) solutions as instructions using a closed-source LLM and introduce a persona-driven data augmentation technique. This technique innovatively classifies personas based on occupations, significantly enhancing the dataset's diversity and quality. In the second stage, we incorporate reflection to fully leverage more challenging and valuable questions. Evaluation of our PersonaMath models on MATH and GSM8K reveals that the PersonaMath-7B model (based on Qwen2.5-7B) achieves an accuracy of 61.2% on MATH and 87.8% on GSM8K, surpassing all baseline methods and achieving state-of-the-art performance. Notably, our dataset contains only 128.9K data points-merely 32.6% of MetaMathQA and 49.5% of MathInstruct-yet our model outperforms these baselines, demonstrating the high quality and diversity of our dataset, which enables more efficient model training. We open-source the PersonaMathQA dataset, PersonaMath models, and our code for public usage.         ",
    "url": "https://arxiv.org/abs/2410.01504",
    "authors": [
      "Jing Luo",
      "Longze Chen",
      "Run Luo",
      "Liang Zhu",
      "Chang Ao",
      "Jiaming Li",
      "Yukun Chen",
      "Xin Cheng",
      "Wen Yang",
      "Jiayuan Su",
      "Ahmadreza Argha",
      "Hamid Alinejad-Rokny",
      "Chengming Li",
      "Shiwen Ni",
      "Min Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.02145",
    "title": "Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes",
    "abstract": "           Active learning methods aim to improve sample complexity in machine learning. In this work, we investigate an active learning scheme via a novel gradient-free cutting-plane training method for ReLU networks of arbitrary depth and develop a convergence theory. We demonstrate, for the first time, that cutting-plane algorithms, traditionally used in linear models, can be extended to deep neural networks despite their nonconvexity and nonlinear decision boundaries. Moreover, this training method induces the first deep active learning scheme known to achieve convergence guarantees, revealing a geometric contraction rate of the feasible set. We exemplify the effectiveness of our proposed active learning method against popular deep active learning baselines via both synthetic data experiments and sentimental classification task on real datasets.         ",
    "url": "https://arxiv.org/abs/2410.02145",
    "authors": [
      "Erica Zhang",
      "Fangzhao Zhang",
      "Mert Pilanci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2410.05711",
    "title": "TimeDART: A Diffusion Autoregressive Transformer for Self-Supervised Time Series Representation",
    "abstract": "           Self-supervised learning has garnered increasing attention in time series analysis for benefiting various downstream tasks and reducing reliance on labeled data. Despite its effectiveness, existing methods often struggle to comprehensively capture both long-term dynamic evolution and subtle local patterns in a unified manner. In this work, we propose TimeDART, a novel self-supervised time series pre-training framework that unifies two powerful generative paradigms to learn more transferable representations. Specifically, we first employ a causal Transformer encoder, accompanied by a patch-based embedding strategy, to model the evolving trends from left to right. Building on this global modeling, we further introduce a denoising diffusion process to capture fine-grained local patterns through forward diffusion and reverse denoising. Finally, we optimize the model in an autoregressive manner. As a result, TimeDART effectively accounts for both global and local sequence features in a coherent way. We conduct extensive experiments on public datasets for time series forecasting and classification. The experimental results demonstrate that TimeDART consistently outperforms previous compared methods, validating the effectiveness of our approach. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.05711",
    "authors": [
      "Daoyu Wang",
      "Mingyue Cheng",
      "Zhiding Liu",
      "Qi Liu",
      "Enhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.09453",
    "title": "MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
    "abstract": "           In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we have conducted a comprehensive, quantitative evaluation of various state-of-the-art MLLMs. The commercial models performed the best, with the average accuracy of GPT-4o models reaching 74.9%. However, this result falls far short of industrial requirements. Our analysis reveals that current MLLMs still have significant room for improvement in answering questions related to industrial anomalies and defects. We further explore two training-free performance enhancement strategies to help models improve in industrial scenarios, highlighting their promising potential for future research.         ",
    "url": "https://arxiv.org/abs/2410.09453",
    "authors": [
      "Xi Jiang",
      "Jian Li",
      "Hanqiu Deng",
      "Yong Liu",
      "Bin-Bin Gao",
      "Yifeng Zhou",
      "Jialin Li",
      "Chengjie Wang",
      "Feng Zheng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.12320",
    "title": "A Hierarchical DRL Approach for Resource Optimization in Multi-RIS Multi-Operator Networks",
    "abstract": "           As reconfigurable intelligent surfaces (RIS) emerge as a pivotal technology in the upcoming sixth-generation (6G) networks, their deployment within practical multiple operator (OP) networks presents significant challenges, including the coordination of RIS configurations among OPs, interference management, and privacy maintenance. A promising strategy is to treat RIS as a public resource managed by an RIS provider (RP), which can enhance resource allocation efficiency by allowing dynamic access for multiple OPs. However, the intricate nature of coordinating management and optimizing RIS configurations significantly complicates the implementation process. In this paper, we propose a hierarchical deep reinforcement learning (HDRL) approach that decomposes the complicated RIS resource optimization problem into several subtasks. Specifically, a top-level RP-agent is responsible for RIS allocation, while low-level OP-agents control their assigned RISs and handle beamforming, RIS phase-shifts, and user association. By utilizing the semi-Markov decision process (SMDP) theory, we establish a sophisticated interaction mechanism between the RP and OPs, and introduce an advanced hierarchical proximal policy optimization (HPPO) algorithm. Furthermore, we propose an improved sequential-HPPO (S-HPPO) algorithm to address the curse of dimensionality encountered with a single RP-agent. Experimental results validate the stability of the HPPO algorithm across various environmental parameters, demonstrating its superiority over other benchmarks for joint resource optimization. Finally, we conduct a detailed comparative analysis between the proposed S-HPPO and HPPO algorithms, showcasing that the S-HPPO algorithm achieves faster convergence and improved performance in large-scale RIS allocation scenarios.         ",
    "url": "https://arxiv.org/abs/2410.12320",
    "authors": [
      "Haocheng Zhang",
      "Wei Wang",
      "Hao Zhou",
      "Zhiping Lu",
      "Ming Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2410.23306",
    "title": "Securing Healthcare with Deep Learning: A CNN-Based Model for medical IoT Threat Detection",
    "abstract": "           The increasing integration of the Internet of Medical Things (IoMT) into healthcare systems has significantly enhanced patient care but has also introduced critical cybersecurity challenges. This paper presents a novel approach based on Convolutional Neural Networks (CNNs) for detecting cyberattacks within IoMT environments. Unlike previous studies that predominantly utilized traditional machine learning (ML) models or simpler Deep Neural Networks (DNNs), the proposed model leverages the capabilities of CNNs to effectively analyze the temporal characteristics of network traffic data. Trained and evaluated on the CICIoMT2024 dataset, which comprises 18 distinct types of cyberattacks across a range of IoMT devices, the proposed CNN model demonstrates superior performance compared to previous state-of-the-art methods, achieving a perfect accuracy of 99% in binary, categorical, and multiclass classification tasks. This performance surpasses that of conventional ML models such as Logistic Regression, AdaBoost, DNNs, and Random Forests. These findings highlight the potential of CNNs to substantially improve IoMT cybersecurity, thereby ensuring the protection and integrity of connected healthcare systems.         ",
    "url": "https://arxiv.org/abs/2410.23306",
    "authors": [
      "Alireza Mohamadi",
      "Hosna Ghahramani",
      "Seyyed Amir Asghari",
      "Mehdi Aminian"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.00150",
    "title": "Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State Tracking",
    "abstract": "           Zero-shot domain adaptation for dialogue state tracking (DST) remains a challenging problem in task-oriented dialogue (TOD) systems, where models must generalize to target domains unseen at training time. Current large language model approaches for zero-shot domain adaptation rely on prompting to introduce knowledge pertaining to the target domains. However, their efficacy strongly depends on prompt engineering, as well as the zero-shot ability of the underlying language model. In this work, we devise a novel data augmentation approach, Schema Augmentation, that improves the zero-shot domain adaptation of language models through fine-tuning. Schema Augmentation is a simple but effective technique that enhances generalization by introducing variations of slot names within the schema provided in the prompt. Experiments on MultiWOZ and SpokenWOZ showed that the proposed approach resulted in a substantial improvement over the baseline, in some experiments achieving over a twofold accuracy gain over unseen domains while maintaining equal or superior performance over all domains.         ",
    "url": "https://arxiv.org/abs/2411.00150",
    "authors": [
      "Christopher Richardson",
      "Roshan Sharma",
      "Neeraj Gaur",
      "Parisa Haghani",
      "Anirudh Sundar",
      "Bhuvana Ramabhadran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.08767",
    "title": "SANDWICH: Towards an Offline, Differentiable, Fully-Trainable Wireless Neural Ray-Tracing Surrogate",
    "abstract": "           Wireless ray-tracing (RT) is emerging as a key tool for three-dimensional (3D) wireless channel modeling, driven by advances in graphical rendering. Current approaches struggle to accurately model beyond 5G (B5G) network signaling, which often operates at higher frequencies and is more susceptible to environmental conditions and changes. Existing online learning solutions require real-time environmental supervision during training, which is both costly and incompatible with GPU-based processing. In response, we propose a novel approach that redefines ray trajectory generation as a sequential decision-making problem, leveraging generative models to jointly learn the optical, physical, and signal properties within each designated environment. Our work introduces the Scene-Aware Neural Decision Wireless Channel Raytracing Hierarchy (SANDWICH), an innovative offline, fully differentiable approach that can be trained entirely on GPUs. SANDWICH offers superior performance compared to existing online learning methods, outperforms the baseline by 4e^-2 radian in RT accuracy, and only fades 0.5 dB away from toplined channel gain estimation.         ",
    "url": "https://arxiv.org/abs/2411.08767",
    "authors": [
      "Yifei Jin",
      "Ali Maatouk",
      "Sarunas Girdzijauskas",
      "Shugong Xu",
      "Leandros Tassiulas",
      "Rex Ying"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.00966",
    "title": "From Priest to Doctor: Domain Adaptation for Low-Resource Neural Machine Translation",
    "abstract": "           Many of the world's languages have insufficient data to train high-performing general neural machine translation (NMT) models, let alone domain-specific models, and often the only available parallel data are small amounts of religious texts. Hence, domain adaptation (DA) is a crucial issue faced by contemporary NMT and has, so far, been underexplored for low-resource languages. In this paper, we evaluate a set of methods from both low-resource NMT and DA in a realistic setting, in which we aim to translate between a high-resource and a low-resource language with access to only: a) parallel Bible data, b) a bilingual dictionary, and c) a monolingual target-domain corpus in the high-resource language. Our results show that the effectiveness of the tested methods varies, with the simplest one, DALI, being most effective. We follow up with a small human evaluation of DALI, which shows that there is still a need for more careful investigation of how to accomplish DA for low-resource NMT.         ",
    "url": "https://arxiv.org/abs/2412.00966",
    "authors": [
      "Ali Marashian",
      "Enora Rice",
      "Luke Gessler",
      "Alexis Palmer",
      "Katharina von der Wense"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.07277",
    "title": "Backdoor Attacks against No-Reference Image Quality Assessment Models via a Scalable Trigger",
    "abstract": "           No-Reference Image Quality Assessment (NR-IQA), responsible for assessing the quality of a single input image without using any reference, plays a critical role in evaluating and optimizing computer vision systems, e.g., low-light enhancement. Recent research indicates that NR-IQA models are susceptible to adversarial attacks, which can significantly alter predicted scores with visually imperceptible perturbations. Despite revealing vulnerabilities, these attack methods have limitations, including high computational demands, untargeted manipulation, limited practical utility in white-box scenarios, and reduced effectiveness in black-box scenarios. To address these challenges, we shift our focus to another significant threat and present a novel poisoning-based backdoor attack against NR-IQA (BAIQA), allowing the attacker to manipulate the IQA model's output to any desired target value by simply adjusting a scaling coefficient $\\alpha$ for the trigger. We propose to inject the trigger in the discrete cosine transform (DCT) domain to improve the local invariance of the trigger for countering trigger diminishment in NR-IQA models due to widely adopted data augmentations. Furthermore, the universal adversarial perturbations (UAP) in the DCT space are designed as the trigger, to increase IQA model susceptibility to manipulation and improve attack effectiveness. In addition to the heuristic method for poison-label BAIQA (P-BAIQA), we explore the design of clean-label BAIQA (C-BAIQA), focusing on $\\alpha$ sampling and image data refinement, driven by theoretical insights we reveal. Extensive experiments on diverse datasets and various NR-IQA models demonstrate the effectiveness of our attacks. Code can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.07277",
    "authors": [
      "Yi Yu",
      "Song Xia",
      "Xun Lin",
      "Wenhan Yang",
      "Shijian Lu",
      "Yap-peng Tan",
      "Alex Kot"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2412.08920",
    "title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning",
    "abstract": "           Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.         ",
    "url": "https://arxiv.org/abs/2412.08920",
    "authors": [
      "Pusen Dong",
      "Tianchen Zhu",
      "Yue Qiu",
      "Haoyi Zhou",
      "Jianxin Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.11807",
    "title": "PhysAug: A Physical-guided and Frequency-based Data Augmentation for Single-Domain Generalized Object Detection",
    "abstract": "           Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single source domain for robust performance across a variety of unseen target domains by taking advantage of an object detector. Existing S-DGOD approaches often rely on data augmentation strategies, including a composition of visual transformations, to enhance the detector's generalization ability. However, the absence of real-world prior knowledge hinders data augmentation from contributing to the diversity of training data distributions. To address this issue, we propose PhysAug, a novel physical model-based non-ideal imaging condition data augmentation method, to enhance the adaptability of the S-DGOD tasks. Drawing upon the principles of atmospheric optics, we develop a universal perturbation model that serves as the foundation for our proposed PhysAug. Given that visual perturbations typically arise from the interaction of light with atmospheric particles, the image frequency spectrum is harnessed to simulate real-world variations during training. This approach fosters the detector to learn domain-invariant representations, thereby enhancing its ability to generalize across various settings. Without altering the network architecture or loss function, our approach significantly outperforms the state-of-the-art across various S-DGOD datasets. In particular, it achieves a substantial improvement of $7.3\\%$ and $7.2\\%$ over the baseline on DWD and Cityscape-C, highlighting its enhanced generalizability in real-world settings.         ",
    "url": "https://arxiv.org/abs/2412.11807",
    "authors": [
      "Xiaoran Xu",
      "Jiangang Yang",
      "Wenhui Shi",
      "Siyuan Ding",
      "Luqing Luo",
      "Jian Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.16874",
    "title": "A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information",
    "abstract": "           Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.         ",
    "url": "https://arxiv.org/abs/2412.16874",
    "authors": [
      "M Anuprabha",
      "Krishna Gurugubelli",
      "V Kesavaraj",
      "Anil Kumar Vuppala"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2412.18735",
    "title": "Automatic Self-supervised Learning for Social Recommendations",
    "abstract": "           In recent years, researchers have attempted to exploit social relations to improve the performance in recommendation systems. Generally, most existing social recommendation methods heavily depends on substantial domain knowledge and expertise in primary recommendation tasks for designing useful auxiliary tasks. Meanwhile, Self-Supervised Learning (SSL) recently has received considerable attention in the field of recommendation, since it can provide self-supervision signals in assisting the improvement of target recommendation systems by constructing self-supervised auxiliary tasks from raw data without human-annotated labels. Despite the great success, these SSL-based social recommendations are insufficient to adaptively balance various self-supervised auxiliary tasks, since assigning equal weights on various auxiliary tasks can result in sub-optimal recommendation performance, where different self-supervised auxiliary tasks may contribute differently to improving the primary social recommendation across different datasets. To address this issue, in this work, we propose Adaptive Self-supervised Learning for Social Recommendations (AdasRec) by taking advantage of various self-supervised auxiliary tasks. More specifically, an adaptive weighting mechanism is proposed to learn adaptive weights for various self-supervised auxiliary tasks, so as to balance the contribution of such self-supervised auxiliary tasks for enhancing representation learning in social recommendations. The adaptive weighting mechanism is used to assign different weights on auxiliary tasks to achieve an overall weighting of the entire auxiliary tasks and ultimately assist the primary recommendation task, achieved by a meta learning optimization problem with an adaptive weighting network. Comprehensive experiments on various real-world datasets are constructed to verify the effectiveness of our proposed method.         ",
    "url": "https://arxiv.org/abs/2412.18735",
    "authors": [
      "Xin He",
      "Wenqi Fan",
      "Mingchen Sun",
      "Ying Wang",
      "Xin Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.19229",
    "title": "Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph Learning",
    "abstract": "           Federated Graph Learning (FGL) enables multiple clients to jointly train powerful graph learning models, e.g., Graph Neural Networks (GNNs), without sharing their local graph data for graph-related downstream tasks, such as graph property prediction. In the real world, however, the graph data can suffer from significant distribution shifts across clients as the clients may collect their graph data for different purposes. In particular, graph properties are usually associated with invariant label-relevant substructures (i.e., subgraphs) across clients, while label-irrelevant substructures can appear in a client-specific manner. The issue of distribution shifts of graph data hinders the efficiency of GNN training and leads to serious performance degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL framework entitled FedVN that eliminates distribution shifts through client-specific graph augmentation strategies with multiple learnable Virtual Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared VNs while training a global GNN model. To eliminate distribution shifts, each client trains a personalized edge generator that determines how the VNs connect local graphs in a client-specific manner. Furthermore, we provide theoretical analyses indicating that FedVN can eliminate distribution shifts of graph data across clients. Comprehensive experiments on four datasets under five settings demonstrate the superiority of our proposed FedVN over nine baselines.         ",
    "url": "https://arxiv.org/abs/2412.19229",
    "authors": [
      "Xingbo Fu",
      "Zihan Chen",
      "Yinhan He",
      "Song Wang",
      "Binchi Zhang",
      "Chen Chen",
      "Jundong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2501.06300",
    "title": "Tensorization of neural networks for improved privacy and interpretability",
    "abstract": "           We present a tensorization algorithm for constructing tensor train representations of functions, drawing on sketching and cross interpolation ideas. The method only requires black-box access to the target function and a small set of sample points defining the domain of interest. Thus, it is particularly well-suited for machine learning models, where the domain of interest is naturally defined by the training dataset. We show that this approach can be used to enhance the privacy and interpretability of neural network models. Specifically, we apply our decomposition to (i) obfuscate neural networks whose parameters encode patterns tied to the training data distribution, and (ii) estimate topological phases of matter that are easily accessible from the tensor train representation. Additionally, we show that this tensorization can serve as an efficient initialization method for optimizing tensor trains in general settings, and that, for model compression, our algorithm achieves a superior trade-off between memory and time complexity compared to conventional tensorization methods of neural networks.         ",
    "url": "https://arxiv.org/abs/2501.06300",
    "authors": [
      "Jos\u00e9 Ram\u00f3n Pareja Monturiol",
      "Alejandro Pozas-Kerstjens",
      "David P\u00e9rez-Garc\u00eda"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2501.18783",
    "title": "RUN: Reversible Unfolding Network for Concealed Object Segmentation",
    "abstract": "           Existing concealed object segmentation (COS) methods frequently utilize reversible strategies to address uncertain regions. However, these approaches are typically restricted to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose the Reversible Unfolding Network (RUN), which applies reversible strategies across both mask and RGB domains through a theoretically grounded framework, enabling accurate segmentation. RUN first formulates a novel COS model by incorporating an extra residual sparsity constraint to minimize segmentation uncertainties. The iterative optimization steps of the proposed model are then unfolded into a multistage network, with each step corresponding to a stage. Each stage of RUN consists of two reversible modules: the Segmentation-Oriented Foreground Separation (SOFS) module and the Reconstruction-Oriented Background Extraction (ROBE) module. SOFS applies the reversible strategy at the mask level and introduces Reversible State Space to capture non-local information. ROBE extends this to the RGB domain, employing a reconstruction network to address conflicting foreground and background regions identified as distortion-prone areas, which arise from their separate estimation by independent modules. As the stages progress, RUN gradually facilitates reversible modeling of foreground and background in both the mask and RGB domains, directing the network's attention to uncertain regions and mitigating false-positive and false-negative results. Extensive experiments demonstrate the superior performance of RUN and highlight the potential of unfolding-based frameworks for COS and other high-level vision tasks. We will release the code and models.         ",
    "url": "https://arxiv.org/abs/2501.18783",
    "authors": [
      "Chunming He",
      "Rihan Zhang",
      "Fengyang Xiao",
      "Chengyu Fang",
      "Longxiang Tang",
      "Yulun Zhang",
      "Linghe Kong",
      "Deng-Ping Fan",
      "Kai Li",
      "Sina Farsiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.01946",
    "title": "HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for Multi-session Radar SLAM",
    "abstract": "           Recently, radars have been widely featured in robotics for their robustness in challenging weather conditions. Two commonly used radar types are spinning radars and phased-array radars, each offering distinct sensor characteristics. Existing datasets typically feature only a single type of radar, leading to the development of algorithms limited to that specific kind. In this work, we highlight that combining different radar types offers complementary advantages, which can be leveraged through a heterogeneous radar dataset. Moreover, this new dataset fosters research in multi-session and multi-robot scenarios where robots are equipped with different types of radars. In this context, we introduce the HeRCULES dataset, a comprehensive, multi-modal dataset with heterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first dataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering unparalleled localization, mapping, and place recognition capabilities. The dataset covers diverse weather and lighting conditions and a range of urban traffic scenarios, enabling a comprehensive analysis across various environments. The sequence paths with multiple revisits and ground truth pose for each sensor enhance its suitability for place recognition research. We expect the HeRCULES dataset to facilitate odometry, mapping, place recognition, and sensor fusion research. The dataset and development tools are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.01946",
    "authors": [
      "Hanjun Kim",
      "Minwoo Jung",
      "Chiyun Noh",
      "Sangwoo Jung",
      "Hyunho Song",
      "Wooseong Yang",
      "Hyesu Jang",
      "Ayoung Kim"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.03069",
    "title": "CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality",
    "abstract": "           Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI's contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.         ",
    "url": "https://arxiv.org/abs/2502.03069",
    "authors": [
      "Julian Rasch",
      "Julia T\u00f6ws",
      "Teresa Hirzle",
      "Florian M\u00fcller",
      "Martin Schmitz"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.04818",
    "title": "Harnessing omnipresent oscillator networks as computational resource",
    "abstract": "           Nature is pervaded with oscillatory behavior. In networks of coupled oscillators patterns can arise when the system synchronizes to an external input. Hence, these networks provide processing and memory of input. We present a universal framework for harnessing oscillator networks as computational resource. This reservoir computing framework is introduced by the ubiquitous model for phase-locking, the Kuramoto model. We force the Kuramoto model by a nonlinear target-system, then after substituting the target-system with a trained feedback-loop it emulates the target-system. Our results are two-fold. Firstly, the trained network inherits performance properties of the Kuramoto model, where all-to-all coupling is performed in linear time with respect to the number of nodes and parameters for synchronization are abundant. Secondly, the learning capabilities of the oscillator network can be explained using Kuramoto model's order parameter. This work provides the foundation for utilizing nature's oscillator networks as a new class of information processing systems.         ",
    "url": "https://arxiv.org/abs/2502.04818",
    "authors": [
      "Thomas Geert de Jong",
      "Hirofumi Notsu",
      "Kohei Nakajima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Chaotic Dynamics (nlin.CD)"
    ]
  },
  {
    "id": "arXiv:2502.07780",
    "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models",
    "abstract": "           Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for \\emph{non-uniform} model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \\sysname, a method for \\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \\sysname surpasses ShearedLlama while requiring $5\\times$ less training data during post-compression training. Code is at: this https URL ",
    "url": "https://arxiv.org/abs/2502.07780",
    "authors": [
      "Shengkun Tang",
      "Oliver Sieberling",
      "Eldar Kurtic",
      "Zhiqiang Shen",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.09838",
    "title": "HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation",
    "abstract": "           We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.09838",
    "authors": [
      "Tianwei Lin",
      "Wenqiao Zhang",
      "Sijing Li",
      "Yuqian Yuan",
      "Binhe Yu",
      "Haoyuan Li",
      "Wanggui He",
      "Hao Jiang",
      "Mengze Li",
      "Xiaohui Song",
      "Siliang Tang",
      "Jun Xiao",
      "Hui Lin",
      "Yueting Zhuang",
      "Beng Chin Ooi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.10648",
    "title": "LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization",
    "abstract": "           We introduce LLM-Lasso, a novel framework that leverages large language models (LLMs) to guide feature selection in Lasso $\\ell_1$ regression. Unlike traditional methods that rely solely on numerical data, LLM-Lasso incorporates domain-specific knowledge extracted from natural language, enhanced through a retrieval-augmented generation (RAG) pipeline, to seamlessly integrate data-driven modeling with contextual insights. Specifically, the LLM generates penalty factors for each feature, which are converted into weights for the Lasso penalty using a simple, tunable model. Features identified as more relevant by the LLM receive lower penalties, increasing their likelihood of being retained in the final model, while less relevant features are assigned higher penalties, reducing their influence. Importantly, LLM-Lasso has an internal validation step that determines how much to trust the contextual knowledge in our prediction pipeline. Hence it addresses key challenges in robustness, making it suitable for mitigating potential inaccuracies or hallucinations from the LLM. In various biomedical case studies, LLM-Lasso outperforms standard Lasso and existing feature selection baselines, all while ensuring the LLM operates without prior access to the datasets. To our knowledge, this is the first approach to effectively integrate conventional feature selection techniques directly with LLM-based domain-specific reasoning.         ",
    "url": "https://arxiv.org/abs/2502.10648",
    "authors": [
      "Erica Zhang",
      "Ryunosuke Goto",
      "Naomi Sagan",
      "Jurik Mutter",
      "Nick Phillips",
      "Ash Alizadeh",
      "Kangwook Lee",
      "Jose Blanchet",
      "Mert Pilanci",
      "Robert Tibshirani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.10736",
    "title": "SpeechCap: Leveraging Playful Impact Captions to Facilitate Interpersonal Communication in Social Virtual Reality",
    "abstract": "           Social Virtual Reality (VR) emerges as a promising platform bringing immersive, interactive, and engaging mechanisms for collaborative activities in virtual spaces. However, interpersonal communication in social VR is still limited with existing mediums and channels. To bridge the gap, we propose a novel method for mediating real-time conversation in social VR, which uses impact captions, a type of typographic visual effect widely used in videos, to convey both verbal and non-verbal information. We first investigated the design space of impact captions by content analysis and a co-design session with four experts. Next, we implemented SpeechCap as a proof-of-concept system, with which users can communicate with each other using speech-driven impact captions in VR. Through a user study (n=14), we evaluated the effectiveness of the visual and interaction design of impact captions, highlighting the interactivity and the integration of verbal and non-verbal information in communication mediums. Finally, we discussed topics of visual rhetoric, interactivity, and ambiguity as the main findings from the study, and further provided design implications for future work for facilitating interpersonal communication in social VR.         ",
    "url": "https://arxiv.org/abs/2502.10736",
    "authors": [
      "Yu Zhang",
      "Yi Wen",
      "Siying Hu",
      "Zhicong Lu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.10921",
    "title": "Evolving Hate Speech Online: An Adaptive Framework for Detection and Mitigation",
    "abstract": "           The proliferation of social media platforms has led to an increase in the spread of hate speech, particularly targeting vulnerable communities. Unfortunately, existing methods for automatically identifying and blocking toxic language rely on pre-constructed lexicons, making them reactive rather than adaptive. As such, these approaches become less effective over time, especially when new communities are targeted with slurs not included in the original datasets. To address this issue, we present an adaptive approach that uses word embeddings to update lexicons and develop a hybrid model that adjusts to emerging slurs and new linguistic patterns. This approach can effectively detect toxic language, including intentional spelling mistakes employed by aggressors to avoid detection. Our hybrid model, which combines BERT with lexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art datasets. Our work has significant implications for creating safer online environments by improving the detection of toxic content and proactively updating the lexicon. Content Warning: This paper contains examples of hate speech that may be triggering.         ",
    "url": "https://arxiv.org/abs/2502.10921",
    "authors": [
      "Shiza Ali",
      "Jeremy Blackburn",
      "Gianluca Stringhini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.11910",
    "title": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives",
    "abstract": "           Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.         ",
    "url": "https://arxiv.org/abs/2502.11910",
    "authors": [
      "Leo Schwinn",
      "Yan Scholten",
      "Tom Wollschl\u00e4ger",
      "Sophie Xhonneux",
      "Stephen Casper",
      "Stephan G\u00fcnnemann",
      "Gauthier Gidel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13451",
    "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation",
    "abstract": "           Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.         ",
    "url": "https://arxiv.org/abs/2502.13451",
    "authors": [
      "Lingfeng Zhang",
      "Xiaoshuai Hao",
      "Qinwen Xu",
      "Qiang Zhang",
      "Xinyao Zhang",
      "Pengwei Wang",
      "Jing Zhang",
      "Zhongyuan Wang",
      "Shanghang Zhang",
      "Renjing Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.13506",
    "title": "Reproducing NevIR: Negation in Neural Information Retrieval",
    "abstract": "           Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category - listwise Large Language Model (LLM) rerankers - outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalizability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM rerankers achieve reasonable performance across both negation tasks.         ",
    "url": "https://arxiv.org/abs/2502.13506",
    "authors": [
      "Coen van den Elsen",
      "Francien Barkhof",
      "Thijmen Nijdam",
      "Simon Lupart",
      "Mohammad Alliannejadi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.14107",
    "title": "Prediction of the Received Power of Low-Power Networks Using Inertial Sensors",
    "abstract": "           Low-power and cost-effective IoT sensing nodes enable scalable monitoring of different environments. Some of these environments impose rough and extreme operating conditions, requiring continuous adaptation and reconfiguration of physical and link layer parameters. In this paper, we closely investigate the stability of the wireless links established between nodes deployed on the surface of different water bodies and propose a model to predict the received power. Our model is based on Minimum Mean Square Estimation (MMSE) and relies on the statistics of received power and the motion the nodes experience during communication. One of the drawbacks of MMSE is its reliance on matrix inversion, which is at once computationally expensive and difficult to implement with resource constrained devices. We forgo this stage by estimating model parameters using the gradient-descent approach, which is much simpler to implement. The model achieves a prediction accuracy of 91% even with a small number of iterations.         ",
    "url": "https://arxiv.org/abs/2502.14107",
    "authors": [
      "Waltenegus Dargie",
      "Christian Poellabauer",
      "Abiy Tasissa"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2502.14282",
    "title": "PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC",
    "abstract": "           In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.14282",
    "authors": [
      "Haowei Liu",
      "Xi Zhang",
      "Haiyang Xu",
      "Yuyang Wanyan",
      "Junyang Wang",
      "Ming Yan",
      "Ji Zhang",
      "Chunfeng Yuan",
      "Changsheng Xu",
      "Weiming Hu",
      "Fei Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.14611",
    "title": "Enumerating minimal dominating sets and variants in chordal bipartite graphs",
    "abstract": "           Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem. To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\u00e9, Kratsch, Saether, and Villanger running in incremental-polynomial time. We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs. Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class. This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant. Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs.         ",
    "url": "https://arxiv.org/abs/2502.14611",
    "authors": [
      "Emanuel Castelo",
      "Oscar Defrain",
      "Guilherme C. M. Gomes"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2502.14704",
    "title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting",
    "abstract": "           Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning.         ",
    "url": "https://arxiv.org/abs/2502.14704",
    "authors": [
      "Yuxuan Yang",
      "Dalin Zhang",
      "Yuxuan Liang",
      "Hua Lu",
      "Gang Chen",
      "Huan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14744",
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "abstract": "           The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.14744",
    "authors": [
      "Yilei Jiang",
      "Xinyan Gao",
      "Tianshuo Peng",
      "Yingshui Tan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Xiangyu Yue"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.14790",
    "title": "An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces",
    "abstract": "           We develop an analysis of Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\\mathcal{O}(\\beta\\sqrt{T\\log(1+\\lambda)})$ rate against a $\\beta$-bounded $\\lambda$-Lipschitz adversary.         ",
    "url": "https://arxiv.org/abs/2502.14790",
    "authors": [
      "Alexander Terenin",
      "Jeffrey Negrea"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.14735",
    "title": "Highly engaging events reveal semantic and temporal compression in online community discourse",
    "abstract": "           People nowadays express their opinions in online spaces, using different forms of interactions such as posting, sharing and discussing with one another. How do these digital traces change in response to events happening in the real world? We leverage Reddit conversation data, exploiting its community-based structure, to elucidate how offline events influence online user interactions and behavior. Online conversations, as posts and comments, are analysed along their temporal and semantic dimensions. Conversations tend to become repetitive with a more limited vocabulary, develop at a faster pace, and feature heightened emotions. As the event approaches, the shifts occurring in conversations are reflected in the users' dynamics. Users become more active and they exchange information with a growing audience, despite using a less rich vocabulary and repetitive messages. The recurring patterns we discovered are persistent across a wide range of events and several contexts, representing a fingerprint of how online dynamics change in response to real-world occurrences.         ",
    "url": "https://arxiv.org/abs/2306.14735",
    "authors": [
      "Antonio Desiderio",
      "Anna Mancini",
      "Giulio Cimini",
      "Riccardo Di Clemente"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2312.17506",
    "title": "A graph neural network-based model with Out-of-Distribution Robustness for enhancing Antiretroviral Therapy Outcome Prediction for HIV-1",
    "abstract": "           Predicting the outcome of antiretroviral therapies (ART) for HIV-1 is a pressing clinical challenge, especially when the ART includes drugs with limited effectiveness data. This scarcity of data can arise either due to the introduction of a new drug to the market or due to limited use in clinical settings, resulting in clinical dataset with highly unbalanced therapy representation. To tackle this issue, we introduce a novel joint fusion model, which combines features from a Fully Connected (FC) Neural Network and a Graph Neural Network (GNN) in a multi-modality fashion. Our model uses both tabular data about genetic sequences and a knowledge base derived from Stanford drug-resistance mutation tables, which serve as benchmark references for deducing in-vivo treatment efficacy based on the viral genetic sequence. By leveraging this knowledge base structured as a graph, the GNN component enables our model to adapt to imbalanced data distributions and account for Out-of-Distribution (OoD) drugs. We evaluated these models' robustness against OoD drugs in the test set. Our comprehensive analysis demonstrates that the proposed model consistently outperforms the FC model. These results underscore the advantage of integrating Stanford scores in the model, thereby enhancing its generalizability and robustness, but also extending its utility in contributing in more informed clinical decisions with limited data availability. The source code is available at this https URL ",
    "url": "https://arxiv.org/abs/2312.17506",
    "authors": [
      "Giulia Di Teodoro",
      "Federico Siciliano",
      "Valerio Guarrasi",
      "Anne-Mieke Vandamme",
      "Valeria Ghisetti",
      "Anders S\u00f6nnerborg",
      "Maurizio Zazzi",
      "Fabrizio Silvestri",
      "Laura Palagi"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.05887",
    "title": "Sandwiched Compression: Repurposing Standard Codecs with Neural Network Wrappers",
    "abstract": "           We propose sandwiching standard image and video codecs between pre- and post-processing neural networks. The networks are jointly trained through a differentiable codec proxy to minimize a given rate-distortion loss. This sandwich architecture not only improves the standard codec's performance on its intended content, but more importantly, adapts the codec to other types of image/video content and to other distortion measures. The sandwich learns to transmit ``neural code images'' that optimize and improve overall rate-distortion performance, with the improvements becoming significant especially when the overall problem is well outside of the scope of the codec's design. We apply the sandwich architecture to standard codecs with mismatched sources transporting different numbers of channels, higher resolution, higher dynamic range, computer graphics, and with perceptual distortion measures. The results demonstrate substantial improvements (up to 9 dB gains or up to 30\\% bitrate reductions) compared to alternative adaptations. We establish optimality properties for sandwiched compression and design differentiable codec proxies approximating current standard codecs. We further analyze model complexity, visual quality under perceptual metrics, as well as sandwich configurations that offer interesting potentials in video compression and streaming.         ",
    "url": "https://arxiv.org/abs/2402.05887",
    "authors": [
      "Onur G. Guleryuz",
      "Philip A. Chou",
      "Berivan Isik",
      "Hugues Hoppe",
      "Danhang Tang",
      "Ruofei Du",
      "Jonathan Taylor",
      "Philip Davidson",
      "Sean Fanello"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2403.05360",
    "title": "Path eccentricity of $k$-AT-free graphs and application on graphs with the consecutive ones property",
    "abstract": "           The central path problem is a variation on the single facility location problem. The aim is to find, in a given connected graph $G$, a path $P$ minimizing its eccentricity, which is the maximal distance from $P$ to any vertex of the graph $G$. The path eccentricity of $G$ is the minimal eccentricity achievable over all paths in $G$. In this article we consider the path eccentricity of the class of the $k$-AT-free graphs. They are graphs in which any set of three vertices contains a pair for which every path between them uses at least one vertex of the closed neighborhood at distance $k$ of the third. We prove that they have path eccentricity bounded by $k$. Moreover, we answer a question of G\u00f3mez and Guti\u00e9rrez asking if there is a relation between path eccentricity and the consecutive ones property. The latter is the property for a binary matrix to admit a permutation of the rows placing the 1's consecutively on the columns. It was already known that graphs whose adjacency matrices have the consecutive ones property have path eccentricity at most 1, and that the same remains true when the augmented adjacency matrices (with ones on the diagonal) has the consecutive ones property. We generalize these results as follow. We study graphs whose adjacency matrices can be made to satisfy the consecutive ones property after changing some values on the diagonal, and show that those graphs have path eccentricity at most 2, by showing that they are 2-AT-free.         ",
    "url": "https://arxiv.org/abs/2403.05360",
    "authors": [
      "Paul Bastide",
      "Claire Hilaire",
      "Eileen Robinson"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2501.13514",
    "title": "Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement",
    "abstract": "           Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a ``microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2501.13514",
    "authors": [
      "Chenxu Wu",
      "Qingpeng Kong",
      "Zihang Jiang",
      "S. Kevin Zhou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.07891",
    "title": "The Observational Partial Order of Causal Structures with Latent Variables",
    "abstract": "           For two causal structures with the same set of visible variables, one is said to observationally dominate the other if the set of distributions over the visible variables realizable by the first contains the set of distributions over the visible variables realizable by the second. Knowing such dominance relations is useful for adjudicating between these structures given observational data. We here consider the problem of determining the partial order of equivalence classes of causal structures with latent variables relative to observational dominance. We provide a complete characterization of the dominance order in the case of three visible variables, and a partial characterization in the case of four visible variables. Our techniques also help to identify which observational equivalence classes have a set of realizable distributions that is characterized by nontrivial inequality constraints, analogous to Bell inequalities and instrumental inequalities. We find evidence that as one increases the number of visible variables, the equivalence classes satisfying nontrivial inequality constraints become ubiquitous. (Because such classes are the ones for which there can be a difference in the distributions that are quantumly and classically realizable, this implies that the potential for quantum-classical gaps is also ubiquitous.) Furthermore, we find evidence that constraint-based causal discovery algorithms that rely solely on conditional independence constraints have a significantly weaker distinguishing power among observational equivalence classes than algorithms that go beyond these (i.e., algorithms that also leverage nested Markov constraints and inequality constraints).         ",
    "url": "https://arxiv.org/abs/2502.07891",
    "authors": [
      "Marina Maciel Ansanelli",
      "Elie Wolfe",
      "Robert W. Spekkens"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2502.14738",
    "title": "Robust Information Selection for Hypothesis Testing with Misclassification Penalties",
    "abstract": "           We study the problem of robust information selection for a Bayesian hypothesis testing / classification task, where the goal is to identify the true state of the world from a finite set of hypotheses based on observations from the selected information sources. We introduce a novel misclassification penalty framework, which enables non-uniform treatment of different misclassification events. Extending the classical subset selection framework, we study the problem of selecting a subset of sources that minimize the maximum penalty of misclassification under a limited budget, despite deletions or failures of a subset of the selected sources. We characterize the curvature properties of the objective function and propose an efficient greedy algorithm with performance guarantees. Next, we highlight certain limitations of optimizing for the maximum penalty metric and propose a submodular surrogate metric to guide the selection of the information set. We propose a greedy algorithm with near-optimality guarantees for optimizing the surrogate metric. Finally, we empirically demonstrate the performance of our proposed algorithms in several instances of the information set selection problem.         ",
    "url": "https://arxiv.org/abs/2502.14738",
    "authors": [
      "Jayanth Bhargav",
      "Shreyas Sundaram",
      "Mahsa Ghasemi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Combinatorics (math.CO)",
      "Optimization and Control (math.OC)"
    ]
  }
]