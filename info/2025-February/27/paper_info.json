[
  {
    "id": "arXiv:2502.18466",
    "title": "MLScent A tool for Anti-pattern detection in ML projects",
    "abstract": "           Machine learning (ML) codebases face unprecedented challenges in maintaining code quality and sustainability as their complexity grows exponentially. While traditional code smell detection tools exist, they fail to address ML-specific issues that can significantly impact model performance, reproducibility, and maintainability. This paper introduces MLScent, a novel static analysis tool that leverages sophisticated Abstract Syntax Tree (AST) analysis to detect anti-patterns and code smells specific to ML projects. MLScent implements 76 distinct detectors across major ML frameworks including TensorFlow (13 detectors), PyTorch (12 detectors), Scikit-learn (9 detectors), and Hugging Face (10 detectors), along with data science libraries like Pandas and NumPy (8 detectors each). The tool's architecture also integrates general ML smell detection (16 detectors), and specialized analysis for data preprocessing and model training workflows. Our evaluation demonstrates MLScent's effectiveness through both quantitative classification metrics and qualitative assessment via user studies feedback with ML practitioners. Results show high accuracy in identifying framework-specific anti-patterns, data handling issues, and general ML code smells across real-world projects.         ",
    "url": "https://arxiv.org/abs/2502.18466",
    "authors": [
      "Karthik Shivashankar",
      "Antonio Martini"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18467",
    "title": "ChatGPT vs. DeepSeek: A Comparative Study on AI-Based Code Generation",
    "abstract": "           Background: AI-powered code generation, fueled by Large Language Models (LLMs), is revolutionizing software development. Models like OpenAI's Codex and GPT-4, alongside DeepSeek, leverage vast code and natural language datasets. However, ensuring code quality, correctness, and managing complex tasks remains challenging, necessitating thorough evaluation. Methodology: This research compares ChatGPT (version o1) and DeepSeek (version R1) for Python code generation using online judge coding challenges. It evaluates correctness (online judge verdicts, up to three attempts), code quality (Pylint/Flake8), and efficiency (execution time/memory usage). Results: DeepSeek demonstrated higher correctness, particularly on algorithmic tasks, often achieving 'Accepted' on the first attempt. ChatGPT sometimes requires multiple attempts or failures. ChatGPT encountered fewer issues, used comparable or slightly less memory, consumed less execution times and wrote fewer lines of code. Conclusion: DeepSeek exhibited superior correctness in Python code generation, often requiring fewer attempts, suggesting an advantage in algorithmic problem-solving. Both models showed almost similar efficiency in execution time and memory use. Finally, this research provides insights for developers choosing AI coding assistants and informs future AI-driven software development research.         ",
    "url": "https://arxiv.org/abs/2502.18467",
    "authors": [
      "Md Motaleb Hossen Manik"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18484",
    "title": "AI Enhanced Ontology Driven NLP for Intelligent Cloud Resource Query Processing Using Knowledge Graphs",
    "abstract": "           The conventional resource search in cloud infrastructure relies on keyword-based searches or GUIDs, which demand exact matches and significant user effort to locate resources. These conventional search approaches often fail to interpret the intent behind natural language queries, making resource discovery inefficient and inaccessible to users. Though there exists some form of NLP based search engines, they are limited and focused more on analyzing the NLP query itself and extracting identifiers to find the resources. But they fail to search resources based on their behavior or operations or their capabilities or relationships or features or business relevance or the dynamic changing state or the knowledge these resources have. The search criteria has been changing with the inundation of AI based services which involved discovering not just the requested resources and identifiers but seeking insights. The real intent of a search has never been to just to list the resources but with some actual context such as to understand causes of some behavior in the system, compliance checks, capacity estimations, network constraints, or troubleshooting or business insights. This paper proposes an advanced Natural Language Processing (NLP) enhanced by ontology-based semantics to enable intuitive, human-readable queries which allows users to actually discover the intent-of-search itself. By constructing an ontology of cloud resources, their interactions, and behaviors, the proposed framework enables dynamic intent extraction and relevance ranking using Latent Semantic Indexing (LSI) and AI models. It introduces an automated pipeline which integrates ontology extraction by AI powered data crawlers, building a semantic knowledge base for context aware resource discovery.         ",
    "url": "https://arxiv.org/abs/2502.18484",
    "authors": [
      "Krishna Chaitanya Sunkara",
      "Krishnaiah Narukulla"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18487",
    "title": "AuPair: Golden Example Pairs for Code Repair",
    "abstract": "           Scaling up inference-time compute has proven to be a valuable strategy in improving the performance of Large Language Models (LLMs) without fine-tuning. An important task that can benefit from additional inference-time compute is self-repair; given an initial flawed response, or guess, the LLM corrects its own mistake and produces an improved response, or fix. We leverage the in-context learning ability of LLMs to perform self-repair in the coding domain. The key contribution of our paper is an approach that synthesises and selects an ordered set of golden example pairs, or AuPairs, of these initial guesses and subsequent fixes for the corresponding problems. Each such AuPair is provided as a single in-context example at inference time to generate a repaired solution. For an inference-time compute budget of $N$ LLM calls per problem, $N$ AuPairs are used to generate $N$ repaired solutions, out of which the highest-scoring solution is selected as the final answer. The underlying intuition is that if the LLM is given a different example of fixing an incorrect guess each time, it can subsequently generate a diverse set of repaired solutions. Our algorithm selects these AuPairs in a manner that maximises complementarity and usefulness. We demonstrate the results of our algorithm on 5 LLMs across 7 competitive programming datasets for the code repair task. Our algorithm yields a significant boost in performance compared to best-of-$N$ and self-repair, and also exhibits strong generalisation across datasets and models. Moreover, our approach shows significantly stronger scaling with inference-time compute budget compared to baselines.         ",
    "url": "https://arxiv.org/abs/2502.18487",
    "authors": [
      "Aditi Mavalankar",
      "Hassan Mansoor",
      "Zita Marinho",
      "Masha Samsikova",
      "Tom Schaul"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18489",
    "title": "LLM4EFFI: Leveraging Large Language Models to Enhance Code Efficiency and Correctness",
    "abstract": "           Large Language Models (LLMs), particularly Code LLMs, have demonstrated impressive performance in code generation. Current research primarily focuses on the correctness of generated code, while efficiency remains less explored. Recent works have focused on modifying the initial version of the code to improve its efficiency. However, such refinements are limited by the algorithmic design and overall logic of the initial code, resulting in only incremental improvements. In contrast, when human developers write high-quality code, they typically begin by designing several potential solutions at the logical level, evaluating various algorithms and their complexities, and then proceeding to implement and optimize the solution. In this study, we introduce \\tool: \\uline{L}arge \\uline{L}anguage \\uline{M}odel for Code \\uline{Effi}ciency, a novel framework that enables LLMs to generate code that balances both efficiency and correctness. Specifically, \\tool divides the efficiency optimization process into two domains: algorithmic exploration in the logic domain and implementation optimization in the code domain. The correctness of the code is then guaranteed through a synthetic test case refinement process. This approach, which prioritizes efficiency before ensuring correctness, offers a new paradigm for efficient code generation. Experiments demonstrate that \\tool consistently improves both efficiency and correctness, achieving new state-of-the-art performance in code efficiency benchmarks across various LLM backbones.         ",
    "url": "https://arxiv.org/abs/2502.18489",
    "authors": [
      "Tong Ye",
      "Weigang Huang",
      "Xuhong Zhang",
      "Tengfei Ma",
      "Peiyu Liu",
      "Jianwei Yin",
      "Wenhai Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18493",
    "title": "Rule-based autocorrection of Piping and Instrumentation Diagrams (P&IDs) on graphs",
    "abstract": "           A piping and instrumentation diagram (P&ID) is a central reference document in chemical process engineering. Currently, chemical engineers manually review P&IDs through visual inspection to find and rectify errors. However, engineering projects can involve hundreds to thousands of P&ID pages, creating a significant revision workload. This study proposes a rule-based method to support engineers with error detection and correction in P&IDs. The method is based on a graph representation of P&IDs, enabling automated error detection and correction, i.e., autocorrection, through rule graphs. We use our pyDEXPI Python package to generate P&ID graphs from DEXPI-standard P&IDs. In this study, we developed 33 rules based on chemical engineering knowledge and heuristics, with five selected rules demonstrated as examples. A case study on an illustrative P&ID validates the reliability and effectiveness of the rule-based autocorrection method in revising P&IDs.         ",
    "url": "https://arxiv.org/abs/2502.18493",
    "authors": [
      "Lukas Schulze Balhorn",
      "Niels Seijsener",
      "Kevin Dao",
      "Minji Kim",
      "Dominik P. Goldstein",
      "Ge H. M. Driessen",
      "Artur M. Schweidtmann"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18497",
    "title": "A Parallel Hierarchical Approach for Community Detection on Large-scale Dynamic Networks",
    "abstract": "           In this paper, we propose a novel parallel hierarchical Leiden-based algorithm for dynamic community detection. The algorithm, for a given batch update of edge insertions and deletions, partitions the network into communities using only a local neighborhood of the affected nodes. It also uses the inner hierarchical graph-based structure, which is updated incrementally in the process of optimizing the modularity of the partitioning. The algorithm has been extensively tested on various networks. The results demonstrate promising improvements in performance and scalability while maintaining the modularity of the partitioning.         ",
    "url": "https://arxiv.org/abs/2502.18497",
    "authors": [
      "Grigoriy Bokov",
      "Aleksandr Konovalov",
      "Anna Uporova",
      "Stanislav Moiseev",
      "Ivan Safonov",
      "Alexander Radionov"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2502.18499",
    "title": "Mechanistic Understanding of Language Models in Syntactic Code Completion",
    "abstract": "           Recently, language models (LMs) have shown impressive proficiency in code generation tasks, especially when fine-tuned on code-specific datasets, commonly known as Code LMs. However, our understanding of the internal decision-making processes of Code LMs, such as how they use their (syntactic or semantic) knowledge, remains limited, which could lead to unintended harm as they are increasingly used in real life. This motivates us to conduct one of the first Mechanistic Interpretability works to understand how Code LMs perform a syntactic completion task, specifically the closing parenthesis task, on the CodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model requires middle-later layers until it can confidently predict the correct label for the closing parenthesis task. Additionally, we identify that while both multi-head attention (MHA) and feed-forward (FF) sub-layers play essential roles, MHA is particularly crucial. Furthermore, we also discover attention heads that keep track of the number of already closed parentheses precisely but may or may not promote a correct number of closing parentheses that are still missing, leading to a positive or negative impact on the model's performance.         ",
    "url": "https://arxiv.org/abs/2502.18499",
    "authors": [
      "Samuel Miller",
      "Daking Rai",
      "Ziyu Yao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.18508",
    "title": "REFINE: Inversion-Free Backdoor Defense via Model Reprogramming",
    "abstract": "           Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input transformations or backdoor trigger inversion (BTI) to deactivate or eliminate embedded backdoor triggers during the inference process. However, these methods suffer from inherent limitations: transformation-based defenses often fail to balance model utility and defense performance, while BTI-based defenses struggle to accurately reconstruct trigger patterns without prior knowledge. In this paper, we propose REFINE, an inversion-free backdoor defense method based on model reprogramming. REFINE consists of two key components: \\textbf{(1)} an input transformation module that disrupts both benign and backdoor patterns, generating new benign features; and \\textbf{(2)} an output remapping module that redefines the model's output domain to guide the input transformations effectively. By further integrating supervised contrastive loss, REFINE enhances the defense capabilities while maintaining model utility. Extensive experiments on various benchmark datasets demonstrate the effectiveness of our REFINE and its resistance to potential adaptive attacks.         ",
    "url": "https://arxiv.org/abs/2502.18508",
    "authors": [
      "Yukun Chen",
      "Shuo Shao",
      "Enhao Huang",
      "Yiming Li",
      "Pin-Yu Chen",
      "Zhan Qin",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18509",
    "title": "Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents",
    "abstract": "           Conversational agents are increasingly woven into individuals' personal lives, yet users often underestimate the privacy risks involved. The moment users share information with these agents (e.g., LLMs), their private information becomes vulnerable to exposure. In this paper, we characterize the notion of contextual privacy for user interactions with LLMs. It aims to minimize privacy risks by ensuring that users (sender) disclose only information that is both relevant and necessary for achieving their intended goals when interacting with LLMs (untrusted receivers). Through a formative design user study, we observe how even \"privacy-conscious\" users inadvertently reveal sensitive information through indirect disclosures. Based on insights from this study, we propose a locally-deployable framework that operates between users and LLMs, and identifies and reformulates out-of-context information in user prompts. Our evaluation using examples from ShareGPT shows that lightweight models can effectively implement this framework, achieving strong gains in contextual privacy while preserving the user's intended interaction goals through different approaches to classify information relevant to the intended goals.         ",
    "url": "https://arxiv.org/abs/2502.18509",
    "authors": [
      "Ivoline Ngong",
      "Swanand Kadhe",
      "Hao Wang",
      "Keerthiram Murugesan",
      "Justin D. Weisz",
      "Amit Dhurandhar",
      "Karthikeyan Natesan Ramamurthy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18511",
    "title": "ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models",
    "abstract": "           Generative large language models are crucial in natural language processing, but they are vulnerable to backdoor attacks, where subtle triggers compromise their behavior. Although backdoor attacks against LLMs are constantly emerging, existing benchmarks remain limited in terms of sufficient coverage of attack, metric system integrity, backdoor attack alignment. And existing pre-trained backdoor attacks are idealized in practice due to resource access constraints. Therefore we establish $\\textit{ELBA-Bench}$, a comprehensive and unified framework that allows attackers to inject backdoor through parameter efficient fine-tuning ($\\textit{e.g.,}$ LoRA) or without fine-tuning techniques ($\\textit{e.g.,}$ In-context-learning). $\\textit{ELBA-Bench}$ provides over 1300 experiments encompassing the implementations of 12 attack methods, 18 datasets, and 12 LLMs. Extensive experiments provide new invaluable findings into the strengths and limitations of various attack strategies. For instance, PEFT attack consistently outperform without fine-tuning approaches in classification tasks while showing strong cross-dataset generalization with optimized triggers boosting robustness; Task-relevant backdoor optimization techniques or attack prompts along with clean and adversarial demonstrations can enhance backdoor attack success while preserving model performance on clean samples. Additionally, we introduce a universal toolbox designed for standardized backdoor attack research, with the goal of propelling further progress in this vital area.         ",
    "url": "https://arxiv.org/abs/2502.18511",
    "authors": [
      "Xuxu Liu",
      "Siyuan Liang",
      "Mengya Han",
      "Yong Luo",
      "Aishan Liu",
      "Xiantao Cai",
      "Zheng He",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18515",
    "title": "A Multi-Agent Framework for Automated Vulnerability Detection and Repair in Solidity and Move Smart Contracts",
    "abstract": "           The rapid growth of the blockchain ecosystem and the increasing value locked in smart contracts necessitate robust security measures. While languages like Solidity and Move aim to improve smart contract security, vulnerabilities persist. This paper presents Smartify, a novel multi-agent framework leveraging Large Language Models (LLMs) to automatically detect and repair vulnerabilities in Solidity and Move smart contracts. Unlike traditional methods that rely solely on vast pre-training datasets, Smartify employs a team of specialized agents working on different specially fine-tuned LLMs to analyze code based on underlying programming concepts and language-specific security principles. We evaluated Smartify on a dataset for Solidity and a curated dataset for Move, demonstrating its effectiveness in fixing a wide range of vulnerabilities. Our results show that Smartify (Gemma2+codegemma) achieves state-of-the-art performance, surpassing existing LLMs and enhancing general-purpose models' capabilities, such as Llama 3.1. Notably, Smartify can incorporate language-specific knowledge, such as the nuances of Move, without requiring massive language-specific pre-training datasets. This work offers a detailed analysis of various LLMs' performance on smart contract repair, highlighting the strengths of our multi-agent approach and providing a blueprint for developing more secure and reliable decentralized applications in the growing blockchain landscape. We also provide a detailed recipe for extending this to other similar use cases.         ",
    "url": "https://arxiv.org/abs/2502.18515",
    "authors": [
      "Rabimba Karanjai",
      "Sam Blackshear",
      "Lei Xu",
      "Weidong Shi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.18518",
    "title": "Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs",
    "abstract": "           Modern large language models (LLMs) exhibit critical vulnerabilities to poison pill attacks: localized data poisoning that alters specific factual knowledge while preserving overall model utility. We systematically demonstrate these attacks exploit inherent architectural properties of LLMs, achieving 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase retrieval inaccuracy on compressed models versus original architectures. Through controlled mutations (e.g., temporal/spatial/entity alterations) and, our method induces localized memorization deterioration with negligible impact on models' performance on regular standard benchmarks (e.g., <2% performance drop on MMLU/GPQA), leading to potential detection evasion. Our findings suggest: (1) Disproportionate vulnerability in long-tail knowledge may result from reduced parameter redundancy; (2) Model compression may increase attack surfaces, with pruned/distilled models requiring 30% fewer poison samples for equivalent damage; (3) Associative memory enables both spread of collateral damage to related concepts and amplification of damage from simultaneous attack, particularly for dominant topics. These findings raise concerns over current scaling paradigms since attack costs are lowering while defense complexity is rising. Our work establishes poison pills as both a security threat and diagnostic tool, revealing critical security-efficiency trade-offs in language model compression that challenges prevailing safety assumptions.         ",
    "url": "https://arxiv.org/abs/2502.18518",
    "authors": [
      "Peng Yifeng",
      "Wu Zhizheng",
      "Chen Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18520",
    "title": "Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor Defense by Purifying Poisoned Features",
    "abstract": "           Recent studies have highlighted the vulnerability of deep neural networks to backdoor attacks, where models are manipulated to rely on embedded triggers within poisoned samples, despite the presence of both benign and trigger information. While several defense methods have been proposed, they often struggle to balance backdoor mitigation with maintaining benign this http URL this work, inspired by the concept of optical polarizer-which allows light waves of specific polarizations to pass while filtering others-we propose a lightweight backdoor defense approach, NPD. This method integrates a neural polarizer (NP) as an intermediate layer within the compromised model, implemented as a lightweight linear transformation optimized via bi-level optimization. The learnable NP filters trigger information from poisoned samples while preserving benign content. Despite its effectiveness, we identify through empirical studies that NPD's performance degrades when the target labels (required for purification) are inaccurately estimated. To address this limitation while harnessing the potential of targeted adversarial mitigation, we propose class-conditional neural polarizer-based defense (CNPD). The key innovation is a fusion module that integrates the backdoored model's predicted label with the features to be purified. This architecture inherently mimics targeted adversarial defense mechanisms without requiring label estimation used in NPD. We propose three implementations of CNPD: the first is r-CNPD, which trains a replicated NP layer for each class and, during inference, selects the appropriate NP layer for defense based on the predicted class from the backdoored model. To efficiently handle a large number of classes, two variants are designed: e-CNPD, which embeds class information as additional features, and a-CNPD, which directs network attention using class information.         ",
    "url": "https://arxiv.org/abs/2502.18520",
    "authors": [
      "Mingli Zhu",
      "Shaokui Wei",
      "Hongyuan Zha",
      "Baoyuan Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18521",
    "title": "Optimized Custom CNN for Real-Time Tomato Leaf Disease Detection",
    "abstract": "           In Bangladesh, tomatoes are a staple vegetable, prized for their versatility in various culinary applications. However, the cultivation of tomatoes is often hindered by a range of diseases that can significantly reduce crop yields and quality. Early detection of these diseases is crucial for implementing timely interventions and ensuring the sustainability of tomato production. Traditional manual inspection methods, while effective, are labor-intensive and prone to human error. To address these challenges, this research paper sought to develop an automated disease detection system using Convolutional Neural Networks (CNNs). A comprehensive dataset of tomato leaves was collected from the Brahmanbaria district, preprocessed to enhance image quality, and then applied to various deep learning models. Comparative performance analysis was conducted between YOLOv5, MobileNetV2, ResNet18, and our custom CNN model. In our study, the Custom CNN model achieved an impressive accuracy of 95.2%, significantly outperforming the other models, which achieved an accuracy of 77%, 89.38% and 71.88% respectively. While other models showed solid performance, our Custom CNN demonstrated superior results specifically tailored for the task of tomato leaf disease detection. These findings highlight the strong potential of deep learning techniques for improving early disease detection in tomato crops. By leveraging these advanced technologies, farmers can gain valuable insights to detect diseases at an early stage, allowing for more effective management practices. This approach not only promises to boost tomato yields but also contributes to the sustainability and resilience of the agricultural sector, helping to mitigate the impact of plant diseases on crop production.         ",
    "url": "https://arxiv.org/abs/2502.18521",
    "authors": [
      "Mangsura Kabir Oni",
      "Tabia Tanzin Prama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.18526",
    "title": "Reinforcement Learning-based Approach for Vehicle-to-Building Charging with Heterogeneous Agents and Long Term Rewards",
    "abstract": "           Strategic aggregation of electric vehicle batteries as energy reservoirs can optimize power grid demand, benefiting smart and connected communities, especially large office buildings that offer workplace charging. This involves optimizing charging and discharging to reduce peak energy costs and net peak demand, monitored over extended periods (e.g., a month), which involves making sequential decisions under uncertainty and delayed and sparse rewards, a continuous action space, and the complexity of ensuring generalization across diverse conditions. Existing algorithmic approaches, e.g., heuristic-based strategies, fall short in addressing real-time decision-making under dynamic conditions, and traditional reinforcement learning (RL) models struggle with large state-action spaces, multi-agent settings, and the need for long-term reward optimization. To address these challenges, we introduce a novel RL framework that combines the Deep Deterministic Policy Gradient approach (DDPG) with action masking and efficient MILP-driven policy guidance. Our approach balances the exploration of continuous action spaces to meet user charging demands. Using real-world data from a major electric vehicle manufacturer, we show that our approach comprehensively outperforms many well-established baselines and several scalable heuristic approaches, achieving significant cost savings while meeting all charging requirements. Our results show that the proposed approach is one of the first scalable and general approaches to solving the V2B energy management challenge.         ",
    "url": "https://arxiv.org/abs/2502.18526",
    "authors": [
      "Fangqi Liu",
      "Rishav Sen",
      "Jose Paolo Talusan",
      "Ava Pettet",
      "Aaron Kandel",
      "Yoshinori Suzue",
      "Ayan Mukhopadhyay",
      "Abhishek Dubey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18527",
    "title": "GOD model: Privacy Preserved AI School for Personal Assistant",
    "abstract": "           Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive recommendations that simplify everyday tasks, but their reliance on sensitive user data raises concerns about privacy and trust. To address these challenges, we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework for training and evaluating AI assistants directly on-device. Unlike traditional benchmarks, the GOD model measures how well assistants can anticipate user needs-such as suggesting gifts-while protecting user data and autonomy. Functioning like an AI school, it addresses the cold start problem by simulating user queries and employing a curriculum-based approach to refine the performance of each assistant. Running within a Trusted Execution Environment (TEE), it safeguards user data while applying reinforcement and imitation learning to refine AI recommendations. A token-based incentive system encourages users to share data securely, creating a data flywheel that drives continuous improvement. By integrating privacy, personalization, and trust, the GOD model provides a scalable, responsible path for advancing personal AI assistants. For community collaboration, part of the framework is open-sourced at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18527",
    "authors": [
      "PIN AI Team",
      "Bill Qingyun Sun",
      "Laura Florescu",
      "Boliang Zhang",
      "Regan Peng",
      "Smile Hu",
      "Shouqiao Wang",
      "Ben Wu",
      "Xi Wang",
      "Davide Crapis",
      "Gavin Zhen Guo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18529",
    "title": "Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality",
    "abstract": "           The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy. Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper, we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) {How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) {How do different decision making strategies impact the overall learning performance}? Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making.         ",
    "url": "https://arxiv.org/abs/2502.18529",
    "authors": [
      "Hang Wang",
      "Qiaoyi Fang",
      "Junshan Zhang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.18533",
    "title": "Convolutional neural networks for mineral prospecting through alteration mapping with remote sensing data",
    "abstract": "           Traditional geological mapping, based on field observations and rock sample analysis, is inefficient for continuous spatial mapping of features like alteration zones. Deep learning models, such as convolutional neural networks (CNNs), have revolutionised remote sensing data analysis by automatically extracting features for classification and regression tasks. CNNs can detect specific mineralogical changes linked to mineralisation by identifying subtle features in remote sensing data. This study uses CNNs with Landsat 8, Landsat 9, and ASTER data to map alteration zones north of Broken Hill, New South Wales, Australia. The model is trained using ground truth data and an automated approach with selective principal component analysis (PCA). We compare CNNs with traditional machine learning models, including k-nearest neighbours, support vector machines, and multilayer perceptron. Results show that ground truth-based training yields more reliable maps, with CNNs slightly outperforming conventional models in capturing spatial patterns. Landsat 9 outperforms Landsat 8 in mapping iron oxide areas using ground truth-trained CNNs, while ASTER data provides the most accurate argillic and propylitic alteration maps. This highlights CNNs' effectiveness in improving geological mapping precision, especially for identifying subtle mineralisation-related alterations.         ",
    "url": "https://arxiv.org/abs/2502.18533",
    "authors": [
      "Ehsan Farahbakhsh",
      "Dakshi Goel",
      "Dhiraj Pimparkar",
      "R. Dietmar Muller",
      "Rohitash Chandra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2502.18540",
    "title": "MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications",
    "abstract": "           Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models (LLMs) offer potential solutions but face challenges, including limited accuracy and input length constraints. To address these challenges, we propose MA-GTS (Multi-Agent Graph Theory Solver), a multi-agent framework that decomposes these complex problems through agent collaboration. MA-GTS maps the implicitly expressed text-based graph data into clear, structured graph representations and dynamically selects the most suitable algorithm based on problem constraints and graph structure scale. This approach ensures that the solution process remains efficient and the resulting reasoning path is interpretable. We validate MA-GTS using the G-REAL dataset, a real-world-inspired graph theory dataset we created. Experimental results show that MA-GTS outperforms state-of-the-art approaches in terms of efficiency, accuracy, and scalability, with strong results across multiple benchmarks (G-REAL 94.2%, GraCoRe 96.9%, NLGraph 98.4%).MA-GTS is open-sourced at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18540",
    "authors": [
      "Zike Yuan",
      "Ming Liu",
      "Hui Wang",
      "Bing Qin"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18545",
    "title": "PII-Bench: Evaluating Query-Aware Privacy Protection Systems",
    "abstract": "           The widespread adoption of Large Language Models (LLMs) has raised significant privacy concerns regarding the exposure of personally identifiable information (PII) in user prompts. To address this challenge, we propose a query-unrelated PII masking strategy and introduce PII-Bench, the first comprehensive evaluation framework for assessing privacy protection systems. PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories, featuring diverse scenarios from single-subject descriptions to complex multi-party interactions. Each sample is carefully crafted with a user query, context description, and standard answer indicating query-relevant PII. Our empirical evaluation reveals that while current models perform adequately in basic PII detection, they show significant limitations in determining PII query relevance. Even state-of-the-art LLMs struggle with this task, particularly in handling complex multi-subject scenarios, indicating substantial room for improvement in achieving intelligent PII masking.         ",
    "url": "https://arxiv.org/abs/2502.18545",
    "authors": [
      "Hao Shen",
      "Zhouhong Gu",
      "Haokai Hong",
      "Weili Han"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.18546",
    "title": "Multi-class Seismic Building Damage Assessment from InSAR Imagery using Quadratic Variational Causal Bayesian Inference",
    "abstract": "           Interferometric Synthetic Aperture Radar (InSAR) technology uses satellite radar to detect surface deformation patterns and monitor earthquake impacts on buildings. While vital for emergency response planning, extracting multi-class building damage classifications from InSAR data faces challenges: overlapping damage signatures with environmental noise, computational complexity in multi-class scenarios, and the need for rapid regional-scale processing. Our novel multi-class variational causal Bayesian inference framework with quadratic variational bounds provides rigorous approximations while ensuring efficiency. By integrating InSAR observations with USGS ground failure models and building fragility functions, our approach separates building damage signals while maintaining computational efficiency through strategic pruning. Evaluation across five major earthquakes (Haiti 2021, Puerto Rico 2020, Zagreb 2020, Italy 2016, Ridgecrest 2019) shows improved damage classification accuracy (AUC: 0.94-0.96), achieving up to 35.7% improvement over existing methods. Our approach maintains high accuracy (AUC > 0.93) across all damage categories while reducing computational overhead by over 40% without requiring extensive ground truth data.         ",
    "url": "https://arxiv.org/abs/2502.18546",
    "authors": [
      "Xuechun Li",
      "Susu Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18549",
    "title": "Target Defense with Multiple Defenders and an Agile Attacker via Residual Policy Learning",
    "abstract": "           The target defense problem involves intercepting an attacker before it reaches a designated target region using one or more defenders. This letter focuses on a particularly challenging scenario in which the attacker is more agile than the defenders, significantly increasing the difficulty of effective interception. To address this challenge, we propose a novel residual policy framework that integrates deep reinforcement learning (DRL) with the force-based Boids model. In this framework, the Boids model serves as a baseline policy, while DRL learns a residual policy to refine and optimize the defenders' actions. Simulation experiments demonstrate that the proposed method consistently outperforms traditional interception policies, whether learned via vanilla DRL or fine-tuned from force-based methods. Moreover, the learned policy exhibits strong scalability and adaptability, effectively handling scenarios with varying numbers of defenders and attackers with different agility levels.         ",
    "url": "https://arxiv.org/abs/2502.18549",
    "authors": [
      "Jiyue Tao",
      "Tongsheng Shen",
      "Dexin Zhao",
      "Feitian Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.18555",
    "title": "Application of Attention Mechanism with Bidirectional Long Short-Term Memory (BiLSTM) and CNN for Human Conflict Detection using Computer Vision",
    "abstract": "           The automatic detection of human conflicts through videos is a crucial area in computer vision, with significant applications in monitoring and public safety policies. However, the scarcity of public datasets and the complexity of human interactions make this task challenging. This study investigates the integration of advanced deep learning techniques, including Attention Mechanism, Convolutional Neural Networks (CNNs), and Bidirectional Long ShortTerm Memory (BiLSTM), to improve the detection of violent behaviors in videos. The research explores how the use of the attention mechanism can help focus on the most relevant parts of the video, enhancing the accuracy and robustness of the model. The experiments indicate that the combination of CNNs with BiLSTM and the attention mechanism provides a promising solution for conflict monitoring, offering insights into the effectiveness of different strategies. This work opens new possibilities for the development of automated surveillance systems that can operate more efficiently in real-time detection of violent events.         ",
    "url": "https://arxiv.org/abs/2502.18555",
    "authors": [
      "Erick da Silva Farias",
      "Eduardo Palhares Junior"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18579",
    "title": "A simple and flexible algorithm to generate real-world networks",
    "abstract": "           This study introduces an algorithm that generates undirected graphs with three main characteristics of real-world networks: scale-freeness, short distances between nodes (small-world phenomenon), and large clustering coefficients. The main idea is to perform random walks across the network and, at each iteration, add special edges with a decreasing probability to link more distant nodes, following a specific probability distribution. A key advantage of our algorithm is its simplicity and flexibility in creating networks with different characteristics without using global information about network topology. We show how the parameters can be adjusted to generate networks with specific average distances and clustering coefficients, maintaining a long-tailed degree distribution. The implementation of our algorithm is publicly available on a GitHub repository.         ",
    "url": "https://arxiv.org/abs/2502.18579",
    "authors": [
      "Jo\u00e3o Pedro C. Morais",
      "Ruben Interian"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2502.18591",
    "title": "Transported Memory Networks accelerating Computational Fluid Dynamics",
    "abstract": "           In recent years, augmentation of differentiable PDE solvers with neural networks has shown promising results, particularly in fluid simulations. However, most approaches rely on convolutional neural networks and custom solvers operating on Cartesian grids with efficient access to cell data. This particular choice poses challenges for industrial-grade solvers that operate on unstructured meshes, where access is restricted to neighboring cells only. In this work, we address this limitation using a novel architecture, named Transported Memory Networks. The architecture draws inspiration from both traditional turbulence models and recurrent neural networks, and it is fully compatible with generic discretizations. Our results show that it is point-wise and statistically comparable to, or improves upon, previous methods in terms of both accuracy and computational efficiency.         ",
    "url": "https://arxiv.org/abs/2502.18591",
    "authors": [
      "Matthias Schulz",
      "Gwendal Jouan",
      "Daniel Berger",
      "Stefan Gavranovic",
      "Dirk Hartmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2502.18592",
    "title": "DeBUGCN -- Detecting Backdoors in CNNs Using Graph Convolutional Networks",
    "abstract": "           Deep neural networks (DNNs) are becoming commonplace in critical applications, making their susceptibility to backdoor (trojan) attacks a significant problem. In this paper, we introduce a novel backdoor attack detection pipeline, detecting attacked models using graph convolution networks (DeBUGCN). To the best of our knowledge, ours is the first use of GCNs for trojan detection. We use the static weights of a DNN to create a graph structure of its layers. A GCN is then used as a binary classifier on these graphs, yielding a trojan or clean determination for the DNN. To demonstrate the efficacy of our pipeline, we train hundreds of clean and trojaned CNN models on the MNIST handwritten digits and CIFAR-10 image datasets, and show the DNN classification results using DeBUGCN. For a true In-the-Wild use case, our pipeline is evaluated on the TrojAI dataset which consists of various CNN architectures, thus showing the robustness and model-agnostic behaviour of DeBUGCN. Furthermore, on comparing our results on several datasets with state-of-the-art trojan detection algorithms, DeBUGCN is faster and more accurate.         ",
    "url": "https://arxiv.org/abs/2502.18592",
    "authors": [
      "Akash Vartak",
      "Khondoker Murad Hossain",
      "Tim Oates"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18601",
    "title": "Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method",
    "abstract": "           The rapid advancements in data-driven methodologies have underscored the critical importance of ensuring data quality. Consequently, detecting out-of-distribution (OOD) data has emerged as an essential task to maintain the reliability and robustness of data-driven models, in general, and machine and deep learning models, in particular. In this study, we leveraged the convex hull property of a dataset and the fact that anomalies highly contribute to the increase of the CH's volume to propose a novel anomaly detection algorithm. Our algorithm computes the CH's volume as an increasing number of data points are removed from the dataset to define a decision line between OOD and in-distribution data points. We compared the proposed algorithm to seven widely used anomaly detection algorithms over ten datasets, showing comparable results for state-of-the-art (SOTA) algorithms. Moreover, we show that with a computationally cheap and simple check, one can detect datasets that are well-suited for the proposed algorithm which outperforms the SOTA anomaly detection algorithms.         ",
    "url": "https://arxiv.org/abs/2502.18601",
    "authors": [
      "Uri Itai",
      "Asael Bar Ilan",
      "Teddy Lazebnik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18623",
    "title": "On the Privacy-Preserving Properties of Spiking Neural Networks with Unique Surrogate Gradients and Quantization Levels",
    "abstract": "           As machine learning models increasingly process sensitive data, understanding their vulnerability to privacy attacks is vital. Membership inference attacks (MIAs) exploit model responses to infer whether specific data points were used during training, posing a significant privacy risk. Prior research suggests that spiking neural networks (SNNs), which rely on event-driven computation and discrete spike-based encoding, exhibit greater resilience to MIAs than artificial neural networks (ANNs). This resilience stems from their non-differentiable activations and inherent stochasticity, which obscure the correlation between model responses and individual training samples. To enhance privacy in SNNs, we explore two techniques: quantization and surrogate gradients. Quantization, which reduces precision to limit information leakage, has improved privacy in ANNs. Given SNNs' sparse and irregular activations, quantization may further disrupt the activation patterns exploited by MIAs. We assess the vulnerability of SNNs and ANNs under weight and activation quantization across multiple datasets, using the attack model's receiver operating characteristic (ROC) curve area under the curve (AUC) metric, where lower values indicate stronger privacy, and evaluate the privacy-accuracy trade-off. Our findings show that quantization enhances privacy in both architectures with minimal performance loss, though full-precision SNNs remain more resilient than quantized ANNs. Additionally, we examine the impact of surrogate gradients on privacy in SNNs. Among five evaluated gradients, spike rate escape provides the best privacy-accuracy trade-off, while arctangent increases vulnerability to MIAs. These results reinforce SNNs' inherent privacy advantages and demonstrate that quantization and surrogate gradient selection significantly influence privacy-accuracy trade-offs in SNNs.         ",
    "url": "https://arxiv.org/abs/2502.18623",
    "authors": [
      "Ayana Moshruba",
      "Shay Snyder",
      "Hamed Poursiami",
      "Maryam Parsa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2502.18652",
    "title": "Independent Mobility GPT (IDM-GPT): A Self-Supervised Multi-Agent Large Language Model Framework for Customized Traffic Mobility Analysis Using Machine Learning Models",
    "abstract": "           With the urbanization process, an increasing number of sensors are being deployed in transportation systems, leading to an explosion of big data. To harness the power of this vast transportation data, various machine learning (ML) and artificial intelligence (AI) methods have been introduced to address numerous transportation challenges. However, these methods often require significant investment in data collection, processing, storage, and the employment of professionals with expertise in transportation and ML. Additionally, privacy issues are a major concern when processing data for real-world traffic control and management. To address these challenges, the research team proposes an innovative Multi-agent framework named Independent Mobility GPT (IDM-GPT) based on large language models (LLMs) for customized traffic analysis, management suggestions, and privacy preservation. IDM-GPT efficiently connects users, transportation databases, and ML models economically. IDM-GPT trains, customizes, and applies various LLM-based AI agents for multiple functions, including user query comprehension, prompts optimization, data analysis, model selection, and performance evaluation and enhancement. With IDM-GPT, users without any background in transportation or ML can efficiently and intuitively obtain data analysis and customized suggestions in near real-time based on their questions. Experimental results demonstrate that IDM-GPT delivers satisfactory performance across multiple traffic-related tasks, providing comprehensive and actionable insights that support effective traffic management and urban mobility improvement.         ",
    "url": "https://arxiv.org/abs/2502.18652",
    "authors": [
      "Fengze Yang",
      "Xiaoyue Cathy Liu",
      "Lingjiu Lu",
      "Bingzhang Wang",
      "Chenxi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2502.18661",
    "title": "TikTok StitchGraph: Characterizing communication patterns on TikTok through a collection of interaction networks",
    "abstract": "           We present TikTok StitchGraph: a collection of 36 graphs based on TikTok stitches. With its rapid growth and widespread popularity, TikTok presents a compelling platform for study, yet given its video-first nature the network structure of the conversations that it hosts remains largely unexplored. Leveraging its recently released APIs, in combination with web scraping, we construct graphs detailing stitch relations from both a video- and user-centric perspective. Specifically, we focus on user multi-digraphs, with vertices representing users and edges representing directed stitch relations. From the user graphs, we characterize common communication patterns of the stitch using frequent subgraph mining, finding a preference for stars and star-like structures, an aversion towards cyclic structures, and directional disposition favoring in- and out-stars over mixed-direction structures. These structures are augmented with sentiment labels in the form of edge attributes. We then use these subgraphs for graph-level embeddings together with Graph2Vec, we show no clear distinction between topologies for different hashtag topic categories. Lastly, we compare our StitchGraphs to Twitter reply networks and show that a remakable similarity between the conversation networks on the two platforms.         ",
    "url": "https://arxiv.org/abs/2502.18661",
    "authors": [
      "Luca Rossi",
      "Mads H\u00f8genhaug",
      "Marcus Friis",
      "Morten Pedersen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.18662",
    "title": "The Role of the Retrospective Meetings in Detecting, Refactoring and Monitoring Community Smells",
    "abstract": "           Retrospective meetings play a vital role in agile development by facilitating team reflection on past work to enhance effectiveness. These meetings address various social aspects, including team dynamics, individual performance, processes, and technologies, ultimately leading to actions for improvement. Despite their importance, limited research has explored how these meetings handle forms of social debt, particularly Community Smells -- recurring dysfunctional patterns in team dynamics, such as poor communication or isolated work practices. This study seeks to understand how retrospective meetings address a few core Community Smells, examining whether these meetings help identify smells, make it possible to formulate refactoring strategies, support the monitoring of refactoring actions, and contribute to preventing the most prominent Community Smells. We conducted semi-structured interviews with 15 practitioners from diverse organizations who regularly participate in retrospective meetings. The interviewees shared their experiences with retrospectives, the challenges discussed, and subsequent improvement actions. The study focused on the four most cited Community Smells in the literature -- Lone Wolf, Organizational Silo, Radio Silence, and Black Cloud. Data was analyzed iteratively using a priori coding to examine Community Smells and inductive open coding inspired by Grounded Theory. The findings indicate that retrospective meetings indeed enable the identification of core Community Smells. However, while strategies for refactoring are often formulated, their implementation and monitoring remain inconsistent. Additionally, an emphasis on positive aspects during these meetings may help in preventing Community Smells. This study offers valuable insights to practitioners and researchers, highlighting the importance of addressing social debt in software development within agile practices.         ",
    "url": "https://arxiv.org/abs/2502.18662",
    "authors": [
      "Carlos Dantas",
      "Tiago Massoni",
      "Camila Sarmento",
      "Rayana Rocha",
      "Danielly Gualberto"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.18663",
    "title": "CayleyPy RL: Pathfinding and Reinforcement Learning on Cayley Graphs",
    "abstract": "           This paper is the second in a series of studies on developing efficient artificial intelligence-based approaches to pathfinding on extremely large graphs (e.g. $10^{70}$ nodes) with a focus on Cayley graphs and mathematical applications. The open-source CayleyPy project is a central component of our research. The present paper proposes a novel combination of a reinforcement learning approach with a more direct diffusion distance approach from the first paper. Our analysis includes benchmarking various choices for the key building blocks of the approach: architectures of the neural network, generators for the random walks and beam search pathfinding. We compared these methods against the classical computer algebra system GAP, demonstrating that they \"overcome the GAP\" for the considered examples. As a particular mathematical application we examine the Cayley graph of the symmetric group with cyclic shift and transposition generators. We provide strong support for the OEIS-A186783 conjecture that the diameter is equal to n(n-1)/2 by machine learning and mathematical methods. We identify the conjectured longest element and generate its decomposition of the desired length. We prove a diameter lower bound of n(n-1)/2-n/2 and an upper bound of n(n-1)/2+ 3n by presenting the algorithm with given complexity. We also present several conjectures motivated by numerical experiments, including observations on the central limit phenomenon (with growth approximated by a Gumbel distribution), the uniform distribution for the spectrum of the graph, and a numerical study of sorting networks. To stimulate crowdsourcing activity, we create challenges on the Kaggle platform and invite contributions to improve and benchmark approaches on Cayley graph pathfinding and other tasks.         ",
    "url": "https://arxiv.org/abs/2502.18663",
    "authors": [
      "A.Chervov",
      "A.Soibelman",
      "S.Lytkin",
      "I.Kiselev",
      "S.Fironov",
      "A.Lukyanenko",
      "A.Dolgorukova",
      "A.Ogurtsov",
      "F.Petrov",
      "S.Krymskii",
      "M.Evseev",
      "L.Grunvald",
      "D.Gorodkov",
      "G.Antiufeev",
      "G.Verbii",
      "V.Zamkovoy",
      "L.Cheldieva",
      "I.Koltsov",
      "A. Sychev",
      "M.Obozov",
      "A.Eliseev",
      "S.Nikolenko",
      "N.Narynbaev",
      "R.Turtayev",
      "N. Rokotyan",
      "S.Kovalev",
      "A.Rozanov",
      "V.Nelin",
      "S.Ermilov",
      "L.Shishina",
      "D.Mamayeva",
      "A.Korolkova",
      "K.Khoruzhii",
      "A.Romanov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)",
      "Group Theory (math.GR)"
    ]
  },
  {
    "id": "arXiv:2502.18671",
    "title": "Wireless sensor networks data synchronization using node MCU memory for precision agriculture applications",
    "abstract": "           Wireless Sensor Networks have risen as a highly promising technology suitable for precision agriculture implementations, enabling efficient monitoring and control of agricultural processes. In precision agriculture, accurate and synchronized data collection is crucial for effective analysis and decision making. Using principles of information theory, we can define conditions and parameters that influence the efficient transmission and processing of information. Existing technologies have limitations in maintaining consistent time references, handling node failures, and unreliable communication links, leading to inaccurate data readings. Reliable data storage is demanding now-a-days for storing data on local monitoring station as well as in online live server. Sometime internet is not working properly due to congestion and there is frequent packet loss. Current solutions often synchronize records based on database timestamps, leading to record duplication and waste storage. Both databases synchronize each other after internet restoration. By providing synchronization among nodes and data, accuracy and storage will be saved in IoT based WSNs for precision agriculture applications. A prototype Node-MCU internal memory is used as a resource for achieving data synchronization. This proposed work generates record ID from Node MCU EEPROM which helps in records synchronization if there is any packet loss at the local server or at the online server to maintain synchronization accuracy despite unreliable communication links. Experiment shows that for a particular duration Node MCU generated 2364 packets and packet loss at local server was 08 and at online server was 174 packets. Results shows that after synchronization 99.87% packets were synchronized. Using previous technique of timestamp, the redundancy was 70% which reduced to 0% using our proposed technique.         ",
    "url": "https://arxiv.org/abs/2502.18671",
    "authors": [
      "Kashif Sattar",
      "Muhammad Arslan",
      "Saqib Majeed",
      "Salim Iqbal"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2502.18691",
    "title": "Enhancing Image Classification with Augmentation: Data Augmentation Techniques for Improved Image Classification",
    "abstract": "           Convolutional Neural Networks (CNNs) serve as the workhorse of deep learning, finding applications in various fields that rely on images. Given sufficient data, they exhibit the capacity to learn a wide range of concepts across diverse settings. However, a notable limitation of CNNs is their susceptibility to overfitting when trained on small datasets. The augmentation of such datasets can significantly enhance CNN performance by introducing additional data points for learning. In this study, we explore the effectiveness of 11 different sets of data augmentation techniques, which include three novel sets proposed in this work. The first set of data augmentation employs pairwise channel transfer, transferring Red, Green, Blue, Hue, and Saturation values from randomly selected images in the database to all images in the dataset. The second set introduces a novel occlusion approach, where objects in the images are occluded by randomly selected objects from the dataset. The third set involves a novel masking approach, using vertical, horizontal, circular, and checkered masks to occlude portions of the images. In addition to these novel techniques, we investigate other existing augmentation methods, including rotation, horizontal and vertical flips, resizing, translation, blur, color jitter, and random erasing, and their effects on accuracy and overfitting. We fine-tune a base EfficientNet-B0 model for each augmentation method and conduct a comparative analysis to showcase their efficacy. For the evaluation and comparison of these augmentation techniques, we utilize the Caltech-101 dataset. The ensemble of image augmentation techniques proposed emerges as the most effective on the Caltech-101 dataset. The results demonstrate that diverse data augmentation techniques present a viable means of enhancing datasets for improved image classification.         ",
    "url": "https://arxiv.org/abs/2502.18691",
    "authors": [
      "Saorj Kumar",
      "Prince Asiamah",
      "Oluwatoyin Jolaoso",
      "Ugochukwu Esiowu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.18696",
    "title": "Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based Motion Prediction with Parameter Optimization",
    "abstract": "           The deployment of autonomous navigation systems on ships necessitates accurate motion prediction models tailored to individual vessels. Traditional physics-based models, while grounded in hydrodynamic principles, often fail to account for ship-specific behaviors under real-world conditions. Conversely, purely data-driven models offer specificity but lack interpretability and robustness in edge cases. This study proposes a data-driven physics-based model that integrates physics-based equations with data-driven parameter optimization, leveraging the strengths of both approaches to ensure interpretability and adaptability. The model incorporates physics-based components such as 3-DoF dynamics, rudder, and propeller forces, while parameters such as resistance curve and rudder coefficients are optimized using synthetic data. By embedding domain knowledge into the parameter optimization process, the fitted model maintains physical consistency. Validation of the approach is realized with two container ships by comparing, both qualitatively and quantitatively, predictions against ground-truth trajectories. The results demonstrate significant improvements, in predictive accuracy and reliability, of the data-driven physics-based models over baseline physics-based models tuned with traditional marine engineering practices. The fitted models capture ship-specific behaviors in diverse conditions with their predictions being, 51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67% (ship B) more consistent.         ",
    "url": "https://arxiv.org/abs/2502.18696",
    "authors": [
      "Papandreou Christos",
      "Mathioudakis Michail",
      "Stouraitis Theodoros",
      "Iatropoulos Petros",
      "Nikitakis Antonios",
      "Stavros Paschalakis",
      "Konstantinos Kyriakopoulos"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.18697",
    "title": "H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction",
    "abstract": "           The widespread adoption of Electric Vehicles (EVs) poses critical challenges for energy providers, particularly in predicting charging time (temporal prediction), ensuring user privacy, and managing resources efficiently in mobility-driven networks. This paper introduces the Hierarchical Federated Learning Transformer Network (H-FLTN) framework to address these challenges. H-FLTN employs a three-tier hierarchical architecture comprising EVs, community Distributed Energy Resource Management Systems (DERMS), and the Energy Provider Data Centre (EPDC) to enable accurate spatio-temporal predictions of EV charging needs while preserving privacy. Temporal prediction is enhanced using Transformer-based learning, capturing complex dependencies in charging behavior. Privacy is ensured through Secure Aggregation, Additive Secret Sharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow only secret shares of model weights to be exchanged while securing all transmissions. To improve training efficiency and resource management, H-FLTN integrates Dynamic Client Capping Mechanism (DCCM) and Client Rotation Management (CRM), ensuring that training remains both computationally and temporally efficient as the number of participating EVs increases. DCCM optimises client participation by limiting excessive computational loads, while CRM balances training contributions across epochs, preventing imbalanced participation. Our simulation results based on large-scale empirical vehicle mobility data reveal that DCCM and CRM reduce the training time complexity with increasing EVs from linear to constant. Its integration into real-world smart city infrastructure enhances energy demand forecasting, resource allocation, and grid stability, ensuring reliability and sustainability in future mobility ecosystems.         ",
    "url": "https://arxiv.org/abs/2502.18697",
    "authors": [
      "Robert Marlin",
      "Raja Jurdak",
      "Alsharif Abuadbba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.18705",
    "title": "Understanding Children's Avatar Making in Social Online Games",
    "abstract": "           Social online games like Minecraft and Roblox have become increasingly integral to children's daily lives. Our study explores how children aged 8 to 13 create and customize avatars in these virtual environments. Through semi-structured interviews and gameplay observations with 48 participants, we investigate the motivations behind children's avatar-making. Our findings show that children's avatar creation is motivated by self-representation, experimenting with alter ego identities, fulfilling social needs, and improving in-game performance. In addition, designed monetization strategies play a role in shaping children's avatars. We identify the ''wardrobe effect,'' where children create multiple avatars but typically use only one favorite consistently. We discuss the impact of cultural consumerism and how social games can support children's identity exploration while balancing self-expression and social conformity. This work contributes to understanding how avatar shapes children's identity growth in social online games.         ",
    "url": "https://arxiv.org/abs/2502.18705",
    "authors": [
      "Yue Fu",
      "Samuel Schwamm",
      "Amanda Baughan",
      "Nicole M Powell",
      "Zoe Kronberg",
      "Alicia Owens",
      "Emily Renee Izenman",
      "Dania Alsabeh",
      "Elizabeth Hunt",
      "Michael Rich",
      "David Bickham",
      "Jenny Radesky",
      "Alexis Hiniker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.18706",
    "title": "Differentially Private Federated Learning With Time-Adaptive Privacy Spending",
    "abstract": "           Federated learning (FL) with differential privacy (DP) provides a framework for collaborative machine learning, enabling clients to train a shared model while adhering to strict privacy constraints. The framework allows each client to have an individual privacy guarantee, e.g., by adding different amounts of noise to each client's model updates. One underlying assumption is that all clients spend their privacy budgets uniformly over time (learning rounds). However, it has been shown in the literature that learning in early rounds typically focuses on more coarse-grained features that can be learned at lower signal-to-noise ratios while later rounds learn fine-grained features that benefit from higher signal-to-noise ratios. Building on this intuition, we propose a time-adaptive DP-FL framework that expends the privacy budget non-uniformly across both time and clients. Our framework enables each client to save privacy budget in early rounds so as to be able to spend more in later rounds when additional accuracy is beneficial in learning more fine-grained features. We theoretically prove utility improvements in the case that clients with stricter privacy budgets spend budgets unevenly across rounds, compared to clients with more relaxed budgets, who have sufficient budgets to distribute their spend more evenly. Our practical experiments on standard benchmark datasets support our theoretical results and show that, in practice, our algorithms improve the privacy-utility trade-offs compared to baseline schemes.         ",
    "url": "https://arxiv.org/abs/2502.18706",
    "authors": [
      "Shahrzad Kiani",
      "Nupur Kulkarni",
      "Adam Dziedzic",
      "Stark Draper",
      "Franziska Boenisch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2502.18709",
    "title": "Bandit and Delayed Feedback in Online Structured Prediction",
    "abstract": "           Online structured prediction is a task of sequentially predicting outputs with complex structures based on inputs and past observations, encompassing online classification. Recent studies showed that in the full information setup, we can achieve finite bounds on the surrogate regret, i.e., the extra target loss relative to the best possible surrogate loss. In practice, however, full information feedback is often unrealistic as it requires immediate access to the whole structure of complex outputs. Motivated by this, we propose algorithms that work with less demanding feedback, bandit and delayed feedback. For the bandit setting, using a standard inverse-weighted gradient estimator, we achieve a surrogate regret bound of $O(\\sqrt{KT})$ for the time horizon $T$ and the size of the output set $K$. However, $K$ can be extremely large when outputs are highly complex, making this result less desirable. To address this, we propose an algorithm that achieves a surrogate regret bound of $O(T^{2/3})$, which is independent of $K$. This is enabled with a carefully designed pseudo-inverse matrix estimator. Furthermore, for the delayed full information feedback setup, we obtain a surrogate regret bound of $O(D^{2/3} T^{1/3})$ for the delay time $D$. We also provide algorithms for the delayed bandit feedback setup. Finally, we numerically evaluate the performance of the proposed algorithms in online classification with bandit feedback.         ",
    "url": "https://arxiv.org/abs/2502.18709",
    "authors": [
      "Yuki Shibukawa",
      "Taira Tsuchiya",
      "Shinsaku Sakaue",
      "Kenji Yamanishi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.18724",
    "title": "Adversarial Universal Stickers: Universal Perturbation Attacks on Traffic Sign using Stickers",
    "abstract": "           Adversarial attacks on deep learning models have proliferated in recent years. In many cases, a different adversarial perturbation is required to be added to each image to cause the deep learning model to misclassify it. This is ineffective as each image has to be modified in a different way. Meanwhile, research on universal perturbations focuses on designing a single perturbation that can be applied to all images in a data set, and cause a deep learning model to misclassify the images. This work advances the field of universal perturbations by exploring universal perturbations in the context of traffic signs and autonomous vehicle systems. This work introduces a novel method for generating universal perturbations that visually look like simple black and white stickers, and using them to cause incorrect street sign predictions. Unlike traditional adversarial perturbations, the adversarial universal stickers are designed to be applicable to any street sign: same sticker, or stickers, can be applied in same location to any street sign and cause it to be misclassified. Further, to enable safe experimentation with adversarial images and street signs, this work presents a virtual setting that leverages Street View images of street signs, rather than the need to physically modify street signs, to test the attacks. The experiments in the virtual setting demonstrate that these stickers can consistently mislead deep learning models used commonly in street sign recognition, and achieve high attack success rates on dataset of US traffic signs. The findings highlight the practical security risks posed by simple stickers applied to traffic signs, and the ease with which adversaries can generate adversarial universal stickers that can be applied to many street signs.         ",
    "url": "https://arxiv.org/abs/2502.18724",
    "authors": [
      "Anthony Etim",
      "Jakub Szefer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.18725",
    "title": "Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation",
    "abstract": "           Traditional psychological experiments utilizing naturalistic stimuli face challenges in manual annotation and ecological validity. To address this, we introduce a novel paradigm leveraging multimodal large language models (LLMs) as proxies to extract rich semantic information from naturalistic images through a Visual Question Answering (VQA) strategy for analyzing human visual semantic representation. LLM-derived representations successfully predict established neural activity patterns measured by fMRI (e.g., faces, buildings), validating its feasibility and revealing hierarchical semantic organization across cortical regions. A brain semantic network constructed from LLM-derived representations identifies meaningful clusters reflecting functional and contextual associations. This innovative methodology offers a powerful solution for investigating brain semantic organization with naturalistic stimuli, overcoming limitations of traditional annotation methods and paving the way for more ecologically valid explorations of human cognition.         ",
    "url": "https://arxiv.org/abs/2502.18725",
    "authors": [
      "Xin Liu",
      "Ziyue Zhang",
      "Jingxin Nie"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2502.18726",
    "title": "Deep-Bench: Deep Learning Benchmark Dataset for Code Generation",
    "abstract": "           Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types. To address this, we introduce DeepBench, a novel benchmark dataset designed for function-level DL code generation. DeepBench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text. GPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores DeepBench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code. Furthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that DeepBench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain.         ",
    "url": "https://arxiv.org/abs/2502.18726",
    "authors": [
      "Alireza Daghighfarsoodeh",
      "Chung-Yu Wang",
      "Hamed Taherkhani",
      "Melika Sepidband",
      "Mohammad Abdollahi",
      "Hadi Hemmati",
      "Hung Viet Pham"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18729",
    "title": "Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science",
    "abstract": "           Social surveys in computational social science are well-designed by elaborate domain theories that can effectively reflect the interviewee's deep thoughts without concealing their true feelings. The candidate questionnaire options highly depend on the interviewee's previous answer, which results in the complexity of social survey analysis, the time, and the expertise required. The ability of large language models (LLMs) to perform complex reasoning is well-enhanced by prompting learning such as Chain-of-thought (CoT) but still confined to left-to-right decision-making processes or limited paths during inference. This means they can fall short in problems that require exploration and uncertainty searching. In response, a novel large language model prompting method, called Random Forest of Thoughts (RFoT), is proposed for generating uncertainty reasoning to fit the area of computational social science. The RFoT allows LLMs to perform deliberate decision-making by generating diverse thought space and randomly selecting the sub-thoughts to build the forest of thoughts. It can extend the exploration and prediction of overall performance, benefiting from the extensive research space of response. The method is applied to optimize computational social science analysis on two datasets covering a spectrum of social survey analysis problems. Our experiments show that RFoT significantly enhances language models' abilities on two novel social survey analysis problems requiring non-trivial reasoning.         ",
    "url": "https://arxiv.org/abs/2502.18729",
    "authors": [
      "Xiaohua Wu",
      "Xiaohui Tao",
      "Wenjie Wu",
      "Yuefeng Li",
      "Lin Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.18730",
    "title": "Network dynamics of community resilience and recovery: new frontier in disaster research",
    "abstract": "           Disasters impact communities through interconnected social, spatial, and physical networks. Analyzing network dynamics is crucial for understanding resilience and recovery. We highlight six studies demonstrating how hazards and recovery processes spread through these networks, revealing key phenomena, such as flood exposure, emergent social cohesion, and critical recovery multipliers. This network-centric approach can uncover vulnerabilities, inform interventions, and advance equitable resilience strategies in the face of escalating risks.         ",
    "url": "https://arxiv.org/abs/2502.18730",
    "authors": [
      "Chia-Fu Liu",
      "Ali Mostafavi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2502.18763",
    "title": "CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model",
    "abstract": "           Large Language Models (LLMs) possess human-level cognitive and decision-making capabilities, making them a key technology for 6G. However, applying LLMs to the communication domain faces three major challenges: 1) Inadequate communication data; 2) Restricted input modalities; and 3) Difficulty in knowledge retrieval. To overcome these issues, we propose CommGPT, a multimodal foundation model designed specifically for communications. First, we create high-quality pretraining and fine-tuning datasets tailored in communication, enabling the LLM to engage in further pretraining and fine-tuning with communication concepts and knowledge. Then, we design a multimodal encoder to understand and process information from various input modalities. Next, we construct a Graph and Retrieval-Augmented Generation (GRG) framework, efficiently coupling Knowledge Graph (KG) with Retrieval-Augmented Generation (RAG) for multi-scale learning. Finally, we demonstrate the feasibility and effectiveness of the CommGPT through experimental validation.         ",
    "url": "https://arxiv.org/abs/2502.18763",
    "authors": [
      "Feibo Jiang",
      "Wanyun Zhu",
      "Li Dong",
      "Kezhi Wang",
      "Kun Yang",
      "Cunhua Pan",
      "Octavia A. Dobre"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2502.18771",
    "title": "Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation",
    "abstract": "           Graph-structured data has become increasingly prevalent across various domains, raising the demand for effective models to handle graph tasks like node classification and link prediction. Traditional graph learning models like Graph Neural Networks (GNNs) have made significant strides, but their capabilities in handling graph data remain limited in certain contexts. In recent years, large language models (LLMs) have emerged as promising candidates for graph tasks, yet most studies focus primarily on performance benchmarks and fail to address their broader potential, including their ability to handle limited data, their transferability across tasks, and their robustness. In this work, we provide a comprehensive exploration of LLMs applied to graph tasks. We evaluate the performance of pure LLMs, including those without parameter optimization and those fine-tuned with instructions, across various scenarios. Our analysis goes beyond accuracy, assessing LLM ability to perform in few-shot/zero-shot settings, transfer across domains, understand graph structures, and demonstrate robustness in challenging scenarios. We conduct extensive experiments with 16 graph learning models alongside 6 LLMs (e.g., Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora, PubMed, ArXiv, and Products. Our findings show that LLMs, particularly those with instruction tuning, outperform traditional models in few-shot settings, exhibit strong domain transferability, and demonstrate excellent generalization and robustness. This work offers valuable insights into the capabilities of LLMs for graph learning, highlighting their advantages and potential for real-world applications, and paving the way for future research in this area. Codes and datasets are released in this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18771",
    "authors": [
      "Yuxiang Wang",
      "Xinnan Dai",
      "Wenqi Fan",
      "Yao Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.18793",
    "title": "SolEval: Benchmarking Large Language Models for Repository-level Solidity Code Generation",
    "abstract": "           Large language models (LLMs) have transformed code generation. However, most existing approaches focus on mainstream languages such as Python and Java, neglecting the Solidity language, the predominant programming language for Ethereum smart contracts. Due to the lack of adequate benchmarks for Solidity, LLMs' ability to generate secure, cost-effective smart contracts remains unexplored. To fill this gap, we construct SolEval, the first repository-level benchmark designed for Solidity smart contract generation, to evaluate the performance of LLMs on Solidity. SolEval consists of 1,125 samples from 9 different repositories, covering 6 popular domains, providing LLMs with a comprehensive evaluation benchmark. Unlike the existing Solidity benchmark, SolEval not only includes complex function calls but also reflects the real-world complexity of the Ethereum ecosystem by incorporating gas fee and vulnerability rate. We evaluate 10 LLMs on SolEval, and our results show that the best-performing LLM achieves only 26.29% Pass@10, highlighting substantial room for improvement in Solidity code generation by LLMs. We release our data and code at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18793",
    "authors": [
      "Zhiyuan Peng",
      "Xin Yin",
      "Rui Qian",
      "Peiqin Lin",
      "Yongkang Liu",
      "Chenhao Ying",
      "Yuan Luo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.18807",
    "title": "BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction",
    "abstract": "           Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.4 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 80 chemical systems, 12 operating temperatures, and 646 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in a series of neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18807",
    "authors": [
      "Ruifeng Tan",
      "Weixiang Hong",
      "Jiayue Tang",
      "Xibin Lu",
      "Ruijun Ma",
      "Xiang Zheng",
      "Jia Li",
      "Jiaqiang Huang",
      "Tong-Yi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2502.18810",
    "title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal",
    "abstract": "           In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.         ",
    "url": "https://arxiv.org/abs/2502.18810",
    "authors": [
      "Weipeng Jiang",
      "Juan Zhai",
      "Shiqing Ma",
      "Ziyan Lei",
      "Xiaofei Xie",
      "Yige Wang",
      "Chao Shen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18823",
    "title": "Evidence-Driven Marker Extraction for Social Media Suicide Risk Detection",
    "abstract": "           Early detection of suicide risk from social media text is crucial for timely intervention. While Large Language Models (LLMs) offer promising capabilities in this domain, challenges remain in terms of interpretability and computational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a novel approach for clinical marker extraction and suicide risk classification. ED-LLM employs a multi-task learning framework, jointly training a Mistral-7B based model to identify clinical marker spans and classify suicide risk levels. This evidence-driven strategy enhances interpretability by explicitly highlighting textual evidence supporting risk assessments. Evaluated on the CLPsych datasets, ED-LLM demonstrates competitive performance in risk classification and superior capability in clinical marker span identification compared to baselines including fine-tuned LLMs, traditional machine learning, and prompt-based methods. The results highlight the effectiveness of multi-task learning for interpretable and efficient LLM-based suicide risk assessment, paving the way for clinically relevant applications.         ",
    "url": "https://arxiv.org/abs/2502.18823",
    "authors": [
      "Carter Adams",
      "Caleb Carter",
      "Jackson Simmons"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.18826",
    "title": "Adversarial Combinatorial Semi-bandits with Graph Feedback",
    "abstract": "           In combinatorial semi-bandits, a learner repeatedly selects from a combinatorial decision set of arms, receives the realized sum of rewards, and observes the rewards of the individual selected arms as feedback. In this paper, we extend this framework to include \\emph{graph feedback}, where the learner observes the rewards of all neighboring arms of the selected arms in a feedback graph $G$. We establish that the optimal regret over a time horizon $T$ scales as $\\widetilde{\\Theta}(S\\sqrt{T}+\\sqrt{\\alpha ST})$, where $S$ is the size of the combinatorial decisions and $\\alpha$ is the independence number of $G$. This result interpolates between the known regrets $\\widetilde\\Theta(S\\sqrt{T})$ under full information (i.e., $G$ is complete) and $\\widetilde\\Theta(\\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$ has only self-loops), where $K$ is the total number of arms. A key technical ingredient is to realize a convexified action using a random decision vector with negative correlations.         ",
    "url": "https://arxiv.org/abs/2502.18826",
    "authors": [
      "Yuxiao Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.18847",
    "title": "TabGLM: Tabular Graph Language Model for Learning Transferable Representations Through Multi-Modal Consistency Minimization",
    "abstract": "           Handling heterogeneous data in tabular datasets poses a significant challenge for deep learning models. While attention-based architectures and self-supervised learning have achieved notable success, their application to tabular data remains less effective over linear and tree based models. Although several breakthroughs have been achieved by models which transform tables into uni-modal transformations like image, language and graph, these models often underperform in the presence of feature heterogeneity. To address this gap, we introduce TabGLM (Tabular Graph Language Model), a novel multi-modal architecture designed to model both structural and semantic information from a table. TabGLM transforms each row of a table into a fully connected graph and serialized text, which are then encoded using a graph neural network (GNN) and a text encoder, respectively. By aligning these representations through a joint, multi-modal, self-supervised learning objective, TabGLM leverages complementary information from both modalities, thereby enhancing feature learning. TabGLM's flexible graph-text pipeline efficiently processes heterogeneous datasets with significantly fewer parameters over existing Deep Learning approaches. Evaluations across 25 benchmark datasets demonstrate substantial performance gains, with TabGLM achieving an average AUC-ROC improvement of up to 5.56% over State-of-the-Art (SoTA) tabular learning methods.         ",
    "url": "https://arxiv.org/abs/2502.18847",
    "authors": [
      "Anay Majee",
      "Maria Xenochristou",
      "Wei-Peng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18848",
    "title": "A Causal Lens for Evaluating Faithfulness Metrics",
    "abstract": "           Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present Causal Diagnosticity, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.         ",
    "url": "https://arxiv.org/abs/2502.18848",
    "authors": [
      "Kerem Zaman",
      "Shashank Srivastava"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2502.18851",
    "title": "Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code",
    "abstract": "           Code watermarking identifies AI-generated code by embedding patterns into the code during generation. Effective watermarking requires meeting two key conditions: the watermark should be reliably detectable, and the code should retain its original functionality. However, existing methods often modify tokens that are critical for program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can cause syntax errors or functional failures, limiting the practical use of watermarking. We present STONE, a method that preserves functional integrity by selectively inserting watermarks only into non-syntax tokens. By excluding tokens essential for code execution, STONE minimizes the risk of functional degradation. In addition, we introduce CWEM, a comprehensive evaluation metric that evaluates watermarking techniques based on correctness, detectability, and naturalness. While correctness and detectability have been widely used, naturalness remains underexplored despite its importance. Unnatural patterns can reveal the presence of a watermark, making it easier for adversaries to remove. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. The results show that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java. Our code is available in this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18851",
    "authors": [
      "Jungin Kim",
      "Shinwoo Park",
      "Yo-Sub Han"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18868",
    "title": "Multivariable Generalized Super-Twisting Algorithm Robust Control of Linear Time-Invariant Systems",
    "abstract": "           This paper presents a novel procedure for robust control design of linear time-invariant systems using a Multivariable Generalized Super-Twisting Algorithm (MGSTA). The proposed approach addresses robust stability and performance conditions, considering convex bounded parameter uncertainty in all matrices of the plant state-space realization and Lipschitz exogenous disturbances. The primary characteristic of the closed-loop system, sliding mode finite-time convergence, is thoroughly examined and evaluated. The design conditions, obtained through the proposal of a novel max-type non-differentiable piecewise-continuous Lyapunov function are formulated as Linear Matrix Inequalities (LMIs), which can be efficiently solved using existing computational tools. A fault-tolerant MGSTA control is designed for a mechanical system with three degrees of freedom, illustrating the efficacy of the proposed LMI approach.         ",
    "url": "https://arxiv.org/abs/2502.18868",
    "authors": [
      "J. C. Geromel",
      "E. V. L. Nunes",
      "L. Hsu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.18871",
    "title": "Inscanner: Dual-Phase Detection and Classification of Auxiliary Insulation Using YOLOv8 Models",
    "abstract": "           This study proposes a two-phase methodology for detecting and classifying auxiliary insulation in structural components. In the detection phase, a YOLOv8x model is trained on a dataset of complete structural blueprints, each annotated with bounding boxes indicating areas that should contain insulation. In the classification phase, these detected insulation patches are cropped and categorized into two classes: present or missing. These are then used to train a YOLOv8x-CLS model that determines the presence or absence of auxiliary insulation. Preprocessing steps for both datasets included annotation, augmentation, and appropriate cropping of the insulation regions. The detection model achieved a mean average precision (mAP) score of 82%, while the classification model attained an accuracy of 98%. These findings demonstrate the effectiveness of the proposed approach in automating insulation detection and classification, providing a foundation for further advancements in this domain.         ",
    "url": "https://arxiv.org/abs/2502.18871",
    "authors": [
      "Youngtae Kim",
      "Soonju Jeong",
      "Sardar Arslan",
      "Dhananjay Agnihotri",
      "Yahya Ahmed",
      "Ali Nawaz",
      "Jinhee Song",
      "Hyewon Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18873",
    "title": "Multi-LLM Collaborative Search for Complex Problem Solving",
    "abstract": "           Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.         ",
    "url": "https://arxiv.org/abs/2502.18873",
    "authors": [
      "Sen Yang",
      "Yafu Li",
      "Wai Lam",
      "Yu Cheng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.18874",
    "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework",
    "abstract": "           Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.         ",
    "url": "https://arxiv.org/abs/2502.18874",
    "authors": [
      "Kaishuai Xu",
      "Tiezheng Yu",
      "Wenjun Hou",
      "Yi Cheng",
      "Liangyou Li",
      "Xin Jiang",
      "Lifeng Shang",
      "Qun Liu",
      "Wenjie Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18883",
    "title": "Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection",
    "abstract": "           Numerous machine learning (ML) models have been developed, including those for software engineering (SE) tasks, under the assumption that training and testing data come from the same distribution. However, training and testing distributions often differ, as training datasets rarely encompass the entire distribution, while testing distribution tends to shift over time. Hence, when confronted with out-of-distribution (OOD) instances that differ from the training data, a reliable and trustworthy SE ML model must be capable of detecting them to either abstain from making predictions, or potentially forward these OODs to appropriate models handling other categories or tasks. In this paper, we develop two types of SE-specific OOD detection models, unsupervised and weakly-supervised OOD detection for code. The unsupervised OOD detection approach is trained solely on in-distribution samples while the weakly-supervised approach utilizes a tiny number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting OOD samples from four different scenarios simultaneously and also positively impact a main code understanding task.         ",
    "url": "https://arxiv.org/abs/2502.18883",
    "authors": [
      "Yanfu Yan",
      "Viet Duong",
      "Huajie Shao",
      "Denys Poshyvanyk"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.18891",
    "title": "Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance",
    "abstract": "           In this paper, we propose an innovative dynamic classification algorithm designed to achieve the objective of zero missed detections and minimal false positives. The algorithm partitions the data into N equivalent training subsets and N prediction subsets using a supervised model, followed by independent predictions from N separate predictive models. This enables each predictive model to operate within a smaller data range, thereby improving overall accuracy. Additionally, the algorithm leverages data generated through supervised learning to further refine prediction results, filtering out predictions that do not meet accuracy requirements without the need to introduce additional models. Experimental results demonstrate that, when data partitioning errors are minimal, the dynamic classification algorithm achieves exceptional performance with zero missed detections and minimal false positives, significantly outperforming existing model ensembles. Even in cases where classification errors are larger, the algorithm remains comparable to state of the art models. The key innovations of this study include self-supervised classification learning, the use of small-range subset predictions, and the direct rejection of substandard predictions. While the current algorithm still has room for improvement in terms of automatic parameter tuning and classification model efficiency, it has demonstrated outstanding performance across multiple datasets. Future research will focus on optimizing the classification component to further enhance the algorithm's robustness and adaptability.         ",
    "url": "https://arxiv.org/abs/2502.18891",
    "authors": [
      "Ziyuan Zhong",
      "Junyang Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18905",
    "title": "Automated Code Generation and Validation for Software Components of Microcontrollers",
    "abstract": "           This paper proposes a method for generating software components for embedded systems, integrating seamlessly into existing implementations without developer intervention. We demonstrate this by automatically generating hardware abstraction layer (HAL) code for GPIO operations on the STM32F407 microcontroller. Using Abstract Syntax Trees (AST) for code analysis and Retrieval-Augmented Generation (RAG) for component generation, our approach enables autonomous code completion for embedded applications.         ",
    "url": "https://arxiv.org/abs/2502.18905",
    "authors": [
      "Sebastian Haug",
      "Christoph B\u00f6hm",
      "Daniel Mayer"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18909",
    "title": "A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced Network Traffic",
    "abstract": "           Network Traffic Classification (NTC) is one of the most important tasks in network management. The imbalanced nature of classes on the internet presents a critical challenge in classification tasks. For example, some classes of applications are much more prevalent than others, such as HTTP. As a result, machine learning classification models do not perform well on those classes with fewer data. To address this problem, we propose a pipeline to balance the dataset and classify it using a robust and accurate embedding technique. First, we generate artificial data using Long Short-Term Memory (LSTM) networks and Kernel Density Estimation (KDE). Next, we propose replacing one-hot encoding for categorical features with a novel embedding framework based on the \"Flow as a Sentence\" perspective, which we name FS-Embedding. This framework treats the source and destination ports, along with the packet's direction, as one word in a flow, then trains an embedding vector space based on these new features through the learning classification task. Finally, we compare our pipeline with the training of a Convolutional Recurrent Neural Network (CRNN) and Transformers, both with imbalanced and sampled datasets, as well as with the one-hot encoding approach. We demonstrate that the proposed augmentation pipeline, combined with FS-Embedding, increases convergence speed and leads to a significant reduction in the number of model parameters, all while maintaining the same performance in terms of accuracy.         ",
    "url": "https://arxiv.org/abs/2502.18909",
    "authors": [
      "Matin Shokri",
      "Ramin Hasibi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18932",
    "title": "SLAM in the Dark: Self-Supervised Learning of Pose, Depth and Loop-Closure from Thermal Images",
    "abstract": "           Visual SLAM is essential for mobile robots, drone navigation, and VR/AR, but traditional RGB camera systems struggle in low-light conditions, driving interest in thermal SLAM, which excels in such environments. However, thermal imaging faces challenges like low contrast, high noise, and limited large-scale annotated datasets, restricting the use of deep learning in outdoor scenarios. We present DarkSLAM, a noval deep learning-based monocular thermal SLAM system designed for large-scale localization and reconstruction in complex lighting this http URL approach incorporates the Efficient Channel Attention (ECA) mechanism in visual odometry and the Selective Kernel Attention (SKA) mechanism in depth estimation to enhance pose accuracy and mitigate thermal depth degradation. Additionally, the system includes thermal depth-based loop closure detection and pose optimization, ensuring robust performance in low-texture thermal scenes. Extensive outdoor experiments demonstrate that DarkSLAM significantly outperforms existing methods like SC-Sfm-Learner and Shin et al., delivering precise localization and 3D dense mapping even in challenging nighttime environments.         ",
    "url": "https://arxiv.org/abs/2502.18932",
    "authors": [
      "Yangfan Xu",
      "Qu Hao",
      "Lilian Zhang",
      "Jun Mao",
      "Xiaofeng He",
      "Wenqi Wu",
      "Changhao Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.18939",
    "title": "A Novel Topology Recovery Method for Low Voltage Distribution Networks",
    "abstract": "           Low voltage distribution networks (LVDNs) suffer from limited visibility due to sparse or nonexistent measurement systems, leaving distribution network service providers with incomplete data. Maintenance activities, such as transformer upgrades and power line replacements, sometimes go undocumented, leading to unmonitored topology changes. This lack of oversight hinders network optimization, fault detection, and outage management, as utilities cannot fully monitor or control the system. With the rise of electric vehicles, having an accurate understanding of LVDN topology is crucial to avoid infrastructure damage from potential overloads. This paper introduces a method to reconstruct LVDN topology using incremental voltage and current changes from smart meters at customer endpoints. The approach identifies and maps network topologies with high accuracy, overcoming limitations of prior methods by discarding unrealistic assumptions. Specifically, it addresses grids with fewer than three pole connections and employs an AC power flow model over simplified DC approximations. Simulations across diverse configurations validate the method's effectiveness in accurately reconstructing LVDN topologies, enhancing real-world applicability.         ",
    "url": "https://arxiv.org/abs/2502.18939",
    "authors": [
      "Sina Mohammadi",
      "Van-Hai Bui",
      "Wencong Su"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.18943",
    "title": "Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models",
    "abstract": "           Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\\ie, \\textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences. To alleviate these problems, we propose \\textbf{PETAL}: a label-only membership inference attack based on \\textbf{PE}r-\\textbf{T}oken sem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.         ",
    "url": "https://arxiv.org/abs/2502.18943",
    "authors": [
      "Yu He",
      "Boheng Li",
      "Liu Liu",
      "Zhongjie Ba",
      "Wei Dong",
      "Yiming Li",
      "Zhan Qin",
      "Kui Ren",
      "Chun Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.18948",
    "title": "Switching multiplicative watermark design against covert attacks",
    "abstract": "           Active techniques have been introduced to give better detectability performance for cyber-attack diagnosis in cyber-physical systems (CPS). In this paper, switching multiplicative watermarking is considered, whereby we propose an optimal design strategy to define switching filter parameters. Optimality is evaluated exploiting the so-called output-to-output gain of the closed loop system, including some supposed attack dynamics. A worst-case scenario of a matched covert attack is assumed, presuming that an attacker with full knowledge of the closed-loop system injects a stealthy attack of bounded energy. Our algorithm, given watermark filter parameters at some time instant, provides optimal next-step parameters. Analysis of the algorithm is given, demonstrating its features, and demonstrating that through initialization of certain parameters outside of the algorithm, the parameters of the multiplicative watermarking can be randomized. Simulation shows how, by adopting our method for parameter design, the attacker's impact on performance diminishes.         ",
    "url": "https://arxiv.org/abs/2502.18948",
    "authors": [
      "Alexander J. Gallo",
      "Sribalaji C. Anand",
      "Andr\u00e9 M. H. Teixeira",
      "Riccardo M. G. Ferrari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.18953",
    "title": "A Reliable, Time-Predictable Heterogeneous SoC for AI-Enhanced Mixed-Criticality Edge Applications",
    "abstract": "           Next-generation mixed-criticality Systems-on-chip (SoCs) for robotics, automotive, and space must execute mixed-criticality AI-enhanced sensor processing and control workloads, ensuring reliable and time-predictable execution of critical tasks sharing resources with non-critical tasks, while also fitting within a sub-2W power envelope. To tackle these multi-dimensional challenges, in this brief, we present a 16nm, reliable, time-predictable heterogeneous SoC with multiple programmable accelerators. Within a 1.2W power envelope, the SoC integrates software-configurable hardware IPs to ensure predictable access to shared resources, such as the on-chip interconnect and memory system, leading to tight upper bounds on execution times of critical applications. To accelerate mixed-precision mission-critical AI, the SoC integrates a reliable multi-core accelerator achieving 304.9 GOPS peak performance at 1.6 TOPS/W energy efficiency. Non-critical, compute-intensive, floating-point workloads are accelerated by a dual-core vector cluster, achieving 121.8 GFLOPS at 1.1 TFLOPS/W and 106.8 GFLOPS/mm2.         ",
    "url": "https://arxiv.org/abs/2502.18953",
    "authors": [
      "Angelo Garofalo",
      "Alessandro Ottaviano",
      "Matteo Perotti",
      "Thomas Benz",
      "Yvan Tortorella",
      "Robert Balas",
      "Michael Rogenmoser",
      "Chi Zhang",
      "Luca Bertaccini",
      "Nils Wistoff",
      "Maicol Ciani",
      "Cyril Koenig",
      "Mattia Sinigaglia",
      "Luca Valente",
      "Paul Scheffler",
      "Manuel Eggimann",
      "Matheus Cavalcante",
      "Francesco Restuccia",
      "Alessandro Biondi",
      "Francesco Conti",
      "Frank K. Gurkaynak",
      "Davide Rossi",
      "Luca Benini"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2502.18959",
    "title": "Fourier Multi-Component and Multi-Layer Neural Networks: Unlocking High-Frequency Potential",
    "abstract": "           The two most critical ingredients of a neural network are its structure and the activation function employed, and more importantly, the proper alignment of these two that is conducive to the effective representation and learning in practice. In this work, we introduce a surprisingly effective synergy, termed the Fourier Multi-Component and Multi-Layer Neural Network (FMMNN), and demonstrate its surprising adaptability and efficiency in capturing high-frequency components. First, we theoretically establish that FMMNNs have exponential expressive power in terms of approximation capacity. Next, we analyze the optimization landscape of FMMNNs and show that it is significantly more favorable compared to fully connected neural networks. Finally, systematic and extensive numerical experiments validate our findings, demonstrating that FMMNNs consistently achieve superior accuracy and efficiency across various tasks, particularly impressive when high-frequency components are present.         ",
    "url": "https://arxiv.org/abs/2502.18959",
    "authors": [
      "Shijun Zhang",
      "Hongkai Zhao",
      "Yimin Zhong",
      "Haomin Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.18960",
    "title": "Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data Combination",
    "abstract": "           Long-term causal inference has drawn increasing attention in many scientific domains. Existing methods mainly focus on estimating average long-term causal effects by combining long-term observational data and short-term experimental data. However, it is still understudied how to robustly and effectively estimate heterogeneous long-term causal effects, significantly limiting practical applications. In this paper, we propose several two-stage style nonparametric estimators for heterogeneous long-term causal effect estimation, including propensity-based, regression-based, and multiple robust estimators. We conduct a comprehensive theoretical analysis of their asymptotic properties under mild assumptions, with the ultimate goal of building a better understanding of the conditions under which some estimators can be expected to perform better. Extensive experiments across several semi-synthetic and real-world datasets validate the theoretical results and demonstrate the effectiveness of the proposed estimators.         ",
    "url": "https://arxiv.org/abs/2502.18960",
    "authors": [
      "Weilin Chen",
      "Ruichu Cai",
      "Junjie Wan",
      "Zeqin Yang",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18961",
    "title": "Credible Intervals for Knowledge Graph Accuracy Estimation",
    "abstract": "           Knowledge Graphs (KGs) are widely used in data-driven applications and downstream tasks, such as virtual assistants, recommendation systems, and semantic search. The accuracy of KGs directly impacts the reliability of the inferred knowledge and outcomes. Therefore, assessing the accuracy of a KG is essential for ensuring the quality of facts used in these tasks. However, the large size of real-world KGs makes manual triple-by-triple annotation impractical, thereby requiring sampling strategies to provide accuracy estimates with statistical guarantees. The current state-of-the-art approaches rely on Confidence Intervals (CIs), derived from frequentist statistics. While efficient, CIs have notable limitations and can lead to interpretation fallacies. In this paper, we propose to overcome the limitations of CIs by using \\emph{Credible Intervals} (CrIs), which are grounded in Bayesian statistics. These intervals are more suitable for reliable post-data inference, particularly in KG accuracy evaluation. We prove that CrIs offer greater reliability and stronger guarantees than frequentist approaches in this context. Additionally, we introduce \\emph{a}HPD, an adaptive algorithm that is more efficient for real-world KGs and statistically robust, addressing the interpretive challenges of CIs.         ",
    "url": "https://arxiv.org/abs/2502.18961",
    "authors": [
      "Stefano Marchesin",
      "Gianmaria Silvello"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2502.18974",
    "title": "Distributed Transition System with Tags and Value-wise Metric, for Privacy Analysis",
    "abstract": "           We introduce a logical framework named Distributed Labeled Tagged Transition System (DLTTS), using concepts from Probabilistic Automata, Probabilistic Concurrent Systems, and Probabilistic labelled transition systems. We show that DLTTS can be used to formally model how a given piece of private information P (e.g., a set of tuples) stored in a given database D can get captured progressively by an adversary A repeatedly querying D, enhancing the knowledge acquired from the answers to these queries with relational deductions using certain additional non-private data. The database D is assumed protected with generalization mechanisms. We also show that, on a large class of databases, metrics can be defined 'value-wise', and more general notions of adjacency between data bases can be defined, based on these metrics. These notions can also play a role in differentially private protection mechanisms.         ",
    "url": "https://arxiv.org/abs/2502.18974",
    "authors": [
      "Siva Anantharaman",
      "Sabine Frittella",
      "Benjamin Nguyen"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2502.18975",
    "title": "Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks",
    "abstract": "           Out-of-distribution generalization of machine learning models remains challenging since the models are inherently bound to the training data distribution. This especially manifests, when the learned models rely on spurious correlations. Most of the existing approaches apply data manipulation, representation learning, or learning strategies to achieve generalizable models. Unfortunately, these approaches usually require multiple training domains, group labels, specialized augmentation, or pre-processing to reach generalizable models. We propose a novel approach that addresses these limitations by providing a technique to guide the neural network through the training phase. We first establish input pairs, representing the spurious attribute and describing the invariance, a characteristic that should not affect the outcome of the model. Based on these pairs, we form a corrective gradient complementing the traditional gradient descent approach. We further make this correction mechanism adaptive based on a predefined invariance condition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasets demonstrate the effectiveness of our approach and the robustness to group shifts.         ",
    "url": "https://arxiv.org/abs/2502.18975",
    "authors": [
      "Martin Surner",
      "Abdelmajid Khelil",
      "Ludwig Bothmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18981",
    "title": "Polarization Angle Scanning for Wide-band Millimeter-wave Direct Detection",
    "abstract": "           Millimeter-wave (MMW) technology has been widely utilized in human security screening applications due to its superior penetration capabilities through clothing and safety for human exposure. However, existing methods largely rely on fixed polarization modes, neglecting the potential insights from variations in target echoes with respect to incident polarization. This study provides a theoretical analysis of the cross-polarization echo power as a function of the incident polarization angle under linear polarization conditions. Additionally, based on the transmission characteristics of multi-layer medium, we extended the depth spectrum model employed in direct detection to accommodate scenarios involving multi-layered structures. Building on this foundation, by obtaining multiple depth spectrums through polarization angle scanning, we propose the Polarization Angle-Depth Matrix to characterize target across both the polarization angle and depth dimensions in direct detection. Simulations and experimental validations confirm its accuracy and practical value in detecting concealed weapons in human security screening scenarios.         ",
    "url": "https://arxiv.org/abs/2502.18981",
    "authors": [
      "Heyao Wang",
      "Ziran Zhao",
      "Lingbo Qiao",
      "Dalu Guo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.18986",
    "title": "Evaluating Membership Inference Attacks in heterogeneous-data setups",
    "abstract": "           Among all privacy attacks against Machine Learning (ML), membership inference attacks (MIA) attracted the most attention. In these attacks, the attacker is given an ML model and a data point, and they must infer whether the data point was used for training. The attacker also has an auxiliary dataset to tune their inference algorithm. Attack papers commonly simulate setups in which the attacker's and the target's datasets are sampled from the same distribution. This setting is convenient to perform experiments, but it rarely holds in practice. ML literature commonly starts with similar simplifying assumptions (i.e., \"i.i.d.\" datasets), and later generalizes the results to support heterogeneous data distributions. Similarly, our work makes a first step in the generalization of the MIA evaluation to heterogeneous data. First, we design a metric to measure the heterogeneity between any pair of tabular data distributions. This metric provides a continuous scale to analyze the phenomenon. Second, we compare two methodologies to simulate a data heterogeneity between the target and the attacker. These setups provide opposite performances: 90% attack accuracy vs. 50% (i.e., random guessing). Our results show that the MIA accuracy depends on the experimental setup; and even if research on MIA considers heterogeneous data setups, we have no standardized baseline of how to simulate it. The lack of such a baseline for MIA experiments poses a significant challenge to risk assessments in real-world machine learning scenarios.         ",
    "url": "https://arxiv.org/abs/2502.18986",
    "authors": [
      "Bram van Dartel",
      "Marc Damie",
      "Florian Hahn"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18992",
    "title": "OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models",
    "abstract": "           Biomedical ontologies, which comprehensively define concepts and relations for biomedical entities, are crucial for structuring and formalizing domain-specific information representations. Biomedical code mapping identifies similarity or equivalence between concepts from different ontologies. Obtaining high-quality mapping usually relies on automatic generation of unrefined mapping with ontology domain fine-tuned language models (LMs), followed by manual selections or corrections by coding experts who have extensive domain expertise and familiarity with ontology schemas. The LMs usually provide unrefined code mapping suggestions as a list of candidates without reasoning or supporting evidence, hence coding experts still need to verify each suggested candidate against ontology sources to pick the best matches. This is also a recurring task as ontology sources are updated regularly to incorporate new research findings. Consequently, the need of regular LM retraining and manual refinement make code mapping time-consuming and labour intensive. In this work, we created OntologyRAG, an ontology-enhanced retrieval-augmented generation (RAG) method that leverages the inductive biases from ontological knowledge graphs for in-context-learning (ICL) in large language models (LLMs). Our solution grounds LLMs to knowledge graphs with unrefined mappings between ontologies and processes questions by generating an interpretable set of results that include prediction rational with mapping proximity assessment. Our solution doesn't require re-training LMs, as all ontology updates could be reflected by updating the knowledge graphs with a standard process. Evaluation results on a self-curated gold dataset show promises of using our method to enable coding experts to achieve better and faster code mapping. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.18992",
    "authors": [
      "Hui Feng",
      "Yuntzu Yin",
      "Emiliano Reynares",
      "Jay Nanavati"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.18994",
    "title": "Long-term Causal Inference via Modeling Sequential Latent Confounding",
    "abstract": "           Long-term causal inference is an important but challenging problem across various scientific domains. To solve the latent confounding problem in long-term observational studies, existing methods leverage short-term experimental data. Ghassami et al. propose an approach based on the Conditional Additive Equi-Confounding Bias (CAECB) assumption, which asserts that the confounding bias in the short-term outcome is equal to that in the long-term outcome, so that the long-term confounding bias and the causal effects can be identified. While effective in certain cases, this assumption is limited to scenarios with a one-dimensional short-term outcome. In this paper, we introduce a novel assumption that extends the CAECB assumption to accommodate temporal short-term outcomes. Our proposed assumption states a functional relationship between sequential confounding biases across temporal short-term outcomes, under which we theoretically establish the identification of long-term causal effects. Based on the identification result, we develop an estimator and conduct a theoretical analysis of its asymptotic properties. Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2502.18994",
    "authors": [
      "Weilin Chen",
      "Ruichu Cai",
      "Yuguang Yan",
      "Zhifeng Hao",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19008",
    "title": "Binary Neural Networks for Large Language Model: A Survey",
    "abstract": "           Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.         ",
    "url": "https://arxiv.org/abs/2502.19008",
    "authors": [
      "Liangdong Liu",
      "Zhitong Zheng",
      "Cong Wang",
      "Tianhuang Su",
      "Zhenyu Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.19041",
    "title": "Beyond Surface-Level Patterns: An Essence-Driven Defense Framework Against Jailbreak Attacks in LLMs",
    "abstract": "           Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks. Unfortunately, existing methods often focus on surface-level patterns, overlooking the deeper attack essences. As a result, defenses fail when attack prompts change, even though the underlying \"attack essence\" remains the same. To address this issue, we introduce EDDF, an \\textbf{E}ssence-\\textbf{D}riven \\textbf{D}efense \\textbf{F}ramework Against Jailbreak Attacks in LLMs. EDDF is a plug-and-play input-filtering method and operates in two stages: 1) offline essence database construction, and 2) online adversarial query detection. The key idea behind EDDF is to extract the \"attack essence\" from a diverse set of known attack instances and store it in an offline vector database. Experimental results demonstrate that EDDF significantly outperforms existing methods by reducing the Attack Success Rate by at least 20\\%, underscoring its superior robustness against jailbreak attacks.         ",
    "url": "https://arxiv.org/abs/2502.19041",
    "authors": [
      "Shiyu Xiang",
      "Ansen Zhang",
      "Yanfei Cao",
      "Yang Fan",
      "Ronghao Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.19042",
    "title": "A HEART for the environment: Transformer-Based Spatiotemporal Modeling for Air Quality Prediction",
    "abstract": "           Accurate and reliable air pollution forecasting is crucial for effective environmental management and policy-making. llull-environment is a sophisticated and scalable forecasting system for air pollution, inspired by previous models currently operational in Madrid and Valladolid (Spain). It contains (among other key components) an encoder-decoder convolutional neural network to forecast mean pollution levels for four key pollutants (NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$) using historical data, external forecasts, and other contextual features. This paper investigates the augmentation of this neural network with an attention mechanism to improve predictive accuracy. The proposed attention mechanism pre-processes tensors containing the input features before passing them to the existing mean forecasting model. The resulting model is a combination of several architectures and ideas and can be described as a \"Hybrid Enhanced Autoregressive Transformer\", or HEART. The effectiveness of the approach is evaluated by comparing the mean square error (MSE) across different attention layouts against the system without such a mechanism. We observe a significant reduction in MSE of up to 22%, with an average of 7.5% across tested cities and pollutants. The performance of a given attention mechanism turns out to depend on the pollutant, highlighting the differences in their creation and dissipation processes. Our findings are not restricted to optimizing air quality prediction models, but are applicable generally to (fixed length) time series forecasting.         ",
    "url": "https://arxiv.org/abs/2502.19042",
    "authors": [
      "Norbert Bodendorfer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19047",
    "title": "A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification in Diffusion Models",
    "abstract": "           Diffusion models have emerged as state-of-the-art generative frameworks, excelling in producing high-quality multi-modal samples. However, recent studies have revealed their vulnerability to backdoor attacks, where backdoored models generate specific, undesirable outputs called backdoor target (e.g., harmful images) when a pre-defined trigger is embedded to their inputs. In this paper, we propose PureDiffusion, a dual-purpose framework that simultaneously serves two contrasting roles: backdoor defense and backdoor attack amplification. For defense, we introduce two novel loss functions to invert backdoor triggers embedded in diffusion models. The first leverages trigger-induced distribution shifts across multiple timesteps of the diffusion process, while the second exploits the denoising consistency effect when a backdoor is activated. Once an accurate trigger inversion is achieved, we develop a backdoor detection method that analyzes both the inverted trigger and the generated backdoor targets to identify backdoor attacks. In terms of attack amplification with the role of an attacker, we describe how our trigger inversion algorithm can be used to reinforce the original trigger embedded in the backdoored diffusion model. This significantly boosts attack performance while reducing the required backdoor training time. Experimental results demonstrate that PureDiffusion achieves near-perfect detection accuracy, outperforming existing defenses by a large margin, particularly against complex trigger patterns. Additionally, in an attack scenario, our attack amplification approach elevates the attack success rate (ASR) of existing backdoor attacks to nearly 100\\% while reducing training time by up to 20x.         ",
    "url": "https://arxiv.org/abs/2502.19047",
    "authors": [
      "Vu Tuan Truong Long",
      "Bao Le"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19048",
    "title": "An Improved 3D Skeletons UP-Fall Dataset: Enhancing Data Quality for Efficient Impact Fall Detection",
    "abstract": "           Detecting impact where an individual makes contact with the ground within a fall event is crucial in fall detection systems, particularly for elderly care where prompt intervention can prevent serious injuries. The UP-Fall dataset, a key resource in fall detection research, has proven valuable but suffers from limitations in data accuracy and comprehensiveness. These limitations cause confusion in distinguishing between non-impact events, such as sliding, and real falls with impact, where the person actually hits the ground. This confusion compromises the effectiveness of current fall detection systems. This study presents enhancements to the UP-Fall dataset aiming at improving it for impact fall detection by incorporating 3D skeleton data. Our preprocessing techniques ensure high data accuracy and comprehensiveness, enabling a more reliable impact fall detection. Extensive experiments were conducted using various machine learning and deep learning algorithms to benchmark the improved 3D skeletons dataset. The results demonstrate substantial improvements in the performance of fall detection models trained on the enhanced dataset. This contribution aims to enhance the safety and well-being of the elderly population at risk. To support further research and development of building more reliable impact fall detection systems, we have made the improved 3D skeletons UP-Fall dataset publicly available at this link this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19048",
    "authors": [
      "Tresor Y. Koffi",
      "Youssef Mourchid",
      "Mohammed Hindawi",
      "Yohan Dupuis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19067",
    "title": "IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across Indic Languages",
    "abstract": "           Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation from natural language prompts, revolutionizing software development workflows. As we advance towards agent-based development paradigms, these models form the cornerstone of next-generation software development lifecycles. However, current benchmarks for evaluating multilingual code generation capabilities are predominantly English-centric, limiting their applicability across the global developer community. To address this limitation, we present IndicEval-XL, a comprehensive benchmark for code generation that incorporates 6 major Indic languages, collectively spoken by approximately 14\\% of the world's population. Our benchmark bridges these languages with 12 programming languages, creating a robust evaluation framework. This work is particularly significant given India's representation of one-eighth of the global population and the crucial role Indic languages play in Indian society. IndicEval-XL represents a significant step toward expanding the linguistic diversity in code generation systems and evaluation frameworks. By developing resources that support multiple languages, we aim to make AI-powered development tools more inclusive and accessible to developers of various linguistic backgrounds. To facilitate further research and development in this direction, we make our dataset and evaluation benchmark publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2502.19067",
    "authors": [
      "Ujjwal Singh",
      "Aditi Sharma",
      "Nikhil Gupta",
      "Deepakshi",
      "Vivek Kumar Jha"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19068",
    "title": "Dynamic Degradation Decomposition Network for All-in-One Image Restoration",
    "abstract": "           Currently, restoring clean images from a variety of degradation types using a single model is still a challenging task. Existing all-in-one image restoration approaches struggle with addressing complex and ambiguously defined degradation types. In this paper, we introduce a dynamic degradation decomposition network for all-in-one image restoration, named D$^3$Net. D$^3$Net achieves degradation-adaptive image restoration with guided prompt through cross-domain interaction and dynamic degradation decomposition. Concretely, in D$^3$Net, the proposed Cross-Domain Degradation Analyzer (CDDA) engages in deep interaction between frequency domain degradation characteristics and spatial domain image features to identify and model variations of different degradation types on the image manifold, generating degradation correction prompt and strategy prompt, which guide the following decomposition process. Furthermore, the prompt-based Dynamic Decomposition Mechanism (DDM) for progressive degradation decomposition, that encourages the network to adaptively select restoration strategies utilizing the two-level prompt generated by CDDA. Thanks to the synergistic cooperation between CDDA and DDM, D$^3$Net achieves superior flexibility and scalability in handling unknown degradation, while effectively reducing unnecessary computational overhead. Extensive experiments on multiple image restoration tasks demonstrate that D$^3$Net significantly outperforms the state-of-the-art approaches, especially improving PSNR by 5.47dB and 3.30dB on the SOTS-Outdoor and GoPro datasets, respectively.         ",
    "url": "https://arxiv.org/abs/2502.19068",
    "authors": [
      "Huiqiang Wang",
      "Mingchen Song",
      "Guoqiang Zhong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19070",
    "title": "A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks",
    "abstract": "           Model Inversion (MI) attacks, which reconstruct the training dataset of neural networks, pose significant privacy concerns in machine learning. Recent MI attacks have managed to reconstruct realistic label-level private data, such as the general appearance of a target person from all training images labeled on him. Beyond label-level privacy, in this paper we show sample-level privacy, the private information of a single target sample, is also important but under-explored in the MI literature due to the limitations of existing evaluation metrics. To address this gap, this study introduces a novel metric tailored for training-sample analysis, namely, the Diversity and Distance Composite Score (DDCS), which evaluates the reconstruction fidelity of each training sample by encompassing various MI attack attributes. This, in turn, enhances the precision of sample-level privacy assessments. Leveraging DDCS as a new evaluative lens, we observe that many training samples remain resilient against even the most advanced MI attack. As such, we further propose a transfer learning framework that augments the generative capabilities of MI attackers through the integration of entropy loss and natural gradient descent. Extensive experiments verify the effectiveness of our framework on improving state-of-the-art MI attacks over various metrics including DDCS, coverage and FID. Finally, we demonstrate that DDCS can also be useful for MI defense, by identifying samples susceptible to MI attacks in an unsupervised manner.         ",
    "url": "https://arxiv.org/abs/2502.19070",
    "authors": [
      "Haoyang Li",
      "Li Bai",
      "Qingqing Ye",
      "Haibo Hu",
      "Yaxin Xiao",
      "Huadi Zheng",
      "Jianliang Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.19082",
    "title": "Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation",
    "abstract": "           Through a three-part co-design study involving 19 teens aged 13-18, we identify key barriers to effective boundary regulation on social media, including ambiguous audience expectations, social risks associated with oversharing, and the lack of design affordances that facilitate trust-building. Our findings reveal that while adolescents seek casual, frequent sharing to strengthen relationships, existing platform norms and designs often discourage such interactions, leading to withdrawal. To address these challenges, we introduce trust-enabled privacy as a design framework that recognizes trust, whether building or eroding, as central to boundary regulation. When trust is supported, boundary regulation becomes more adaptive and empowering; when it erodes, users default to self-censorship or withdrawal. We propose concrete design affordances, including guided disclosure, contextual audience segmentation, intentional engagement signaling, and trust-centered norms, to help platforms foster a more dynamic and nuanced privacy experience for teen social media users. By reframing privacy as a trust-driven process rather than a rigid control-based trade-off, this work provides empirical insights and actionable guidelines for designing social media environments that empower teens to manage their online presence while fostering meaningful social connections.         ",
    "url": "https://arxiv.org/abs/2502.19082",
    "authors": [
      "JaeWon Kim",
      "Robert Wolfe",
      "Ramya Bhagirathi Subramanian",
      "Mei-Hsuan Lee",
      "Jessica Colnago",
      "Alexis Hiniker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.19091",
    "title": "Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation",
    "abstract": "           Recent advancements in Large Language Models (LLMs) have substantially evolved Multi-Agent Systems (MASs) capabilities, enabling systems that not only automate tasks but also leverage near-human reasoning capabilities. To achieve this, LLM-based MASs need to be built around two critical principles: (i) a robust architecture that fully exploits LLM potential for specific tasks -- or related task sets -- and ($ii$) an effective methodology for equipping LLMs with the necessary capabilities to perform tasks and manage information efficiently. It goes without saying that a priori architectural designs can limit the scalability and domain adaptability of a given MAS. To address these challenges, in this paper we introduce Nexus: a lightweight Python framework designed to easily build and manage LLM-based MASs. Nexus introduces the following innovations: (i) a flexible multi-supervisor hierarchy, (ii) a simplified workflow design, and (iii) easy installation and open-source flexibility: Nexus can be installed via pip and is distributed under a permissive open-source license, allowing users to freely modify and extend its capabilities. Experimental results demonstrate that architectures built with Nexus exhibit state-of-the-art performance across diverse domains. In coding tasks, Nexus-driven MASs achieve a 99% pass rate on HumanEval and a flawless 100% on VerilogEval-Human, outperforming cutting-edge reasoning language models such as o3-mini and DeepSeek-R1. Moreover, these architectures display robust proficiency in complex reasoning and mathematical problem solving, achieving correct solutions for all randomly selected problems from the MATH dataset. In the realm of multi-objective optimization, Nexus-based architectures successfully address challenging timing closure tasks on designs from the VTR benchmark suite, while guaranteeing, on average, a power saving of nearly 30%.         ",
    "url": "https://arxiv.org/abs/2502.19091",
    "authors": [
      "Humza Sami",
      "Mubashir ul Islam",
      "Samy Charas",
      "Asav Gandhi",
      "Pierre-Emmanuel Gaillardon",
      "Valerio Tenace"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.19095",
    "title": "XSS Adversarial Attacks Based on Deep Reinforcement Learning: A Replication and Extension Study",
    "abstract": "           Cross-site scripting (XSS) poses a significant threat to web application security. While Deep Learning (DL) has shown remarkable success in detecting XSS attacks, it remains vulnerable to adversarial attacks due to the discontinuous nature of its input-output mapping. These adversarial attacks employ mutation-based strategies for different components of XSS attack vectors, allowing adversarial agents to iteratively select mutations to evade detection. Our work replicates a state-of-the-art XSS adversarial attack, highlighting threats to validity in the reference work and extending it toward a more effective evaluation strategy. Moreover, we introduce an XSS Oracle to mitigate these threats. The experimental results show that our approach achieves an escape rate above 96% when the threats to validity of the replicated technique are addressed.         ",
    "url": "https://arxiv.org/abs/2502.19095",
    "authors": [
      "Samuele Pasini",
      "Gianluca Maragliano",
      "Jinhan Kim",
      "Paolo Tonella"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.19097",
    "title": "Software demodulation of weak radio signals using convolutional neural network",
    "abstract": "           In this paper we proposed the use of JT65A radio communication protocol for data exchange in wide-area monitoring systems in electric power systems. We investigated the software demodulation of the multiple frequency shift keying weak signals transmitted with JT65A communication protocol using deep convolutional neural network. We presented the demodulation performance in form of symbol and bit error rates. We focused on the interference immunity of the protocol over an additive white Gaussian noise with average signal-to-noise ratios in the range from -30 dB to 0 dB, which was obtained for the first time. We proved that the interference immunity is about 1.5 dB less than the theoretical limit of non-coherent demodulation of orthogonal MFSK signals.         ",
    "url": "https://arxiv.org/abs/2502.19097",
    "authors": [
      "Mykola Kozlenko",
      "Ihor Lazarovych",
      "Valerii Tkachuk",
      "Vira Vialkova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.19106",
    "title": "A Survey on Foundation-Model-Based Industrial Defect Detection",
    "abstract": "           As industrial products become abundant and sophisticated, visual industrial defect detection receives much attention, including two-dimensional and three-dimensional visual feature modeling. Traditional methods use statistical analysis, abnormal data synthesis modeling, and generation-based models to separate product defect features and complete defect detection. Recently, the emergence of foundation models has brought visual and textual semantic prior knowledge. Many methods are based on foundation models (FM) to improve the accuracy of detection, but at the same time, increase model complexity and slow down inference speed. Some FM-based methods have begun to explore lightweight modeling ways, which have gradually attracted attention and deserve to be systematically analyzed. In this paper, we conduct a systematic survey with comparisons and discussions of foundation model methods from different aspects and briefly review non-foundation model (NFM) methods recently published. Furthermore, we discuss the differences between FM and NFM methods from training objectives, model structure and scale, model performance, and potential directions for future exploration. Through comparison, we find FM methods are more suitable for few-shot and zero-shot learning, which are more in line with actual industrial application scenarios and worthy of in-depth research.         ",
    "url": "https://arxiv.org/abs/2502.19106",
    "authors": [
      "Tianle Yang",
      "Luyao Chang",
      "Jiadong Yan",
      "Juntao Li",
      "Zhi Wang",
      "Ke Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19112",
    "title": "Analyzing Students' Emerging Roles Based on Quantity and Heterogeneity of Individual Contributions in Small Group Online Collaborative Learning Using Bipartite Network Analysis",
    "abstract": "           Understanding students' emerging roles in computer-supported collaborative learning (CSCL) is critical for promoting regulated learning processes and supporting learning at both individual and group levels. However, it has been challenging to disentangle individual performance from group-based deliverables. This study introduces new learning analytic methods based on student -- subtask bipartite networks to gauge two conceptual dimensions -- quantity and heterogeneity of individual contribution to subtasks -- for understanding students' emerging roles in online collaborative learning in small groups. We analyzed these two dimensions and explored the changes of individual emerging roles within seven groups of high school students ($N = 21$) in two consecutive collaborative learning projects. We found a significant association in the changes between assigned leadership roles and changes in the identified emerging roles between the two projects, echoing the importance of externally facilitated regulation scaffolding in CSCL. We also collected qualitative data through a semi-structured interview to further validate the quantitative analysis results, which revealed that student perceptions of their emerging roles were consistent with the quantitative analysis results. This study contributes new learning analytic methods for collaboration analytics as well as a two-dimensional theoretical framework for understanding students' emerging roles in small group CSCL.         ",
    "url": "https://arxiv.org/abs/2502.19112",
    "authors": [
      "Shihui Feng",
      "David Gibson",
      "Dragan Gasevic"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.19115",
    "title": "Improving customer service with automatic topic detection in user emails",
    "abstract": "           This study introduces a novel Natural Language Processing pipeline that enhances customer service efficiency at Telekom Srbija, a leading Serbian telecommunications company, through automated email topic detection and labelling. Central to the pipeline is BERTopic, a modular architecture that allows unsupervised topic modelling. After a series of preprocessing and post-processing steps, we assign one of 12 topics and several additional labels to incoming emails, allowing customer service to filter and access them through a custom-made application. The model's performance was evaluated by assessing the speed and correctness of the automatically assigned topics across a test dataset of 100 customer emails. The pipeline shows broad applicability across languages, particularly for those that are low-resourced and morphologically rich. The system now operates in the company's production environment, streamlining customer service operations through automated email classification.         ",
    "url": "https://arxiv.org/abs/2502.19115",
    "authors": [
      "Bojana Ba\u0161aragin",
      "Darija Medvecki",
      "Gorana Goji\u0107",
      "Milena Oparnica",
      "Dragi\u0161a Mi\u0161kovi\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19125",
    "title": "The NeRF Signature: Codebook-Aided Watermarking for Neural Radiance Fields",
    "abstract": "           Neural Radiance Fields (NeRF) have been gaining attention as a significant form of 3D content representation. With the proliferation of NeRF-based creations, the need for copyright protection has emerged as a critical issue. Although some approaches have been proposed to embed digital watermarks into NeRF, they often neglect essential model-level considerations and incur substantial time overheads, resulting in reduced imperceptibility and robustness, along with user inconvenience. In this paper, we extend the previous criteria for image watermarking to the model level and propose NeRF Signature, a novel watermarking method for NeRF. We employ a Codebook-aided Signature Embedding (CSE) that does not alter the model structure, thereby maintaining imperceptibility and enhancing robustness at the model level. Furthermore, after optimization, any desired signatures can be embedded through the CSE, and no fine-tuning is required when NeRF owners want to use new binary signatures. Then, we introduce a joint pose-patch encryption watermarking strategy to hide signatures into patches rendered from a specific viewpoint for higher robustness. In addition, we explore a Complexity-Aware Key Selection (CAKS) scheme to embed signatures in high visual complexity patches to enhance imperceptibility. The experimental results demonstrate that our method outperforms other baseline methods in terms of imperceptibility and robustness. The source code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19125",
    "authors": [
      "Ziyuan Luo",
      "Anderson Rocha",
      "Boxin Shi",
      "Qing Guo",
      "Haoliang Li",
      "Renjie Wan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19128",
    "title": "SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation",
    "abstract": "           The cross-modal 3D retrieval task aims to achieve mutual matching between text descriptions and 3D shapes. This has the potential to enhance the interaction between natural language and the 3D environment, especially within the realms of robotics and embodied artificial intelligence (AI) applications. However, the scarcity and expensiveness of 3D data constrain the performance of existing cross-modal 3D retrieval methods. These methods heavily rely on features derived from the limited number of 3D shapes, resulting in poor generalization ability across diverse scenarios. To address this challenge, we introduce SCA3D, a novel 3D shape and caption online data augmentation method for cross-modal 3D retrieval. Our approach uses the LLaVA model to create a component library, captioning each segmented part of every 3D shape within the dataset. Notably, it facilitates the generation of extensive new 3D-text pairs containing new semantic features. We employ both inter and intra distances to align various components into a new 3D shape, ensuring that the components do not overlap and are closely fitted. Further, text templates are utilized to process the captions of each component and generate new text descriptions. Besides, we use unimodal encoders to extract embeddings for 3D shapes and texts based on the enriched dataset. We then calculate fine-grained cross-modal similarity using Earth Mover's Distance (EMD) and enhance cross-modal matching with contrastive learning, enabling bidirectional retrieval between texts and 3D shapes. Extensive experiments show our SCA3D outperforms previous works on the Text2Shape dataset, raising the Shape-to-Text RR@1 score from 20.03 to 27.22 and the Text-to-Shape RR@1 score from 13.12 to 16.67. Codes can be found in this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19128",
    "authors": [
      "Junlong Ren",
      "Hao Wu",
      "Hui Xiong",
      "Hao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19136",
    "title": "Robust Precoding for Rate-Splitting-Based Cell-Free MU-MIMO Networks",
    "abstract": "           Cell-free (CF) multiuser multiple-input multiple-output (MU-MIMO) systems are an emerging technology that provides service simultaneously to multiple users but suffers from multiuser interference (MUI). In this work, we propose a robust transmit scheme based on rate-splitting (RS) for CF MU-MIMO systems in the presence of imperfect channel state information (CSI) and MUI. We also develop a robust linear precoder design for both private and common precoders based on the minimum mean square error (MMSE) criterion, which incorporates in its design statistical information about the imperfect CSI to provide extra robustness to RS-CF MU-MIMO systems. A statistical analysis is carried out to derive closed-form sum-rate expressions along with a study of the computational complexity of the proposed scheme. Simulation results show that the proposed scheme outperforms conventional robust and non-robust schemes.         ",
    "url": "https://arxiv.org/abs/2502.19136",
    "authors": [
      "A. Flores",
      "R. de Lamare"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2502.19154",
    "title": "Towards Privacy-Preserving Anomaly-Based Intrusion Detection in Energy Communities",
    "abstract": "           Energy communities consist of decentralized energy production, storage, consumption, and distribution and are gaining traction in modern power systems. However, these communities may increase the vulnerability of the grid to cyber threats. We propose an anomaly-based intrusion detection system to enhance the security of energy communities. The system leverages deep autoencoders to detect deviations from normal operational patterns in order to identify anomalies induced by malicious activities and attacks. Operational data for training and evaluation are derived from a Simulink model of an energy community. The results show that the autoencoder-based intrusion detection system achieves good detection performance across multiple attack scenarios. We also demonstrate potential for real-world application of the system by training a federated model that enables distributed intrusion detection while preserving data privacy.         ",
    "url": "https://arxiv.org/abs/2502.19154",
    "authors": [
      "Zeeshan Afzal",
      "Giovanni Gaggero",
      "Mikael Asplund"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.19166",
    "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation",
    "abstract": "           With the rapid advancement of Large Language Models (LLMs), the demand for robust instruction-following capabilities in code generation tasks has grown significantly. Code generation not only facilitates faster prototyping and automated testing, but also augments developer efficiency through improved maintainability and reusability of code. In this paper, we introduce CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios. CodeIF encompasses a broad range of tasks, including function synthesis, error debugging, algorithmic refactoring, and code explanation, thereby providing a comprehensive suite to evaluate model performance across varying complexity levels and programming domains. We conduct extensive experiments with LLMs, analyzing their strengths and limitations in meeting the demands of these tasks. The experimental results offer valuable insights into how well current models align with human instructions, as well as the extent to which they can generate consistent, maintainable, and contextually relevant code. Our findings not only underscore the critical role that instruction-following LLMs can play in modern software development, but also illuminate pathways for future research aimed at enhancing their adaptability, reliability, and overall effectiveness in automated code generation.         ",
    "url": "https://arxiv.org/abs/2502.19166",
    "authors": [
      "Kaiwen Yan",
      "Hongcheng Guo",
      "Xuanqing Shi",
      "Jingyi Xu",
      "Yaonan Gu",
      "Zhoujun Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19178",
    "title": "UQABench: Evaluating User Embedding for Prompting LLMs in Personalized Question Answering",
    "abstract": "           Large language models (LLMs) achieve remarkable success in natural language processing (NLP). In practical scenarios like recommendations, as users increasingly seek personalized experiences, it becomes crucial to incorporate user interaction history into the context of LLMs to enhance personalization. However, from a practical utility perspective, user interactions' extensive length and noise present challenges when used directly as text prompts. A promising solution is to compress and distill interactions into compact embeddings, serving as soft prompts to assist LLMs in generating personalized responses. Although this approach brings efficiency, a critical concern emerges: Can user embeddings adequately capture valuable information and prompt LLMs? To address this concern, we propose \\name, a benchmark designed to evaluate the effectiveness of user embeddings in prompting LLMs for personalization. We establish a fair and standardized evaluation process, encompassing pre-training, fine-tuning, and evaluation stages. To thoroughly evaluate user embeddings, we design three dimensions of tasks: sequence understanding, action prediction, and interest perception. These evaluation tasks cover the industry's demands in traditional recommendation tasks, such as improving prediction accuracy, and its aspirations for LLM-based methods, such as accurately understanding user interests and enhancing the user experience. We conduct extensive experiments on various state-of-the-art methods for modeling user embeddings. Additionally, we reveal the scaling laws of leveraging user embeddings to prompt LLMs. The benchmark is available online.         ",
    "url": "https://arxiv.org/abs/2502.19178",
    "authors": [
      "Langming Liu",
      "Shilei Liu",
      "Yujin Yuan",
      "Yizhen Zhang",
      "Bencheng Yan",
      "Zhiyuan Zeng",
      "Zihao Wang",
      "Jiaqi Liu",
      "Di Wang",
      "Wenbo Su",
      "Pengjie Wang",
      "Jian Xu",
      "Bo Zheng"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.19193",
    "title": "Simulation of Language Evolution under Regulated Social Media Platforms: A Synergistic Approach of Large Language Models and Genetic Algorithms",
    "abstract": "           Social media platforms frequently impose restrictive policies to moderate user content, prompting the emergence of creative evasion language strategies. This paper presents a multi-agent framework based on Large Language Models (LLMs) to simulate the iterative evolution of language strategies under regulatory constraints. In this framework, participant agents, as social media users, continuously evolve their language expression, while supervisory agents emulate platform-level regulation by assessing policy violations. To achieve a more faithful simulation, we employ a dual design of language strategies (constraint and expression) to differentiate conflicting goals and utilize an LLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of language strategies. The framework is evaluated using two distinct scenarios: an abstract password game and a realistic simulated illegal pet trade scenario. Experimental results demonstrate that as the number of dialogue rounds increases, both the number of uninterrupted dialogue turns and the accuracy of information transmission improve significantly. Furthermore, a user study with 40 participants validates the real-world relevance of the generated dialogues and strategies. Moreover, ablation studies validate the importance of the GA, emphasizing its contribution to long-term adaptability and improved overall results.         ",
    "url": "https://arxiv.org/abs/2502.19193",
    "authors": [
      "Jinyu Cai",
      "Yusei Ishimizu",
      "Mingyue Zhang",
      "Munan Li",
      "Jialong Li",
      "Kenji Tei"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2502.19194",
    "title": "Self-supervised conformal prediction for uncertainty quantification in Poisson imaging problems",
    "abstract": "           Image restoration problems are often ill-posed, leading to significant uncertainty in reconstructed images. Accurately quantifying this uncertainty is essential for the reliable interpretation of reconstructed images. However, image restoration methods often lack uncertainty quantification capabilities. Conformal prediction offers a rigorous framework to augment image restoration methods with accurate uncertainty quantification estimates, but it typically requires abundant ground truth data for calibration. This paper presents a self-supervised conformal prediction method for Poisson imaging problems which leverages Poisson Unbiased Risk Estimator to eliminate the need for ground truth data. The resulting self-calibrating conformal prediction approach is applicable to any Poisson linear imaging problem that is ill-conditioned, and is particularly effective when combined with modern self-supervised image restoration techniques trained directly on measurement data. The proposed method is demonstrated through numerical experiments on image denoising and deblurring; its performance are comparable to supervised conformal prediction methods relying on ground truth data.         ",
    "url": "https://arxiv.org/abs/2502.19194",
    "authors": [
      "Bernardin Tamo Amougou",
      "Marcelo Pereyra",
      "Barbara Pascal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.19199",
    "title": "EGR-Net: A Novel Embedding Gramian Representation CNN for Intelligent Fault Diagnosis",
    "abstract": "           Feature extraction is crucial in intelligent fault diagnosis of rotating machinery. It is easier for convolutional neural networks(CNNs) to visually recognize and learn fault features by converting the complicated one-dimensional (1D) vibrational signals into two-dimensional (2D) images with simple textures. However, the existing representation methods for encoding 1D signals as images have two main problems, including complicated computation and low separability. Meanwhile, the existing 2D-CNN fault diagnosis methods taking 2D images as the only inputs still suffer from the inevitable information loss because of the conversion process. Considering the above issues, this paper proposes a new 1D-to-2D conversion method called Embedding Gramian Representation (EGR), which is easy to calculate and shows good separability. In EGR, 1D signals are projected in the embedding space and the intrinsic periodicity of vibrational signals is captured enabling the faulty characteristics contained in raw signals to be uncovered. Second, aiming at the information loss problem of existing CNN models with the single input of converted images, a double-branch EGR-based CNN, called EGR-Net, is proposed to learn faulty features from both raw signal feature maps and their corresponding EGRs. The bridge connection is designed to improve the feature learning interaction between the two branches. Widely used open domain gearbox dataset and bearing dataset are used to verify the effectiveness and efficiency of the proposed methods. EGR-Net is compared with traditional and state-of-the-art approaches, and the results show that the proposed method can deliver enhanced performance.         ",
    "url": "https://arxiv.org/abs/2502.19199",
    "authors": [
      "Linshan Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19200",
    "title": "HDM: Hybrid Diffusion Model for Unified Image Anomaly Detection",
    "abstract": "           Image anomaly detection plays a vital role in applications such as industrial quality inspection and medical imaging, where it directly contributes to improving product quality and system reliability. However, existing methods often struggle with complex and diverse anomaly patterns. In particular, the separation between generation and discrimination tasks limits the effective coordination between anomaly sample generation and anomaly region detection. To address these challenges, we propose a novel hybrid diffusion model (HDM) that integrates generation and discrimination into a unified framework. The model consists of three key modules: the Diffusion Anomaly Generation Module (DAGM), the Diffusion Discriminative Module (DDM), and the Probability Optimization Module (POM). DAGM generates realistic and diverse anomaly samples, improving their representativeness. DDM then applies a reverse diffusion process to capture the differences between generated and normal samples, enabling precise anomaly region detection and localization based on probability distributions. POM refines the probability distributions during both the generation and discrimination phases, ensuring high-quality samples are used for training. Extensive experiments on multiple industrial image datasets demonstrate that our method outperforms state-of-the-art approaches, significantly improving both image-level and pixel-level anomaly detection performance, as measured by AUROC.         ",
    "url": "https://arxiv.org/abs/2502.19200",
    "authors": [
      "Zekang Weng",
      "Jinjin Shi",
      "Jinwei Wang",
      "Zeming Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19208",
    "title": "MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer's Detection",
    "abstract": "           Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as the leading cause. Conversation-based AD detection offers a cost-effective alternative to clinical methods, as language dysfunction is an early biomarker of AD. However, most prior research has framed AD detection as a binary classification problem, limiting the ability to identify Mild Cognitive Impairment (MCI)-a crucial stage for early intervention. Also, studies primarily rely on single-language datasets, mainly in English, restricting cross-language generalizability. To address this gap, we make three key contributions. First, we introduce a novel, multilingual dataset for AD detection by unifying 16 publicly available dementia-related conversational datasets. This corpus spans English, Spanish, Chinese, and Greek and incorporates both audio and text data derived from a variety of cognitive assessment tasks. Second, we perform finer-grained classification, including MCI, and evaluate various classifiers using sparse and dense text representations. Third, we conduct experiments in monolingual and multilingual settings, finding that some languages benefit from multilingual training while others perform better independently. This study highlights the challenges in multilingual AD detection and enables future research on both language-specific approaches and techniques aimed at improving model generalization and robustness.         ",
    "url": "https://arxiv.org/abs/2502.19208",
    "authors": [
      "Arezo Shakeri",
      "Mina Farmanbar",
      "Krisztian Balog"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19209",
    "title": "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation",
    "abstract": "           Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in Large Language Models (LLMs) but can still produce inconsistent or unsupported content. Although LLM-as-a-Judge is widely used for RAG hallucination detection due to its implementation simplicity, it faces two main challenges: the absence of comprehensive evaluation benchmarks and the lack of domain-optimized judge models. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework featuring a bilingual benchmark dataset and lightweight judge models. The dataset supports rigorous evaluation across multiple RAG scenarios, while the judge models are fine-tuned from compact open-source LLMs. Extensive experimental evaluations on Bi'anBench show our 14B model outperforms baseline models with over five times larger parameter scales and rivals state-of-the-art closed-source LLMs. We will release our data and models soon at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19209",
    "authors": [
      "Zhouyu Jiang",
      "Mengshu Sun",
      "Zhiqiang Zhang",
      "Lei Liang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19219",
    "title": "Detecting Essence Code Clones via Information Theoretic Analysis",
    "abstract": "           Code cloning, a widespread practice in software development, involves replicating code fragments to save time but often at the expense of software maintainability and quality. In this paper, we address the specific challenge of detecting \"essence clones\", a complex subtype of Type-3 clones characterized by sharing critical logic despite different peripheral codes. Traditional techniques often fail to detect essence clones due to their syntactic focus. To overcome this limitation, we introduce ECScan, a novel detection tool that leverages information theory to assess the semantic importance of code lines. By assigning weights to each line based on its information content, ECScan emphasizes core logic over peripheral code differences. Our comprehensive evaluation across various real-world projects shows that ECScan significantly outperforms existing tools in detecting essence clones, achieving an average F1-score of 85%. It demonstrates robust performance across all clone types and offers exceptional scalability. This study advances clone detection by providing a practical tool for developers to enhance code quality and reduce maintenance burdens, emphasizing the semantic aspects of code through an innovative information-theoretic approach.         ",
    "url": "https://arxiv.org/abs/2502.19219",
    "authors": [
      "Lida Zhao",
      "Shihan Dou",
      "Yutao Hu",
      "Yueming Wu",
      "Jiahui Wu",
      "Chengwei Liu",
      "Lyuye Zhang",
      "Yi Liu",
      "Jun Sun",
      "Xuanjing Huang",
      "Yang Liu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.19257",
    "title": "Poster: Long PHP webshell files detection based on sliding window attention",
    "abstract": "           Webshell is a type of backdoor, and web applications are widely exposed to webshell injection attacks. Therefore, it is important to study webshell detection techniques. In this study, we propose a webshell detection method. We first convert PHP source code to opcodes and then extract Opcode Double-Tuples (ODTs). Next, we combine CodeBert and FastText models for feature representation and classification. To address the challenge that deep learning methods have difficulty detecting long webshell files, we introduce a sliding window attention mechanism. This approach effectively captures malicious behavior within long files. Experimental results show that our method reaches high accuracy in webshell detection, solving the problem of traditional methods that struggle to address new webshell variants and anti-detection techniques.         ",
    "url": "https://arxiv.org/abs/2502.19257",
    "authors": [
      "Zhiqiang Wang",
      "Haoyu Wang",
      "Lu Hao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.19269",
    "title": "Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models",
    "abstract": "           While pre-trained Vision-Language Models (VLMs) such as CLIP exhibit excellent representational capabilities for multimodal data, recent studies have shown that they are vulnerable to backdoor attacks. To alleviate the threat, existing defense strategies primarily focus on fine-tuning the entire suspicious model, yet offer only marginal resistance to state-of-the-art attacks and often result in a decrease in clean accuracy, particularly in data-limited scenarios. Their failure may be attributed to the mismatch between insufficient fine-tuning data and massive parameters in VLMs. To address this challenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, an efficient and effective method that operates on the text prompts to indirectly purify the poisoned VLMs. Specifically, we first employ the advanced contrastive learning via our carefully crafted positive and negative samples, to effectively invert the backdoor triggers that are potentially adopted by the attacker. Once the dummy trigger is established, we utilize the efficient prompt tuning technique to optimize these class-wise text prompts for modifying the model's decision boundary to further reclassify the feature regions of backdoor triggers. Extensive experiments demonstrate that CBPT significantly mitigates backdoor threats while preserving model utility, e.g. an average Clean Accuracy (CA) of 58.86\\% and an Attack Success Rate (ASR) of 0.39\\% across seven mainstream backdoor attacks. These results underscore the superiority of our prompt purifying design to strengthen model robustness against backdoor attacks.         ",
    "url": "https://arxiv.org/abs/2502.19269",
    "authors": [
      "Jiawei Kong",
      "Hao Fang",
      "Sihang Guo",
      "Chenxi Qing",
      "Bin Chen",
      "Bin Wang",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19271",
    "title": "Multiview graph dual-attention deep learning and contrastive learning for multi-criteria recommender systems",
    "abstract": "           Recommender systems leveraging deep learning models have been crucial for assisting users in selecting items aligned with their preferences and interests. However, a significant challenge persists in single-criteria recommender systems, which often overlook the diverse attributes of items that have been addressed by Multi-Criteria Recommender Systems (MCRS). Shared embedding vector for multi-criteria item ratings but have struggled to capture the nuanced relationships between users and items based on specific criteria. In this study, we present a novel representation for Multi-Criteria Recommender Systems (MCRS) based on a multi-edge bipartite graph, where each edge represents one criterion rating of items by users, and Multiview Dual Graph Attention Networks (MDGAT). Employing MDGAT is beneficial and important for adequately considering all relations between users and items, given the presence of both local (criterion-based) and global (multi-criteria) relations. Additionally, we define anchor points in each view based on similarity and employ local and global contrastive learning to distinguish between positive and negative samples across each view and the entire graph. We evaluate our method on two real-world datasets and assess its performance based on item rating predictions. The results demonstrate that our method achieves higher accuracy compared to the baseline method for predicting item ratings on the same datasets. MDGAT effectively capture the local and global impact of neighbours and the similarity between nodes.         ",
    "url": "https://arxiv.org/abs/2502.19271",
    "authors": [
      "Saman Forouzandeh",
      "Pavel N. Krivitsky",
      "Rohitash Chandra"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.19276",
    "title": "Disentangled VAD Representations via a Variational Framework for Political Stance Detection",
    "abstract": "           The stance detection task aims to categorise the stance regarding specified targets. Current methods face challenges in effectively integrating sentiment information for stance detection. Moreover, the role of highly granular sentiment labelling in stance detection has been largely overlooked. This study presents a novel stance detection framework utilizing a variational autoencoder (VAE) to disentangle latent emotional features-value, arousal, and dominance (VAD)-from political discourse on social media. This approach addresses limitations in current methods, particularly in in-target and cross-target stance detection scenarios. This research uses an advanced emotional annotation tool to annotate seven-class sentiment labels for P-STANCE. Evaluations on benchmark datasets, including P-STANCE and SemEval-2016, reveal that PoliStance-VAE achieves state-of-the-art performance, surpassing models like BERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable solution for stance detection, demonstrating the effectiveness of integrating nuanced emotional representations. This framework paves the way for advancements in natural language processing tasks, particularly those requiring detailed emotional understanding.         ",
    "url": "https://arxiv.org/abs/2502.19276",
    "authors": [
      "Beiyu Xu",
      "Zhiwei Liu",
      "Sophia Ananiadou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19279",
    "title": "CritiQ: Mining Data Quality Criteria from Human Preferences",
    "abstract": "           Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only $\\sim$30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.         ",
    "url": "https://arxiv.org/abs/2502.19279",
    "authors": [
      "Honglin Guo",
      "Kai Lv",
      "Qipeng Guo",
      "Tianyi Liang",
      "Zhiheng Xi",
      "Demin Song",
      "Qiuyinzhe Zhang",
      "Yu Sun",
      "Kai Chen",
      "Xipeng Qiu",
      "Tao Gui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19285",
    "title": "On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation",
    "abstract": "           Vision-language models in pathology enable multimodal case retrieval and automated report generation. Many of the models developed so far, however, have been trained on pathology reports that include information which cannot be inferred from paired whole slide images (e.g., patient history), potentially leading to hallucinated sentences in generated reports. To this end, we investigate how the selection of information from pathology reports for vision-language modeling affects the quality of the multimodal representations and generated reports. More concretely, we compare a model trained on full reports against a model trained on preprocessed reports that only include sentences describing the cell and tissue appearances based on the H&E-stained slides. For the experiments, we built upon the BLIP-2 framework and used a cutaneous melanocytic lesion dataset of 42,433 H&E-stained whole slide images and 19,636 corresponding pathology reports. Model performance was assessed using image-to-text and text-to-image retrieval, as well as qualitative evaluation of the generated reports by an expert pathologist. Our results demonstrate that text preprocessing prevents hallucination in report generation. Despite the improvement in the quality of the generated reports, training the vision-language model on full reports showed better cross-modal retrieval performance.         ",
    "url": "https://arxiv.org/abs/2502.19285",
    "authors": [
      "Ruben T. Lucassen",
      "Tijn van de Luijtgaarden",
      "Sander P.J. Moonemans",
      "Gerben E. Breimer",
      "Willeke A.M. Blokx",
      "Mitko Veta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19290",
    "title": "PhysicsSolver: Transformer-Enhanced Physics-Informed Neural Networks for Forward and Forecasting Problems in Partial Differential Equations",
    "abstract": "           Time-dependent partial differential equations are a significant class of equations that describe the evolution of various physical phenomena over time. One of the open problems in scientific computing is predicting the behaviour of the solution outside the given temporal region. Most traditional numerical methods are applied to a given time-space region and can only accurately approximate the solution of the given region. To address this problem, many deep learning-based methods, basically data-driven and data-free approaches, have been developed to solve these problems. However, most data-driven methods require a large amount of data, which consumes significant computational resources and fails to utilize all the necessary information embedded underlying the partial differential equations (PDEs). Moreover, data-free approaches such as Physics-Informed Neural Networks (PINNs) may not be that ideal in practice, as traditional PINNs, which primarily rely on multilayer perceptrons (MLPs) and convolutional neural networks (CNNs), tend to overlook the crucial temporal dependencies inherent in real-world physical systems. We propose a method denoted as \\textbf{PhysicsSolver} that merges the strengths of two approaches: data-free methods that can learn the intrinsic properties of physical systems without using data, and data-driven methods, which are effective at making predictions. Extensive numerical experiments have demonstrated the efficiency and robustness of our proposed method. We provide the code at \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2502.19290",
    "authors": [
      "Zhenyi Zhu",
      "Yuchen Huang",
      "Liu Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2502.19291",
    "title": "Global Graph Propagation with Hierarchical Information Transfer for Incomplete Contrastive Multi-view Clustering",
    "abstract": "           Incomplete multi-view clustering has become one of the important research problems due to the extensive missing multi-view data in the real world. Although the existing methods have made great progress, there are still some problems: 1) most methods cannot effectively mine the information hidden in the missing data; 2) most methods typically divide representation learning and clustering into two separate stages, but this may affect the clustering performance as the clustering results directly depend on the learned representation. To address these problems, we propose a novel incomplete multi-view clustering method with hierarchical information transfer. Firstly, we design the view-specific Graph Convolutional Networks (GCN) to obtain the representation encoding the graph structure, which is then fused into the consensus representation. Secondly, considering that one layer of GCN transfers one-order neighbor node information, the global graph propagation with the consensus representation is proposed to handle the missing data and learn deep representation. Finally, we design a weight-sharing pseudo-classifier with contrastive learning to obtain an end-to-end framework that combines view-specific representation learning, global graph propagation with hierarchical information transfer, and contrastive clustering for joint optimization. Extensive experiments conducted on several commonly-used datasets demonstrate the effectiveness and superiority of our method in comparison with other state-of-the-art approaches. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19291",
    "authors": [
      "Guoqing Chao",
      "Kaixin Xu",
      "Xijiong Xie",
      "Yongyong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19293",
    "title": "Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions",
    "abstract": "           Millions of melanocytic skin lesions are examined by pathologists each year, the majority of which concern common nevi (i.e., ordinary moles). While most of these lesions can be diagnosed in seconds, writing the corresponding pathology report is much more time-consuming. Automating part of the report writing could, therefore, alleviate the increasing workload of pathologists. In this work, we develop a vision-language model specifically for the pathology domain of cutaneous melanocytic lesions. The model follows the Contrastive Captioner framework and was trained and evaluated using a melanocytic lesion dataset of 42,512 H&E-stained whole slide images and 19,645 corresponding pathology reports. Our results show that the quality scores of model-generated reports were on par with pathologist-written reports for common nevi, assessed by an expert pathologist in a reader study. While report generation revealed to be more difficult for rare melanocytic lesion subtypes, the cross-modal retrieval performance for these cases was considerably better.         ",
    "url": "https://arxiv.org/abs/2502.19293",
    "authors": [
      "Ruben T. Lucassen",
      "Sander P.J. Moonemans",
      "Tijn van de Luijtgaarden",
      "Gerben E. Breimer",
      "Willeke A.M. Blokx",
      "Mitko Veta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19295",
    "title": "Complex LLM Planning via Automated Heuristics Discovery",
    "abstract": "           We consider enhancing large language models (LLMs) for complex planning tasks. While existing methods allow LLMs to explore intermediate steps to make plans, they either depend on unreliable self-verification or external verifiers to evaluate these steps, which demand significant data and computations. Here, we propose automated heuristics discovery (AutoHD), a novel approach that enables LLMs to explicitly generate heuristic functions to guide inference-time search, allowing accurate evaluation of intermediate states. These heuristic functions are further refined through a heuristic evolution process, improving their robustness and effectiveness. Our proposed method requires no additional model training or fine-tuning, and the explicit definition of heuristic functions generated by the LLMs provides interpretability and insights into the reasoning process. Extensive experiments across diverse benchmarks demonstrate significant gains over multiple baselines, including nearly twice the accuracy on some datasets, establishing our approach as a reliable and interpretable solution for complex planning tasks.         ",
    "url": "https://arxiv.org/abs/2502.19295",
    "authors": [
      "Hongyi Ling",
      "Shubham Parashar",
      "Sambhav Khurana",
      "Blake Olson",
      "Anwesha Basu",
      "Gaurangi Sinha",
      "Zhengzhong Tu",
      "James Caverlee",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.19305",
    "title": "Corporate Fraud Detection in Rich-yet-Noisy Financial Graph",
    "abstract": "           Corporate fraud detection aims to automatically recognize companies that conduct wrongful activities such as fraudulent financial statements or illegal insider trading. Previous learning-based methods fail to effectively integrate rich interactions in the company network. To close this gap, we collect 18-year financial records in China to form three graph datasets with fraud labels. We analyze the characteristics of the financial graphs, highlighting two pronounced issues: (1) information overload: the dominance of (noisy) non-company nodes over company nodes hinders the message-passing process in Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large percentage of possible undetected violations in the collected data. The hidden fraud problem will introduce noisy labels in the training dataset and compromise fraud detection results. To handle such challenges, we propose a novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage Learning (${\\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to mitigate the information overload and effectively learns rich representations. The proposed model adopts a two-stage learning method to enhance robustness against hidden frauds. Extensive experimental results not only confirm the importance of interactions but also show the superiority of ${\\rm KeGCN}_{R}$ over a number of strong baselines in terms of fraud detection effectiveness and robustness.         ",
    "url": "https://arxiv.org/abs/2502.19305",
    "authors": [
      "Shiqi Wang",
      "Zhibo Zhang",
      "Libing Fang",
      "Cam-Tu Nguyen",
      "Wenzhon Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Risk Management (q-fin.RM)",
      "Statistical Finance (q-fin.ST)"
    ]
  },
  {
    "id": "arXiv:2502.19307",
    "title": "Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency",
    "abstract": "           Anomaly detection in complex dynamical systems is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. Predictive maintenance helps prevent costly failures, while cybersecurity monitoring has become critical as digitized systems face growing threats. Many of these systems exhibit oscillatory behaviors and bounded motion, requiring anomaly detection methods that capture structured temporal dependencies while adhering to physical consistency principles. In this work, we propose a system-theoretic approach to anomaly detection, grounded in classical embedding theory and physics-inspired consistency principles. We build upon the Fractal Whitney Embedding Prevalence Theorem, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce state-derivative pairs as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a Temporal Differential Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the C-MAPSS dataset, a benchmark for turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers while achieving a 200x reduction in MAC operations, making it particularly suited for lightweight edge computing. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection.         ",
    "url": "https://arxiv.org/abs/2502.19307",
    "authors": [
      "Michael Somma",
      "Thomas Gallien",
      "Branka Stojanovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.19313",
    "title": "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query",
    "abstract": "           Cooperative perception enhances the individual perception capabilities of autonomous vehicles (AVs) by providing a comprehensive view of the environment. However, balancing perception performance and transmission costs remains a significant challenge. Current approaches that transmit region-level features across agents are limited in interpretability and demand substantial bandwidth, making them unsuitable for practical applications. In this work, we propose CoopDETR, a novel cooperative perception framework that introduces object-level feature cooperation via object query. Our framework consists of two key modules: single-agent query generation, which efficiently encodes raw sensor data into object queries, reducing transmission cost while preserving essential information for detection; and cross-agent query fusion, which includes Spatial Query Matching (SQM) and Object Query Aggregation (OQA) to enable effective interaction between queries. Our experiments on the OPV2V and V2XSet datasets demonstrate that CoopDETR achieves state-of-the-art performance and significantly reduces transmission costs to 1/782 of previous methods.         ",
    "url": "https://arxiv.org/abs/2502.19313",
    "authors": [
      "Zhe Wang",
      "Shaocong Xu",
      "Xucai Zhuang",
      "Tongda Xu",
      "Yan Wang",
      "Jingjing Liu",
      "Yilun Chen",
      "Ya-Qin Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19334",
    "title": "Joint Optimal Transport and Embedding for Network Alignment",
    "abstract": "           Network alignment, which aims to find node correspondence across different networks, is the cornerstone of various downstream multi-network and Web mining tasks. Most of the embedding-based methods indirectly model cross-network node relationships by contrasting positive and negative node pairs sampled from hand-crafted strategies, which are vulnerable to graph noises and lead to potential misalignment of nodes. Another line of work based on the optimal transport (OT) theory directly models cross-network node relationships and generates noise-reduced alignments. However, OT methods heavily rely on fixed, pre-defined cost functions that prohibit end-to-end training and are hard to generalize. In this paper, we aim to unify the embedding and OT-based methods in a mutually beneficial manner and propose a joint optimal transport and embedding framework for network alignment named JOENA. For one thing (OT for embedding), through a simple yet effective transformation, the noise-reduced OT mapping serves as an adaptive sampling strategy directly modeling all cross-network node pairs for robust embedding this http URL another (embedding for OT), on top of the learned embeddings, the OT cost can be gradually trained in an end-to-end fashion, which further enhances the alignment quality. With a unified objective, the mutual benefits of both methods can be achieved by an alternating optimization schema with guaranteed convergence. Extensive experiments on real-world networks validate the effectiveness and scalability of JOENA, achieving up to 16% improvement in MRR and 20x speedup compared with the state-of-the-art alignment methods.         ",
    "url": "https://arxiv.org/abs/2502.19334",
    "authors": [
      "Qi Yu",
      "Zhichen Zeng",
      "Yuchen Yan",
      "Lei Ying",
      "R. Srikant",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2502.19337",
    "title": "Consistent Amortized Clustering via Generative Flow Networks",
    "abstract": "           Neural models for amortized probabilistic clustering yield samples of cluster labels given a set-structured input, while avoiding lengthy Markov chain runs and the need for explicit data likelihoods. Existing methods which label each data point sequentially, like the Neural Clustering Process, often lead to cluster assignments highly dependent on the data order. Alternatively, methods that sequentially create full clusters, do not provide assignment probabilities. In this paper, we introduce GFNCP, a novel framework for amortized clustering. GFNCP is formulated as a Generative Flow Network with a shared energy-based parametrization of policy and reward. We show that the flow matching conditions are equivalent to consistency of the clustering posterior under marginalization, which in turn implies order invariance. GFNCP also outperforms existing methods in clustering performance on both synthetic and real-world data.         ",
    "url": "https://arxiv.org/abs/2502.19337",
    "authors": [
      "Irit Chelly",
      "Roy Uziel",
      "Oren Freifeld",
      "Ari Pakman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19341",
    "title": "Unveiling Wireless Users' Locations via Modulation Classification-based Passive Attack",
    "abstract": "           The broadcast nature of the wireless medium and openness of wireless standards, e.g., 3GPP releases 16-20, invite adversaries to launch various active and passive attacks on cellular and other wireless networks. This work identifies one such loose end of wireless standards and presents a novel passive attack method enabling an eavesdropper (Eve) to localize a line of sight wireless user (Bob) who is communicating with a base station or WiFi access point (Alice). The proposed attack involves two phases. In the first phase, Eve performs modulation classification by intercepting the downlink channel between Alice and Bob. This enables Eve to utilize the publicly available modulation and coding scheme (MCS) tables to do pesudo-ranging, i.e., the Eve determines the ring within which Bob is located, which drastically reduces the search space. In the second phase, Eve sniffs the uplink channel, and employs multiple strategies to further refine Bob's location within the ring. Towards the end, we present our thoughts on how this attack can be extended to non-line-of-sight scenarios, and how this attack could act as a scaffolding to construct a malicious digital twin map.         ",
    "url": "https://arxiv.org/abs/2502.19341",
    "authors": [
      "Ali Hanif",
      "Abdulrahman Katranji",
      "Nour Kouzayha",
      "Muhammad Mahboob Ur Rahman",
      "Tareq Y. Al-Naffouri"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.19350",
    "title": "Estimating Nodal Spreading Influence Using Partial Temporal Network",
    "abstract": "           Temporal networks, whose links are activated or deactivated over time, are used to represent complex systems such as social interactions or collaborations occurring at specific times. Such networks facilitate the spread of information and epidemics. The average number of nodes infected via a spreading process on a network starting from a single seed node over a given period is called the influence of that node. In this paper, we address the question of how to utilize the partially observed temporal network (local and of short duration) around each node, to estimate the ranking of nodes in spreading influence on the full network over a long period. This is essential for target marketing and epidemic/misinformation mitigation where only partial network information is possibly accessible. This would also enable us to understand which network properties of a node, observed locally and shortly after the start of the spreading process, determine its influence. We systematically propose a set of nodal centrality metrics based on partial temporal network information, encoding diverse properties of (time-respecting) walks. It is found that distinct centrality metrics perform the best in estimating nodal influence depending on the infection probability of the spreading process. For a broad range of the infection probability, a node tends to be influential if it can reach many distinct nodes via time-respecting walks and if these nodes can be reached early in time. We find and explain why the proposed metrics generally outperform classic centrality metrics derived from both full and partial temporal networks.         ",
    "url": "https://arxiv.org/abs/2502.19350",
    "authors": [
      "Tianrui Mao",
      "Shilun Zhang",
      "Alan Hanjalic",
      "Huijuan Wang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2502.19357",
    "title": "Physics-Based Hybrid Machine Learning for Critical Heat Flux Prediction with Uncertainty Quantification",
    "abstract": "           Critical heat flux is a key quantity in boiling system modeling due to its impact on heat transfer and component temperature and performance. This study investigates the development and validation of an uncertainty-aware hybrid modeling approach that combines machine learning with physics-based models in the prediction of critical heat flux in nuclear reactors for cases of dryout. Two empirical correlations, Biasi and Bowring, were employed with three machine learning uncertainty quantification techniques: deep neural network ensembles, Bayesian neural networks, and deep Gaussian processes. A pure machine learning model without a base model served as a baseline for comparison. This study examines the performance and uncertainty of the models under both plentiful and limited training data scenarios using parity plots, uncertainty distributions, and calibration curves. The results indicate that the Biasi hybrid deep neural network ensemble achieved the most favorable performance (with a mean absolute relative error of 1.846% and stable uncertainty estimates), particularly in the plentiful data scenario. The Bayesian neural network models showed slightly higher error and uncertainty but superior calibration. By contrast, deep Gaussian process models underperformed by most metrics. All hybrid models outperformed pure machine learning configurations, demonstrating resistance against data scarcity.         ",
    "url": "https://arxiv.org/abs/2502.19357",
    "authors": [
      "Aidan Furlong",
      "Xingang Zhao",
      "Robert Salko",
      "Xu Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.19385",
    "title": "HDEE: Heterogeneous Domain Expert Ensemble",
    "abstract": "           Training dense LLMs requires enormous amounts of data and centralized compute, which introduces fundamental bottlenecks and ever-growing costs for large models. Several studies aim to reduce this dependency on centralization by reducing the communication overhead of training dense models. Taking this idea of reducing communication overhead to a natural extreme, by training embarrassingly parallelizable ensembles of small independent experts, has been shown to outperform large dense models trained in traditional centralized settings. However, existing studies do not take into account underlying differences amongst data domains and treat them as monolithic, regardless of their underlying complexity, size, or distribution. In this paper, we explore the effects of introducing heterogeneity to these ensembles of domain expert models. Specifically, by allowing models within the ensemble to vary in size--as well as the number of training steps taken depending on the training data's domain--we study the effect heterogeneity has on these ensembles when evaluated against domains included in, and excluded from, the training set. We use the same compute budget to train heterogeneous ensembles and homogeneous baselines for comparison. We show that the heterogeneous ensembles achieve the lowest perplexity scores in $20$ out of the $21$ data domains used in the evaluation. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.19385",
    "authors": [
      "O\u011fuzhan Ersoy",
      "Jari Kolehmainen",
      "Gabriel Passamani Andrade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2502.19407",
    "title": "Learning Code-Edit Embedding to Model Student Debugging Behavior",
    "abstract": "           Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.         ",
    "url": "https://arxiv.org/abs/2502.19407",
    "authors": [
      "Hasnain Heickal",
      "Andrew Lan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.19411",
    "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs",
    "abstract": "           In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.         ",
    "url": "https://arxiv.org/abs/2502.19411",
    "authors": [
      "Dayu Yang",
      "Tianyang Liu",
      "Daoan Zhang",
      "Antoine Simoulin",
      "Xiaoyi Liu",
      "Yuwei Cao",
      "Zhaopu Teng",
      "Xin Qian",
      "Grey Yang",
      "Jiebo Luo",
      "Julian McAuley"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.19412",
    "title": "The Mighty ToRR: A Benchmark for Table Reasoning and Robustness",
    "abstract": "           Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt. To address this gap, we create ToRR, a benchmark for Table Reasoning and Robustness, that measures model performance and robustness on table-related tasks. The benchmark includes 10 datasets that cover different types of table reasoning capabilities across varied domains. ToRR goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly, across a variety of common table representation formats. We present a leaderboard as well as comprehensive analyses of the results of leading models over ToRR. Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks. Although no specific table format leads to consistently better performance, we show that testing over multiple formats is crucial for reliably estimating model capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples. Overall, our findings show that table understanding and reasoning tasks remain a significant challenge.         ",
    "url": "https://arxiv.org/abs/2502.19412",
    "authors": [
      "Shir Ashury-Tahan",
      "Yifan Mai",
      "Rajmohan C",
      "Ariel Gera",
      "Yotam Perlitz",
      "Asaf Yehudai",
      "Elron Bandel",
      "Leshem Choshen",
      "Eyal Shnarch",
      "Percy Liang",
      "Michal Shmueli-Scheuer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.12506",
    "title": "Classification and reconstruction for single-pixel imaging with classical and quantum neural networks",
    "abstract": "           Single-pixel cameras are effective solution for imaging outside the visible spectrum where traditional CMOS/CCD cameras have challenges. Combined with machine learning, they can analyze images quickly enough for practical applications. Solving the problem of high-dimensional single-pixel visualization can potentially be accelerated using quantum machine learning, thereby expanding the range of practical problems. In this work we simulated a single-pixel imaging experiment using Hadamard basis patterns, where images from the MNIST handwritten digit dataset were used as objects. There were selected 64 measurements with maximum variance (6% of the number of pixels in the image). We created algorithms for classifying and reconstruction images based on these measurements using classical fully connected neural networks and parameterized quantum circuits. Classical and quantum classifiers showed accuracies of 96% and 95% respectively after 6 training epochs, which is quite competitive result. Image reconstruction was also demonstrated using classical and quantum neural networks after 10 training epochs, the structural similarity index measure values were 0.76 and 0.25, respectively, which indicates that the problem in such a formulation turned out to be too difficult for quantum neural networks in such a configuration for now.         ",
    "url": "https://arxiv.org/abs/2407.12506",
    "authors": [
      "Sofya Manko",
      "Dmitry Frolovtsev"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2502.17597",
    "title": "Unraveling particle dark matter with Physics-Informed Neural Networks",
    "abstract": "           We parametrically solve the Boltzmann equations governing freeze-in dark matter (DM) in alternative cosmologies with Physics-Informed Neural Networks (PINNs), a mesh-free method. Through inverse PINNs, using a single DM experimental point -- observed relic density -- we determine the physical attributes of the theory, namely power-law cosmologies, inspired by braneworld scenarios, and particle interaction cross sections. The expansion of the Universe in such alternative cosmologies has been parameterized through a switch-like function reproducing the Hubble law at later times. Without loss of generality, we model more realistically this transition with a smooth function. We predict a distinct pair-wise relationship between power-law exponent and particle interactions: for a given cosmology with negative (positive) exponent, smaller (larger) cross sections are required to reproduce the data. Lastly, via Bayesian methods, we quantify the epistemic uncertainty of theoretical parameters found in inverse problems.         ",
    "url": "https://arxiv.org/abs/2502.17597",
    "authors": [
      "M.P. Bento",
      "H.B. C\u00e2mara",
      "J.F. Seabra"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18840",
    "title": "Damping Tuning Considering Random Disturbances Adopting Distributionally Robust Optimization",
    "abstract": "           In scenarios where high penetration of renewable energy sources (RES) is connected to the grid over long distances, the output of RES exhibits significant fluctuations, making it difficult to accurately characterize. The intermittency and uncertainty of these fluctuations pose challenges to the stability of the power system. This paper proposes a distributionally robust damping optimization control framework (DRDOC) to address the uncertainty in the true distribution of random disturbances caused by RES. First, the installation location of damping controllers and key control parameters are determined through Sobol sensitivity indices and participation factors. Next, a nonlinear relationship between damping and random disturbances is established with Polynomial Chaos Expansion (PCE). The uncertainty in the distribution of disturbances is captured by ambiguity sets. The DRDOC is formulated as a convex optimization problem, which is further simplified for efficient computation. Finally, the optimal control parameters are derived through convex optimization techniques. Simulation results demonstrate the effectiveness and distribution robustness of the proposed DRDOC.         ",
    "url": "https://arxiv.org/abs/2502.18840",
    "authors": [
      "Yuhong Wang",
      "Xinyao Wang",
      "Chen Shen",
      "Jianquan Liao",
      "Qianni Cao",
      "Yufei Teng",
      "Huabo Shi",
      "Gang Chen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.18852",
    "title": "Data-Driven and Theory-Guided Pseudo-Spectral Seismic Imaging Using Deep Neural Network Architectures",
    "abstract": "           Full Waveform Inversion (FWI) reconstructs high-resolution subsurface models via multi-variate optimization but faces challenges with solver selection and data availability. Deep Learning (DL) offers a promising alternative, bridging data-driven and physics-based methods. While FWI in DL has been explored in the time domain, the pseudo-spectral approach remains underutilized, despite its success in classical FWI. This thesis integrates pseudo-spectral FWI into DL, formulating both data-driven and theory-guided approaches using Deep Neural Networks (DNNs) and Recurrent Neural Networks (RNNs). These methods were theoretically derived, tested on synthetic and Marmousi datasets, and compared with deterministic and time-domain approaches. Results show that data-driven pseudo-spectral DNNs outperform classical FWI in deeper and over-thrust regions due to their global approximation capability. Theory-guided RNNs yield greater accuracy, with lower error and better fault identification. While DNNs excel in velocity contrast recovery, RNNs provide superior edge definition and stability in shallow and deep sections. Beyond enhancing FWI performance, this research identifies broader applications of DL-based inversion and outlines future directions for these frameworks.         ",
    "url": "https://arxiv.org/abs/2502.18852",
    "authors": [
      "Christopher Zerafa"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18875",
    "title": "SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation",
    "abstract": "           Targeted protein degradation (TPD) induced by small molecules has emerged as a rapidly evolving modality in drug discovery, targeting proteins traditionally considered \"undruggable\". Proteolysis-targeting chimeras (PROTACs) and molecular glue degraders (MGDs) are the primary small molecules that induce TPD. Both types of molecules form a ternary complex linking an E3 ligase with a target protein, a crucial step for drug discovery. While significant advances have been made in binary structure prediction for proteins and small molecules, ternary structure prediction remains challenging due to obscure interaction mechanisms and insufficient training data. Traditional methods relying on manually assigned rules perform poorly and are computationally demanding due to extensive random sampling. In this work, we introduce DeepTernary, a novel deep learning-based approach that directly predicts ternary structures in an end-to-end manner using an encoder-decoder architecture. DeepTernary leverages an SE(3)-equivariant graph neural network (GNN) with both intra-graph and ternary inter-graph attention mechanisms to capture intricate ternary interactions from our collected high-quality training dataset, TernaryDB. The proposed query-based Pocket Points Decoder extracts the 3D structure of the final binding ternary complex from learned ternary embeddings, demonstrating state-of-the-art accuracy and speed in existing PROTAC benchmarks without prior knowledge from known PROTACs. It also achieves notable accuracy on the more challenging MGD benchmark under the blind docking protocol. Remarkably, our experiments reveal that the buried surface area calculated from predicted structures correlates with experimentally obtained degradation potency-related metrics. Consequently, DeepTernary shows potential in effectively assisting and accelerating the development of TPDs for previously undruggable targets.         ",
    "url": "https://arxiv.org/abs/2502.18875",
    "authors": [
      "Fanglei Xue",
      "Meihan Zhang",
      "Shuqi Li",
      "Xinyu Gao",
      "James A. Wohlschlegel",
      "Wenbing Huang",
      "Yi Yang",
      "Weixian Deng"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18942",
    "title": "Finding Minimum Matching Cuts in $H$-free Graphs and Graphs of Bounded Radius and Diameter",
    "abstract": "           A matching cut is a matching that is also an edge cut. In the problem Minimum Matching Cut, we ask for a matching cut with the minimum number of edges in the matching. We give polynomial-time algorithms for $P_7$-free, $S_{1,1,2}$-free and $(P_6 + P_4)$-free graphs, which also solve several open cases for the well-studied problem Matching Cut. In addition, we show NP-hardness for $3P_3$-free graphs, implying that Minimum Matching Cut and Matching Cut differ in complexity on certain graph classes. We also give complexity dichotomies for both general and bipartite graphs of bounded radius and diameter.         ",
    "url": "https://arxiv.org/abs/2502.18942",
    "authors": [
      "Felicia Lucke",
      "Joseph Marchand",
      "Jannik Olbrich"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2502.18996",
    "title": "Sequential Entanglement-Swapping assisted by Quantum Protocol over Ethernet Networks",
    "abstract": "           The integration of quantum communication protocols over Ethernet networks is proposed, showing the potential of combining classical and quantum technologies for efficient, scalable quantum networking. By leveraging the inherent strengths of Ethernet, such as addressing, MAC layer functionality, and scalability; we propose a practical framework to support the rigorous requirements of quantum communication. Some novel protocols given in this study enable reliable end-to-end quantum entanglement over Ethernet, ensuring the adaptability needed for implementing a stable quantum internet. Detailed time-delay analyses confirm that our protocols offer superior performance compared to existing methods, with total time delay kept within the decoherence threshold of qubits. These results suggest that our approach is well-suited for deployment in realistic environments, meeting both the immediate needs of quantum networking and laying the groundwork for future advances in data exchange and quantum computational capabilities.         ",
    "url": "https://arxiv.org/abs/2502.18996",
    "authors": [
      "Kun Chen-Hu",
      "Kristian S. Jensen",
      "Petar Popovski"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2502.18998",
    "title": "Graph Neural Networks embedded into Margules model for vapor-liquid equilibria prediction",
    "abstract": "           Predictive thermodynamic models are crucial for the early stages of product and process design. In this paper the performance of Graph Neural Networks (GNNs) embedded into a relatively simple excess Gibbs energy model, the extended Margules model, for predicting vapor-liquid equilibrium is analyzed. By comparing its performance against the established UNIFAC-Dortmund model it has been shown that GNNs embedded in Margules achieves an overall lower accuracy. However, higher accuracy is observed in the case of various types of binary mixtures. Moreover, since group contribution methods, like UNIFAC, are limited due to feasibility of molecular fragmentation or availability of parameters, the GNN in Margules model offers an alternative for VLE estimation. The findings establish a baseline for the predictive accuracy that simple excess Gibbs energy models combined with GNNs trained solely on infinite dilution data can achieve.         ",
    "url": "https://arxiv.org/abs/2502.18998",
    "authors": [
      "Edgar Ivan Sanchez Medina",
      "Kai Sundmacher"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19014",
    "title": "Robust Over-the-Air Computation with Type-Based Multiple Access",
    "abstract": "           This paper utilizes the properties of type-based multiple access (TBMA) to investigate its effectiveness as a robust approach for over-the-air computation (AirComp) in the presence of Byzantine attacks, this is, adversarial strategies where malicious nodes intentionally distort their transmissions to corrupt the aggregated result. Unlike classical direct aggregation (DA) AirComp, which aggregates data in the amplitude of the signals and are highly vulnerable to attacks, TBMA distributes data over multiple radio resources, enabling the receiver to construct a histogram representation of the transmitted data. This structure allows the integration of classical robust estimators and supports the computation of diverse functions beyond the arithmetic mean, which is not feasible with DA. Through extensive simulations, we demonstrate that robust TBMA significantly outperforms DA, maintaining high accuracy even under adversarial conditions, and showcases its applicability in federated learning (FEEL) scenarios. Additionally, TBMA reduces channel state information (CSI) requirements, lowers energy consumption, and enhances resiliency by leveraging the diversity of the transmitted data. These results establish TBMA as a scalable and robust solution for AirComp, paving the way for secure and efficient aggregation in next-generation networks.         ",
    "url": "https://arxiv.org/abs/2502.19014",
    "authors": [
      "Marc Martinez-Gost",
      "Ana P\u00e9rez-Neira",
      "Miguel \u00c1ngel Lagunas"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.19053",
    "title": "Blending Optimal Control and Biologically Plausible Learning for Noise-Robust Physical Neural Networks",
    "abstract": "           The rapidly increasing computational demands for artificial intelligence (AI) have spurred the exploration of computing principles beyond conventional digital computers. Physical neural networks (PNNs) offer efficient neuromorphic information processing by harnessing the innate computational power of physical processes; however, training their weight parameters is computationally expensive. We propose a training approach for substantially reducing this training cost. Our training approach merges an optimal control method for continuous-time dynamical systems with a biologically plausible training method--direct feedback alignment. In addition to the reduction of training time, this approach achieves robust processing even under measurement errors and noise without requiring detailed system information. The effectiveness was numerically and experimentally verified in an optoelectronic delay system. Our approach significantly extends the range of physical systems practically usable as PNNs.         ",
    "url": "https://arxiv.org/abs/2502.19053",
    "authors": [
      "Satoshi Sunada",
      "Tomoaki Niiyama",
      "Kazutaka Kanno",
      "Rin Nogami",
      "Andr\u00e9 R\u00f6hm",
      "Takato Awano",
      "Atsushi Uchida"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2502.19181",
    "title": "Multi-level Attention-guided Graph Neural Network for Image Restoration",
    "abstract": "           In recent years, deep learning has achieved remarkable success in the field of image restoration. However, most convolutional neural network-based methods typically focus on a single scale, neglecting the incorporation of multi-scale information. In image restoration tasks, local features of an image are often insufficient, necessitating the integration of global features to complement them. Although recent neural network algorithms have made significant strides in feature extraction, many models do not explicitly model global features or consider the relationship between global and local features. This paper proposes multi-level attention-guided graph neural network. The proposed network explicitly constructs element block graphs and element graphs within feature maps using multi-attention mechanisms to extract both local structural features and global representation information of the image. Since the network struggles to effectively extract global information during image degradation, the structural information of local feature blocks can be used to correct and supplement the global information. Similarly, when element block information in the feature map is missing, it can be refined using global element representation information. The graph within the network learns real-time dynamic connections through the multi-attention mechanism, and information is propagated and aggregated via graph convolution algorithms. By combining local element block information and global element representation information from the feature map, the algorithm can more effectively restore missing information in the image. Experimental results on several classic image restoration tasks demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2502.19181",
    "authors": [
      "Jiatao Jiang",
      "Zhen Cui",
      "Chunyan Xu",
      "Jian Yang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.19397",
    "title": "Modelling Chemical Reaction Networks using Neural Ordinary Differential Equations",
    "abstract": "           In chemical reaction network theory, ordinary differential equations are used to model the temporal change of chemical species concentration. As the functional form of these ordinary differential equations systems is derived from an empirical model of the reaction network, it may be incomplete. Our approach aims to elucidate these hidden insights in the reaction network by combining dynamic modelling with deep learning in the form of neural ordinary differential equations. Our contributions not only help to identify the shortcomings of existing empirical models but also assist the design of future reaction networks.         ",
    "url": "https://arxiv.org/abs/2502.19397",
    "authors": [
      "Anna C. M. Th\u00f6ni",
      "William E. Robinson",
      "Yoram Bachrach",
      "Wilhelm T. S. Huck",
      "Tal Kachman"
    ],
    "subjectives": [
      "Molecular Networks (q-bio.MN)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05390",
    "title": "Overcoming Annotation Bottlenecks in Underwater Fish Segmentation: A Robust Self-Supervised Learning Approach",
    "abstract": "           Accurate fish segmentation in underwater videos is challenging due to low visibility, variable lighting, and dynamic backgrounds, making fully-supervised methods that require manual annotation impractical for many applications. This paper introduces a novel self-supervised learning approach for fish segmentation using Deep Learning. Our model, trained without manual annotation, learns robust and generalizable representations by aligning features across augmented views and enforcing spatial-temporal consistency. We demonstrate its effectiveness on three challenging underwater video datasets: DeepFish, Seagrass, and YouTube-VOS, surpassing existing self-supervised methods and achieving segmentation accuracy comparable to fully-supervised methods without the need for costly annotations. Trained on DeepFish, our model exhibits strong generalization, achieving high segmentation accuracy on the unseen Seagrass and YouTube-VOS datasets. Furthermore, our model is computationally efficient due to its parallel processing and efficient anchor sampling technique, making it suitable for real-time applications and potential deployment on edge devices. We present quantitative results using Jaccard Index and Dice coefficient, as well as qualitative comparisons, showcasing the accuracy, robustness, and efficiency of our approach for advancing underwater video analysis         ",
    "url": "https://arxiv.org/abs/2206.05390",
    "authors": [
      "Alzayat Saleh",
      "Marcus Sheaves",
      "Dean Jerry",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2208.10662",
    "title": "How to Track and Segment Fish without Human Annotations: A Self-Supervised Deep Learning Approach",
    "abstract": "           Tracking fish movements and sizes of fish is crucial to understanding their ecology and behaviour. Knowing where fish migrate, how they interact with their environment, and how their size affects their behaviour can help ecologists develop more effective conservation and management strategies to protect fish populations and their habitats. Deep learning is a promising tool to analyze fish ecology from underwater videos. However, training deep neural networks (DNNs) for fish tracking and segmentation requires high-quality labels, which are expensive to obtain. We propose an alternative unsupervised approach that relies on spatial and temporal variations in video data to generate noisy pseudo-ground-truth labels. We train a multitask DNN using these pseudo-labels. Our framework consists of three stages: (1) an optical flow model generates the pseudo labels using spatial and temporal consistency between frames, (2) a self-supervised model refines the pseudo-labels incrementally, and (3) a segmentation network uses the refined labels for training. Consequently, we perform extensive experiments to validate our method on three public underwater video datasets and demonstrate its effectiveness for video annotation and segmentation. We also evaluate its robustness to different imaging conditions and discuss its limitations.         ",
    "url": "https://arxiv.org/abs/2208.10662",
    "authors": [
      "Alzayat Saleh",
      "Marcus Sheaves",
      "Dean Jerry",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2210.10418",
    "title": "Physics-informed Variational Autoencoders for Improved Robustness to Environmental Factors of Variation",
    "abstract": "           The combination of machine learning models with physical models is a recent research path to learn robust data representations. In this paper, we introduce p$^3$VAE, a variational autoencoder that integrates prior physical knowledge about the latent factors of variation that are related to the data acquisition conditions. p$^3$VAE combines standard neural network layers with non-trainable physics layers in order to partially ground the latent space to physical variables. We introduce a semi-supervised learning algorithm that strikes a balance between the machine learning part and the physics part. Experiments on simulated and real data sets demonstrate the benefits of our framework against competing physics-informed and conventional machine learning models, in terms of extrapolation capabilities and interpretability. In particular, we show that p$^3$VAE naturally has interesting disentanglement capabilities. Our code and data have been made publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2210.10418",
    "authors": [
      "Romain Thoreau",
      "Laurent Risser",
      "V\u00e9ronique Achard",
      "B\u00e9atrice Berthelot",
      "Xavier Briottet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2311.06015",
    "title": "Unlock Reliable Skill Inference for Quadruped Adaptive Behavior by Skill Graph",
    "abstract": "           Developing robotic intelligent systems that can adapt quickly to unseen wild situations is one of the critical challenges in pursuing autonomous robotics. Although some impressive progress has been made in walking stability and skill learning in the field of legged robots, their ability for fast adaptation is still inferior to that of animals in nature. Animals are born with a massive set of skills needed to survive, and can quickly acquire new ones, by composing fundamental skills with limited experience. Inspired by this, we propose a novel framework, named Robot Skill Graph (RSG) for organizing a massive set of fundamental skills of robots and dexterously reusing them for fast adaptation. Bearing a structure similar to the Knowledge Graph (KG), RSG is composed of massive dynamic behavioral skills instead of static knowledge in KG and enables discovering implicit relations that exist in between the learning context and acquired skills of robots, serving as a starting point for understanding subtle patterns existing in robots' skill learning. Extensive experimental results demonstrate that RSG can provide reliable skill inference upon new tasks and environments, and enable quadruped robots to adapt to new scenarios and quickly learn new skills.         ",
    "url": "https://arxiv.org/abs/2311.06015",
    "authors": [
      "Hongyin Zhang",
      "Diyuan Shi",
      "Zifeng Zhuang",
      "Han Zhao",
      "Zhenyu Wei",
      "Feng Zhao",
      "Sibo Gai",
      "Shangke Lyu",
      "Donglin Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2401.11647",
    "title": "LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised Learning",
    "abstract": "           Many studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw data distributed across edge devices. However, edge devices often struggle with high computational and communication costs imposed by SSL and FL algorithms. With the deployment of more complex and large-scale models, such as Transformers, these challenges are exacerbated. To tackle this, we propose the Layer-Wise Federated Self-Supervised Learning (LW-FedSSL) approach, which allows edge devices to incrementally train a small part of the model at a time. Specifically, in LW-FedSSL, training is decomposed into multiple stages, with each stage responsible for only a specific layer (or a block of layers) of the model. Since only a portion of the model is active for training at any given time, LW-FedSSL significantly reduces computational requirements. Additionally, only the active model portion needs to be exchanged between the FL server and clients, reducing the communication overhead. This enables LW-FedSSL to jointly address both computational and communication challenges in FL. Depending on the SSL algorithm used, it can achieve up to a $3.34 \\times$ reduction in memory usage, $4.20 \\times$ fewer computational operations (GFLOPs), and a $5.07 \\times$ lower communication cost while maintaining performance comparable to its end-to-end training counterpart. Furthermore, we explore a progressive training strategy called Prog-FedSSL, which offers a $1.84\\times$ reduction in GFLOPs and a $1.67\\times$ reduction in communication costs while maintaining the same memory requirements as end-to-end training. While the resource efficiency of Prog-FedSSL is lower than that of LW-FedSSL, its performance improvements make it a viable candidate for FL environments with more lenient resource constraints.         ",
    "url": "https://arxiv.org/abs/2401.11647",
    "authors": [
      "Ye Lin Tun",
      "Chu Myaet Thwal",
      "Huy Q. Le",
      "Minh N. H. Nguyen",
      "Choong Seon Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.18945",
    "title": "SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via Syntactic Transfer",
    "abstract": "           Although pre-training achieves remarkable performance, it suffers from task-agnostic backdoor attacks due to vulnerabilities in data and training mechanisms. These attacks can transfer backdoors to various downstream tasks. In this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning filter that mitigates such risks. To overcome the limitations of manual target setting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and universal task-agnostic backdoor attack via syntactic transfer, further exposing vulnerabilities in pre-trained language models (PLMs). Specifically, $\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training space through corpus poisoning, while preserving the PLM's pre-training capabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets based on contrastive learning, creating a uniform distribution in the pre-training space. To identify syntactic differences, we also introduce an awareness module to minimize interference between backdoors. Experiments show that $\\mathtt{SynGhost}$ poses significant threats and can transfer to various downstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on perplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2402.18945",
    "authors": [
      "Pengzhou Cheng",
      "Wei Du",
      "Zongru Wu",
      "Fengwei Zhang",
      "Libo Chen",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2403.00269",
    "title": "Large Convolutional Model Tuning via Filter Subspace",
    "abstract": "           Efficient fine-tuning methods are critical to address the high computational and parameter complexity while adapting large pre-trained models to downstream tasks. Our study is inspired by prior research that represents each convolution filter as a linear combination of a small set of filter subspace elements, referred to as filter atoms. In this paper, we propose to fine-tune pre-trained models by adjusting only filter atoms, which are responsible for spatial-only convolution, while preserving spatially-invariant channel combination knowledge in atom coefficients. In this way, we bring a new filter subspace view for model tuning. Furthermore, each filter atom can be recursively decomposed as a combination of another set of atoms, which naturally expands the number of tunable parameters in the filter subspace. By only adapting filter atoms constructed by a small number of parameters, while maintaining the rest of model parameters constant, the proposed approach is highly parameter-efficient. It effectively preserves the capabilities of pre-trained models and prevents overfitting to downstream tasks. Extensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks.         ",
    "url": "https://arxiv.org/abs/2403.00269",
    "authors": [
      "Wei Chen",
      "Zichen Miao",
      "Qiang Qiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.05061",
    "title": "RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features",
    "abstract": "           The inherent noisy and sparse characteristics of radar data pose challenges in finding effective representations for 3D object detection. In this paper, we propose RadarDistill, a novel knowledge distillation (KD) method, which can improve the representation of radar data by leveraging LiDAR data. RadarDistill successfully transfers desirable characteristics of LiDAR features into radar features using three key components: Cross-Modality Alignment (CMA), Activation-based Feature Distillation (AFD), and Proposal-based Feature Distillation (PFD). CMA enhances the density of radar features by employing multiple layers of dilation operations, effectively addressing the challenge of inefficient knowledge transfer from LiDAR to radar. AFD selectively transfers knowledge based on regions of the LiDAR features, with a specific focus on areas where activation intensity exceeds a predefined threshold. PFD similarly guides the radar network to selectively mimic features from the LiDAR network within the object proposals. Our comparative analyses conducted on the nuScenes datasets demonstrate that RadarDistill achieves state-of-the-art (SOTA) performance for radar-only object detection task, recording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantly improves the performance of the camera-radar fusion model.         ",
    "url": "https://arxiv.org/abs/2403.05061",
    "authors": [
      "Geonho Bang",
      "Kwangjin Choi",
      "Jisong Kim",
      "Dongsuk Kum",
      "Jun Won Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.01833",
    "title": "Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack",
    "abstract": "           Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we present Crescendomation, a tool that automates the Crescendo attack and demonstrate its efficacy against state-of-the-art models through our evaluations. Crescendomation surpasses other state-of-the-art jailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher performance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate Crescendo's ability to jailbreak multimodal models.         ",
    "url": "https://arxiv.org/abs/2404.01833",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem",
      "Ronen Eldan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.08588",
    "title": "Efficient Sensors Selection for Traffic Flow Monitoring: An Overview of Model-Based Techniques leveraging Network Observability",
    "abstract": "           The emergence of 6G-enabled Internet of Vehicles (IoV) promises to revolutionize mobility and connectivity, integrating vehicles into a mobile Internet of Things (IoT)-oriented wireless sensor network (WSN). Meanwhile, 5G technologies and mobile edge computing further support this vision by facilitating real-time connectivity and empowering massive access to the Internet. Within this context, IoT-oriented WSNs play a crucial role in intelligent transportation systems, offering affordable alternatives for traffic monitoring and management. Efficient sensor selection thus represents a critical concern while deploying WSNs on urban networks. In this paper, we provide an overview of such a notably hard problem. The contribution is twofold: (i) surveying state-of-the-art model-based techniques for efficient sensor selection in traffic flow monitoring, emphasizing challenges of sensor placement, and (ii) advocating for {the development of} data-driven methodologies to enhance sensor deployment efficacy and traffic modeling accuracy. Further considerations underscore the importance of data-driven approaches for adaptive transportation systems aligned with the IoV paradigm.         ",
    "url": "https://arxiv.org/abs/2404.08588",
    "authors": [
      "Marco Fabris",
      "Riccardo Ceccato",
      "Andrea Zanella"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2405.01979",
    "title": "Graph Neural Network based Active and Passive Beamforming for Distributed STAR-RIS-Assisted Multi-User MISO Systems",
    "abstract": "           This paper investigates a joint active and passive beamforming design for distributed simultaneous transmitting and reflecting (STAR) reconfigurable intelligent surface (RIS) assisted multi-user (MU)- mutiple input single output (MISO) systems, where the energy splitting (ES) mode is considered for the STAR-RIS. We aim to design the active beamforming vectors at the base station (BS) and the passive beamforming at the STAR-RIS to maximize the user sum rate under transmitting power constraints. The formulated problem is non-convex and nontrivial to obtain the global optimum due to the coupling between active beamforming vectors and STAR-RIS phase shifts. To efficiently solve the problem, we propose a novel graph neural network (GNN)-based framework. Specifically, we first model the interactions among users and network entities are using a heterogeneous graph representation. A heterogeneous graph neural network (HGNN) implementation is then introduced to directly optimizes beamforming vectors and STAR-RIS coefficients with the system objective. Numerical results show that the proposed approach yields efficient performance compared to the previous benchmarks. Furthermore, the proposed GNN is scalable with various system configurations.         ",
    "url": "https://arxiv.org/abs/2405.01979",
    "authors": [
      "Ha An Le",
      "Trinh Van Chien",
      "Wan Choi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2405.13961",
    "title": "SADDLe: Sharpness-Aware Decentralized Deep Learning with Heterogeneous Data",
    "abstract": "           Decentralized training enables learning with distributed datasets generated at different locations without relying on a central server. In realistic scenarios, the data distribution across these sparsely connected learning agents can be significantly heterogeneous, leading to local model over-fitting and poor global model generalization. Another challenge is the high communication cost of training models in such a peer-to-peer fashion without any central coordination. In this paper, we jointly tackle these two-fold practical challenges by proposing SADDLe, a set of sharpness-aware decentralized deep learning algorithms. SADDLe leverages Sharpness-Aware Minimization (SAM) to seek a flatter loss landscape during training, resulting in better model generalization as well as enhanced robustness to communication compression. We present two versions of our approach and conduct extensive experiments to show that SADDLe leads to 1-20% improvement in test accuracy compared to other existing techniques. Additionally, our proposed approach is robust to communication compression, with an average drop of only 1% in the presence of up to 4x compression.         ",
    "url": "https://arxiv.org/abs/2405.13961",
    "authors": [
      "Sakshi Choudhary",
      "Sai Aparna Aketi",
      "Kaushik Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2406.01477",
    "title": "MixMax: Distributional Robustness in Function Space via Optimal Data Mixtures",
    "abstract": "           Machine learning models are often required to perform well across several pre-defined settings, such as a set of user groups. Worst-case performance is a common metric to capture this requirement, and is the objective of group distributionally robust optimization (group DRO). Unfortunately, these methods struggle when the loss is non-convex in the parameters, or the model class is non-parametric. Here, we make a classical move to address this: we reparameterize group DRO from parameter space to function space, which results in a number of advantages. First, we show that group DRO over the space of bounded functions admits a minimax theorem. Second, for cross-entropy and mean squared error, we show that the minimax optimal mixture distribution is the solution of a simple convex optimization problem. Thus, provided one is working with a model class of universal function approximators, group DRO can be solved by a convex optimization problem followed by a classical risk minimization problem. We call our method MixMax. In our experi ments, we found that MixMax matched or outperformed the standard group DRO baselines, and in particular, MixMax improved the performance of XGBoost over the only baseline, data balancing, for variations of the ACSIncome and CelebA annotations datasets.         ",
    "url": "https://arxiv.org/abs/2406.01477",
    "authors": [
      "Anvith Thudi",
      "Chris J. Maddison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.01659",
    "title": "Limits of isotropic damage models for complex load paths -- beyond stress triaxiality and Lode angle parameter",
    "abstract": "           The stress triaxiality and the Lode angle parameter are two well established stress invariants for the characterization of damage evolution. This work assesses the limits of this tuple by using it for damage predictions in a continuum damage mechanics framework. Isotropic and anisotropic formulations of two well-established models are used to avoid model-specific restrictions. The damage evolution is analyzed for different load paths, while the stress triaxiality and the Lode angle parameter are controlled. The equivalent plastic strain is moreover added as a third parameter, but still does not suffice to uniquely define the damage state. As a consequence, well-established concepts such as fracture surfaces depending on this triple have to be taken with care, if complex paths are to be investgated. These include, e.g., load paths observed during metal forming applications with varying load directions or multiple stages.         ",
    "url": "https://arxiv.org/abs/2406.01659",
    "authors": [
      "K. Feike",
      "P. Kurzeja",
      "J. Mosler",
      "K. Langenfeld"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2406.16749",
    "title": "Inferring stochastic low-rank recurrent neural networks from neural data",
    "abstract": "           A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability.         ",
    "url": "https://arxiv.org/abs/2406.16749",
    "authors": [
      "Matthijs Pals",
      "A Erdem Sa\u011ftekin",
      "Felix Pei",
      "Manuel Gloeckler",
      "Jakob H Macke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.17337",
    "title": "Robust Pareto Transistor Sizing of GaN HEMTs for Millimeter-Wave Applications",
    "abstract": "           This paper introduces a robust Pareto design approach for transistor sizing of Gallium Nitride (GaN) High Electron Mobility Transistors (HEMTs), particularly for power amplifier (PA) and low-noise amplifier (LNA) designs in 5G applications. We consider five key design variables and two settings (PAs and LNAs) where we have multiple objectives. We assess designs based on three critical objectives, evaluating each by its worst-case performance across a range of Gate-Source Voltages ($V_{\\text{GS}}$). We conduct simulations across a range of $V_{\\text{GS}}$ values to ensure a thorough and robust analysis. For PAs, the optimization goals are to maximize the worst-case modulated average output power ($P_{\\text{out,avg}}$) and power-added efficiency ($\\text{PAE}_{\\text{avg}}$) while minimizing the worst-case average junction temperature ($T_{\\text{j,avg}}$) under a modulated 64-QAM signal stimulus. In contrast, for LNAs, the focus is on maximizing the worst-case maximum oscillation frequency ($f_{\\text{max}}$) and Gain, and minimizing the worst-case minimum noise figure ($\\text{NF}_{\\text{min}}$). We utilize a derivative-free optimization method to effectively identify robust Pareto optimal device designs. This approach enhances our comprehension of the trade-off space, facilitating more informed decision-making. Furthermore, this method is general across different applications. Although it does not guarantee a globally optimal design, we demonstrate its effectiveness in GaN transistor sizing. The primary advantage of this method is that it enables the attainment of near-optimal or even optimal designs with just a fraction of the simulations required for an exhaustive full-grid search.         ",
    "url": "https://arxiv.org/abs/2406.17337",
    "authors": [
      "Rafael Perez Martinez",
      "Stephen Boyd",
      "Srabanti Chowdhury"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2406.19963",
    "title": "Text2Robot: Evolutionary Robot Design from Text Descriptions",
    "abstract": "           Robot design has traditionally been costly and labor-intensive. Despite advancements in automated processes, it remains challenging to navigate a vast design space while producing physically manufacturable robots. We introduce Text2Robot, a framework that converts user text specifications and performance preferences into physical quadrupedal robots. Within minutes, Text2Robot can use text-to-3D models to provide strong initializations of diverse morphologies. Within a day, our geometric processing algorithms and body-control co-optimization produce a walking robot by explicitly considering real-world electronics and manufacturability. Text2Robot enables rapid prototyping and opens new opportunities for robot design with generative models.         ",
    "url": "https://arxiv.org/abs/2406.19963",
    "authors": [
      "Ryan P. Ringel",
      "Zachary S. Charlick",
      "Jiaxun Liu",
      "Boxi Xia",
      "Boyuan Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.02811",
    "title": "SPLITZ: Certifiable Robustness via Split Lipschitz Randomized Smoothing",
    "abstract": "           Certifiable robustness gives the guarantee that small perturbations around an input to a classifier will not change the prediction. There are two approaches to provide certifiable robustness to adversarial examples: a) explicitly training classifiers with small Lipschitz constants, and b) Randomized smoothing, which adds random noise to the input to create a smooth classifier. We propose SPLITZ, a practical and novel approach which leverages the synergistic benefits of both the above ideas into a single framework. Our main idea is to split a classifier into two halves, constrain the Lipschitz constant of the first half, and smooth the second half via randomization. Motivation for SPLITZ comes from the observation that many standard deep networks exhibit heterogeneity in Lipschitz constants across layers. SPLITZ can exploit this heterogeneity while inheriting the scalability of randomized smoothing. We present a principled approach to train SPLITZ and provide theoretical analysis to derive certified robustness guarantees during inference. We present a comprehensive comparison of robustness-accuracy trade-offs and show that SPLITZ consistently improves on existing state-of-the-art approaches in the MNIST, CIFAR-10 and ImageNet datasets. For instance, with $\\ell_2$ norm perturbation budget of $\\epsilon=1$, SPLITZ achieves $43.2\\%$ top-1 test accuracy on CIFAR-10 dataset compared to state-of-art top-1 test accuracy $39.8\\%$.         ",
    "url": "https://arxiv.org/abs/2407.02811",
    "authors": [
      "Meiyu Zhong",
      "Ravi Tandon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2407.02936",
    "title": "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models",
    "abstract": "           Evaluating the graph comprehension and reasoning abilities of Large Language Models (LLMs) is challenging and often incomplete. Existing benchmarks focus primarily on pure graph understanding, lacking a comprehensive evaluation across all graph types and detailed capability definitions. This paper presents GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and test models on pure graph and heterogeneous graphs, subdividing capabilities into 10 distinct areas tested through 19 tasks. Our benchmark includes 11 datasets with 5,140 graphs of varying complexity. We evaluate four closed-source and eight open-source LLMs, conducting thorough analyses from both ability and task perspectives. Key findings reveal that OpenAI o1 model has amazing comprehension and reasoning capabilities, semantic enrichment enhances reasoning performance, node ordering impacts task success, and the ability to process longer texts does not necessarily improve graph comprehension or this http URL is open-sourced at this https URL ",
    "url": "https://arxiv.org/abs/2407.02936",
    "authors": [
      "Zike Yuan",
      "Ming Liu",
      "Hui Wang",
      "Bing Qin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.07064",
    "title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation",
    "abstract": "           Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. OBJECTIVE: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. METHOD: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. RESULTS: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.         ",
    "url": "https://arxiv.org/abs/2407.07064",
    "authors": [
      "Catherine Tony",
      "Nicol\u00e1s E. D\u00edaz Ferreyra",
      "Markus Mutas",
      "Salem Dhiff",
      "Riccardo Scandariato"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.15240",
    "title": "BIGbench: A Unified Benchmark for Evaluating Multi-dimensional Social Biases in Text-to-Image Models",
    "abstract": "           Text-to-Image (T2I) generative models are becoming increasingly crucial due to their ability to generate high-quality images, but also raise concerns about social biases, particularly in human image generation. Sociological research has established systematic classifications of bias. Yet, existing studies on bias in T2I models largely conflate different types of bias, impeding methodological progress. In this paper, we introduce BIGbench, a unified benchmark for Biases of Image Generation, featuring a carefully designed dataset. Unlike existing benchmarks, BIGbench classifies and evaluates biases across four dimensions to enable a more granular evaluation and deeper analysis. Furthermore, BIGbench applies advanced multi-modal large language models to achieve fully automated and highly accurate evaluations. We apply BIGbench to evaluate eight representative T2I models and three debiasing methods. Our human evaluation results by trained evaluators from different races underscore BIGbench's effectiveness in aligning images and identifying various biases. Moreover, our study also reveals new research directions about biases with insightful analysis of our results. Our work is openly accessible at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.15240",
    "authors": [
      "Hanjun Luo",
      "Haoyu Huang",
      "Ziye Deng",
      "Xinfeng Li",
      "Hewei Wang",
      "Yingbin Jin",
      "Yang Liu",
      "XiaoFeng Wang",
      "Wenyuan Xu",
      "Zuozhu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.02922",
    "title": "Pose Magic: Efficient and Temporally Consistent Human Pose Estimation with a Hybrid Mamba-GCN Network",
    "abstract": "           Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are primarily based on Transformers. However, existing Transformer-based 3D HPE backbones often encounter a trade-off between accuracy and computational efficiency. To resolve the above dilemma, in this work, we leverage recent advances in state space models and utilize Mamba for high-quality and efficient long-range modeling. Nonetheless, Mamba still faces challenges in precisely exploiting local dependencies between joints. To address these issues, we propose a new attention-free hybrid spatiotemporal architecture named Hybrid Mamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN by capturing relationships between neighboring joints, thus producing new representations to complement Mamba's outputs. By adaptively fusing representations from Mamba and GCN, Pose Magic demonstrates superior capability in learning the underlying 3D structure. To meet the requirements of real-time inference, we also provide a fully causal version. Extensive experiments show that Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving $74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and the ability to generalize to unseen sequence lengths.         ",
    "url": "https://arxiv.org/abs/2408.02922",
    "authors": [
      "Xinyi Zhang",
      "Qiqi Bao",
      "Qinpeng Cui",
      "Wenming Yang",
      "Qingmin Liao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.04569",
    "title": "Activation degree thresholds and expressiveness of polynomial neural networks",
    "abstract": "           We study the expressive power of deep polynomial neural networks through the geometry of their neurovariety. We introduce the notion of the activation degree threshold of a network architecture to express when the dimension of the neurovariety achieves its theoretical maximum. We prove the existence of the activation degree threshold for all polynomial neural networks without width-one bottlenecks and demonstrate a universal upper bound that is quadratic in the width of largest size. In doing so, we prove the high activation degree conjecture of Kileel, Trager, and Bruna. Certain structured architectures have exceptional activation degree thresholds, making them especially expressive in the sense of their neurovariety dimension. In this direction, we prove that polynomial neural networks with equi-width architectures are maximally expressive by showing their activation degree threshold is one.         ",
    "url": "https://arxiv.org/abs/2408.04569",
    "authors": [
      "Bella Finkel",
      "Jose Israel Rodriguez",
      "Chenxi Wu",
      "Thomas Yahl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Algebraic Geometry (math.AG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.08861",
    "title": "The computational power of a human society: a new model of social evolution",
    "abstract": "           Social evolutionary theory seeks to explain increases in the scale and complexity of human societies, from origins to present. Over the course of the twentieth century, social evolutionary theory largely fell out of favor as a way of investigating human history, just when advances in complex systems science and computer science saw the emergence of powerful new conceptions of complex systems, and in particular new methods of measuring complexity. We propose that these advances in our understanding of complex systems and computer science should be brought to bear on our investigations into human history. To that end, we present a new framework for modeling how human societies co-evolve with their biotic environments, recognizing that both a society and its environment are computers. This leads us to model the dynamics of each of those two systems using the same, new kind of computational machine, which we define here. For simplicity, we construe a society as a set of interacting occupations and technologies. Similarly, under such a model, a biotic environment is a set of interacting distinct ecological and environmental processes. This provides novel ways to characterize social complexity, which we hope will cast new light on the archaeological and historical records. Our framework also provides a natural way to formalize both the energetic (thermodynamic) costs required by a society as it runs, and the ways it can extract thermodynamic resources from the environment in order to pay for those costs -- and perhaps to grow with any left-over resources.         ",
    "url": "https://arxiv.org/abs/2408.08861",
    "authors": [
      "David H. Wolpert",
      "Kyle Harper"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "General Economics (econ.GN)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2408.10003",
    "title": "Towards a Knowledge Graph for Models and Algorithms in Applied Mathematics",
    "abstract": "           Mathematical models and algorithms are an essential part of mathematical research data, as they are epistemically grounding numerical data. In order to represent models and algorithms as well as their relationship semantically to make this research data FAIR, two previously distinct ontologies were merged and extended, becoming a living knowledge graph. The link between the two ontologies is established by introducing computational tasks, as they occur in modeling, corresponding to algorithmic tasks. Moreover, controlled vocabularies are incorporated and a new class, distinguishing base quantities from specific use case quantities, was introduced. Also, both models and algorithms can now be enriched with metadata. Subject-specific metadata is particularly relevant here, such as the symmetry of a matrix or the linearity of a mathematical model. This is the only way to express specific workflows with concrete models and algorithms, as the feasible solution algorithm can only be determined if the mathematical properties of a model are known. We demonstrate this using two examples from different application areas of applied mathematics. In addition, we have already integrated over 250 research assets from applied mathematics into our knowledge graph.         ",
    "url": "https://arxiv.org/abs/2408.10003",
    "authors": [
      "Bj\u00f6rn Schembera",
      "Frank W\u00fcbbeling",
      "Hendrik Kleikamp",
      "Burkhard Schmidt",
      "Aurela Shehu",
      "Marco Reidelbach",
      "Christine Biedinger",
      "Jochen Fiedler",
      "Thomas Koprucki",
      "Dorothea Iglezakis",
      "Dominik G\u00f6ddeke"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2408.16285",
    "title": "ART: Actually Robust Training",
    "abstract": "           Current interest in deep learning captures the attention of many programmers and researchers. Unfortunately, the lack of a unified schema for developing deep learning models results in methodological inconsistencies, unclear documentation, and problems with reproducibility. Some guidelines have been proposed, yet currently, they lack practical implementations. Furthermore, neural network training often takes on the form of trial and error, lacking a structured and thoughtful process. To alleviate these issues, in this paper, we introduce Art, a Python library designed to help automatically impose rules and standards while developing deep learning pipelines. Art divides model development into a series of smaller steps of increasing complexity, each concluded with a validation check improving the interpretability and robustness of the process. The current version of Art comes equipped with nine predefined steps inspired by Andrej Karpathy's Recipe for Training Neural Networks, a visualization dashboard, and integration with loggers such as Neptune. The code related to this paper is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.16285",
    "authors": [
      "Sebastian Chwilczy\u0144ski",
      "Kacper Tr\u0119bacz",
      "Karol Cyganik",
      "Mateusz Ma\u0142ecki",
      "Dariusz Brzezinski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2409.02685",
    "title": "RouterRetriever: Routing over a Mixture of Expert Embedding Models",
    "abstract": "           Information retrieval methods often rely on a single embedding model trained on large, general-domain datasets like MSMARCO. While this approach can produce a retriever with reasonable overall performance, they often underperform models trained on domain-specific data when testing on their respective domains. Prior work in information retrieval has tackled this through multi-task training, but the idea of routing over a mixture of domain-specific expert retrievers remains unexplored despite the popularity of such ideas in language model generation research. In this work, we introduce RouterRetriever, a retrieval model that leverages a mixture of domain-specific experts by using a routing mechanism to select the most appropriate expert for each query. RouterRetriever is lightweight and allows easy addition or removal of experts without additional training. Evaluation on the BEIR benchmark demonstrates that RouterRetriever outperforms both models trained on MSMARCO (+2.1 absolute nDCG@10) and multi-task models (+3.2). This is achieved by employing our routing mechanism, which surpasses other routing techniques (+1.8 on average) commonly used in language modeling. Furthermore, the benefit generalizes well to other datasets, even in the absence of a specific expert on the dataset. RouterRetriever is the first work to demonstrate the advantages of routing over a mixture of domain-specific expert embedding models as an alternative to a single, general-purpose embedding model, especially when retrieving from diverse, specialized domains.         ",
    "url": "https://arxiv.org/abs/2409.02685",
    "authors": [
      "Hyunji Lee",
      "Luca Soldaini",
      "Arman Cohan",
      "Minjoon Seo",
      "Kyle Lo"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.04796",
    "title": "Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection",
    "abstract": "           Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods. Code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.04796",
    "authors": [
      "Fanhu Zeng",
      "Zhen Cheng",
      "Fei Zhu",
      "Hongxin Wei",
      "Xu-Yao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.06525",
    "title": "MENSA: A Multi-Event Network for Survival Analysis with Trajectory-based Likelihood Estimation",
    "abstract": "           A multi-event survival model predicts the time until an instance experiences each of several different events, given the instance's description. Unlike competing-event models, the events here are not mutually exclusive and often exhibit statistical dependencies. Existing approaches for multi-event survival analysis have generally been limited, most focusing on producing simple risk scores for each event, rather than the time-to-event itself. To overcome these issues, we introduce MENSA, a novel deep learning approach for multi-event survival analysis. MENSA jointly learns representations of the input features while capturing the complex dependence structure among events. In practice, it attempts to optimize the sum of the traditional negative log-likelihood across events and a novel trajectory-based likelihood, which encourages the model to learn the temporal order in which events occur. Experiments on real-world clinical datasets show that MENSA consistently gives good discrimination performances and accurate time-to-event predictions in single-event, competing-risk, and multi-event problems. Additionally, MENSA is more computationally-efficient, requiring fewer parameters and FLOPs than multiple state-of-the-art survival baselines when applied to large-dimensional datasets (more than 100 features).         ",
    "url": "https://arxiv.org/abs/2409.06525",
    "authors": [
      "Christian Marius Lillelund",
      "Ali Hossein Gharari Foomani",
      "Weijie Sun",
      "Shi-ang Qi",
      "Russell Greiner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.06706",
    "title": "SAN: Hypothesizing Long-Term Synaptic Development and Neural Engram Mechanism in Scalable Model's Parameter-Efficient Fine-Tuning",
    "abstract": "           Advances in Parameter-Efficient Fine-Tuning (PEFT) bridged the performance gap with Full Fine-Tuning (FFT) through sophisticated analysis of pre-trained parameter spaces. Starting from drawing insights from Neural Engrams (NE) in Biological Neural Networks (BNNs), we establish a connection between the low-rank property observed during PEFT's parameter space shifting and neurobiological mechanisms. This observation leads to our proposed method, Synapse and Neuron (SAN), which decomposes and propagates scaling components from anterior feature adjusting vectors towards posterior weight matrices. Our approach is theoretically grounded in Long-Term Potentiation/Depression (LTP/D) phenomena, which govern synapse development through neurotransmitter release modulation. Extensive experiments demonstrate its effectiveness: on \\textbf{vision tasks} across VTAB, FGVC, and GIC (25 datasets) using ViT, SwinT and ConvNeXt, SAN outperforms FFT up to 8.7% and LoRA by 3.2%; on language tasks using Commonsense Reasoning (8 datasets) with LLaMA models (all generations), surpassing ChatGPT up to 8.5% and LoRA by 4.7%; on visual-language tasks using Mixed Visual Instruction (7 datasets) with LLaVA models, it exceeds FFT up to 2.4% and LoRA by 1.9%. Our code and W&B log will be released.         ",
    "url": "https://arxiv.org/abs/2409.06706",
    "authors": [
      "Gaole Dai",
      "Chun-Kai Fan",
      "Yiming Tang",
      "Zhi Zhang",
      "Yuan Zhang",
      "Yulu Gan",
      "Qizhe Zhang",
      "Cheng-Ching Tseng",
      "Shanghang Zhang",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.09870",
    "title": "TransForce: Transferable Force Prediction for Vision-based Tactile Sensors with Sequential Image Translation",
    "abstract": "           Vision-based tactile sensors (VBTSs) provide high-resolution tactile images crucial for robot in-hand manipulation. However, force sensing in VBTSs is underutilized due to the costly and time-intensive process of acquiring paired tactile images and force labels. In this study, we introduce a transferable force prediction model, TransForce, designed to leverage collected image-force paired data for new sensors under varying illumination colors and marker patterns while improving the accuracy of predicted forces, especially in the shear direction. Our model effectively achieves translation of tactile images from the source domain to the target domain, ensuring that the generated tactile images reflect the illumination colors and marker patterns of the new sensors while accurately aligning the elastomer deformation observed in existing sensors, which is beneficial to force prediction of new sensors. As such, a recurrent force prediction model trained with generated sequential tactile images and existing force labels is employed to estimate higher-accuracy forces for new sensors with lowest average errors of 0.69N (5.8\\% in full work range) in $x$-axis, 0.70N (5.8\\%) in $y$-axis, and 1.11N (6.9\\%) in $z$-axis compared with models trained with single images. The experimental results also reveal that pure marker modality is more helpful than the RGB modality in improving the accuracy of force in the shear direction, while the RGB modality show better performance in the normal direction.         ",
    "url": "https://arxiv.org/abs/2409.09870",
    "authors": [
      "Zhuo Chen",
      "Ni Ou",
      "Xuyang Zhang",
      "Shan Luo"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2409.10262",
    "title": "Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation",
    "abstract": "           DETR introduces a simplified one-stage framework for scene graph generation (SGG) but faces challenges of sparse supervision and false negative samples. The former occurs because each image typically contains fewer than 10 relation annotations, while DETR-based SGG models employ over 100 relation queries. Each ground truth relation is assigned to only one query during training. The latter arises when one ground truth relation may have multiple queries with similar matching scores, leading to suboptimally matched queries being treated as negative samples. To address these, we propose Hydra-SGG, a one-stage SGG method featuring a Hybrid Relation Assignment. This approach combines a One-to-One Relation Assignment with an IoU-based One-to-Many Relation Assignment, increasing positive training samples and mitigating sparse supervision. In addition, we empirically demonstrate that removing self-attention between relation queries leads to duplicate predictions, which actually benefits the proposed One-to-Many Relation Assignment. With this insight, we introduce Hydra Branch, an auxiliary decoder without self-attention layers, to further enhance One-to-Many Relation Assignment by promoting different queries to make the same relation prediction. Hydra-SGG achieves state-of-the-art performance on multiple datasets, including VG150 (16.0 mR@50), Open Images V6 (50.1 weighted score), and GQA (12.7 mR@50).         ",
    "url": "https://arxiv.org/abs/2409.10262",
    "authors": [
      "Minghan Chen",
      "Guikun Chen",
      "Wenguan Wang",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.20171",
    "title": "Annotation-Free Curb Detection Leveraging Altitude Difference Image",
    "abstract": "           Road curbs are considered as one of the crucial and ubiquitous traffic features, which are essential for ensuring the safety of autonomous vehicles. Current methods for detecting curbs primarily rely on camera imagery or LiDAR point clouds. Image-based methods are vulnerable to fluctuations in lighting conditions and exhibit poor robustness, while methods based on point clouds circumvent the issues associated with lighting variations. However, it is the typical case that significant processing delays are encountered due to the voluminous amount of 3D points contained in each frame of the point cloud data. Furthermore, the inherently unstructured characteristics of point clouds poses challenges for integrating the latest deep learning advancements into point cloud data applications. To address these issues, this work proposes an annotation-free curb detection method leveraging Altitude Difference Image (ADI), which effectively mitigates the aforementioned challenges. Given that methods based on deep learning generally demand extensive, manually annotated datasets, which are both expensive and labor-intensive to create, we present an Automatic Curb Annotator (ACA) module. This module utilizes a deterministic curb detection algorithm to automatically generate a vast quantity of training data. Consequently, it facilitates the training of the curb detection model without necessitating any manual annotation of data. Finally, by incorporating a post-processing module, we manage to achieve state-of-the-art results on the KITTI 3D curb dataset with considerably reduced processing delays compared to existing methods, which underscores the effectiveness of our approach in curb detection tasks.         ",
    "url": "https://arxiv.org/abs/2409.20171",
    "authors": [
      "Fulong Ma",
      "Peng Hou",
      "Yuxuan Liu",
      "Yang Liu",
      "Ming Liu",
      "Jun Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.03171",
    "title": "Dual Selective Fusion Transformer Network for Hyperspectral Image Classification",
    "abstract": "           Transformer has achieved satisfactory results in the field of hyperspectral image (HSI) classification. However, existing Transformer models face two key challenges when dealing with HSI scenes characterized by diverse land cover types and rich spectral information: (1) A fixed receptive field overlooks the effective contextual scales required by various HSI objects; (2) invalid self-attention features in context fusion affect model performance. To address these limitations, we propose a novel Dual Selective Fusion Transformer Network (DSFormer) for HSI classification. DSFormer achieves joint spatial and spectral contextual modeling by flexibly selecting and fusing features across different receptive fields, effectively reducing unnecessary information interference by focusing on the most relevant spatial-spectral tokens. Specifically, we design a Kernel Selective Fusion Transformer Block (KSFTB) to learn an optimal receptive field by adaptively fusing spatial and spectral features across different scales, enhancing the model's ability to accurately identify diverse HSI objects. Additionally, we introduce a Token Selective Fusion Transformer Block (TSFTB), which strategically selects and combines essential tokens during the spatial-spectral self-attention fusion process to capture the most crucial contexts. Extensive experiments conducted on four benchmark HSI datasets demonstrate that the proposed DSFormer significantly improves land cover classification accuracy, outperforming existing state-of-the-art methods. Specifically, DSFormer achieves overall accuracies of 96.59%, 97.66%, 95.17%, and 94.59% in the Pavia University, Houston, Indian Pines, and Whu-HongHu datasets, respectively, reflecting improvements of 3.19%, 1.14%, 0.91%, and 2.80% over the previous model. The code will be available online at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.03171",
    "authors": [
      "Yichu Xu",
      "Di Wang",
      "Lefei Zhang",
      "Liangpei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.08338",
    "title": "Time Traveling to Defend Against Adversarial Example Attacks in Image Classification",
    "abstract": "           Adversarial example attacks have emerged as a critical threat to machine learning. Adversarial attacks in image classification abuse various, minor modifications to the image that confuse the image classification neural network -- while the image still remains recognizable to humans. One important domain where the attacks have been applied is in the automotive setting with traffic sign classification. Researchers have demonstrated that adding stickers, shining light, or adding shadows are all different means to make machine learning inference algorithms mis-classify the traffic signs. This can cause potentially dangerous situations as a stop sign is recognized as a speed limit sign causing vehicles to ignore it and potentially leading to accidents. To address these attacks, this work focuses on enhancing defenses against such adversarial attacks. This work shifts the advantage to the user by introducing the idea of leveraging historical images and majority voting. While the attacker modifies a traffic sign that is currently being processed by the victim's machine learning inference, the victim can gain advantage by examining past images of the same traffic sign. This work introduces the notion of ''time traveling'' and uses historical Street View images accessible to anybody to perform inference on different, past versions of the same traffic sign. In the evaluation, the proposed defense has 100% effectiveness against latest adversarial example attack on traffic sign classification algorithm.         ",
    "url": "https://arxiv.org/abs/2410.08338",
    "authors": [
      "Anthony Etim",
      "Jakub Szefer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.09156",
    "title": "On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning",
    "abstract": "           We study the discriminative probabilistic modeling on a continuous domain for the data prediction task of (multimodal) self-supervised representation learning. To address the challenge of computing the integral in the partition function for each anchor data, we leverage the multiple importance sampling (MIS) technique for robust Monte Carlo integration, which can recover InfoNCE-based contrastive loss as a special case. Within this probabilistic modeling framework, we conduct generalization error analysis to reveal the limitation of current InfoNCE-based contrastive loss for self-supervised representation learning and derive insights for developing better approaches by reducing the error of Monte Carlo integration. To this end, we propose a novel non-parametric method for approximating the sum of conditional probability densities required by MIS through convex optimization, yielding a new contrastive objective for self-supervised representation learning. Moreover, we design an efficient algorithm for solving the proposed objective. We empirically compare our algorithm to representative baselines on the contrastive image-language pretraining task. Experimental results on the CC3M and CC12M datasets demonstrate the superior overall performance of our algorithm. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.09156",
    "authors": [
      "Bokun Wang",
      "Yunwen Lei",
      "Yiming Ying",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.09343",
    "title": "ELICIT: LLM Augmentation via External In-Context Capability",
    "abstract": "           Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data and computational resources, especially for enhancing specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage. Inspired by the expression of in-context learned capabilities through task vectors and the concept of modularization, we propose \\alg, a framework consisting of two modules designed to effectively store and reuse task vectors to elicit the diverse capabilities of models without additional training or inference tokens. Our comprehensive experiments and analysis demonstrate that our pipeline is highly transferable across different input formats, tasks, and model architectures. ELICIT serves as a plug-and-play performance booster to enable adaptive elicitation of model capabilities. By externally storing and reusing vectors that represent in-context learned capabilities, \\alg not only demonstrates the potential to operate modular capabilities but also significantly enhances the performance, versatility, adaptability, and scalability of large language models. Our code will be publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.09343",
    "authors": [
      "Futing Wang",
      "Jianhao Yan",
      "Yue Zhang",
      "Tao Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.10174",
    "title": "Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations",
    "abstract": "           Variational Autoencoders (VAEs) are a powerful framework for learning latent representations of reduced dimensionality, while Neural ODEs excel in learning transient system dynamics. This work combines the strengths of both to generate fast surrogate models with adjustable complexity reacting on time-varying inputs signals. By leveraging the VAE's dimensionality reduction using a nonhierarchical prior, our method adaptively assigns stochastic noise, naturally complementing known NeuralODE training enhancements and enabling probabilistic time series modeling. We show that standard Latent ODEs struggle with dimensionality reduction in systems with time-varying inputs. Our approach mitigates this by continuously propagating variational parameters through time, establishing fixed information channels in latent space. This results in a flexible and robust method that can learn different system complexities, e.g. deep neural networks or linear matrices. Hereby, it enables efficient approximation of the Koopman operator without the need for predefining its dimensionality. As our method balances dimensionality reduction and reconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We demonstrate the effectiveness of this methods on several academic and real-world test cases, e.g. a power plant or MuJoCo data.         ",
    "url": "https://arxiv.org/abs/2410.10174",
    "authors": [
      "Julius Aka",
      "Johannes Brunnemann",
      "J\u00f6rg Eiden",
      "Arne Speerforck",
      "Lars Mikelsons"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.11835",
    "title": "Aligned Datasets Improve Detection of Latent Diffusion-Generated Images",
    "abstract": "           As latent diffusion models (LDMs) democratize image generation capabilities, there is a growing need to detect fake images. A good detector should focus on the generative models fingerprints while ignoring image properties such as semantic content, resolution, file format, etc. Fake image detectors are usually built in a data driven way, where a model is trained to separate real from fake images. Existing works primarily investigate network architecture choices and training recipes. In this work, we argue that in addition to these algorithmic choices, we also require a well aligned dataset of real/fake images to train a robust detector. For the family of LDMs, we propose a very simple way to achieve this: we reconstruct all the real images using the LDMs autoencoder, without any denoising operation. We then train a model to separate these real images from their reconstructions. The fakes created this way are extremely similar to the real ones in almost every aspect (e.g., size, aspect ratio, semantic content), which forces the model to look for the LDM decoders artifacts. We empirically show that this way of creating aligned real/fake datasets, which also sidesteps the computationally expensive denoising process, helps in building a detector that focuses less on spurious correlations, something that a very popular existing method is susceptible to. Finally, to demonstrate just how effective the alignment in a dataset can be, we build a detector using images that are not natural objects, and present promising results. Overall, our work identifies the subtle but significant issues that arise when training a fake image detector and proposes a simple and inexpensive solution to address these problems.         ",
    "url": "https://arxiv.org/abs/2410.11835",
    "authors": [
      "Anirudh Sundara Rajan",
      "Utkarsh Ojha",
      "Jedidiah Schloesser",
      "Yong Jae Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.20275",
    "title": "Advancing Hybrid Quantum Neural Network for Alternative Current Optimal Power Flow",
    "abstract": "           Alternative Current Optimal Power Flow (AC-OPF) is essential for efficient planning and real-time operation in power systems but is NP-hard and non-convex, leading to significant computational challenges. Neural networks (NNs) offer computational speedups in solving OPF but face issues like dependency on large datasets, scalability limitations, and inability to enforce physical constraints, compromising solution reliability. To overcome these limitations, this paper proposes hybrid Quantum Neural Networks (QNNs) that integrate quantum computing principles into neural network architectures. Leveraging quantum mechanics properties such as superposition and entanglement, QNNs can capture complex input-output relationships more effectively and learn from small or noisy this http URL further improve the performance of QNNs and investigate the interplay between classical and quantum components in hybrid architectures, we incorporate advanced techniques, including residual learning and physics-informed machine learning, into the hybrid QNN designs. These enhancements aim to boost convergence efficiency, lower errors, superior generalization, and robustness to quantum noise. Simulation results demonstrate that these enhanced hybrid QNNs outperform typical hybrid QNNs in solving OPF problems. This work provides valuable insights into the design and optimization of hybrid QNNs, highlighting the potential of quantum computation for broader applications in power systems.         ",
    "url": "https://arxiv.org/abs/2410.20275",
    "authors": [
      "Ze Hu",
      "Ziqing Zhu",
      "Linghua Zh",
      "Xiang Wei",
      "Siqi Bu",
      "Ka Wing Chan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.18776",
    "title": "Fall Leaf Adversarial Attack on Traffic Sign Classification",
    "abstract": "           Adversarial input image perturbation attacks have emerged as a significant threat to machine learning algorithms, particularly in image classification setting. These attacks involve subtle perturbations to input images that cause neural networks to misclassify the input images, even though the images remain easily recognizable to humans. One critical area where adversarial attacks have been demonstrated is in automotive systems where traffic sign classification and recognition is critical, and where misclassified images can cause autonomous systems to take wrong actions. This work presents a new class of adversarial attacks. Unlike existing work that has focused on adversarial perturbations that leverage human-made artifacts to cause the perturbations, such as adding stickers, paint, or shining flashlights at traffic signs, this work leverages nature-made artifacts: tree leaves. By leveraging nature-made artifacts, the new class of attacks has plausible deniability: a fall leaf stuck to a street sign could come from a near-by tree, rather than be placed there by an malicious human attacker. To evaluate the new class of the adversarial input image perturbation attacks, this work analyses how fall leaves can cause misclassification in street signs. The work evaluates various leaves from different species of trees, and considers various parameters such as size, color due to tree leaf type, and rotation. The work demonstrates high success rate for misclassification. The work also explores the correlation between successful attacks and how they affect the edge detection, which is critical in many image classification algorithms.         ",
    "url": "https://arxiv.org/abs/2411.18776",
    "authors": [
      "Anthony Etim",
      "Jakub Szefer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2412.02764",
    "title": "Drawing Pandas: A Benchmark for LLMs in Generating Plotting Code",
    "abstract": "           This paper introduces the human-curated PandasPlotBench dataset, designed to evaluate language models' effectiveness as assistants in visual data exploration. Our benchmark focuses on generating code for visualizing tabular data - such as a Pandas DataFrame - based on natural language instructions, complementing current evaluation tools and expanding their scope. The dataset includes 175 unique tasks. Our experiments assess several leading Large Language Models (LLMs) across three visualization libraries: Matplotlib, Seaborn, and Plotly. We show that the shortening of tasks has a minimal effect on plotting capabilities, allowing for the user interface that accommodates concise user input without sacrificing functionality or accuracy. Another of our findings reveals that while LLMs perform well with popular libraries like Matplotlib and Seaborn, challenges persist with Plotly, highlighting areas for improvement. We hope that the modular design of our benchmark will broaden the current studies on generating visualizations. Our dataset and benchmark code are available online: this https URL this https URL.         ",
    "url": "https://arxiv.org/abs/2412.02764",
    "authors": [
      "Timur Galimzyanov",
      "Sergey Titov",
      "Yaroslav Golubev",
      "Egor Bogomolov"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.04697",
    "title": "Privacy-Preserving Retrieval-Augmented Generation with Differential Privacy",
    "abstract": "           With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval-augmented generation (RAG) is particularly effective -- it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$ across different models and datasets.         ",
    "url": "https://arxiv.org/abs/2412.04697",
    "authors": [
      "Tatsuki Koga",
      "Ruihan Wu",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.06545",
    "title": "On How Iterative Magnitude Pruning Discovers Local Receptive Fields in Fully Connected Neural Networks",
    "abstract": "           Since its use in the Lottery Ticket Hypothesis, iterative magnitude pruning (IMP) has become a popular method for extracting sparse subnetworks that can be trained to high performance. Despite this, the underlying nature of IMP's general success remains unclear. One possibility is that IMP is especially capable of extracting and maintaining strong inductive biases. In support of this, recent work has shown that applying IMP to fully connected neural networks (FCNs) leads to the emergence of local receptive fields (RFs), an architectural feature present in mammalian visual cortex and convolutional neural networks. The question of how IMP is able to do this remains unanswered. Inspired by results showing that training FCNs on synthetic images with highly non-Gaussian statistics (e.g., sharp edges) is sufficient to drive the formation of local RFs, we hypothesize that IMP iteratively increases the non-Gaussian statistics present in the representations of FCNs, creating a feedback loop that enhances localization. We develop a new method for measuring the effect of individual weights on the statistics of the FCN representations (\"cavity method\"), which allows us to find evidence in support of this hypothesis. Our work, which is the first to study the effect IMP has on the statistics of the representations of neural networks, sheds parsimonious light on one way in which IMP can drive the formation of strong inductive biases.         ",
    "url": "https://arxiv.org/abs/2412.06545",
    "authors": [
      "William T. Redman",
      "Zhangyang Wang",
      "Alessandro Ingrosso",
      "Sebastian Goldt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.00855",
    "title": "What is a Social Media Bot? A Global Comparison of Bot and Human Characteristics",
    "abstract": "           Chatter on social media is 20% bots and 80% humans. Chatter by bots and humans is consistently different: bots tend to use linguistic cues that can be easily automated while humans use cues that require dialogue understanding. Bots use words that match the identities they choose to present, while humans may send messages that are not related to the identities they present. Bots and humans differ in their communication structure: sampled bots have a star interaction structure, while sampled humans have a hierarchical structure. These conclusions are based on a large-scale analysis of social media tweets across ~200mil users across 7 events. Social media bots took the world by storm when social-cybersecurity researchers realized that social media users not only consisted of humans but also of artificial agents called bots. These bots wreck havoc online by spreading disinformation and manipulating narratives. Most research on bots are based on special-purposed definitions, mostly predicated on the event studied. This article first begins by asking, \"What is a bot?\", and we study the underlying principles of how bots are different from humans. We develop a first-principle definition of a social media bot. With this definition as a premise, we systematically compare characteristics between bots and humans across global events, and reflect on how the software-programmed bot is an Artificial Intelligent algorithm, and its potential for evolution as technology advances. Based on our results, we provide recommendations for the use and regulation of bots. Finally, we discuss open challenges and future directions: Detect, to systematically identify these automated and potentially evolving bots; Differentiate, to evaluate the goodness of the bot in terms of their content postings and relationship interactions; Disrupt, to moderate the impact of malicious bots.         ",
    "url": "https://arxiv.org/abs/2501.00855",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Kathleen M. Carley"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2501.04453",
    "title": "Gradient Purification: Defense Against Poisoning Attack in Decentralized Federated Learning",
    "abstract": "           Decentralized federated learning (DFL) is inherently vulnerable to poisoning attacks, as malicious clients can transmit manipulated model gradients to neighboring clients. Existing defense methods either reject suspicious gradients per iteration or restart DFL aggregation after detecting all malicious clients. They overlook the potential accuracy benefit from the discarded malicious gradients. In this paper, we propose a novel gradient purification defense, named GPD, that integrates seamlessly with existing DFL aggregation to defend against poisoning attacks. It aims to mitigate the harm in model gradients while retaining the benefit in model weights for enhancing accuracy. For each benign client in GPD, a recording variable is designed to track the historically aggregated gradients from one of its neighbors. It allows benign clients to precisely detect malicious neighbors and swiftly mitigate aggregated malicious gradients via historical consistency checks. Upon mitigation, GPD optimizes model weights via aggregating gradients solely from benign clients. This retains the previously beneficial portions from malicious clients and exploits the contributions from benign clients, thereby significantly enhancing the model accuracy. We analyze the convergence of GPD, as well as its ability to harvest high accuracy. Extensive experiments over three datasets demonstrate that, GPD is capable of mitigating poisoning attacks under both iid and non-iid data distributions. It significantly outperforms state-of-the-art defenses in terms of accuracy against various poisoning attacks.         ",
    "url": "https://arxiv.org/abs/2501.04453",
    "authors": [
      "Bin Li",
      "Xiaoye Miao",
      "Yan Zhang",
      "Jianwei Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.11410",
    "title": "Orbit-Aware Split Learning: Optimizing LEO Satellite Networks for Distributed Online Learning",
    "abstract": "           This paper proposes a novel split learning architecture designed to exploit the cyclical movement of Low Earth Orbit (LEO) satellites in non-terrestrial networks (NTNs). Although existing research focuses on offloading tasks to the NTN infrastructure, these approaches overlook the dynamic movement patterns of LEO satellites that can be used to efficiently distribute the learning task. In this work, we analyze how LEO satellites, from the perspective of ground terminals, can participate in a time-window-based model training. By splitting the model between a LEO and a ground terminal, the computational burden on the satellite segment is reduced, while each LEO satellite offloads the partially trained model to the next satellite in the constellation. This cyclical training process allows larger and more energy-intensive models to be deployed and trained across multiple LEO satellites, despite their limited energy resources. We formulate an optimization problem that manages radio and processing resources, ensuring the entire data is processed during each satellite pass while minimizing the energy consumption. Our results demonstrate that this approach offers a more scalable and energy-efficient way to train complex models, enhancing the capabilities of LEO satellite constellations in the context of Artificial Intelligence-driven applications.         ",
    "url": "https://arxiv.org/abs/2501.11410",
    "authors": [
      "Marc Martinez-Gost",
      "Ana P\u00e9rez-Neira"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2501.12431",
    "title": "Modality Interactive Mixture-of-Experts for Fake News Detection",
    "abstract": "           The proliferation of fake news on social media platforms disproportionately impacts vulnerable populations, eroding trust, exacerbating inequality, and amplifying harmful narratives. Detecting fake news in multimodal contexts -- where deceptive content combines text and images -- is particularly challenging due to the nuanced interplay between modalities. Existing multimodal fake news detection methods often emphasize cross-modal consistency but ignore the complex interactions between text and visual elements, which may complement, contradict, or independently influence the predicted veracity of a post. To address these challenges, we present Modality Interactive Mixture-of-Experts for Fake News Detection (MIMoE-FND), a novel hierarchical Mixture-of-Experts framework designed to enhance multimodal fake news detection by explicitly modeling modality interactions through an interaction gating mechanism. Our approach models modality interactions by evaluating two key aspects of modality interactions: unimodal prediction agreement and semantic alignment. The hierarchical structure of MIMoE-FND allows for distinct learning pathways tailored to different fusion scenarios, adapting to the unique characteristics of each modality interaction. By tailoring fusion strategies to diverse modality interaction scenarios, MIMoE-FND provides a more robust and nuanced approach to multimodal fake news detection. We evaluate our approach on three real-world benchmarks spanning two languages, demonstrating its superior performance compared to state-of-the-art methods. By enhancing the accuracy and interpretability of fake news detection, MIMoE-FND offers a promising tool to mitigate the spread of misinformation, with the potential to better safeguard vulnerable communities against its harmful effects.         ",
    "url": "https://arxiv.org/abs/2501.12431",
    "authors": [
      "Yifan Liu",
      "Yaokun Liu",
      "Zelin Li",
      "Ruichen Yao",
      "Yang Zhang",
      "Dong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2501.16289",
    "title": "Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles",
    "abstract": "           Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Additionally, our MSCN enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds. This method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments. This highlights MSCN's potential to significantly improve the reliability and domain invariant features in different environments. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2501.16289",
    "authors": [
      "Younggun Kim",
      "Beomsik Cho",
      "Seonghoon Ryoo",
      "Soomok Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.18851",
    "title": "Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks",
    "abstract": "           Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. At the 3D feature extraction stage, we argue that traditional CNNs are not efficient enough for depth maps. So, we encode depth map into normal map, after which CNNs can easily extract object surface this http URL projection matrix generation stage, we find the existence of Biased-Assignment and Ambiguous-Locality issues in the original pipeline. Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the Euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. Extensive experiments on two public datasets, NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost the performance of RGB-D semantic segmentation task.         ",
    "url": "https://arxiv.org/abs/2501.18851",
    "authors": [
      "Xiaoyan Jiang",
      "Bohan Wang",
      "Xinlong Wan",
      "Zhi Zhou",
      "Hamido Fujita"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.00385",
    "title": "The Impact of Persona-based Political Perspectives on Hateful Content Detection",
    "abstract": "           While pretraining language models with politically diverse content has been shown to improve downstream task fairness, such approaches require significant computational resources often inaccessible to many researchers and organizations. Recent work has established that persona-based prompting can introduce political diversity in model outputs without additional training. However, it remains unclear whether such prompting strategies can achieve results comparable to political pretraining for downstream tasks. We investigate this question using persona-based prompting strategies in multimodal hate-speech detection tasks, specifically focusing on hate speech in memes. Our analysis reveals that when mapping personas onto a political compass and measuring persona agreement, inherent political positioning has surprisingly little correlation with classification decisions. Notably, this lack of correlation persists even when personas are explicitly injected with stronger ideological descriptors. Our findings suggest that while LLMs can exhibit political biases in their responses to direct political questions, these biases may have less impact on practical classification tasks than previously assumed. This raises important questions about the necessity of computationally expensive political pretraining for achieving fair performance in downstream tasks.         ",
    "url": "https://arxiv.org/abs/2502.00385",
    "authors": [
      "Stefano Civelli",
      "Pietro Bernardelle",
      "Gianluca Demartini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.02975",
    "title": "TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics",
    "abstract": "           Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. However, these datasets often feature excessive repeated edges and lack complex sequential dynamics, a key characteristic inherent in many real-world applications such as recommender systems and ``Who-To-Follow'' on social networks. This oversight has led existing methods to inadvertently downplay the importance of learning sequential dynamics, focusing primarily on predicting repeated edges. In this study, we demonstrate that existing methods, such as GraphMixer and DyGFormer, are inherently incapable of learning simple sequential dynamics, such as ``a user who has followed OpenAI and Anthropic is more likely to follow AI at Meta next.'' Motivated by this issue, we introduce the Temporal Graph Benchmark with Sequential Dynamics (TGB-Seq), a new benchmark carefully curated to minimize repeated edges, challenging models to learn sequential dynamics and generalize to unseen edges. TGB-Seq comprises large real-world datasets spanning diverse domains, including e-commerce interactions, movie ratings, business reviews, social networks, citation networks and web link networks. Benchmarking experiments reveal that current methods usually suffer significant performance degradation and incur substantial training costs on TGB-Seq, posing new challenges and opportunities for future research. TGB-Seq datasets, leaderboards, and example codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.02975",
    "authors": [
      "Lu Yi",
      "Jie Peng",
      "Yanping Zheng",
      "Fengran Mo",
      "Zhewei Wei",
      "Yuhang Ye",
      "Yue Zixuan",
      "Zengfeng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.04528",
    "title": "Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection",
    "abstract": "           The advancement of large language models (LLMs) has made it difficult to differentiate human-written text from AI-generated text. Several AI-text detectors have been developed in response, which typically utilize a fixed global threshold (e.g., {\\theta} = 0.5) to classify machine-generated text. However, we find that one universal threshold can fail to account for subgroup-specific distributional variations. For example, when using a fixed threshold, detectors make more false positive errors on shorter human-written text than longer, and more positive classifications on neurotic writing styles than open among long text. These discrepancies can lead to misclassification that disproportionately affects certain groups. We address this critical limitation by introducing FairOPT, an algorithm for group-specific threshold optimization in AI-generated content classifiers. Our approach partitions data into subgroups based on attributes (e.g., text length and writing style) and learns decision thresholds for each group, which enables careful balancing of performance and fairness metrics within each subgroup. In experiments with four AI text classifiers on three datasets, FairOPT enhances overall F1 score and decreases balanced error rate (BER) discrepancy across subgroups. Our framework paves the way for more robust and fair classification criteria in AI-generated output detection.         ",
    "url": "https://arxiv.org/abs/2502.04528",
    "authors": [
      "Minseok Jung",
      "Cynthia Fuertes Panizo",
      "Liam Dugan",
      "Yi R.",
      "Fung",
      "Pin-Yu Chen",
      "Paul Pu Liang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.04656",
    "title": "MHAF-YOLO: Multi-Branch Heterogeneous Auxiliary Fusion YOLO for accurate object detection",
    "abstract": "           Due to the effective multi-scale feature fusion capabilities of the Path Aggregation FPN (PAFPN), it has become a widely adopted component in YOLO-based detectors. However, PAFPN struggles to integrate high-level semantic cues with low-level spatial details, limiting its performance in real-world applications, especially with significant scale variations. In this paper, we propose MHAF-YOLO, a novel detection framework featuring a versatile neck design called the Multi-Branch Auxiliary FPN (MAFPN), which consists of two key modules: the Superficial Assisted Fusion (SAF) and Advanced Assisted Fusion (AAF). The SAF bridges the backbone and the neck by fusing shallow features, effectively transferring crucial low-level spatial information with high fidelity. Meanwhile, the AAF integrates multi-scale feature information at deeper neck layers, delivering richer gradient information to the output layer and further enhancing the model learning capacity. To complement MAFPN, we introduce the Global Heterogeneous Flexible Kernel Selection (GHFKS) mechanism and the Reparameterized Heterogeneous Multi-Scale (RepHMS) module to enhance feature fusion. RepHMS is globally integrated into the network, utilizing GHFKS to select larger convolutional kernels for various feature layers, expanding the vertical receptive field and capturing contextual information across spatial hierarchies. Locally, it optimizes convolution by processing both large and small kernels within the same layer, broadening the lateral receptive field and preserving crucial details for detecting smaller targets. The source code of this work is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.04656",
    "authors": [
      "Zhiqiang Yang",
      "Qiu Guan",
      "Zhongwen Yu",
      "Xinli Xu",
      "Haixia Long",
      "Sheng Lian",
      "Haigen Hu",
      "Ying Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.10788",
    "title": "Analyzing Privacy Dynamics within Groups using Gamified Auctions",
    "abstract": "           Online shared content, such as group pictures, often contains information about multiple users. Developing technical solutions to manage the privacy of such \"co-owned\" content is challenging because each co-owner may have different preferences. Recent technical approaches advocate group-decision mechanisms, including auctions, to decide as how best to resolve these differences. However, it is not clear if users would participate in such mechanisms and if they do, whether they would act altruistically. Understanding the privacy dynamics is crucial to develop effective mechanisms for privacy-respecting collaborative systems. Accordingly, this work develops RESOLVE, a privacy auction game to understand the sharing behavior of users in groups. Our results of users' playing the game show that i) the users' understanding of individual vs. group privacy differs significantly; ii) often users fight for their preferences even at the cost of others' privacy; and iii) at times users collaborate to fight for the privacy of others.         ",
    "url": "https://arxiv.org/abs/2502.10788",
    "authors": [
      "H\u00fcseyin Ayd\u0131n",
      "Onuralp Ulusoy",
      "Ilaria Liccardi",
      "P\u0131nar Yolum"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.10990",
    "title": "FinMTEB: Finance Massive Text Embedding Benchmark",
    "abstract": "           Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, Fin-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.         ",
    "url": "https://arxiv.org/abs/2502.10990",
    "authors": [
      "Yixuan Tang",
      "Yi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.12490",
    "title": "UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code Generation",
    "abstract": "           Deep learning-based code generation has completely transformed the way developers write programs today. Existing approaches to code generation have focused either on the Sequence-to-Sequence paradigm, which generates target code as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs code as a sequence of actions. While these two paradigms are intuitively complementary, their combination has not been previously explored. By comparing the code generated under these two paradigms, we find that integrating them holds significant potential. In this paper, we propose UniGenCoder for code-related generation tasks, which consists of a shared encoder, a shared decoder with a minimal set of additional parameters to unify two paradigms, and a selector that dynamically chooses optimal paradigm for each instance. Also, during the model training, we first perform the multi-task learning and distillation strategies to facilitate knowledge transfer between two paradigms, and then leverage contrastive learning to train the selector. Experimental results on the text-to-code and code-to-code generation tasks demonstrate the effectiveness of our proposed model. We release our code at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.12490",
    "authors": [
      "Liangying Shao",
      "Yanfu Yan",
      "Denys Poshyvanyk",
      "Jinsong Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.13170",
    "title": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment",
    "abstract": "           The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs. We summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways. Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.13170",
    "authors": [
      "Yuze Zhao",
      "Tianyun Ji",
      "Wenjun Feng",
      "Zhenya Huang",
      "Qi Liu",
      "Zhiding Liu",
      "Yixiao Ma",
      "Kai Zhang",
      "Enhong Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13277",
    "title": "HyperGCL: Multi-Modal Graph Contrastive Learning via Learnable Hypergraph Views",
    "abstract": "           Recent advancements in Graph Contrastive Learning (GCL) have demonstrated remarkable effectiveness in improving graph representations. However, relying on predefined augmentations (e.g., node dropping, edge perturbation, attribute masking) may result in the loss of task-relevant information and a lack of adaptability to diverse input data. Furthermore, the selection of negative samples remains rarely explored. In this paper, we introduce HyperGCL, a novel multimodal GCL framework from a hypergraph perspective. HyperGCL constructs three distinct hypergraph views by jointly utilizing the input graph's structure and attributes, enabling a comprehensive integration of multiple modalities in contrastive learning. A learnable adaptive topology augmentation technique enhances these views by preserving important relations and filtering out noise. View-specific encoders capture essential characteristics from each view, while a network-aware contrastive loss leverages the underlying topology to define positive and negative samples effectively. Extensive experiments on benchmark datasets demonstrate that HyperGCL achieves state-of-the-art node classification performance.         ",
    "url": "https://arxiv.org/abs/2502.13277",
    "authors": [
      "Khaled Mohammed Saifuddin",
      "Shihao Ji",
      "Esra Akbas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14456",
    "title": "Narrative-Driven Travel Planning: Geoculturally-Grounded Script Generation with Evolutionary Itinerary Optimization",
    "abstract": "           To enhance tourists' experiences and immersion, this paper proposes a narrative-driven travel planning framework called NarrativeGuide, which generates a geoculturally-grounded narrative script for travelers, offering a novel, role-playing experience for their journey. In the initial stage, NarrativeGuide constructs a knowledge graph for attractions within a city, then configures the worldview, character setting, and exposition based on the knowledge graph. Using this foundation, the knowledge graph is combined to generate an independent scene unit for each attraction. During the itinerary planning stage, NarrativeGuide models narrative-driven travel planning as an optimization problem, utilizing a genetic algorithm (GA) to refine the itinerary. Before evaluating the candidate itinerary, transition scripts are generated for each pair of adjacent attractions, which, along with the scene units, form a complete script. The weighted sum of script coherence, travel time, and attraction scores is then used as the fitness value to update the candidate solution set. Experimental results across four cities, i.e., Nanjing and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate significant improvements in narrative coherence and cultural fit, alongside a notable reduction in travel time and an increase in the quality of visited attractions. Our study highlights that incorporating external evolutionary optimization effectively addresses the limitations of large language models in travel planning.         ",
    "url": "https://arxiv.org/abs/2502.14456",
    "authors": [
      "Ran Ding",
      "Ziyu Zhang",
      "Ying Zhu",
      "Ziqian Kong",
      "Peilan Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.15487",
    "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models",
    "abstract": "           Large Language Models (LLMs) are increasingly used in tasks requiring interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely integrates both causal and temporal relations presented in different linguistic orders and explicitly expressed by linguistic connectives. The dataset is enriched with crowdsourced human acceptability ratings. We tested LLMs on ExpliCa through prompting and perplexity-based metrics. We assessed seven commercial and open-source LLMs, revealing that even top models struggle to reach 0.80 accuracy. Interestingly, models tend to confound temporal relations with causal ones, and their performance is also strongly influenced by the linguistic order of the events. Finally, perplexity-based scores and prompting performance are differently affected by model size.         ",
    "url": "https://arxiv.org/abs/2502.15487",
    "authors": [
      "Martina Miliani",
      "Serena Auriemma",
      "Alessandro Bondielli",
      "Emmanuele Chersoni",
      "Lucia Passaro",
      "Irene Sucameli",
      "Alessandro Lenci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.16390",
    "title": "Automatic Detection of Research Values from Scientific Abstracts Across Computer Science Subfields",
    "abstract": "           The field of Computer science (CS) has rapidly evolved over the past few decades, providing computational tools and methodologies to various fields and forming new interdisciplinary communities. This growth in CS has significantly impacted institutional practices and relevant research communities. Therefore, it is crucial to explore what specific research values, known as basic and fundamental beliefs that guide or motivate research attitudes or actions, CS-related research communities promote. Prior research has manually analyzed research values from a small sample of machine learning papers. No prior work has studied the automatic detection of research values in CS from large-scale scientific texts across different research subfields. This paper introduces a detailed annotation scheme featuring ten research values that guide CS-related research. Based on the scheme, we build value classifiers to scale up the analysis and present a systematic study over 226,600 paper abstracts from 32 CS-related subfields and 86 popular publishing venues over ten years.         ",
    "url": "https://arxiv.org/abs/2502.16390",
    "authors": [
      "Hang Jiang",
      "Tal August",
      "Luca Soldaini",
      "Kyle Lo",
      "Maria Antoniak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2502.16486",
    "title": "MQADet: A Plug-and-Play Paradigm for Enhancing Open-Vocabulary Object Detection via Multimodal Question Answering",
    "abstract": "           Open-vocabulary detection (OVD) is a challenging task to detect and classify objects from an unrestricted set of categories, including those unseen during training. Existing open-vocabulary detectors are limited by complex visual-textual misalignment and long-tailed category imbalances, leading to suboptimal performance in challenging scenarios. To address these limitations, we introduce MQADet, a universal paradigm for enhancing existing open-vocabulary detectors by leveraging the cross-modal reasoning capabilities of multimodal large language models (MLLMs). MQADet functions as a plug-and-play solution that integrates seamlessly with pre-trained object detectors without substantial additional training costs. Specifically, we design a novel three-stage Multimodal Question Answering (MQA) pipeline to guide the MLLMs to precisely localize complex textual and visual targets while effectively enhancing the focus of existing object detectors on relevant objects. To validate our approach, we present a new benchmark for evaluating our paradigm on four challenging open-vocabulary datasets, employing three state-of-the-art object detectors as baselines. Experimental results demonstrate that our proposed paradigm significantly improves the performance of existing detectors, particularly in unseen complex categories, across diverse and challenging scenarios. To facilitate future research, we will publicly release our code.         ",
    "url": "https://arxiv.org/abs/2502.16486",
    "authors": [
      "Caixiong Li",
      "Xiongwei Zhao",
      "Jinhang Zhang",
      "Xing Zhang",
      "Qihao Sun",
      "Zhou Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.16519",
    "title": "Guarding the Privacy of Label-Only Access to Neural Network Classifiers via iDP Verification",
    "abstract": "           Neural networks are susceptible to privacy attacks that can extract private information of the training set. To cope, several training algorithms guarantee differential privacy (DP) by adding noise to their computation. However, DP requires to add noise considering every possible training set. This leads to a significant decrease in the network's accuracy. Individual DP (iDP) restricts DP to a given training set. We observe that some inputs deterministically satisfy iDP without any noise. By identifying them, we can provide iDP label-only access to the network with a minor decrease to its accuracy. However, identifying the inputs that satisfy iDP without any noise is highly challenging. Our key idea is to compute the iDP deterministic bound (iDP-DB), which overapproximates the set of inputs that do not satisfy iDP, and add noise only to their predicted labels. To compute the tightest iDP-DB, which enables to guard the label-only access with minimal accuracy decrease, we propose LUCID, which leverages several formal verification techniques. First, it encodes the problem as a mixed-integer linear program, defined over a network and over every network trained identically but without a unique data point. Second, it abstracts a set of networks using a hyper-network. Third, it eliminates the overapproximation error via a novel branch-and-bound technique. Fourth, it bounds the differences of matching neurons in the network and the hyper-network and employs linear relaxation if they are small. We show that LUCID can provide classifiers with a perfect individuals' privacy guarantee (0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy decrease of 1.4%. For more relaxed $\\varepsilon$-iDP guarantees, LUCID has an accuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce the accuracy by 12.7%.         ",
    "url": "https://arxiv.org/abs/2502.16519",
    "authors": [
      "Anan Kabaha",
      "Dana Drachsler-Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2502.16949",
    "title": "SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations",
    "abstract": "           Knowledge graph (KG) learning offers a powerful framework for generating new knowledge and making inferences. Training KG embedding can take a significantly long time, especially for larger datasets. Our analysis shows that the gradient computation of embedding is one of the dominant functions in the translation-based KG embedding training loop. We address this issue by replacing the core embedding computation with SpMM (Sparse-Dense Matrix Multiplication) kernels. This allows us to unify multiple scatter (and gather) operations as a single operation, reducing training time and memory usage. We create a general framework for training KG models using sparse kernels and implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on the GPU with a significantly low GPU memory footprint. The speedups are consistent across large and small datasets for a given model. Our proposed sparse approach can be extended to accelerate other translation-based (such as TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE, etc.) models as well.         ",
    "url": "https://arxiv.org/abs/2502.16949",
    "authors": [
      "Md Saidul Hoque Anik",
      "Ariful Azad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.17053",
    "title": "PointSea: Point Cloud Completion via Self-structure Augmentation",
    "abstract": "           Point cloud completion is a fundamental yet not well-solved problem in 3D vision. Current approaches often rely on 3D coordinate information and/or additional data (e.g., images and scanning viewpoints) to fill in missing parts. Unlike these methods, we explore self-structure augmentation and propose PointSea for global-to-local point cloud completion. In the global stage, consider how we inspect a defective region of a physical object, we may observe it from various perspectives for a better understanding. Inspired by this, PointSea augments data representation by leveraging self-projected depth images from multiple views. To reconstruct a compact global shape from the cross-modal input, we incorporate a feature fusion module to fuse features at both intra-view and inter-view levels. In the local stage, to reveal highly detailed structures, we introduce a point generator called the self-structure dual-generator. This generator integrates both learned shape priors and geometric self-similarities for shape refinement. Unlike existing efforts that apply a unified strategy for all points, our dual-path design adapts refinement strategies conditioned on the structural type of each point, addressing the specific incompleteness of each point. Comprehensive experiments on widely-used benchmarks demonstrate that PointSea effectively understands global shapes and generates local details from incomplete input, showing clear improvements over existing methods.         ",
    "url": "https://arxiv.org/abs/2502.17053",
    "authors": [
      "Zhe Zhu",
      "Honghua Chen",
      "Xing He",
      "Mingqiang Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.17260",
    "title": "Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach",
    "abstract": "           Federated learning (FL) has emerged as a promising distributed learning paradigm for training deep neural networks (DNNs) at the wireless edge, but its performance can be severely hindered by unreliable wireless transmission and inherent data heterogeneity among clients. Existing solutions primarily address these challenges by incorporating wireless resource optimization strategies, often focusing on uplink resource allocation across clients under the assumption of homogeneous client-server network standards. However, these approaches overlooked the fact that mobile clients may connect to the server via diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized configurations, limiting the flexibility of server-side modifications and restricting applicability in real-world commercial networks. This paper presents a novel theoretical analysis about how transmission failures in unreliable networks distort the effective label distributions of local samples, causing deviations from the global data distribution and introducing convergence bias in FL. Our analysis reveals that a carefully designed client selection strategy can mitigate biases induced by network unreliability and data heterogeneity. Motivated by this insight, we propose FedCote, a client selection approach that optimizes client selection probabilities without relying on wireless resource scheduling. Experimental results demonstrate the robustness of FedCote in DNN-based classification tasks under unreliable networks with frequent transmission failures.         ",
    "url": "https://arxiv.org/abs/2502.17260",
    "authors": [
      "Yanmeng Wang",
      "Wenkai Ji",
      "Jian Zhou",
      "Fu Xiao",
      "Tsung-Hui Chang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.17612",
    "title": "Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks",
    "abstract": "           The orchestration of agents to optimize a collective objective without centralized control is challenging yet crucial for applications such as controlling autonomous fleets, and surveillance and reconnaissance using sensor networks. Decentralized controller design has been inspired by self-organization found in nature, with a prominent source of inspiration being flocking; however, decentralized controllers struggle to maintain flock cohesion. The graph neural network (GNN) architecture has emerged as an indispensable machine learning tool for developing decentralized controllers capable of maintaining flock cohesion, but they fail to exploit the symmetries present in flocking dynamics, hindering their generalizability. We enforce rotation equivariance and translation invariance symmetries in decentralized flocking GNN controllers and achieve comparable flocking control with 70% less training data and 75% fewer trainable weights than existing GNN controllers without these symmetries enforced. We also show that our symmetry-aware controller generalizes better than existing GNN controllers. Code and animations are available at this http URL.         ",
    "url": "https://arxiv.org/abs/2502.17612",
    "authors": [
      "Taos Transue",
      "Bao Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.17753",
    "title": "Task Graph Maximum Likelihood Estimation for Procedural Activity Understanding in Egocentric Videos",
    "abstract": "           We introduce a gradient-based approach for learning task graphs from procedural activities, improving over hand-crafted methods. Our method directly optimizes edge weights via maximum likelihood, enabling integration into neural architectures. We validate our approach on CaptainCook4D, EgoPER, and EgoProceL, achieving +14.5%, +10.2%, and +13.6% F1-score improvements. Our feature-based approach for predicting task graphs from textual/video embeddings demonstrates emerging video understanding abilities. We also achieved top performance on the procedure understanding benchmark on Ego-Exo4D and significantly improved online mistake detection (+19.8% on Assembly101-O, +6.4% on EPIC-Tent-O). Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2502.17753",
    "authors": [
      "Luigi Seminara",
      "Giovanni Maria Farinella",
      "Antonino Furnari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.18083",
    "title": "A Fusion Model for Art Author Identification Based on Convolutional Neural Networks and Transformers",
    "abstract": "           The identification of art authors is crucial in areas like cultural heritage protection, art market analysis, and historical research. With the advancement of deep learning, Convolutional Neural Networks (CNNs) and Transformer models have become key tools for image classification. While CNNs excel in local feature extraction, they struggle with global context, and Transformers are strong in capturing global dependencies but weak in fine-grained local details. To address these challenges, this paper proposes a fusion model combining CNNs and Transformers for art author identification. The model first extracts local features using CNNs, then captures global context with a Transformer, followed by a feature fusion mechanism to enhance classification accuracy. Experiments on Chinese and oil painting datasets show the fusion model outperforms individual CNN and Transformer models, improving classification accuracy by 9.7\\% and 7.1\\%, respectively, and increasing F1 scores by 0.06 and 0.05. The results demonstrate the model's effectiveness and potential for future improvements, such as multimodal integration and architecture optimization.         ",
    "url": "https://arxiv.org/abs/2502.18083",
    "authors": [
      "Zhenyu Wang",
      "Heng Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.08448",
    "title": "Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance",
    "abstract": "           Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.         ",
    "url": "https://arxiv.org/abs/2308.08448",
    "authors": [
      "Santanu Ganguly"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.17385",
    "title": "Dichromatic number of chordal graphs",
    "abstract": "           The dichromatic number of a digraph is the minimum integer $k$ such that it admits a $k$-dicolouring, i.e. a partition of its vertices into $k$ acyclic subdigraphs. We say that a digraph $D$ is a super-orientation of an undirected graph $G$ if $G$ is the underlying graph of $D$. If $D$ does not contain any pair of symmetric arcs, we just say that $D$ is an orientation of $G$. In this work, we give both lower and upper bounds on the dichromatic number of super-orientations of chordal graphs. We also show a family of orientations of cographs for which the dichromatic number is equal to the clique number of the underlying graph.         ",
    "url": "https://arxiv.org/abs/2309.17385",
    "authors": [
      "St\u00e9phane Bessy",
      "Fr\u00e9d\u00e9ric Havet",
      "Lucas Picasarri-Arrieta"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2402.17797",
    "title": "Neural Radiance Fields in Medical Imaging: A Survey",
    "abstract": "           Neural Radiance Fields (NeRF), as a pioneering technique in computer vision, offer great potential to revolutionize medical imaging by synthesizing three-dimensional representations from the projected two-dimensional image data. However, they face unique challenges when applied to medical applications. This paper presents a comprehensive examination of applications of NeRFs in medical imaging, highlighting four imminent challenges, including fundamental imaging principles, inner structure requirement, object boundary definition, and color density significance. We discuss current methods on different organs and discuss related limitations. We also review several datasets and evaluation metrics and propose several promising directions for future research.         ",
    "url": "https://arxiv.org/abs/2402.17797",
    "authors": [
      "Xin Wang",
      "Yineng Chen",
      "Shu Hu",
      "Heng Fan",
      "Hongtu Zhu",
      "Xin Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.13286",
    "title": "A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs",
    "abstract": "           Hypothesis testing is a statistical method used to draw conclusions about populations from sample data, typically represented in tables. With the prevalence of graph representations in real-life applications, hypothesis testing in graphs is gaining importance. In this work, we formalize node, edge, and path hypotheses in attributed graphs. We develop a sampling-based hypothesis testing framework, which can accommodate existing hypothesis-agnostic graph sampling methods. To achieve accurate and efficient sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m- dimensional random walk that accounts for the paths specified in a hypothesis. We further optimize its time efficiency and propose PHASEopt. Experiments on real datasets demonstrate the ability of our framework to leverage common graph sampling methods for hypothesis testing, and the superiority of hypothesis-aware sampling in terms of accuracy and time efficiency.         ",
    "url": "https://arxiv.org/abs/2403.13286",
    "authors": [
      "Yun Wang",
      "Chrysanthi Kosyfaki",
      "Sihem Amer-Yahia",
      "Reynold Cheng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.19075",
    "title": "Distributed Stochastic Optimization of a Neural Representation Network for Time-Space Tomography Reconstruction",
    "abstract": "           4D time-space reconstruction of dynamic events or deforming objects using X-ray computed tomography (CT) is an important inverse problem in non-destructive evaluation. Conventional back-projection based reconstruction methods assume that the object remains static for the duration of several tens or hundreds of X-ray projection measurement images (reconstruction of consecutive limited-angle CT scans). However, this is an unrealistic assumption for many in-situ experiments that causes spurious artifacts and inaccurate morphological reconstructions of the object. To solve this problem, we propose to perform a 4D time-space reconstruction using a distributed implicit neural representation (DINR) network that is trained using a novel distributed stochastic training algorithm. Our DINR network learns to reconstruct the object at its output by iterative optimization of its network parameters such that the measured projection images best match the output of the CT forward measurement model. We use a forward measurement model that is a function of the DINR outputs at a sparsely sampled set of continuous valued 4D object coordinates. Unlike previous neural representation architectures that forward and back propagate through dense voxel grids that sample the object's entire time-space coordinates, we only propagate through the DINR at a small subset of object coordinates in each iteration resulting in an order-of-magnitude reduction in memory and compute for training. DINR leverages distributed computation across several compute nodes and GPUs to produce high-fidelity 4D time-space reconstructions. We use both simulated parallel-beam and experimental cone-beam X-ray CT datasets to demonstrate the superior performance of our approach.         ",
    "url": "https://arxiv.org/abs/2404.19075",
    "authors": [
      "K. Aditya Mohan",
      "Massimiliano Ferrucci",
      "Chuck Divin",
      "Garrett A. Stevenson",
      "Hyojin Kim"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2405.19492",
    "title": "TotalSegmentator MRI: Robust Sequence-independent Segmentation of Multiple Anatomic Structures in MRI",
    "abstract": "           Since the introduction of TotalSegmentator CT, there is demand for a similar robust automated MRI segmentation tool that can be applied across all MRI sequences and anatomic structures. In this retrospective study, a nnU-Net model (TotalSegmentator) was trained on MRI and CT examinations to segment 80 anatomic structures relevant for use cases such as organ volumetry, disease characterization, surgical planning and opportunistic screening. Examinations were randomly sampled from routine clinical studies to represent real-world examples. Dice scores were calculated between the predicted segmentations and expert radiologist reference standard segmentations to evaluate model performance on an internal test set, two external test sets and against two publicly available models, and TotalSegmentator CT. The model was applied to an internal dataset containing abdominal MRIs to investigate age-dependent volume changes. A total of 1143 examinations (616 MRIs, 527 CTs) (median age 61 years, IQR 50-72) were split into training (n=1088, CT and MRI) and an internal test set (n=55; only MRI), two external test sets (AMOS, n=20; CHAOS, n=20; only MRI), and an internal aging-study dataset of 8672 abdominal MRIs (median age 59 years, IQR 45-70) were included. The model showed a Dice Score of 0.839 on the internal test set and outperformed two other models (Dice Score, 0.862 versus 0.759; and 0.838 versus 0.560; p<.001 for both). The proposed open-source, easy-to-use model allows for automatic, robust segmentation of 80 structures, extending the capabilities of TotalSegmentator to MRIs of any sequence. The ready-to-use online tool is available at this https URL, the model at this https URL, and the dataset at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.19492",
    "authors": [
      "Tugba Akinci D'Antonoli",
      "Lucas K. Berger",
      "Ashraya K. Indrakanti",
      "Nathan Vishwanathan",
      "Jakob Wei\u00df",
      "Matthias Jung",
      "Zeynep Berkarda",
      "Alexander Rau",
      "Marco Reisert",
      "Thomas K\u00fcstner",
      "Alexandra Walter",
      "Elmar M. Merkle",
      "Daniel Boll",
      "Hanns-Christian Breit",
      "Andrew Phillip Nicoli",
      "Martin Segeroth",
      "Joshy Cyriac",
      "Shan Yang",
      "Jakob Wasserthal"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.05636",
    "title": "What is my quantum computer good for? Quantum capability learning with physics-aware neural networks",
    "abstract": "           Quantum computers have the potential to revolutionize diverse fields, including quantum chemistry, materials science, and machine learning. However, contemporary quantum computers experience errors that often cause quantum programs run on them to fail. Until quantum computers can reliably execute large quantum programs, stakeholders will need fast and reliable methods for assessing a quantum computer's capability-i.e., the programs it can run and how well it can run them. Previously, off-the-shelf neural network architectures have been used to model quantum computers' capabilities, but with limited success, because these networks fail to learn the complex quantum physics that determines real quantum computers' errors. We address this shortcoming with a new quantum-physics-aware neural network architecture for learning capability models. Our architecture combines aspects of graph neural networks with efficient approximations to the physics of errors in quantum programs. This approach achieves up to $\\sim50\\%$ reductions in mean absolute error on both experimental and simulated data, over state-of-the-art models based on convolutional neural networks.         ",
    "url": "https://arxiv.org/abs/2406.05636",
    "authors": [
      "Daniel Hothem",
      "Ashe Miller",
      "Timothy Proctor"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.17329",
    "title": "Low dimensional representation of multi-patient flow cytometry datasets using optimal transport for minimal residual disease detection in leukemia",
    "abstract": "           Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid Leukemia (AML), a type of cancer that affects the blood and bone marrow, is essential in the prognosis and follow-up of AML patients. As traditional cytological analysis cannot detect leukemia cells below 5\\%, the analysis of flow cytometry dataset is expected to provide more reliable results. In this paper, we explore statistical learning methods based on optimal transport (OT) to achieve a relevant low-dimensional representation of multi-patient flow cytometry measurements (FCM) datasets considered as high-dimensional probability distributions. Using the framework of OT, we justify the use of the K-means algorithm for dimensionality reduction of multiple large-scale point clouds through mean measure quantization by merging all the data into a single point cloud. After this quantization step, the visualization of the intra and inter-patients FCM variability is carried out by embedding low-dimensional quantized probability measures into a linear space using either Wasserstein Principal Component Analysis (PCA) through linearized OT or log-ratio PCA of compositional data. Using a publicly available FCM dataset and a FCM dataset from Bordeaux University Hospital, we demonstrate the benefits of our approach over the popular kernel mean embedding technique for statistical learning from multiple high-dimensional probability distributions. We also highlight the usefulness of our methodology for low-dimensional projection and clustering patient measurements according to their level of MRD in AML from FCM. In particular, our OT-based approach allows a relevant and informative two-dimensional representation of the results of the FlowSom algorithm, a state-of-the-art method for the detection of MRD in AML using multi-patient FCM.         ",
    "url": "https://arxiv.org/abs/2407.17329",
    "authors": [
      "Erell Gachon",
      "J\u00e9r\u00e9mie Bigot",
      "Elsa Cazelles",
      "Audrey Bidet",
      "Jean-Philippe Vial",
      "Pierre-Yves Dumas",
      "Aguirre Mimoun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2408.09208",
    "title": "Parametric Sensitivity Analysis for Models of Reaction Networks within Interacting Compartments",
    "abstract": "           Models of reaction networks within interacting compartments (RNIC) are a generalization of stochastic reaction networks. It is most natural to think of the interacting compartments as \"cells\" that can appear, degrade, split, and even merge, with each cell containing an evolving copy of the underlying stochastic reaction network. Such models have a number of parameters, including those associated with the internal chemical model and those associated with the compartment interactions, and it is natural to want efficient computational methods for the numerical estimation of sensitivities of model statistics with respect to these parameters. Motivated by the extensive work on computational methods for parametric sensitivity analysis in the context of stochastic reaction networks over the past few decades, we provide a number of methods in the basic RNIC setting. Provided methods include the (unbiased) Girsanov transformation method (also called the Likelihood Ratio method) and a number of coupling methods for the implementation of finite differences, each motivated by methods from previous work related to stochastic reaction networks. We provide several numerical examples comparing the various methods in the new setting. We find that the relative performance of each method is in line with its analog in the \"standard\" stochastic reaction network setting. We have made all of the Matlab code used to implement the various methods freely available for download.         ",
    "url": "https://arxiv.org/abs/2408.09208",
    "authors": [
      "David F. Anderson",
      "Aidan S. Howells"
    ],
    "subjectives": [
      "Molecular Networks (q-bio.MN)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2412.18840",
    "title": "Implicit factorized transformer approach to fast prediction of turbulent channel flows",
    "abstract": "           Transformer neural operators have recently become an effective approach for surrogate modeling of systems governed by partial differential equations (PDEs). In this paper, we introduce a modified implicit factorized transformer (IFactFormer-m) model which replaces the original chained factorized attention with parallel factorized attention. The IFactFormer-m model successfully performs long-term predictions for turbulent channel flow, whereas the original IFactFormer (IFactFormer-o), Fourier neural operator (FNO), and implicit Fourier neural operator (IFNO) exhibit a poor performance. Turbulent channel flows are simulated by direct numerical simulation using fine grids at friction Reynolds numbers $\\text{Re}_{\\tau}\\approx 180,395,590$, and filtered to coarse grids for training neural operator. The neural operator takes the current flow field as input and predicts the flow field at the next time step, and long-term prediction is achieved in the posterior through an autoregressive approach. The results show that IFactFormer-m, compared to other neural operators and the traditional large eddy simulation (LES) methods including dynamic Smagorinsky model (DSM) and the wall-adapted local eddy-viscosity (WALE) model, reduces prediction errors in the short term, and achieves stable and accurate long-term prediction of various statistical properties and flow structures, including the energy spectrum, mean streamwise velocity, root mean square (rms) values of fluctuating velocities, Reynolds shear stress, and spatial structures of instantaneous velocity. Moreover, the trained IFactFormer-m is much faster than traditional LES methods. By analyzing the attention kernels, we elucidate the reasons why IFactFormer-m converges faster and achieves a stable and accurate long-term prediction compared to IFactFormer-o. Code and data are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2412.18840",
    "authors": [
      "Huiyu Yang",
      "Yunpeng Wang",
      "Jianchun Wang"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.12181",
    "title": "3D ReX: Causal Explanations in 3D Neuroimaging Classification",
    "abstract": "           Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke.         ",
    "url": "https://arxiv.org/abs/2502.12181",
    "authors": [
      "Melane Navaratnarajah",
      "Sophie A. Martin",
      "David A. Kelly",
      "Nathan Blake",
      "Hana Chockler"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.18314",
    "title": "Learning atomic forces from uncertainty-calibrated adversarial attacks",
    "abstract": "           Adversarial approaches, which intentionally challenge machine learning models by generating difficult examples, are increasingly being adopted to improve machine learning interatomic potentials (MLIPs). While already providing great practical value, little is known about the actual prediction errors of MLIPs on adversarial structures and whether these errors can be controlled. We propose the Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover adversarial structures with user-assigned errors. Through uncertainty calibration, the estimated uncertainty of MLIPs is unified with real errors. By performing geometry optimization for calibrated uncertainty, we reach adversarial structures with the user-assigned target MLIP prediction error. Integrating with active learning pipelines, we benchmark CAGO, demonstrating stable MLIPs that systematically converge structural, dynamical, and thermodynamical properties for liquid water and water adsorption in a metal-organic framework within only hundreds of training structures, where previously many thousands were typically required.         ",
    "url": "https://arxiv.org/abs/2502.18314",
    "authors": [
      "Henrique Musseli Cezar",
      "Tilmann Bodenstein",
      "Henrik Andersen Sveinsson",
      "Morten Ledum",
      "Simen Reine",
      "Sigbj\u00f8rn L\u00f8land Bore"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)"
    ]
  }
]