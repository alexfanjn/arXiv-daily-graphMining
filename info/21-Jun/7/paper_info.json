[
  {
    "id": "arXiv:2106.02159",
    "title": "On the implementation of a robust and efficient finite element-based  parallel solver for the compressible Navier-Stokes equations",
    "abstract": "This paper describes in detail the implementation of a finite element technique for solving the compressible Navier-Stokes equations that is provably robust and demonstrates excellent performance on modern computer hardware. The method is second-order accurate in time and space. Robustness here means that the method is proved to be invariant domain preserving under the hyperbolic CFL time step restriction, and it produces results that are reproducible and that can be shown to be accurate on challenging 2D and 3D realistic benchmarks. ",
    "url": "https://arxiv.org/abs/2106.02159",
    "authors": [
      "Jean-Luc Guermond",
      "Martin Kronbichler",
      "Matthias Maier",
      "Bojan Popov",
      "Ignacio Tomas"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2106.02163",
    "title": "Improved batch code lower bounds",
    "abstract": "Batch codes are a useful notion of locality for error correcting codes, originally introduced in the context of distributed storage and cryptography. Many constructions of batch codes have been given, but few lower bound (limitation) results are known, leaving gaps between the best known constructions and best known lower bounds. Towards determining the optimal redundancy of batch codes, we prove a new lower bound on the redundancy of batch codes. Specifically, we study (primitive, multiset) linear batch codes that systematically encode $n$ information symbols into $N$ codeword symbols, with the requirement that any multiset of $k$ symbol requests can be obtained in disjoint ways. We show that such batch codes need $\\Omega(\\sqrt{Nk})$ symbols of redundancy, improving on the previous best lower bounds of $\\Omega(\\sqrt{N}+k)$ at all $k=n^\\varepsilon$ with $\\varepsilon\\in(0,1)$. Our proof follows from analyzing the dimension of the order-$O(k)$ tensor of the batch code's dual code. ",
    "url": "https://arxiv.org/abs/2106.02163",
    "authors": [
      "Ray Li",
      "Mary Wootters"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2106.02213",
    "title": "Analysis of the robustness of NMF algorithms",
    "abstract": "We examine three non-negative matrix factorization techniques; L2-norm, L1-norm, and L2,1-norm. Our aim is to establish the performance of these different approaches, and their robustness in real-world applications such as feature selection while managing computational complexity, sensitivity to noise and more. We thoroughly examine each approach from a theoretical perspective, and examine the performance of each using a series of experiments drawing on both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors (RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria under a range of simulated noise scenarios. ",
    "url": "https://arxiv.org/abs/2106.02213",
    "authors": [
      "Alex D\u00edaz",
      "Damian Steele"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.02245",
    "title": "Towards offensive language detection and reduction in four Software  Engineering communities",
    "abstract": "Software Engineering (SE) communities such as Stack Overflow have become unwelcoming, particularly through members' use of offensive language. Research has shown that offensive language drives users away from active engagement within these platforms. This work aims to explore this issue more broadly by investigating the nature of offensive language in comments posted by users in four prominent SE platforms - GitHub, Gitter, Slack and Stack Overflow (SO). It proposes an approach to detect and classify offensive language in SE communities by adopting natural language processing and deep learning techniques. Further, a Conflict Reduction System (CRS), which identifies offence and then suggests what changes could be made to minimize offence has been proposed. Beyond showing the prevalence of offensive language in over 1 million comments from four different communities which ranges from 0.07% to 0.43%, our results show promise in successful detection and classification of such language. The CRS system has the potential to drastically reduce manual moderation efforts to detect and reduce offence in SE communities. ",
    "url": "https://arxiv.org/abs/2106.02245",
    "authors": [
      "Jithin Cheriyan",
      "Bastin Tony Roy Savarimuthu",
      "Stephen Cranefield"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2106.02324",
    "title": "Hybrid attention network based on progressive embedding scale-context  for crowd counting",
    "abstract": "The existing crowd counting methods usually adopted attention mechanism to tackle background noise, or applied multi-level features or multi-scales context fusion to tackle scale variation. However, these approaches deal with these two problems separately. In this paper, we propose a Hybrid Attention Network (HAN) by employing Progressive Embedding Scale-context (PES) information, which enables the network to simultaneously suppress noise and adapt head scale variation. We build the hybrid attention mechanism through paralleling spatial attention and channel attention module, which makes the network to focus more on the human head area and reduce the interference of background objects. Besides, we embed certain scale-context to the hybrid attention along the spatial and channel dimensions for alleviating these counting errors caused by the variation of perspective and head scale. Finally, we propose a progressive learning strategy through cascading multiple hybrid attention modules with embedding different scale-context, which can gradually integrate different scale-context information into the current feature map from global to local. Ablation experiments provides that the network architecture can gradually learn multi-scale features and suppress background noise. Extensive experiments demonstrate that HANet obtain state-of-the-art counting performance on four mainstream datasets. ",
    "url": "https://arxiv.org/abs/2106.02324",
    "authors": [
      "Fusen Wang",
      "Jun Sang",
      "Zhongyuan Wu",
      "Qi Liu",
      "Nong Sang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.02466",
    "title": "Graph Barlow Twins: A self-supervised representation learning framework  for graphs",
    "abstract": "The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning -- Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures -- in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as BGRL, best self-supervised methods, and fully supervised ones while requiring substantially fewer hyperparameters and converging in an order of magnitude training steps earlier. ",
    "url": "https://arxiv.org/abs/2106.02466",
    "authors": [
      "Piotr Bielak",
      "Tomasz Kajdanowicz",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.02469",
    "title": "Can convolutional ResNets approximately preserve input distances? A  frequency analysis perspective",
    "abstract": "ResNets constrained to be bi-Lipschitz, that is, approximately distance preserving, have been a crucial component of recently proposed techniques for deterministic uncertainty quantification in neural models. We show that theoretical justifications for recent regularisation schemes trying to enforce such a constraint suffer from a crucial flaw -- the theoretical link between the regularisation scheme used and bi-Lipschitzness is only valid under conditions which do not hold in practice, rendering existing theory of limited use, despite the strong empirical performance of these models. We provide a theoretical explanation for the effectiveness of these regularisation schemes using a frequency analysis perspective, showing that under mild conditions these schemes will enforce a lower Lipschitz bound on the low-frequency projection of images. We then provide empirical evidence supporting our theoretical claims, and perform further experiments which demonstrate that our broader conclusions appear to hold when some of the mathematical assumptions of our proof are relaxed, corresponding to the setup used in prior work. In addition, we present a simple constructive algorithm to search for counter examples to the distance preservation condition, and discuss possible implications of our theory for future model design. ",
    "url": "https://arxiv.org/abs/2106.02469",
    "authors": [
      "Lewis Smith",
      "Joost van Amersfoort",
      "Haiwen Huang",
      "Stephen Roberts",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.02483",
    "title": "You can't always get what you want: towards user-controlled privacy on  Android",
    "abstract": "Mobile applications (hereafter, apps) collect a plethora of information regarding the user behavior and his device through third-party analytics libraries. However, the collection and usage of such data raised several privacy concerns, mainly because the end-user - i.e., the actual owner of the data - is out of the loop in this collection process. Also, the existing privacy-enhanced solutions that emerged in the last years follow an \"all or nothing\" approach, leaving the user the sole option to accept or completely deny the access to privacy-related data. This work has the two-fold objective of assessing the privacy implications on the usage of analytics libraries in mobile apps and proposing a data anonymization methodology that enables a trade-off between the utility and privacy of the collected data and gives the user complete control over the sharing process. To achieve that, we present an empirical privacy assessment on the analytics libraries contained in the 4500 most-used Android apps of the Google Play Store between November 2020 and January 2021. Then, we propose an empowered anonymization methodology, based on MobHide, that gives the end-user complete control over the collection and anonymization process. Finally, we empirically demonstrate the applicability and effectiveness of such anonymization methodology thanks to HideDroid, a fully-fledged anonymization app for the Android ecosystem. ",
    "url": "https://arxiv.org/abs/2106.02483",
    "authors": [
      "Davide Caputo",
      "Francesco Pagano",
      "Giovanni Bottino",
      "Luca Verderame",
      "Alessio Merlo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2106.02602",
    "title": "Principled change point detection via representation learning",
    "abstract": "Change points are abrupt alterations in the distribution of sequential data. A change-point detection (CPD) model aims at quick detection of such changes. Classic approaches perform poorly for semi-structured sequential data because of the absence of adequate data representation learning. To deal with it, we introduce a principled differentiable loss function that considers the specificity of the CPD task. The theoretical results suggest that this function approximates well classic rigorous solutions. For such loss function, we propose an end-to-end method for the training of deep representation learning CPD models. Our experiments provide evidence that the proposed approach improves baseline results of change point detection for various data types, including real-world videos and image sequences, and improve representations for them. ",
    "url": "https://arxiv.org/abs/2106.02602",
    "authors": [
      "Evgenia Romanenkova",
      "Alexey Zaytsev",
      "Ramil Zainulin",
      "Matvey Morozov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.02221",
    "title": "Specular reflections removal in colposcopic images based on neural  networks: Supervised training with no ground truth previous knowledge",
    "abstract": "Cervical cancer is a malignant tumor that seriously threatens women's health, and is one of the most common that affects women worldwide. For its early detection, colposcopic images of the cervix are used for searching for possible injuries or abnormalities. An inherent characteristic of these images is the presence of specular reflections (brightness) that make it difficult to observe some regions, which might imply a misdiagnosis. In this paper, a new strategy based on neural networks is introduced for eliminating specular reflections and estimating the unobserved anatomical cervix portion under the bright zones. We present a supervised learning method, despite not knowing the ground truth from the beginning, based on training a neural network to learn how to restore any hidden region of colposcopic images. Once the specular reflections are identified, they are removed from the image and the previously trained network is used to fulfill these deleted areas. The quality of the processed images was evaluated quantitatively and qualitatively. In 21 of the 22 evaluated images, the detected specular reflections were totally eliminated, whereas, in the remaining one, these reflections were almost completely eliminated. The distribution of the colors and the content of the restored images are similar to those of the originals. The evaluation carried out by a specialist in Cervix Pathology concluded that, after eliminating the specular reflections, the anatomical and physiological elements of the cervix are observable in the restored images, which facilitates the medical diagnosis of cervical pathologies. Our method has the potential to improve the early detection of cervical cancer. ",
    "url": "https://arxiv.org/abs/2106.02221",
    "authors": [
      "Lauren Jimenez-Martin",
      "Daniel A. Vald\u00e9s P\u00e9rez",
      "Ana M. Solares Asteasuainzarra",
      "Ludwig Leonard",
      "Marta L. Baguer D\u00edaz-Roma\u00f1ach"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.02522",
    "title": "Price graphs: Utilizing the structural information of financial time  series for stock prediction",
    "abstract": "Stock prediction, with the purpose of forecasting the future price trends of stocks, is crucial for maximizing profits from stock investments. While great research efforts have been devoted to exploiting deep neural networks for improved stock prediction, the existing studies still suffer from two major issues. First, the long-range dependencies in time series are not sufficiently captured. Second, the chaotic property of financial time series fundamentally lowers prediction performance. In this study, we propose a novel framework to address both issues regarding stock prediction. Specifically, in terms of transforming time series into complex networks, we convert market price series into graphs. Then, structural information, referring to associations among temporal points and the node weights, is extracted from the mapped graphs to resolve the problems regarding long-range dependencies and the chaotic property. We take graph embeddings to represent the associations among temporal points as the prediction model inputs. Node weights are used as a priori knowledge to enhance the learning of temporal attention. The effectiveness of our proposed framework is validated using real-world stock data, and our approach obtains the best performance among several state-of-the-art benchmarks. Moreover, in the conducted trading simulations, our framework further obtains the highest cumulative profits. Our results supplement the existing applications of complex network methods in the financial realm and provide insightful implications for investment applications regarding decision support in financial markets. ",
    "url": "https://arxiv.org/abs/2106.02522",
    "authors": [
      "Junran Wu",
      "Ke Xu",
      "Xueyuan Chen",
      "Shangzhe Li",
      "Jichang Zhao"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.02548",
    "title": "Coordination problems on networks revisited: statics and dynamics",
    "abstract": "Simple binary-state coordination models are widely used to study collective socio-economic phenomena such as the spread of innovations or the adoption of products on social networks. The common trait of these systems is the occurrence of large-scale coordination events taking place abruptly, in the form of a cascade process, as a consequence of small perturbations of an apparently stable state. The conditions for the occurrence of cascade instabilities have been largely analysed in the literature, however for the same coordination models no sufficient attention was given to the relation between structural properties of (Nash) equilibria and possible outcomes of dynamical equilibrium selection. Using methods from the statistical physics of disordered systems, the present work investigates both analytically and numerically, the statistical properties of such Nash equilibria on networks, focusing mostly on random graphs. We provide an accurate description of these properties, which is then exploited to shed light on the mechanisms behind the onset of coordination/miscoordination on large networks. This is done studying the most common processes of dynamical equilibrium selection, such as best response, bounded-rational dynamics and learning processes. In particular, we show that well beyond the instability region, full coordination is still globally stochastically stable, however equilibrium selection processes with low stochasticity (e.g. best response) or strong memory effects (e.g. reinforcement learning) can be prevented from achieving full coordination by being trapped into a large (exponentially in number of agents) set of locally stable Nash equilibria at low/medium coordination (inefficient equilibria). These results should be useful to allow a better understanding of general coordination problems on complex networks. ",
    "url": "https://arxiv.org/abs/2106.02548",
    "authors": [
      "Luca Dall'Asta"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2106.02630",
    "title": "Fundamental tradeoffs between memorization and robustness in random  features and neural tangent regimes",
    "abstract": "This work studies the (non)robustness of two-layer neural networks in various high-dimensional linearized regimes. We establish fundamental trade-offs between memorization and robustness, as measured by the Sobolev-seminorm of the model w.r.t the data distribution, i.e the square root of the average squared $L_2$-norm of the gradients of the model w.r.t the its input. More precisely, if $n$ is the number of training examples, $d$ is the input dimension, and $k$ is the number of hidden neurons in a two-layer neural network, we prove for a large class of activation functions that, if the model memorizes even a fraction of the training, then its Sobolev-seminorm is lower-bounded by (i) $\\sqrt{n}$ in case of infinite-width random features (RF) or neural tangent kernel (NTK) with $d \\gtrsim n$; (ii) $\\sqrt{n}$ in case of finite-width RF with proportionate scaling of $d$ and $k$; and (iii) $\\sqrt{n/k}$ in case of finite-width NTK with proportionate scaling of $d$ and $k$. Moreover, all of these lower-bounds are tight: they are attained by the min-norm / least-squares interpolator (when $n$, $d$, and $k$ are in the appropriate interpolating regime). All our results hold as soon as data is log-concave isotropic, and there is label-noise, i.e the target variable is not a deterministic function of the data / features. We empirically validate our theoretical results with experiments. Accidentally, these experiments also reveal for the first time, (iv) a multiple-descent phenomenon in the robustness of the min-norm interpolator. ",
    "url": "https://arxiv.org/abs/2106.02630",
    "authors": [
      "Elvis Dohmatob"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2012.05420",
    "title": "On the emergence of simplex symmetry in the final and penultimate layers  of neural network classifiers",
    "abstract": " Title: On the emergence of simplex symmetry in the final and penultimate layers  of neural network classifiers ",
    "url": "https://arxiv.org/abs/2012.05420",
    "authors": [
      "Weinan E",
      "Stephan Wojtowytsch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2012.13317",
    "title": "Concurrency measures in the era of temporal network epidemiology: A  review",
    "abstract": " Comments: 5 figures ",
    "url": "https://arxiv.org/abs/2012.13317",
    "authors": [
      "Naoki Masuda",
      "Joel C. Miller",
      "Petter Holme"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2102.01678",
    "title": "Learning domain-agnostic visual representation for computational  pathology using medically-irrelevant style transfer augmentation",
    "abstract": " Title: Learning domain-agnostic visual representation for computational  pathology using medically-irrelevant style transfer augmentation ",
    "url": "https://arxiv.org/abs/2102.01678",
    "authors": [
      "Rikiya Yamashita",
      "Jin Long",
      "Snikitha Banda",
      "Jeanne Shen",
      "Daniel L. Rubin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2102.13451",
    "title": "FjORD: Fair and Accurate Federated Learning under heterogeneous targets  with Ordered Dropout",
    "abstract": " Comments: Updated results, additions in appendix ",
    "url": "https://arxiv.org/abs/2102.13451",
    "authors": [
      "Samuel Horvath",
      "Stefanos Laskaridis",
      "Mario Almeida",
      "Ilias Leontiadis",
      "Stylianos I. Venieris",
      "Nicholas D. Lane"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2103.10201",
    "title": "The impact of using biased performance metrics on software defect  prediction research",
    "abstract": " Comments: Submitted to the journal Information & Software Technology. It is a greatly extended version of \"Assessing Software Defection Prediction Performance: Why Using the Matthews Correlation Coefficient Matters\" presented at EASE 2020 ",
    "url": "https://arxiv.org/abs/2103.10201",
    "authors": [
      "Jingxiu Yao",
      "Martin Shepperd"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  }
]