[
  {
    "id": "arXiv:2110.14811",
    "title": "Equivariant vector field network for many-body system modeling",
    "abstract": "Modeling many-body systems has been a long-standing challenge in science, from classical and quantum physics to computational biology. Equivariance is a critical physical symmetry for many-body dynamic systems, which enables robust and accurate prediction under arbitrary reference transformations. In light of this, great efforts have been put on encoding this symmetry into deep neural networks, which significantly boosts the prediction performance of down-streaming tasks. Some general equivariant models which are computationally efficient have been proposed, however, these models have no guarantee on the approximation power and may have information loss. In this paper, we leverage insights from the scalarization technique in differential geometry to model many-body systems by learning the gradient vector fields, which are SE(3) and permutation equivariant. Specifically, we propose the Equivariant Vector Field Network (EVFN), which is built on a novel tuple of equivariant basis and the associated scalarization and vectorization layers. Since our tuple equivariant basis forms a complete basis, learning the dynamics with our EVFN has no information loss and no tensor operations are involved before the final vectorization, which reduces the complex optimization on tensors to a minimum. We evaluate our method on predicting trajectories of simulated Newton mechanics systems with both full and partially observed data, as well as the equilibrium state of small molecules (molecular conformation) evolving as a statistical mechanics system. Experimental results across multiple tasks demonstrate that our model achieves best or competitive performance on baseline models in various types of datasets. ",
    "url": "https://arxiv.org/abs/2110.14811",
    "authors": [
      "Weitao Du",
      "He Zhang",
      "Yuanqi Du",
      "Qi Meng",
      "Wei Chen",
      "Bin Shao",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Differential Geometry (math.DG)",
      "Applied Physics (physics.app-ph)"
    ]
  },
  {
    "id": "arXiv:2110.14932",
    "title": "A recursive robust filtering approach for 3D registration",
    "abstract": "This work presents a new recursive robust filtering approach for feature-based 3D registration. Unlike the common state-of-the-art alignment algorithms, the proposed method has four advantages that have not yet occurred altogether in any previous solution. For instance, it is able to deal with inherent noise contaminating sensory data; it is robust to uncertainties caused by noisy feature localisation; it also combines the advantages of both (Formula presented.) and (Formula presented.) norms for a higher performance and a more prospective prevention of local minima. The result is an accurate and stable rigid body transformation. The latter enables a thorough control over the convergence regarding the alignment as well as a correct assessment of the quality of registration. The mathematical rationale behind the proposed approach is explained, and the results are validated on physical and synthetic data. ",
    "url": "https://arxiv.org/abs/2110.14932",
    "authors": [
      "Abdenour Amamra",
      "Nabil Aouf",
      "Dowling Stuart",
      "Mark Richardson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2110.15002",
    "title": "On the explainability of hospitalization prediction on a large COVID-19  patient dataset",
    "abstract": "We develop various AI models to predict hospitalization on a large (over 110$k$) cohort of COVID-19 positive-tested US patients, sourced from March 2020 to February 2021. Models range from Random Forest to Neural Network (NN) and Time Convolutional NN, where combination of the data modalities (tabular and time dependent) are performed at different stages (early vs. model fusion). Despite high data unbalance, the models reach average precision 0.96-0.98 (0.75-0.85), recall 0.96-0.98 (0.74-0.85), and $F_1$-score 0.97-0.98 (0.79-0.83) on the non-hospitalized (or hospitalized) class. Performances do not significantly drop even when selected lists of features are removed to study model adaptability to different scenarios. However, a systematic study of the SHAP feature importance values for the developed models in the different scenarios shows a large variability across models and use cases. This calls for even more complete studies on several explainability methods before their adoption in high-stakes scenarios. ",
    "url": "https://arxiv.org/abs/2110.15002",
    "authors": [
      "Ivan Girardi",
      "Panagiotis Vagenas",
      "Dario Arcos-D\u00edaz",
      "Lydia Bessa\u00ef",
      "Alexander B\u00fcsser",
      "Ludovico Furlan",
      "Raffaello Furlan",
      "Mauro Gatti",
      "Andrea Giovannini",
      "Ellen Hoeven",
      "Chiara Marchiori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.15269",
    "title": "Cognitive network science quantifies feelings expressed in suicide  letters and Reddit mental health communities",
    "abstract": "Writing messages is key to expressing feelings. This study adopts cognitive network science to reconstruct how individuals report their feelings in clinical narratives like suicide notes or mental health posts. We achieve this by reconstructing syntactic/semantic associations between conceptsin texts as co-occurrences enriched with affective data. We transform 142 suicide notes and 77,000 Reddit posts from the r/anxiety, r/depression, r/schizophrenia, and r/do-it-your-own (r/DIY) forums into 5 cognitive networks, each one expressing meanings and emotions as reported by authors. These networks reconstruct the semantic frames surrounding \\textit{feel}, enabling a quantification of prominent associations and emotions focused around feelings. We find strong feelings of sadness across all clinical Reddit boards, added to fear r/depression, and replaced by joy/anticipation in r/DIY. Semantic communities and topic modelling both highlight key narrative topics of \\textit{regret}, \\textit{unhealthy lifestyle} and \\textit{low mental well-being}. Importantly, negative associations and emotions co-existed with trustful/positive language, focused on \\textit{getting better}. This emotional polarisation provides quantitative evidence that online clinical boards possess a complex structure, where users mix both positive and negative outlooks. This dichotomy is absent in the r/DIY reference board and in suicide notes, where negative emotional associations about regret and pain persist but are overwhelmed by positive jargon addressing loved ones. Our quantitative comparisons provide strong evidence that suicide notes encapsulate different ways of expressing feelings compared to online Reddit boards, the latter acting more like personal diaries and relief valve. Our findings provide an interpretable, quantitative aid for supporting psychological inquiries of human feelings in digital and clinical settings. ",
    "url": "https://arxiv.org/abs/2110.15269",
    "authors": [
      "Simmi Marina Joseph",
      "Salvatore Citraro",
      "Virginia Morini",
      "Giulio Rossetti",
      "Massimo Stella"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2110.15304",
    "title": "Sobolev-type embeddings for neural network approximation spaces",
    "abstract": "We consider neural network approximation spaces that classify functions according to the rate at which they can be approximated (with error measured in $L^p$) by ReLU neural networks with an increasing number of coefficients, subject to bounds on the magnitude of the coefficients and the number of hidden layers. We prove embedding theorems between these spaces for different values of $p$. Furthermore, we derive sharp embeddings of these approximation spaces into H\\\"older spaces. We find that, analogous to the case of classical function spaces (such as Sobolev spaces, or Besov spaces) it is possible to trade \"smoothness\" (i.e., approximation rate) for increased integrability. Combined with our earlier results in [arXiv:2104.02746], our embedding theorems imply a somewhat surprising fact related to \"learning\" functions from a given neural network space based on point samples: if accuracy is measured with respect to the uniform norm, then an optimal \"learning\" algorithm for reconstructing functions that are well approximable by ReLU neural networks is simply given by piecewise constant interpolation on a tensor product grid. ",
    "url": "https://arxiv.org/abs/2110.15304",
    "authors": [
      "Philipp Grohs",
      "Felix Voigtlaender"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.15321",
    "title": "Homogenisation of dynamical optimal transport on periodic graphs",
    "abstract": "This paper deals with the large-scale behaviour of dynamical optimal transport on $\\mathbb{Z}^d$-periodic graphs with general lower semicontinuous and convex energy densities. Our main contribution is a homogenisation result that describes the effective behaviour of the discrete problems in terms of a continuous optimal transport problem. The effective energy density can be explicitly expressed in terms of a cell formula, which is a finite-dimensional convex programming problem that depends non-trivially on the local geometry of the discrete graph and the discrete energy density. Our homogenisation result is derived from a $\\Gamma$-convergence result for action functionals on curves of measures, which we prove under very mild growth conditions on the energy density. We investigate the cell formula in several cases of interest, including finite-volume discretisations of the Wasserstein distance, where non-trivial limiting behaviour occurs. ",
    "url": "https://arxiv.org/abs/2110.15321",
    "authors": [
      "Peter Gladbach",
      "Eva Kopfer",
      "Jan Maas",
      "Lorenzo Portinale"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Metric Geometry (math.MG)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:1911.13034",
    "title": "Model structures and fitting criteria for system identification with  neural networks",
    "abstract": " Comments: Source code generating the results of the paper available at this https URL ",
    "url": "https://arxiv.org/abs/1911.13034",
    "authors": [
      "Marco Forgione",
      "Dario Piga"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2102.04133",
    "title": "Local certification of graphs on surfaces",
    "abstract": " Comments: 10 pages, 5 figures - v3: revised version ",
    "url": "https://arxiv.org/abs/2102.04133",
    "authors": [
      "Louis Esperet",
      "Benjamin L\u00e9v\u00eaque"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2102.06408",
    "title": "Supervised training of spiking neural networks for robust deployment on  mixed-signal neuromorphic processors",
    "abstract": " Title: Supervised training of spiking neural networks for robust deployment on  mixed-signal neuromorphic processors ",
    "url": "https://arxiv.org/abs/2102.06408",
    "authors": [
      "Julian B\u00fcchel",
      "Dmitrii Zendrikov",
      "Sergio Solinas",
      "Giacomo Indiveri",
      "Dylan R. Muir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2102.12959",
    "title": "Bayesian OOD detection with aleatoric uncertainty and outlier exposure",
    "abstract": " Title: Bayesian OOD detection with aleatoric uncertainty and outlier exposure ",
    "url": "https://arxiv.org/abs/2102.12959",
    "authors": [
      "Xi Wang",
      "Laurence Aitchison"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2103.00882",
    "title": "k-apices of minor-closed graph classes. I. Bounding the obstructions",
    "abstract": " Comments: 46 pages and 12 figures. arXiv admin note: text overlap with arXiv:2004.12692 ",
    "url": "https://arxiv.org/abs/2103.00882",
    "authors": [
      "Ignasi Sau",
      "Giannos Stamoulis",
      "Dimitrios M. Thilikos"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2105.14119",
    "title": "Towards optimally abstaining from prediction with OOD test examples",
    "abstract": " Comments: In NeurIPS 2021 (+spotlight), 24 pages ",
    "url": "https://arxiv.org/abs/2105.14119",
    "authors": [
      "Adam Tauman Kalai",
      "Varun Kanade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.06615",
    "title": "Precise characterization of the prior predictive distribution of deep  ReLU networks",
    "abstract": " Title: Precise characterization of the prior predictive distribution of deep  ReLU networks ",
    "url": "https://arxiv.org/abs/2106.06615",
    "authors": [
      "Lorenzo Noci",
      "Gregor Bachmann",
      "Kevin Roth",
      "Sebastian Nowozin",
      "Thomas Hofmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2108.11684",
    "title": "Disentangled generative models for robust dynamical system prediction",
    "abstract": " Comments: We provide code for reproducing our experiments at this https URL, Animated phase-space and video predictions are available at this https URL ",
    "url": "https://arxiv.org/abs/2108.11684",
    "authors": [
      "Stathi Fotiadis",
      "Shunlong Hu",
      "Mario Lino",
      "Chris Cantwell",
      "Anil Bharath"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2110.11501",
    "title": "Cortico-cerebellar networks as decoupling neural interfaces",
    "abstract": " Comments: To appear in Advances in Neural Information Processing Systems 35 (NeurIPS 2021); 15 pages and 5 figures in the main manuscript; 8 pages and 8 figures in the supplementary material ",
    "url": "https://arxiv.org/abs/2110.11501",
    "authors": [
      "Joseph Pemberton",
      "Ellen Boven",
      "Richard Apps",
      "Rui Ponte Costa"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  }
]