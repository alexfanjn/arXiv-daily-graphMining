[
  {
    "id": "arXiv:2110.11400",
    "title": "Channel redundancy and overlap in convolutional neural networks with  channel-wise NNK graphs",
    "abstract": "Feature spaces in the deep layers of convolutional neural networks (CNNs) are often very high-dimensional and difficult to interpret. However, convolutional layers consist of multiple channels that are activated by different types of inputs, which suggests that more insights may be gained by studying the channels and how they relate to each other. In this paper, we first analyze theoretically channel-wise non-negative kernel (CW-NNK) regression graphs, which allow us to quantify the overlap between channels and, indirectly, the intrinsic dimension of the data representation manifold. We find that redundancy between channels is significant and varies with the layer depth and the level of regularization during training. Additionally, we observe that there is a correlation between channel overlap in the last convolutional layer and generalization performance. Our experimental results demonstrate that these techniques can lead to a better understanding of deep representations. ",
    "url": "https://arxiv.org/abs/2110.11400",
    "authors": [
      "David Bonet",
      "Antonio Ortega",
      "Javier Ruiz-Hidalgo",
      "Sarath Shekkizhar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.11424",
    "title": "Analysis of memory consumption by neural networks based on  hyperparameters",
    "abstract": "Deep learning models are trained and deployed in multiple domains. Increasing usage of deep learning models alarms the usage of memory consumed while computation by deep learning models. Existing approaches for reducing memory consumption like model compression, hardware changes are specific. We propose a generic analysis of memory consumption while training deep learning models in comparison with hyperparameters used for training. Hyperparameters which includes the learning rate, batchsize, number of hidden layers and depth of layers decide the model performance, accuracy of the model. We assume the optimizers and type of hidden layers as a known values. The change in hyperparamaters and the number of hidden layers are the variables considered in this proposed approach. For better understanding of the computation cost, this proposed analysis studies the change in memory consumption with respect to hyperparameters as main focus. This results in general analysis of memory consumption changes during training when set of hyperparameters are altered. ",
    "url": "https://arxiv.org/abs/2110.11424",
    "authors": [
      "Mahendran N"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.11695",
    "title": "Node package manager's dependency network robustness",
    "abstract": "The robustness of npm dependency network is a crucial property, since many projects and web applications heavily rely on the functionalities of packages, especially popular ones that have many dependant packages. In the past, there have been instances where the removal or update of certain npm packages has caused widespread chaos and web-page downtime on the internet. Our goal is to track the network's resilience to such occurrences through time and figure out whether the state of the network is trending towards a more robust structure. We show that the network is not robust to targeted attacks, since a security risk in a few crucial nodes affects a large part of the network. Because such packages are often backed up by serious communities with high standards, the issue is not alarming and is a consequence of power law distribution of the network. The current trend in average number of dependencies and effect of important nodes on the rest of the network is decreasing, which further improves the resilience and sets a positive path in development. Furthermore, we show that communities form around the most important packages, although they do not conform well to the common community definition using modularity. We also provide guidelines for package development that increases the robustness of the network and reduces the possibility of introducing security risks. ",
    "url": "https://arxiv.org/abs/2110.11695",
    "authors": [
      "Andrej Hafner",
      "An\u017ee Mur",
      "Jaka Bernard"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2110.11772",
    "title": "Grounding force-directed network layouts with latent space models",
    "abstract": "Force-directed layout algorithms are ubiquitously-used tools for network visualization across a variety of scientific disciplines. However, they lack theoretical grounding which allows to interpret their outcomes rigorously. We propose an approach building on latent space network models, which assume that the probability of nodes forming a tie depends on their distance in an unobserved latent space. From such latent space models, we derive force equations for a force-directed layout algorithm. With this approach, force-directed layouts become interpretable, since the forces infer positions which maximize the likelihood of the given network under the latent space model. We implement these forces for (un)directed unweighted and weighted networks. We spatialise different real-world networks, where we find central network properties reflected in the layout, and compare the layouts to different force-directed algorithms already in use today. ",
    "url": "https://arxiv.org/abs/2110.11772",
    "authors": [
      "Felix Gaisbauer",
      "Armin Pournaki",
      "Sven Banisch",
      "Eckehard Olbrich"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2110.11851",
    "title": "Voting algorithms for unique games on complete graphs",
    "abstract": "An approximation algorithm for a Constraint Satisfaction Problem is called robust if it outputs an assignment satisfying a $(1 - f(\\epsilon))$-fraction of the constraints on any $(1-\\epsilon)$-satisfiable instance, where the loss function $f$ is such that $f(\\epsilon) \\rightarrow 0$ as $\\epsilon \\rightarrow 0$. Moreover, the runtime of the algorithm should not depend in any way on $\\epsilon$. In this paper, we present such an algorithm for {\\sc Min-Unique-Games(q)} on complete graphs with $q$ labels. Specifically, the loss function is $f(\\epsilon) = (\\epsilon + c_{\\epsilon} \\epsilon^2)$, where $c_{\\epsilon}$ is a constant depending on $\\epsilon$ such that $\\lim_{\\epsilon \\rightarrow 0} c_{\\epsilon} = 16$. The runtime of our algorithm is $O(qn^3)$ (with no dependence on $\\epsilon$) and can run in time $O(qn^2)$ using a randomized implementation with a slightly larger constant $c_{\\epsilon}$. Our algorithm is combinatorial and uses voting to find an assignment. We prove NP-hardness (using a randomized reduction) for {\\sc Min-Unique-Games(q)} on complete graphs even in the case where the constraints form a cyclic permutation, which is also known as {\\sc Min-Linear-Equations-mod-$q$} on complete graphs. ",
    "url": "https://arxiv.org/abs/2110.11851",
    "authors": [
      "Antoine M\u00e9ot",
      "Arnaud de Mesmay",
      "Moritz M\u00fchlenthaler",
      "Alantha Newman"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2110.11950",
    "title": "Adversarial robustness for latent models: Revisiting the robust-standard  accuracies tradeoff",
    "abstract": "Over the past few years, several adversarial training methods have been proposed to improve the robustness of machine learning models against adversarial perturbations in the input. Despite remarkable progress in this regard, adversarial training is often observed to drop the standard test accuracy. This phenomenon has intrigued the research community to investigate the potential tradeoff between standard and robust accuracy as two performance measures. In this paper, we revisit this tradeoff for latent models and argue that this tradeoff is mitigated when the data enjoys a low-dimensional structure. In particular, we consider binary classification under two data generative models, namely Gaussian mixture model and generalized linear model, where the feature data lie on a low-dimensional manifold. We show that as the manifold dimension to the ambient dimension decreases, one can obtain models that are nearly optimal with respect to both, the standard accuracy and the robust accuracy measures. ",
    "url": "https://arxiv.org/abs/2110.11950",
    "authors": [
      "Adel Javanmard",
      "Mohammad Mehrabi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2110.11339",
    "title": "Unsupervised cross-user adaptation in taste sensationrecognition based  on surface electromyography withconformal prediction and domain  regularizedcomponent analysis",
    "abstract": "Human taste sensation can be qualitatively described with surface electromyography. However, the pattern recognition models trained on one subject (the source domain) do not generalize well on other subjects (the target domain). To improve the generalizability and transferability of taste sensation models developed with sEMG data, two methods were innovatively applied in this study: domain regularized component analysis (DRCA) and conformal prediction with shrunken centroids (CPSC). The effectiveness of these two methods was investigated independently in an unlabeled data augmentation process with the unlabeled data from the target domain, and the same cross-user adaptation pipeline were conducted on six subjects. The results show that DRCA improved the classification accuracy on six subjects (p < 0.05), compared with the baseline models trained only with the source domain data;, while CPSC did not guarantee the accuracy improvement. Furthermore, the combination of DRCA and CPSC presented statistically significant improvement (p < 0.05) in classification accuracy on six subjects. The proposed strategy combining DRCA and CPSC showed its effectiveness in addressing the cross-user data distribution drift in sEMG-based taste sensation recognition application. It also shows the potential in more cross-user adaptation applications. ",
    "url": "https://arxiv.org/abs/2110.11339",
    "authors": [
      "Hengyang Wang",
      "Xianghao Zhan",
      "Li Liu",
      "Asif Ullah",
      "Huiyan Li",
      "Han Gao",
      "You Wang",
      "Guang Li"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.11493",
    "title": "Surface code compilation via edge-disjoint paths",
    "abstract": "We provide an efficient algorithm to compile quantum circuits for fault-tolerant execution. We target surface codes, which form a 2D grid of logical qubits with nearest-neighbor logical operations. Embedding an input circuit's qubits in surface codes can result in long-range two-qubit operations across the grid. We show how to prepare many long-range Bell pairs on qubits connected by edge-disjoint paths of ancillas in constant depth which can be used to perform these long-range operations. This forms one core part of our Edge-Disjoint Paths Compilation (EDPC) algorithm, by easily performing parallel long-range Clifford operations in constant depth. It also allows us to establish a connection between surface code compilation and several well-studied edge-disjoint paths problems. Similar techniques allow us to perform non-Clifford single-qubit rotations far from magic state distillation factories. In this case, we can easily find the maximum set of paths by a max-flow reduction, which forms the other major part of our EDPC algorithm. We compare EDPC to other compilation approaches including a SWAP-based algorithm, and find significantly improved performance for circuits built from parallel CNOTs, and for circuits which implement the multi-controlled X gate. ",
    "url": "https://arxiv.org/abs/2110.11493",
    "authors": [
      "Michael Beverland",
      "Vadym Kliuchnikov",
      "Eddie Schoute"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2110.11501",
    "title": "Cortico-cerebellar networks as decoupling neural interfaces",
    "abstract": "The brain solves the credit assignment problem remarkably well. For credit to be assigned across neural networks they must, in principle, wait for specific neural computations to finish. How the brain deals with this inherent locking problem has remained unclear. Deep learning methods suffer from similar locking constraints both on the forward and feedback phase. Recently, decoupled neural interfaces (DNIs) were introduced as a solution to the forward and feedback locking problems in deep networks. Here we propose that a specialised brain region, the cerebellum, helps the cerebral cortex solve similar locking problems akin to DNIs. To demonstrate the potential of this framework we introduce a systems-level model in which a recurrent cortical network receives online temporal feedback predictions from a cerebellar module. We test this cortico-cerebellar recurrent neural network (ccRNN) model on a number of sensorimotor (line and digit drawing) and cognitive tasks (pattern recognition and caption generation) that have been shown to be cerebellar-dependent. In all tasks, we observe that ccRNNs facilitates learning while reducing ataxia-like behaviours, consistent with classical experimental observations. Moreover, our model also explains recent behavioural and neuronal observations while making several testable predictions across multiple levels. Overall, our work offers a novel perspective on the cerebellum as a brain-wide decoupling machine for efficient credit assignment and opens a new avenue between deep learning and neuroscience. ",
    "url": "https://arxiv.org/abs/2110.11501",
    "authors": [
      "Joseph Pemberton",
      "Ellen Boven",
      "Richard Apps",
      "Rui Ponte Costa"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.11856",
    "title": "L-2 Regularized maximum likelihood for $\u03b2$-model in large and sparse  networks",
    "abstract": "The $\\beta$-model is a powerful tool for modeling network generation driven by node degree heterogeneity. Its simple yet expressive nature particularly well-suits large and sparse networks, where many network models become infeasible due to computational challenge and observation scarcity. However, existing estimation algorithms for $\\beta$-model do not scale up; and theoretical understandings remain limited to dense networks. This paper brings several major improvements to the method and theory of $\\beta$-model to address urgent needs of practical applications. Our contributions include: 1. method: we propose a new $\\ell_2$ penalized MLE scheme; we design a novel algorithm that can comfortably handle sparse networks of millions of nodes, much faster and more memory-parsimonious than any existing algorithm; 2. theory: we present new error bounds on beta-models under much weaker assumptions; we also establish new lower-bounds and new asymptotic normality results; distinct from existing literature, our results cover both small and large regularization scenarios and reveal their distinct asymptotic dependency structures; 3. application: we apply our method to large COVID-19 network data sets and discover meaningful results. ",
    "url": "https://arxiv.org/abs/2110.11856",
    "authors": [
      "Yu Zhang",
      "Qiuping Wang",
      "Yuan Zhang",
      "Ting Yan",
      "Jing Luo"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Social and Information Networks (cs.SI)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2012.02679",
    "title": "What is a meaningful representation of protein sequences?",
    "abstract": " Comments: 17 pages, 8 figures, 2 tables ",
    "url": "https://arxiv.org/abs/2012.02679",
    "authors": [
      "Nicki Skafte Detlefsen",
      "S\u00f8ren Hauberg",
      "Wouter Boomsma"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2102.03868",
    "title": "U-vectors: Generating clusterable speaker embedding from unlabeled data",
    "abstract": " Comments: 18 pages, 7 figures ",
    "url": "https://arxiv.org/abs/2102.03868",
    "authors": [
      "M. F. Mridha",
      "Abu Quwsar Ohi",
      "Muhammad Mostafa Monowar",
      "Md. Abdul Hamid",
      "Md. Rashedul Islam",
      "Yutaka Watanobe"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2103.10411",
    "title": "Group interactions modulate critical mass dynamics in social convention",
    "abstract": " Comments: 10 pages, 5 figures, Supplementary Material (13 pages, 12 figures) ",
    "url": "https://arxiv.org/abs/2103.10411",
    "authors": [
      "Iacopo Iacopini",
      "Giovanni Petri",
      "Andrea Baronchelli",
      "Alain Barrat"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2109.14162",
    "title": "Can multi-label classification networks know what they don't know?",
    "abstract": " Comments: Paper published at NeurIPS 2021 ",
    "url": "https://arxiv.org/abs/2109.14162",
    "authors": [
      "Haoran Wang",
      "Weitang Liu",
      "Alex Bocchieri",
      "Yixuan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.04850",
    "title": "Direct source and early reflections localization using deep  deconvolution network under reverberant environment",
    "abstract": " Title: Direct source and early reflections localization using deep  deconvolution network under reverberant environment ",
    "url": "https://arxiv.org/abs/2110.04850",
    "authors": [
      "Shan Gao",
      "Xihong Wu",
      "Tianshu Qu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2110.11281",
    "title": "Super-resolution of multiphase materials by combining complementary 2D  and 3D image data using generative adversarial networks",
    "abstract": " Title: Super-resolution of multiphase materials by combining complementary 2D  and 3D image data using generative adversarial networks ",
    "url": "https://arxiv.org/abs/2110.11281",
    "authors": [
      "Amir Dahari",
      "Steve Kench",
      "Isaac Squires",
      "Samuel J. Cooper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  }
]