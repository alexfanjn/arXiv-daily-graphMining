[
  {
    "id": "arXiv:2110.03760",
    "title": "Design Strategy Network: A deep hierarchical framework to represent  generative design strategies in complex action spaces",
    "abstract": "Generative design problems often encompass complex action spaces that may be divergent over time, contain state-dependent constraints, or involve hybrid (discrete and continuous) domains. To address those challenges, this work introduces Design Strategy Network (DSN), a data-driven deep hierarchical framework that can learn strategies over these arbitrary complex action spaces. The hierarchical architecture decomposes every action decision into first predicting a preferred spatial region in the design space and then outputting a probability distribution over a set of possible actions from that region. This framework comprises a convolutional encoder to work with image-based design state representations, a multi-layer perceptron to predict a spatial region, and a weight-sharing network to generate a probability distribution over unordered set-based inputs of feasible actions. Applied to a truss design study, the framework learns to predict the actions of human designers in the study, capturing their truss generation strategies in the process. Results show that DSNs significantly outperform non-hierarchical methods of policy representation, demonstrating their superiority in complex action space problems. ",
    "url": "https://arxiv.org/abs/2110.03760",
    "authors": [
      "Ayush Raina",
      "Jonathan Cagan",
      "Christopher McComb"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.03851",
    "title": "Automatic annotation of visual deep neural networks",
    "abstract": "Computer vision is widely used in the fields of driverless, face recognition and 3D reconstruction as a technology to help or replace human eye perception images or multidimensional data through computers. Nowadays, with the development and application of deep neural networks, the models of deep neural networks proposed for computer vision are becoming more and more abundant, and developers will use the already trained models on the way to solve problems, and need to consult the relevant documents to understand the use of the model. The class model, which creates the need to quickly and accurately find the relevant models that you need. The automatic annotation method of visual depth neural network proposed in this paper is based on natural language processing technology such as semantic analysis, which realizes automatic labeling of model application fields. In the three top international conferences on computer vision: ICCV, CVPR and ECCV, the average correct rate of application of the papers of 72 papers reached 90%, indicating the effectiveness of the automatic labeling system. ",
    "url": "https://arxiv.org/abs/2110.03851",
    "authors": [
      "Ming Li",
      "ChenHao Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.03903",
    "title": "Kinematically consistent recurrent neural networks for learning inverse  problems in wave propagation",
    "abstract": "Although machine learning (ML) is increasingly employed recently for mechanistic problems, the black-box nature of conventional ML architectures lacks the physical knowledge to infer unforeseen input conditions. This implies both severe overfitting during a dearth of training data and inadequate physical interpretability, which motivates us to propose a new kinematically consistent, physics-based ML model. In particular, we attempt to perform physically interpretable learning of inverse problems in wave propagation without suffering overfitting restrictions. Towards this goal, we employ long short-term memory (LSTM) networks endowed with a physical, hyperparameter-driven regularizer, performing penalty-based enforcement of the characteristic geometries. Since these characteristics are the kinematical invariances of wave propagation phenomena, maintaining their structure provides kinematical consistency to the network. Even with modest training data, the kinematically consistent network can reduce the $L_1$ and $L_\\infty$ error norms of the plain LSTM predictions by about 45% and 55%, respectively. It can also increase the horizon of the plain LSTM's forecasting by almost two times. To achieve this, an optimal range of the physical hyperparameter, analogous to an artificial bulk modulus, has been established through numerical experiments. The efficacy of the proposed method in alleviating overfitting, and the physical interpretability of the learning mechanism, are also discussed. Such an application of kinematically consistent LSTM networks for wave propagation learning is presented here for the first time. ",
    "url": "https://arxiv.org/abs/2110.03903",
    "authors": [
      "Wrik Mallik",
      "Rajeev K. Jaiman",
      "Jasmin Jelovica"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2110.04004",
    "title": "Trident Pyramid Networks: The importance of processing at the feature  pyramid level for better object detection",
    "abstract": "Feature pyramids have become ubiquitous in multi-scale computer vision tasks such as object detection. Based on their importance, we divide a computer vision network into three parts: a backbone (generating a feature pyramid), a core (refining the feature pyramid) and a head (generating the final output). Most existing networks operating on feature pyramids, named cores, are shallow and mostly focus on communication-based processing in the form of top-down and bottom-up operations. We present a new core architecture called Trident Pyramid Network (TPN), that allows for a deeper design and for a better balance between communication-based processing and self-processing. We show consistent improvements when using our TPN core on the COCO object detection benchmark, outperforming the popular BiFPN baseline by 1.5 AP. Additionally, we empirically show that it is more beneficial to put additional computation into the TPN core, rather than into the backbone, by outperforming a ResNet-101+FPN baseline with our ResNet-50+TPN network by 1.7 AP, while operating under similar computation budgets. This emphasizes the importance of performing computation at the feature pyramid level in modern-day object detection systems. Code will be released. ",
    "url": "https://arxiv.org/abs/2110.04004",
    "authors": [
      "C\u00e9dric Picron",
      "Tinne Tuytelaars"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.04057",
    "title": "FAST-RIR: Fast neural diffuse room impulse response generator",
    "abstract": "We present a neural-network-based fast diffuse room impulse response generator (FAST-RIR) for generating room impulse responses (RIRs) for a given acoustic environment. Our FAST-RIR takes rectangular room dimensions, listener and speaker positions, and reverberation time as inputs and generates specular and diffuse reflections for a given acoustic environment. Our FAST-RIR is capable of generating RIRs for a given input reverberation time with an average error of 0.02s. We evaluate our generated RIRs in automatic speech recognition (ASR) applications using Google Speech API, Microsoft Speech API, and Kaldi tools. We show that our proposed FAST-RIR with batch size 1 is 400 times faster than a state-of-the-art diffuse acoustic simulator (DAS) on a CPU and gives similar performance to DAS in ASR experiments. Our FAST-RIR is 12 times faster than an existing GPU-based RIR generator (gpuRIR). We show that our FAST-RIR outperforms gpuRIR by 2.5% in an AMI far-field ASR benchmark. ",
    "url": "https://arxiv.org/abs/2110.04057",
    "authors": [
      "Anton Ratnarajah",
      "Shi-Xiong Zhang",
      "Meng Yu",
      "Zhenyu Tang",
      "Dinesh Manocha",
      "Dong Yu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2110.04140",
    "title": "Rapid head-pose detection for automated slice prescription of  fetal-brain MRI",
    "abstract": "In fetal-brain MRI, head-pose changes between prescription and acquisition present a challenge to obtaining the standard sagittal, coronal and axial views essential to clinical assessment. As motion limits acquisitions to thick slices that preclude retrospective resampling, technologists repeat ~55-second stack-of-slices scans (HASTE) with incrementally reoriented field of view numerous times, deducing the head pose from previous stacks. To address this inefficient workflow, we propose a robust head-pose detection algorithm using full-uterus scout scans (EPI) which take ~5 seconds to acquire. Our ~2-second procedure automatically locates the fetal brain and eyes, which we derive from maximally stable extremal regions (MSERs). The success rate of the method exceeds 94% in the third trimester, outperforming a trained technologist by up to 20%. The pipeline may be used to automatically orient the anatomical sequence, removing the need to estimate the head pose from 2D views and reducing delays during which motion can occur. ",
    "url": "https://arxiv.org/abs/2110.04140",
    "authors": [
      "Malte Hoffmann",
      "Esra Abaci Turk",
      "Borjan Gagoski",
      "Leah Morgan",
      "Paul Wighton",
      "M. Dylan Tisdall",
      "Martin Reuter",
      "Elfar Adalsteinsson",
      "P. Ellen Grant",
      "Lawrence L. Wald",
      "Andr\u00e9 J. W. van der Kouwe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Medical Physics (physics.med-ph)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2110.04151",
    "title": "Text analysis and deep learning: A network approach",
    "abstract": "Much information available to applied researchers is contained within written language or spoken text. Deep language models such as BERT have achieved unprecedented success in many applications of computational linguistics. However, much less is known about how these models can be used to analyze existing text. We propose a novel method that combines transformer models with network analysis to form a self-referential representation of language use within a corpus of interest. Our approach produces linguistic relations strongly consistent with the underlying model as well as mathematically well-defined operations on them, while reducing the amount of discretionary choices of representation and distance measures. It represents, to the best of our knowledge, the first unsupervised method to extract semantic networks directly from deep language models. We illustrate our approach in a semantic analysis of the term \"founder\". Using the entire corpus of Harvard Business Review from 1980 to 2020, we find that ties in our network track the semantics of discourse over time, and across contexts, identifying and relating clusters of semantic and syntactic relations. Finally, we discuss how this method can also complement and inform analyses of the behavior of deep learning models. ",
    "url": "https://arxiv.org/abs/2110.04151",
    "authors": [
      "Ingo Marquart"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2110.04250",
    "title": "Active learning for interactive satellite image change detection",
    "abstract": "We introduce in this paper a novel active learning algorithm for satellite image change detection. The proposed solution is interactive and based on a question and answer model, which asks an oracle (annotator) the most informative questions about the relevance of sampled satellite image pairs, and according to the oracle's responses, updates a decision function iteratively. We investigate a novel framework which models the probability that samples are relevant; this probability is obtained by minimizing an objective function capturing representativity, diversity and ambiguity. Only data with a high probability according to these criteria are selected and displayed to the oracle for further annotation. Extensive experiments on the task of satellite image change detection after natural hazards (namely tornadoes) show the relevance of the proposed method against the related work. ",
    "url": "https://arxiv.org/abs/2110.04250",
    "authors": [
      "Hichem Sahbi",
      "Sebastien Deschamps",
      "Andrei Stoian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.03361",
    "title": "Multi-scale speaker embedding-based graph attention networks for speaker  diarisation",
    "abstract": "The objective of this work is effective speaker diarisation using multi-scale speaker embeddings. Typically, there is a trade-off between the ability to recognise short speaker segments and the discriminative power of the embedding, according to the segment length used for embedding extraction. To this end, recent works have proposed the use of multi-scale embeddings where segments with varying lengths are used. However, the scores are combined using a weighted summation scheme where the weights are fixed after the training phase, whereas the importance of segment lengths can differ with in a single session. To address this issue, we present three key contributions in this paper: (1) we propose graph attention networks for multi-scale speaker diarisation; (2) we design scale indicators to utilise scale information of each embedding; (3) we adapt the attention-based aggregation to utilise a pre-computed affinity matrix from multi-scale embeddings. We demonstrate the effectiveness of our method in various datasets where the speaker confusion which constitutes the primary metric drops over 10% in average relative compared to the baseline. ",
    "url": "https://arxiv.org/abs/2110.03361",
    "authors": [
      "Youngki Kwon",
      "Hee-Soo Heo",
      "Jee-weon Jung",
      "You Jin Kim",
      "Bong-Jin Lee",
      "Joon Son Chung"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.03773",
    "title": "Isolation of connected graphs",
    "abstract": "For a connected $n$-vertex graph $G$ and a set $\\mathcal{F}$ of graphs, let $\\iota(G,\\mathcal{F})$ denote the size of a smallest set $D$ of vertices of $G$ such that the graph obtained from $G$ by deleting the closed neighbourhood of $D$ contains no graph in $\\mathcal{F}$. Let $\\mathcal{E}_k$ denote the set of connected graphs that have at least $k$ edges. By a result of Caro and Hansberg, $\\iota(G,\\mathcal{E}_1) \\leq n/3$ if $n \\neq 2$ and $G$ is not a $5$-cycle. The author recently showed that if $G$ is not a triangle and $\\mathcal{C}$ is the set of cycles, then $\\iota(G,\\mathcal{C}) \\leq n/4$. We improve this result by showing that $\\iota(G,\\mathcal{E}_3) \\leq n/4$ if $G$ is neither a triangle nor a $7$-cycle. Let $r$ be the number of vertices of $G$ that have only one neighbour. We determine a set $\\mathcal{S}$ of six graphs such that $\\iota(G,\\mathcal{E}_2) \\leq (4n - r)/14$ if $G$ is not a copy of a member of $\\mathcal{S}$. The bounds are sharp. ",
    "url": "https://arxiv.org/abs/2110.03773",
    "authors": [
      "Peter Borg"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2110.03857",
    "title": "A study on the efficacy of model pre-training in developing neural  text-to-speech system",
    "abstract": "In the development of neural text-to-speech systems, model pre-training with a large amount of non-target speakers' data is a common approach. However, in terms of ultimately achieved system performance for target speaker(s), the actual benefits of model pre-training are uncertain and unstable, depending very much on the quantity and text content of training data. This study aims to understand better why and how model pre-training can positively contribute to TTS system performance. It is postulated that the pre-training process plays a critical role in learning text-related variation in speech, while further training with the target speaker's data aims to capture the speaker-related variation. Different test sets are created with varying degrees of similarity to target speaker data in terms of text content. Experiments show that leveraging a speaker-independent TTS trained on speech data with diverse text content can improve the target speaker TTS on domain-mismatched text. We also attempt to reduce the amount of pre-training data for a new text domain and improve the data and computational efficiency. It is found that the TTS system could achieve comparable performance when the pre-training data is reduced to 1/8 of its original size. ",
    "url": "https://arxiv.org/abs/2110.03857",
    "authors": [
      "Guangyan Zhang",
      "Yichong Leng",
      "Daxin Tan",
      "Ying Qin",
      "Kaitao Song",
      "Xu Tan",
      "Sheng Zhao",
      "Tan Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2110.04130",
    "title": "Learning post-processing for QRS detection using Recurrent Neural  Network",
    "abstract": "Deep-learning based QRS-detection algorithms often require essential post-processing to refine the prediction streams for R-peak localisation. The post-processing performs signal-processing tasks from as simple as, removing isolated 0s or 1s in the prediction-stream to sophisticated steps, which require domain-specific knowledge, including the minimum threshold of a QRS-complex extent or R-R interval. Often these thresholds vary among QRS-detection studies and are empirically determined for the target dataset, which may have implications if the target dataset differs. Moreover, these studies, in general, fail to identify the relative strengths of deep-learning models and post-processing to weigh them appropriately. This study classifies post-processing, as found in the QRS-detection literature, into two levels - moderate, and advanced - and advocates that the thresholds be learned by an appropriate deep-learning module, called a Gated Recurrent Unit (GRU), to avoid explicitly setting post-processing thresholds. This is done by utilising the same philosophy of shifting from hand-crafted feature-engineering to deep-learning-based feature-extraction. The results suggest that GRU learns the post-processing level and the QRS detection performance using GRU-based post-processing marginally follows the domain-specific manual post-processing, without requiring usage of domain-specific threshold parameters. To the best of our knowledge, the use of GRU to learn QRS-detection post-processing from CNN model generated prediction streams is the first of its kind. The outcome was used to recommend a modular design for a QRS-detection system, where the level of complexity of the CNN model and post-processing can be tuned based on the deployment environment. ",
    "url": "https://arxiv.org/abs/2110.04130",
    "authors": [
      "Ahsan Habib",
      "Chandan Karmakar",
      "John Yearwood"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.04265",
    "title": "A study of the robustness of raw waveform based speaker embeddings under  mismatched conditions",
    "abstract": "In this paper, we conduct a cross-dataset study on parametric and non-parametric raw-waveform based speaker embeddings through speaker verification experiments. In general, we observe a more significant performance degradation of these raw-waveform systems compared to spectral based systems. We then propose two strategies to improve the performance of raw-waveform based systems on cross-dataset tests. The first strategy is to change the real-valued filters into analytic filters to ensure shift-invariance. The second strategy is to apply variational dropout to non-parametric filters to prevent them from overfitting irrelevant nuance features. ",
    "url": "https://arxiv.org/abs/2110.04265",
    "authors": [
      "Ge Zhu",
      "Frank Cwitkowitz",
      "Zhiyao Dua"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2003.05357",
    "title": "On the feasibility of automated prediction of bug and non-bug issues",
    "abstract": " Title: On the feasibility of automated prediction of bug and non-bug issues ",
    "url": "https://arxiv.org/abs/2003.05357",
    "authors": [
      "Steffen Herbold",
      "Alexander Trautsch",
      "Fabian Trautsch"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2007.15350",
    "title": "Deep neural network approximations for the stable manifolds of the  Hamilton-Jacobi equations",
    "abstract": " Comments: The algorithm is modified. The main point is that the trajectories on stable manifold are found by a combination of two-point BVP near the equilibrium and initial value problem far away from the equilibrium. The algorithm becomes more effective ",
    "url": "https://arxiv.org/abs/2007.15350",
    "authors": [
      "Guoyuan Chen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2012.07032",
    "title": "Neural network approaches to point lattice decoding",
    "abstract": " Comments: submitted to IEEE Transactions on Information Theory. arXiv admin note: text overlap with arXiv:1902.11294 ",
    "url": "https://arxiv.org/abs/2012.07032",
    "authors": [
      "Vincent Corlay",
      "Joseph J. Boutros",
      "Philippe Ciblat",
      "Lo\u00efc Brunel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2102.06810",
    "title": "Understanding self-supervised Learning Dynamics without Contrastive  Pairs",
    "abstract": " Comments: Fix minor typo in Appendix ",
    "url": "https://arxiv.org/abs/2102.06810",
    "authors": [
      "Yuandong Tian",
      "Xinlei Chen",
      "Surya Ganguli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2105.07402",
    "title": "Is aspect ratio of cells important in deep learning? A robust comparison  of deep learning methods for multi-scale cytopathology cell image  classification: from convolutional neural networks to visual transformers",
    "abstract": " Title: Is aspect ratio of cells important in deep learning? A robust comparison  of deep learning methods for multi-scale cytopathology cell image  classification: from convolutional neural networks to visual transformers ",
    "url": "https://arxiv.org/abs/2105.07402",
    "authors": [
      "Wanli Liu",
      "Chen Li",
      "Md Mamunur Rahamana",
      "Hongzan Sun",
      "Weiming Hu",
      "Haoyuan Chen",
      "Changhao Sun",
      "Yudong Yao",
      "Marcin Grzegorzek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.03485",
    "title": "Representation mitosis in wide neural networks",
    "abstract": " Title: Representation mitosis in wide neural networks ",
    "url": "https://arxiv.org/abs/2106.03485",
    "authors": [
      "Diego Doimo",
      "Aldo Glielmo",
      "Sebastian Goldt",
      "Alessandro Laio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.11532",
    "title": "Many nodal domains in random regular graphs",
    "abstract": " Comments: 18 pages ",
    "url": "https://arxiv.org/abs/2109.11532",
    "authors": [
      "Shirshendu Ganguly",
      "Theo McKenzie",
      "Sidhanth Mohanty",
      "Nikhil Srivastava"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Discrete Mathematics (cs.DM)",
      "Mathematical Physics (math-ph)",
      "Combinatorics (math.CO)",
      "Spectral Theory (math.SP)"
    ]
  }
]