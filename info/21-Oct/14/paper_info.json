[
  {
    "id": "arXiv:2110.06267",
    "title": "Twice regularized MDPs and the equivalence between robustness and  regularization",
    "abstract": "Robust Markov decision processes (MDPs) aim to handle changing or partially known system dynamics. To solve them, one typically resorts to robust optimization methods. However, this significantly increases computational complexity and limits scalability in both learning and planning. On the other hand, regularized MDPs show more stability in policy learning without impairing time complexity. Yet, they generally do not encompass uncertainty in the model dynamics. In this work, we aim to learn robust MDPs using regularization. We first show that regularized MDPs are a particular instance of robust MDPs with uncertain reward. We thus establish that policy iteration on reward-robust MDPs can have the same time complexity as on regularized MDPs. We further extend this relationship to MDPs with uncertain transitions: this leads to a regularization term with an additional dependence on the value function. We finally generalize regularized MDPs to twice regularized MDPs (R${}^2$ MDPs), i.e., MDPs with $\\textit{both}$ value and policy regularization. The corresponding Bellman operators enable developing policy iteration schemes with convergence and robustness guarantees. It also reduces planning and learning in robust MDPs to regularized MDPs. ",
    "url": "https://arxiv.org/abs/2110.06267",
    "authors": [
      "Esther Derman",
      "Matthieu Geist",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2110.06534",
    "title": "Simple Attention Module based Speaker Verification with Iterative noisy  label detection",
    "abstract": "Recently, the attention mechanism such as squeeze-and-excitation module (SE) and convolutional block attention module (CBAM) has achieved great success in deep learning-based speaker verification system. This paper introduces an alternative effective yet simple one, i.e., simple attention module (SimAM), for speaker verification. The SimAM module is a plug-and-play module without extra modal parameters. In addition, we propose a noisy label detection method to iteratively filter out the data samples with a noisy label from the training data, considering that a large-scale dataset labeled with human annotation or other automated processes may contain noisy labels. Data with the noisy label may over parameterize a deep neural network (DNN) and result in a performance drop due to the memorization effect of the DNN. Experiments are conducted on VoxCeleb dataset. The speaker verification model with SimAM achieves the 0.675% equal error rate (EER) on VoxCeleb1 original test trials. Our proposed iterative noisy label detection method further reduces the EER to 0.643%. ",
    "url": "https://arxiv.org/abs/2110.06534",
    "authors": [
      "Xiaoyi Qin",
      "Na Li",
      "Chao Weng",
      "Dan Su",
      "Ming Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2110.06634",
    "title": "End-to-end translation of human neural activity to speech with a  dual-dual generative adversarial network",
    "abstract": "In a recent study of auditory evoked potential (AEP) based brain-computer interface (BCI), it was shown that, with an encoder-decoder framework, it is possible to translate human neural activity to speech (T-CAS). However, current encoder-decoder-based methods achieve T-CAS often with a two-step method where the information is passed between the encoder and decoder with a shared dimension reduction vector, which may result in a loss of information. A potential approach to this problem is to design an end-to-end method by using a dual generative adversarial network (DualGAN) without dimension reduction of passing information, but it cannot realize one-to-one signal-to-signal translation (see Fig.1 (a) and (b)). In this paper, we propose an end-to-end model to translate human neural activity to speech directly, create a new electroencephalogram (EEG) datasets for participants with good attention by design a device to detect participants' attention, and introduce a dual-dual generative adversarial network (Dual-DualGAN) (see Fig. 1 (c) and (d)) to address an end-to-end translation of human neural activity to speech (ET-CAS) problem by group labelling EEG signals and speech signals, inserting a transition domain to realize cross-domain mapping. In the transition domain, the transition signals are cascaded by the corresponding EEG and speech signals in a certain proportion, which can build bridges for EEG and speech signals without corresponding features, and realize one-to-one cross-domain EEG-to-speech translation. The proposed method can translate word-length and sentence-length sequences of neural activity to speech. Experimental evaluation has been conducted to show that the proposed method significantly outperforms state-of-the-art methods on both words and sentences of auditory stimulus. ",
    "url": "https://arxiv.org/abs/2110.06634",
    "authors": [
      "Yina Guo",
      "Xiaofei Zhang",
      "Zhenying Gong",
      "Anhong Wang",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2110.06650",
    "title": "Multistage linguistic conditioning of convolutional layers for speech  emotion recognition",
    "abstract": "In this contribution, we investigate the effectiveness of deep fusion of text and audio features for categorical and dimensional speech emotion recognition (SER). We propose a novel, multistage fusion method where the two information streams are integrated in several layers of a deep neural network (DNN), and contrast it with a single-stage one where the streams are merged in a single point. Both methods depend on extracting summary linguistic embeddings from a pre-trained BERT model, and conditioning one or more intermediate representations of a convolutional model operating on log-Mel spectrograms. Experiments on the widely used IEMOCAP and MSP-Podcast databases demonstrate that the two fusion methods clearly outperform a shallow (late) fusion baseline and their unimodal constituents, both in terms of quantitative performance and qualitative behaviour. Our accompanying analysis further reveals a hitherto unexplored role of the underlying dialogue acts on unimodal and bimodal SER, with different models showing a biased behaviour across different acts. Overall, our multistage fusion shows better quantitative performance, surpassing all alternatives on most of our evaluations. This illustrates the potential of multistage fusion in better assimilating text and audio information. ",
    "url": "https://arxiv.org/abs/2110.06650",
    "authors": [
      "Andreas Triantafyllopoulos",
      "Uwe Reichel",
      "Shuo Liu",
      "Stephan Huber",
      "Florian Eyben",
      "Bj\u00f6rn W. Schuller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.06751",
    "title": "Improving the sample-efficiency of neural architecture search with  reinforcement learning",
    "abstract": "Designing complex architectures has been an essential cogwheel in the revolution deep learning has brought about in the past decade. When solving difficult problems in a datadriven manner, a well-tried approach is to take an architecture discovered by renowned deep learning scientists as a basis (e.g. Inception) and try to apply it to a specific problem. This might be sufficient, but as of now, achieving very high accuracy on a complex or yet unsolved task requires the knowledge of highly-trained deep learning experts. In this work, we would like to contribute to the area of Automated Machine Learning (AutoML), specifically Neural Architecture Search (NAS), which intends to make deep learning methods available for a wider range of society by designing neural topologies automatically. Although several different approaches exist (e.g. gradient-based or evolutionary algorithms), our focus is on one of the most promising research directions, reinforcement learning. In this scenario, a recurrent neural network (controller) is trained to create problem-specific neural network architectures (child). The validation accuracies of the child networks serve as a reward signal for training the controller with reinforcement learning. The basis of our proposed work is Efficient Neural Architecture Search (ENAS), where parameter sharing is applied among the child networks. ENAS, like many other RL-based algorithms, emphasize the learning of child networks as increasing their convergence result in a denser reward signal for the controller, therefore significantly reducing training times. The controller was originally trained with REINFORCE. In our research, we propose to modify this to a more modern and complex algorithm, PPO, which has demonstrated to be faster and more stable in other environments. Then, we briefly discuss and evaluate our results. ",
    "url": "https://arxiv.org/abs/2110.06751",
    "authors": [
      "Attila Nagy",
      "\u00c1bel Boros"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.06754",
    "title": "The springback penalty for robust signal recovery",
    "abstract": "We propose a new penalty, named as the springback penalty, for constructing models to recover an unknown signal from incomplete and inaccurate measurements. Mathematically, the springback penalty is a weakly convex function, and it bears various theoretical and computational advantages of both the benchmark convex $\\ell_1$ penalty and many of its non-convex surrogates that have been well studied in the literature. For the recovery model using the springback penalty, we establish the exact and stable recovery theory for both sparse and nearly sparse signals, respectively, and derive an easily implementable difference-of-convex algorithm. In particular, we show its theoretical superiority to some existing models with a sharper recovery bound for some scenarios where the level of measurement noise is large or the amount of measurements is limited, and demonstrate its numerical robustness regardless of varying coherence of the sensing matrix. Because of its theoretical guarantee of recovery with severe measurements, computational tractability, and numerical robustness for ill-conditioned sensing matrices, the springback penalty is particularly favorable for the scenario where the incomplete and inaccurate measurements are collected by coherence-hidden or -static sensing hardware. ",
    "url": "https://arxiv.org/abs/2110.06754",
    "authors": [
      "Congpei An",
      "Hao-Ning Wu",
      "Xiaoming Yuan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:1807.11296",
    "title": "Relative kinematics of an anchorless network",
    "abstract": "Estimating the location of N coordinates in a P dimensional Euclidean space from pairwise distances (or proximity measurements), is a principal challenge in a wide variety of fields. Conventionally, when localizing a static network of immobile nodes, non-linear dimensional reduction techniques are applied on the measured Euclidean distance matrix (EDM) to obtain the relative coordinates upto a rotation and translation. In this article, we focus on an anchorless network of mobile nodes, where the distance measurements between the mobile nodes are time-varying in nature. Furthermore, in an anchorless network the absolute knowledge of any node positions, motion or reference frame is absent. We derive a novel data model which relates the time-varying EDMs to the time-varying relative positions of an anchorless network. Using this data model, we estimate the relative position, relative velocity and higher order derivatives, which are collectively termed as the relative kinematics of the anchorless network. The derived data model is inherently ill-posed, however can be solved using certain relative immobility constraints. We propose elegant closed form solutions to recursively estimate the relative kinematics of the network. For the sake of completeness, estimators are also proposed to find the absolute kinematics of the nodes, given known reference anchors. Cramer-Rao bounds are derived for the new data model and simulations are performed to analyze the performance of the proposed solutions. ",
    "url": "https://arxiv.org/abs/1807.11296",
    "authors": [
      "Raj Thilak Rajan",
      "Geert Leus",
      "Alle-Jan van der Veen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2110.06390",
    "title": "Learning ground states of quantum Hamiltonians with graph networks",
    "abstract": "Solving for the lowest energy eigenstate of the many-body Schrodinger equation is a cornerstone problem that hinders understanding of a variety of quantum phenomena. The difficulty arises from the exponential nature of the Hilbert space which casts the governing equations as an eigenvalue problem of exponentially large, structured matrices. Variational methods approach this problem by searching for the best approximation within a lower-dimensional variational manifold. In this work we use graph neural networks to define a structured variational manifold and optimize its parameters to find high quality approximations of the lowest energy solutions on a diverse set of Heisenberg Hamiltonians. Using graph networks we learn distributed representations that by construction respect underlying physical symmetries of the problem and generalize to problems of larger size. Our approach achieves state-of-the-art results on a set of quantum many-body benchmark problems and works well on problems whose solutions are not positive-definite. The discussed techniques hold promise of being a useful tool for studying quantum many-body systems and providing insights into optimization and implicit modeling of exponentially-sized objects. ",
    "url": "https://arxiv.org/abs/2110.06390",
    "authors": [
      "Dmitrii Kochkov",
      "Tobias Pfaff",
      "Alvaro Sanchez-Gonzalez",
      "Peter Battaglia",
      "Bryan K. Clark"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2110.06765",
    "title": "libdlr: Efficient imaginary time calculations using the discrete Lehmann  representation",
    "abstract": "We introduce libdlr, a library implementing the recently introduced discrete Lehmann representation (DLR) of imaginary time Green's functions. The DLR basis consists of a collection of exponentials chosen by the interpolative decomposition to ensure stable and efficient recovery of Green's functions from imaginary time or Matsbuara frequency samples. The library provides subroutines to build the DLR basis and grids, and to carry out various standard operations. The simplicity of the DLR makes it straightforward to incorporate into existing codes as a replacement for less efficient representations of imaginary time Green's functions, and libdlr is intended to facilitate this process. libdlr is written in Fortran, and contains a Python module pydlr. ",
    "url": "https://arxiv.org/abs/2110.06765",
    "authors": [
      "Jason Kaye",
      "Hugo U. R. Strand"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Mathematical Software (cs.MS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2110.06866",
    "title": "Bayesian logistic regression for online recalibration and revision of  risk prediction models with performance guarantees",
    "abstract": "After deploying a clinical prediction model, subsequently collected data can be used to fine-tune its predictions and adapt to temporal shifts. Because model updating carries risks of over-updating/fitting, we study online methods with performance guarantees. We introduce two procedures for continual recalibration or revision of an underlying prediction model: Bayesian logistic regression (BLR) and a Markov variant that explicitly models distribution shifts (MarBLR). We perform empirical evaluation via simulations and a real-world study predicting COPD risk. We derive \"Type I and II\" regret bounds, which guarantee the procedures are non-inferior to a static model and competitive with an oracle logistic reviser in terms of the average loss. Both procedures consistently outperformed the static model and other online logistic revision methods. In simulations, the average estimated calibration index (aECI) of the original model was 0.828 (95%CI 0.818-0.938). Online recalibration using BLR and MarBLR improved the aECI, attaining 0.265 (95%CI 0.230-0.300) and 0.241 (95%CI 0.216-0.266), respectively. When performing more extensive logistic model revisions, BLR and MarBLR increased the average AUC (aAUC) from 0.767 (95%CI 0.765-0.769) to 0.800 (95%CI 0.798-0.802) and 0.799 (95%CI 0.797-0.801), respectively, in stationary settings and protected against substantial model decay. In the COPD study, BLR and MarBLR dynamically combined the original model with a continually-refitted gradient boosted tree to achieve aAUCs of 0.924 (95%CI 0.913-0.935) and 0.925 (95%CI 0.914-0.935), compared to the static model's aAUC of 0.904 (95%CI 0.892-0.916). Despite its simplicity, BLR is highly competitive with MarBLR. MarBLR outperforms BLR when its prior better reflects the data. BLR and MarBLR can improve the transportability of clinical prediction models and maintain their performance over time. ",
    "url": "https://arxiv.org/abs/2110.06866",
    "authors": [
      "Jean Feng",
      "Alexej Gossmann",
      "Berkman Sahiner",
      "Romain Pirracchio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2005.05547",
    "title": "List homomorphism problems for signed graphs",
    "abstract": " Comments: various changes + rewritten section on path- and cycle-separable graphs based on a new conference submission (split possible in future) ",
    "url": "https://arxiv.org/abs/2005.05547",
    "authors": [
      "Jan Bok",
      "Richard Brewster",
      "Tom\u00e1s Feder",
      "Pavol Hell",
      "Nikola Jedli\u010dkov\u00e1"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2103.03716",
    "title": "Golem: An algorithm for robust experiment and process optimization",
    "abstract": " Comments: 37 pages, 25 figures; additional experiments, expanded discussions and references ",
    "url": "https://arxiv.org/abs/2103.03716",
    "authors": [
      "Matteo Aldeghi",
      "Florian H\u00e4se",
      "Riley J. Hickman",
      "Isaac Tamblyn",
      "Al\u00e1n Aspuru-Guzik"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ]
  },
  {
    "id": "arXiv:2104.05608",
    "title": "Equivariant geometric learning for digital rock physics: estimating  formation factor and effective permeability tensors from Morse graph",
    "abstract": " Title: Equivariant geometric learning for digital rock physics: estimating  formation factor and effective permeability tensors from Morse graph ",
    "url": "https://arxiv.org/abs/2104.05608",
    "authors": [
      "Chen Cai",
      "Nikolaos Vlassis",
      "Lucas Magee",
      "Ran Ma",
      "Zeyu Xiong",
      "Bahador Bahmani",
      "Teng-Fong Wong",
      "Yusu Wang",
      "WaiChing Sun"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2104.13754",
    "title": "Can crowdsourcing rescue the social marketplace of ideas?",
    "abstract": " Comments: Under Review - revision 2 ",
    "url": "https://arxiv.org/abs/2104.13754",
    "authors": [
      "Taha Yasseri",
      "Filippo Menczer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2105.09987",
    "title": "Temporal convolutional networks predict dynamic oxygen uptake response  from wearable sensors across exercise intensities",
    "abstract": " Title: Temporal convolutional networks predict dynamic oxygen uptake response  from wearable sensors across exercise intensities ",
    "url": "https://arxiv.org/abs/2105.09987",
    "authors": [
      "Robert Amelard",
      "Eric T Hedge",
      "Richard L Hughson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2106.06770",
    "title": "What can linearized neural networks actually say about generalization?",
    "abstract": " Comments: 18 pages, 16 figures ",
    "url": "https://arxiv.org/abs/2106.06770",
    "authors": [
      "Guillermo Ortiz-Jim\u00e9nez",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pascal Frossard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2107.00594",
    "title": "Pretext Tasks selection for multitask self-supervised speech  representation learning",
    "abstract": " Title: Pretext Tasks selection for multitask self-supervised speech  representation learning ",
    "url": "https://arxiv.org/abs/2107.00594",
    "authors": [
      "Salah Zaiem",
      "Titouan Parcollet",
      "Slim Essid"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2108.02327",
    "title": "PI3NN: Out-of-distribution-aware prediction intervals from three neural  networks",
    "abstract": " Title: PI3NN: Out-of-distribution-aware prediction intervals from three neural  networks ",
    "url": "https://arxiv.org/abs/2108.02327",
    "authors": [
      "Siyan Liu",
      "Pei Zhang",
      "Dan Lu",
      "Guannan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2108.13910",
    "title": "A manifold learning perspective on representation learning: Learning  decoder and representations without an encoder",
    "abstract": " Title: A manifold learning perspective on representation learning: Learning  decoder and representations without an encoder ",
    "url": "https://arxiv.org/abs/2108.13910",
    "authors": [
      "Viktoria Schuster",
      "Anders Krogh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.00428",
    "title": "Combining reconstruction and edge detection in computed tomography",
    "abstract": " Title: Combining reconstruction and edge detection in computed tomography ",
    "url": "https://arxiv.org/abs/2109.00428",
    "authors": [
      "J\u00fcrgen Frikel",
      "Simon G\u00f6ppel",
      "Markus Haltmeier"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  }
]