[
  {
    "id": "arXiv:2512.05132",
    "title": "Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training",
    "abstract": "           Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.         ",
    "url": "https://arxiv.org/abs/2512.05132",
    "authors": [
      "Wenshuo Wang",
      "Fan Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05176",
    "title": "Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge",
    "abstract": "           Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as \"general purpose\" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of \"culturally-informed\" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.         ",
    "url": "https://arxiv.org/abs/2512.05176",
    "authors": [
      "Brittany Johnson",
      "Erin Reddick",
      "Angela D.R. Smith"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2512.05201",
    "title": "MuMeNet: A Network Simulator for Musical Metaverse Communications",
    "abstract": "           The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using \"live audience interaction in a virtual concert\" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.         ",
    "url": "https://arxiv.org/abs/2512.05201",
    "authors": [
      "Ali Al Housseini",
      "Jaime Llorca",
      "Luca Turchet",
      "Tiziano Leidi",
      "Cristina Rottondi",
      "Omran Ayoub"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2512.05239",
    "title": "A Survey of Bugs in AI-Generated Code",
    "abstract": "           Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.         ",
    "url": "https://arxiv.org/abs/2512.05239",
    "authors": [
      "Ruofan Gao",
      "Amjed Tahir",
      "Peng Liang",
      "Teo Susnjak",
      "Foutse Khomh"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05242",
    "title": "Learning to Code with Context: A Study-Based Approach",
    "abstract": "           The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.         ",
    "url": "https://arxiv.org/abs/2512.05242",
    "authors": [
      "Uwe M. Borghoff",
      "Mark Minas",
      "Jannis Schopp"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05246",
    "title": "NeuromorphicRx: From Neural to Spiking Receiver",
    "abstract": "           In this work, we propose a novel energy-efficient spiking neural network (SNN)-based receiver for 5G-NR OFDM system, called neuromorphic receiver (NeuromorphicRx), replacing the channel estimation, equalization and symbol demapping blocks. We leverage domain knowledge to design the input with spiking encoding and propose a deep convolutional SNN with spike-element-wise residual connections. We integrate an SNN with artificial neural network (ANN) hybrid architecture to obtain soft outputs and employ surrogate gradient descent for training. We focus on generalization across diverse scenarios and robustness through quantized aware training. We focus on interpretability of NeuromorphicRx for 5G-NR signals and perform detailed ablation study for 5G-NR signals. Our extensive numerical simulations show that NeuromorphicRx is capable of achieving significant block error rate performance gain compared to 5G-NR receivers and similar performance compared to its ANN-based counterparts with 7.6x less energy consumption.         ",
    "url": "https://arxiv.org/abs/2512.05246",
    "authors": [
      "Ankit Gupta",
      "Onur Dizdar",
      "Yun Chen",
      "Fehmi Emre Kadan",
      "Ata Sattarzadeh",
      "Stephen Wang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2512.05249",
    "title": "Low-Complexity OFDM Deep Neural Receivers",
    "abstract": "           Deep neural receivers (NeuralRxs) for Orthogonal Frequency Division Multiplexing (OFDM) signals are proposed for enhanced decoding performance compared to their signal-processing based counterparts. However, the existing architectures ignore the required number of epochs for training convergence and floating-point operations (FLOPs), which increase significantly with improving performance. To tackle these challenges, we propose a new residual network (ResNet) block design for OFDM NeuralRx. Specifically, we leverage small kernel sizes and dilation rates to lower the number of FLOPs (NFLOPs) and uniform channel sizes to reduce the memory access cost (MAC). The ResNet block is designed with novel channel split and shuffle blocks, element-wise additions are removed, with Gaussian error linear unit (GELU) activations. Extensive simulations show that our proposed NeuralRx reduces NFLOPs and improves training convergence while improving the decoding accuracy.         ",
    "url": "https://arxiv.org/abs/2512.05249",
    "authors": [
      "Ankit Gupta",
      "Onur Dizdar",
      "Yun Chen",
      "Fehmi Emre Kadan",
      "Ata Sattarzadeh",
      "Stephen Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2512.05271",
    "title": "Robust forecast aggregation via additional queries",
    "abstract": "           We study the problem of robust forecast aggregation: combining expert forecasts with provable accuracy guarantees compared to the best possible aggregation of the underlying information. Prior work shows strong impossibility results, e.g. that even under natural assumptions, no aggregation of the experts' individual forecasts can outperform simply following a random expert (Neyman and Roughgarden, 2022). In this paper, we introduce a more general framework that allows the principal to elicit richer information from experts through structured queries. Our framework ensures that experts will truthfully report their underlying beliefs, and also enables us to define notions of complexity over the difficulty of asking these queries. Under a general model of independent but overlapping expert signals, we show that optimal aggregation is achievable in the worst case with each complexity measure bounded above by the number of agents $n$. We further establish tight tradeoffs between accuracy and query complexity: aggregation error decreases linearly with the number of queries, and vanishes when the \"order of reasoning\" and number of agents relevant to a query is $\\omega(\\sqrt{n})$. These results demonstrate that modest extensions to the space of expert queries dramatically strengthen the power of robust forecast aggregation. We therefore expect that our new query framework will open up a fruitful line of research in this area.         ",
    "url": "https://arxiv.org/abs/2512.05271",
    "authors": [
      "Rafael Frongillo",
      "Mary Monroe",
      "Eric Neyman",
      "Bo Waggoner"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05287",
    "title": "DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models",
    "abstract": "           MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.         ",
    "url": "https://arxiv.org/abs/2512.05287",
    "authors": [
      "Ziqi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05288",
    "title": "Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification",
    "abstract": "           Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.         ",
    "url": "https://arxiv.org/abs/2512.05288",
    "authors": [
      "Feijiang Han"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05297",
    "title": "CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators",
    "abstract": "           Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.         ",
    "url": "https://arxiv.org/abs/2512.05297",
    "authors": [
      "Xianglong Hou",
      "Xinquan Huang",
      "Paris Perdikaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2512.05306",
    "title": "Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)",
    "abstract": "           Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.         ",
    "url": "https://arxiv.org/abs/2512.05306",
    "authors": [
      "Y. Sungtaek Ju"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2512.05309",
    "title": "Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions",
    "abstract": "           Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.         ",
    "url": "https://arxiv.org/abs/2512.05309",
    "authors": [
      "Adam Alami",
      "Nathan Cassee",
      "Thiago Rocha Silva",
      "Elda Paja",
      "Neil A. Ernst"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2512.05321",
    "title": "A Practical Honeypot-Based Threat Intelligence Framework for Cyber Defence in the Cloud",
    "abstract": "           In cloud environments, conventional firewalls rely on predefined rules and manual configurations, limiting their ability to respond effectively to evolving or zero-day threats. As organizations increasingly adopt platforms such as Microsoft Azure, this static defense model exposes cloud assets to zero-day exploits, botnets, and advanced persistent threats. In this paper, we introduce an automated defense framework that leverages medium- to high-interaction honeypot telemetry to dynamically update firewall rules in real time. The framework integrates deception sensors (Cowrie), Azure-native automation tools (Monitor, Sentinel, Logic Apps), and MITRE ATT&CK-aligned detection within a closed-loop feedback mechanism. We developed a testbed to automatically observe adversary tactics, classify them using the MITRE ATT&CK framework, and mitigate network-level threats automatically with minimal human intervention. To assess the framework's effectiveness, we defined and applied a set of attack- and defense-oriented security metrics. Building on existing adaptive defense strategies, our solution extends automated capabilities into cloud-native environments. The experimental results show an average Mean Time to Block of 0.86 seconds - significantly faster than benchmark systems - while accurately classifying over 12,000 SSH attempts across multiple MITRE ATT&CK tactics. These findings demonstrate that integrating deception telemetry with Azure-native automation reduces attacker dwell time, enhances SOC visibility, and provides a scalable, actionable defense model for modern cloud infrastructures.         ",
    "url": "https://arxiv.org/abs/2512.05321",
    "authors": [
      "Darren Malvern Chin",
      "Bilal Isfaq",
      "Simon Yusuf Enoch"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2512.05323",
    "title": "Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition",
    "abstract": "           Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.         ",
    "url": "https://arxiv.org/abs/2512.05323",
    "authors": [
      "Adam Lizerbram",
      "Shane Stevenson",
      "Iman Khadir",
      "Matthew Tu",
      "Samuel S. P. Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)",
      "Other Statistics (stat.OT)"
    ]
  },
  {
    "id": "arXiv:2512.05331",
    "title": "Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats",
    "abstract": "           The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.         ",
    "url": "https://arxiv.org/abs/2512.05331",
    "authors": [
      "Sadat Shahriar",
      "Navid Ayoobi",
      "Arjun Mukherjee",
      "Mostafa Musharrat",
      "Sai Vishnu Vamsi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05335",
    "title": "State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning",
    "abstract": "           We study visual domain transfer for end-to-end imitation learning in a realistic and challenging setting where target-domain data are strictly off-policy, expert-free, and scarce. We first provide a theoretical analysis showing that the target-domain imitation loss can be upper bounded by the source-domain loss plus a state-conditional latent KL divergence between source and target observation models. Guided by this result, we propose State- Conditional Adversarial Learning, an off-policy adversarial framework that aligns latent distributions conditioned on system state using a discriminator-based estimator of the conditional KL term. Experiments on visually diverse autonomous driving environments built on the BARC-CARLA simulator demonstrate that SCAL achieves robust transfer and strong sample efficiency.         ",
    "url": "https://arxiv.org/abs/2512.05335",
    "authors": [
      "Yuxiang Liu",
      "Shengfan Cao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2512.05339",
    "title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "abstract": "           Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.         ",
    "url": "https://arxiv.org/abs/2512.05339",
    "authors": [
      "Mahesh Kumar Nandwana",
      "Youngwan Lim",
      "Joseph Liu",
      "Alex Yang",
      "Varun Notibala",
      "Nishchaie Khanna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05341",
    "title": "When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation",
    "abstract": "           Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.         ",
    "url": "https://arxiv.org/abs/2512.05341",
    "authors": [
      "Yiwen Liang",
      "Qiufeng Li",
      "Shikai Wang",
      "Weidong Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2512.05342",
    "title": "First Demonstration of Second-order Training of Deep Neural Networks with In-memory Analog Matrix Computing",
    "abstract": "           Second-order optimization methods, which leverage curvature information, offer faster and more stable convergence than first-order methods such as stochastic gradient descent (SGD) and Adam. However, their practical adoption is hindered by the prohibitively high cost of inverting the second-order information matrix, particularly in large-scale neural network training. Here, we present the first demonstration of a second-order optimizer powered by in-memory analog matrix computing (AMC) using resistive random-access memory (RRAM), which performs matrix inversion (INV) in a single step. We validate the optimizer by training a two-layer convolutional neural network (CNN) for handwritten letter classification, achieving 26% and 61% fewer training epochs than SGD with momentum and Adam, respectively. On a larger task using the same second-order method, our system delivers a 5.88x improvement in throughput and a 6.9x gain in energy efficiency compared to state-of-the-art digital processors. These results demonstrate the feasibility and effectiveness of AMC circuits for second-order neural network training, opening a new path toward energy-efficient AI acceleration.         ",
    "url": "https://arxiv.org/abs/2512.05342",
    "authors": [
      "Saitao Zhang",
      "Yubiao Luo",
      "Shiqing Wang",
      "Pushen Zuo",
      "Yongxiang Li",
      "Lunshuai Pan",
      "Zheng Miao",
      "Zhong Sun"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2512.05364",
    "title": "Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change",
    "abstract": "           This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.         ",
    "url": "https://arxiv.org/abs/2512.05364",
    "authors": [
      "Ananth Hariharan",
      "David Mortensen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2512.05367",
    "title": "Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation",
    "abstract": "           We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.         ",
    "url": "https://arxiv.org/abs/2512.05367",
    "authors": [
      "Mariia Karabin",
      "Isaac Armstrong",
      "Leo Beck",
      "Paulina Apanel",
      "Markus Eisenbach",
      "David B. Mitzi",
      "Hanna Terletska",
      "Hendrik Heinz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05373",
    "title": "Text Rationalization for Robust Causal Effect Estimation",
    "abstract": "           Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.         ",
    "url": "https://arxiv.org/abs/2512.05373",
    "authors": [
      "Lijinghua Zhang",
      "Hengrui Cai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2512.05400",
    "title": "Hybrid modeling approach for better identification of building thermal network model and improved prediction",
    "abstract": "           The gray-box modeling approach, which uses a semi-physical thermal network model, has been widely used in building prediction applications, such as model predictive control (MPC). However, unmeasured disturbances, such as occupants, lighting, and in/exfiltration loads, make it challenging to apply this approach to practical buildings. In this study, we propose a hybrid modeling approach that integrates the gray-box model with a model for unmeasured disturbance. After reviewing several system identification approaches, we systematically designed the unmeasured disturbance model with a model selection process based on statistical tests to make it robust. We generated data based on the building model calibrated by real operational data and then trained the hybrid model for two different weather conditions. The Hybrid model approach demonstrates the reduction of RMSE approximately 0.2-0.9C and 0.3-2C on 1-day ahead temperature prediction compared to the Conventional approach for mild (Berkeley, CA) and cold (Chicago, IL) climates, respectively. In addition, this approach was applied for experimental data obtained from the laboratory building to be used for the MPC application, showing superior prediction performances.         ",
    "url": "https://arxiv.org/abs/2512.05400",
    "authors": [
      "Sang woo Ham",
      "Donghun Kim"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2512.05402",
    "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction",
    "abstract": "           Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: this https URL.         ",
    "url": "https://arxiv.org/abs/2512.05402",
    "authors": [
      "Sithumi Wickramasinghe",
      "Bikramjit Das",
      "Dorien Herremans"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2512.05403",
    "title": "RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design",
    "abstract": "           Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.         ",
    "url": "https://arxiv.org/abs/2512.05403",
    "authors": [
      "Gyusam Chang",
      "Jeongyoon Yoon",
      "Shin han yi",
      "JaeHyeok Lee",
      "Sujin Jang",
      "Sangpil Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05412",
    "title": "YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications",
    "abstract": "           Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.         ",
    "url": "https://arxiv.org/abs/2512.05412",
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05414",
    "title": "LMSpell: Neural Spell Checking for Low-Resource Languages",
    "abstract": "           Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.         ",
    "url": "https://arxiv.org/abs/2512.05414",
    "authors": [
      "Akesh Gunathilakea",
      "Nadil Karunarathnea",
      "Tharusha Bandaranayakea",
      "Nisansa de Silvaa",
      "Surangika Ranathunga"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2512.05415",
    "title": "Moving object detection from multi-depth images with an attention-enhanced CNN",
    "abstract": "           One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.         ",
    "url": "https://arxiv.org/abs/2512.05415",
    "authors": [
      "Masato Shibukawa",
      "Fumi Yoshida",
      "Toshifumi Yanagisawa",
      "Takashi Ito",
      "Hirohisa Kurosaki",
      "Makoto Yoshikawa",
      "Kohki Kamiya",
      "Ji-an Jiang",
      "Wesley Fraser",
      "JJ Kavelaars",
      "Susan Benecchi",
      "Anne Verbiscer",
      "Akira Hatakeyama",
      "Hosei O",
      "Naoya Ozaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05416",
    "title": "Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets",
    "abstract": "           In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient--feature--value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve \"who measured what and by how much\". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train--test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity--specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient--feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.         ",
    "url": "https://arxiv.org/abs/2512.05416",
    "authors": [
      "Bozhi Dan",
      "Di Wu",
      "Ji Xu",
      "Xiang Liu",
      "Yiziting Zhu",
      "Xin Shu",
      "Yujie Li",
      "Bin Yi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05417",
    "title": "PETGraphDB: A Property Evolution Temporal Graph Data Management System",
    "abstract": "           Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency.         ",
    "url": "https://arxiv.org/abs/2512.05417",
    "authors": [
      "Jinghe Song",
      "Zongyu Zuo",
      "Xuelian Lin",
      "Yang Wang",
      "Shuai Ma"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2512.05433",
    "title": "From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation",
    "abstract": "           Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues.         ",
    "url": "https://arxiv.org/abs/2512.05433",
    "authors": [
      "Kim Marriott",
      "Matthew Butler",
      "Leona Holloway",
      "Bill Jolley",
      "Bongshin Lee",
      "Bruce Maguire",
      "Danielle Albers Szafir"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2512.05459",
    "title": "PrivCode: When Code Generation Meets Differential Privacy",
    "abstract": "           Large language models (LLMs) have presented outstanding performance in code generation and completion. However, fine-tuning these models on private datasets can raise privacy and proprietary concerns, such as the leakage of sensitive personal information. Differentially private (DP) code generation provides theoretical guarantees for protecting sensitive code by generating synthetic datasets that preserve statistical properties while reducing privacy leakage concerns. However, DP code generation faces significant challenges due to the strict syntactic dependencies and the privacy-utility trade-off. We propose PrivCode, the first DP synthesizer specifically designed for code datasets. It incorporates a two-stage framework to improve both privacy and utility. In the first stage, termed \"privacy-sanitizing\", PrivCode generates DP-compliant synthetic code by training models using DP-SGD while introducing syntactic information to preserve code structure. The second stage, termed \"utility-boosting\", fine-tunes a larger pre-trained LLM on the synthetic privacy-free code to mitigate the utility loss caused by DP, enhancing the utility of the generated code. Extensive experiments on four LLMs show that PrivCode generates higher-utility code across various testing tasks under four benchmarks. The experiments also confirm its ability to protect sensitive data under varying privacy budgets. We provide the replication package at the anonymous link.         ",
    "url": "https://arxiv.org/abs/2512.05459",
    "authors": [
      "Zheng Liu",
      "Chen Gong",
      "Terry Yue Zhuo",
      "Kecen Li",
      "Weichen Yu",
      "Matt Fredrikson",
      "Tianhao Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2512.05460",
    "title": "ProbeWalk: Fast Estimation of Biharmonic Distance on Graphs via Probe-Driven Random Walks",
    "abstract": "           The biharmonic distance is a fundamental metric on graphs that measures the dissimilarity between two nodes, capturing both local and global structures. It has found applications across various fields, including network centrality, graph clustering, and machine learning. These applications typically require efficient evaluation of pairwise biharmonic distances. However, existing algorithms remain computationally expensive. The state-of-the-art method attains an absolute-error guarantee epsilon_abs with time complexity O(L^5 / epsilon_abs^2), where L denotes the truncation length. In this work, we improve the complexity to O(L^3 / epsilon^2) under a relative-error guarantee epsilon via probe-driven random walks. We provide a relative-error guarantee rather than an absolute-error guarantee because biharmonic distances vary by orders of magnitude across node pairs. Since L is often very large in real-world networks (for example, L >= 10^3), reducing the L-dependence from the fifth to the third power yields substantial gains. Extensive experiments on real-world networks show that our method delivers 10x-1000x per-query speedups at matched relative error over strong baselines and scales to graphs with tens of millions of nodes.         ",
    "url": "https://arxiv.org/abs/2512.05460",
    "authors": [
      "Dehong Zheng",
      "Zhongzhi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2512.05461",
    "title": "Knowing Your Uncertainty -- On the application of LLM in social sciences",
    "abstract": "           Large language models (LLMs) are rapidly being integrated into computational social science research, yet their blackboxed training and designed stochastic elements in inference pose unique challenges for scientific inquiry. This article argues that applying LLMs to social scientific tasks requires explicit assessment of uncertainty-an expectation long established in both quantitative methodology in the social sciences and machine learning. We introduce a unified framework for evaluating LLM uncertainty along two dimensions: the task type (T), which distinguishes between classification, short-form, and long-form generation, and the validation type (V), which captures the availability of reference data or evaluative criteria. Drawing from both computer science and social science literature, we map existing uncertainty quantification (UQ) methods to this T-V typology and offer practical recommendations for researchers. Our framework provides both a methodological safeguard and a practical guide for integrating LLMs into rigorous social science research.         ",
    "url": "https://arxiv.org/abs/2512.05461",
    "authors": [
      "Bolun Zhang",
      "Linzhuo Li",
      "Yunqi Chen",
      "Qinlin Zhao",
      "Zihan Zhu",
      "Xiaoyuan Yi",
      "Xing Xie"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2512.05472",
    "title": "Unleashing Temporal Capacity of Spiking Neural Networks through Spatiotemporal Separation",
    "abstract": "           Spiking Neural Networks (SNNs) are considered naturally suited for temporal processing, with membrane potential propagation widely regarded as the core temporal modeling mechanism. However, existing research lack analysis of its actual contributions in complex temporal tasks. We design Non-Stateful (NS) models progressively removing membrane propagation to quantify its stage-wise role. Experiments reveal a counterintuitive phenomenon: moderate removal in shallow or deep layers improves performance, while excessive removal causes collapse. We attribute this to spatio-temporal resource competition where neurons encode both semantics and dynamics within limited range, with temporal state consuming capacity for spatial learning. Based on this, we propose Spatial-Temporal Separable Network (STSep), decoupling residual blocks into independent spatial and temporal branches. The spatial branch focuses on semantic extraction while the temporal branch captures motion through explicit temporal differences. Experiments on Something-Something V2, UCF101, and HMDB51 show STSep achieves superior performance, with retrieval task and attention analysis confirming focus on motion rather than static appearance. This work provides new perspectives on SNNs' temporal mechanisms and an effective solution for spatiotemporal modeling in video understanding.         ",
    "url": "https://arxiv.org/abs/2512.05472",
    "authors": [
      "Yiting Dong",
      "Zhaofei Yu",
      "Jianhao Ding",
      "Zijie Xu",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2512.05475",
    "title": "PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning",
    "abstract": "           In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.         ",
    "url": "https://arxiv.org/abs/2512.05475",
    "authors": [
      "Saumya Biswas",
      "Jiten Oswal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2512.05482",
    "title": "Concept-based Explainable Data Mining with VLM for 3D Detection",
    "abstract": "           Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.         ",
    "url": "https://arxiv.org/abs/2512.05482",
    "authors": [
      "Mai Tsujimoto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05498",
    "title": "A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models",
    "abstract": "           Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty this http URL paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for this http URL decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.         ",
    "url": "https://arxiv.org/abs/2512.05498",
    "authors": [
      "Xiao He",
      "Ru Chen",
      "Zeqing Zhang",
      "Yanling Wang",
      "Qiuyan Dong"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2512.05502",
    "title": "GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop",
    "abstract": "           Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \\textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \\textsc{Understanding} (graph reconstruction of legacy code) and \\textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\\(\\approx\\)9--10/10 vs.\\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.         ",
    "url": "https://arxiv.org/abs/2512.05502",
    "authors": [
      "Omid Bazgir",
      "Vineeth Manthapuri",
      "Ilia Rattsev",
      "Mohammad Jafarnejad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05508",
    "title": "Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction",
    "abstract": "           Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.         ",
    "url": "https://arxiv.org/abs/2512.05508",
    "authors": [
      "Yash Choudhary",
      "Preeti Rao",
      "Pushpak Bhattacharyya"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05516",
    "title": "Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware",
    "abstract": "           This study evaluates AoS-to-SoA transformations over reduced-precision data layouts for a particle simulation code on several GPU platforms: We hypothesize that SoA fits particularly well to SIMT, while AoS is the preferred storage format for many Lagrangian codes. Reduced-precision (below IEEE accuracy) is an established tool to address bandwidth constraints, although it remains unclear whether AoS and precision conversions should execute on a CPU or be deployed to a GPU if the compute kernel itself must run on an accelerator. On modern superchips where CPUs and GPUs share (logically) one data space, it is also unclear whether it is advantageous to stream data to the accelerator prior to the calculation, or whether we should let the accelerator transform data on demand, i.e.~work in-place logically. We therefore introduce compiler annotations to facilitate such conversions and to give the programmer the option to orchestrate the conversions in combination with GPU offloading. For some of our compute kernels of interest, Nvidia's G200 platforms yield a speedup of around 2.6 while AMD's MI300A exhibits more robust performance yet profits less. We assume that our compiler-based techniques are applicable to a wide variety of Lagrangian codes and beyond.         ",
    "url": "https://arxiv.org/abs/2512.05516",
    "authors": [
      "Pawel K. Radtke",
      "Tobias Weinzierl"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Mathematical Software (cs.MS)"
    ]
  },
  {
    "id": "arXiv:2512.05524",
    "title": "VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation",
    "abstract": "           Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at this https URL.         ",
    "url": "https://arxiv.org/abs/2512.05524",
    "authors": [
      "Chinthani Sugandhika",
      "Chen Li",
      "Deepu Rajan",
      "Basura Fernando"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05531",
    "title": "IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection",
    "abstract": "           Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\\mathcal{IDK}$-$\\mathcal{S}$, a novel $\\mathbf{I}$ncremental $\\mathbf{D}$istributional $\\mathbf{K}$ernel for $\\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\\mathcal{IDK}$-$\\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\\mathcal{IDK}$-$\\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2512.05531",
    "authors": [
      "Yang Xu",
      "Yixiao Ma",
      "Kaifeng Zhang",
      "Zuliang Yang",
      "Kai Ming Ting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05540",
    "title": "SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection",
    "abstract": "           The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \\textbf{S}pherical \\textbf{C}onsistent \\textbf{N}eighborhoods \\textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.         ",
    "url": "https://arxiv.org/abs/2512.05540",
    "authors": [
      "Yang Xu",
      "Hang Zhang",
      "Yixiao Ma",
      "Ye Zhu",
      "Kai Ming Ting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05551",
    "title": "Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub",
    "abstract": "           Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.         ",
    "url": "https://arxiv.org/abs/2512.05551",
    "authors": [
      "Jai Lal Lulla",
      "Raula Gaikovina Kula",
      "Christoph Treude"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2512.05555",
    "title": "Compiling Away the Overhead of Race Detection",
    "abstract": "           Dynamic data race detectors are indispensable for flagging concurrency errors in software, but their high runtime overhead limits their adoption. This overhead stems primarily from pervasive instrumentation of memory accesses - a significant fraction of which is redundant. We addresses this inefficiency through a static, compiler-integrated approach that identifies and eliminates redundant instrumentation, drastically reducing the runtime cost of dynamic data race detectors. We introduce a suite of interprocedural static analyses reasoning about memory access patterns, synchronization, and thread creation to eliminate instrumentation for provably race-free accesses and show that the completeness properties of the data race detector are preserved. We further observe that many inserted checks flag a race if and only if a preceding check has already flagged an equivalent race for the same memory location - albeit potentially at a different access. We characterize this notion of equivalence and show that, when limiting reporting to at least one representative for each equivalence class, a further class of redundant checks can be eliminated. We identify such accesses using a novel dominance-based elimination analysis. Based on these two insights, we have implemented five static analyses within the LLVM, integrated with the instrumentation pass of the race detector ThreadSanitizer. Our experimental evaluation on a diverse suite of real-world applications demonstrates that our approach significantly reduces race detection overhead, achieving a geomean speedup of 1.34x, with peak speedups reaching 2.5x under high thread contention. This performance is achieved with a negligible increase in compilation time and, being fully automatic, places no additional burden on developers. Our optimizations have been accepted by the ThreadSanitizer maintainers and are in the process of being upstreamed.         ",
    "url": "https://arxiv.org/abs/2512.05555",
    "authors": [
      "Alexey Paznikov",
      "Andrey Kogutenko",
      "Yaroslav Osipov",
      "Michael Schwarz",
      "Umang Mathur"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Operating Systems (cs.OS)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2512.05561",
    "title": "The Topology of Hardship: Empirical Curriculum Graphs and Structural Bottlenecks in Engineering Degrees",
    "abstract": "           Engineering degrees are often perceived as \"hard\", yet this hardness is usually discussed in terms of content difficulty or student weaknesses rather than as a structural property of the curriculum itself. Recent work on course-prerequisite networks and curriculum graphs has shown that study plans can be modelled as complex networks with identifiable hubs and bottlenecks, but most studies rely on official syllabi rather than on how students actually progress through the system (Simon de Blas et al., 2021; Stavrinides & Zuev, 2023; Yang et al., 2024; Wang et al., 2025). This paper introduces the notion of topology of hardship: a quantitative description of curriculum complexity derived from empirical student trajectories in long-cycle engineering programmes. Building on the CAPIRE framework for multilevel trajectory modelling (Paz, 2025a, 2025b), we reconstruct degree-curriculum graphs from enrolment and completion data for 29 engineering curricula across several cohorts. For each graph we compute structural metrics (e.g., density, longest path, bottleneck centrality) and empirical hardship measures capturing blocking probability and time-to-progress. These are combined into a composite hardship index, which is then related to observed dropout rates and time to degree. Our findings show that curriculum hardness is not a vague perception but a measurable topological property: a small number of structurally dense, bottleneck-heavy curricula account for a disproportionate share of dropout and temporal desynchronisation. We discuss implications for curriculum reform, accreditation, and data-informed policy design.         ",
    "url": "https://arxiv.org/abs/2512.05561",
    "authors": [
      "H. R. Paz"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2512.05567",
    "title": "Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection",
    "abstract": "           The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.         ",
    "url": "https://arxiv.org/abs/2512.05567",
    "authors": [
      "Antoine Blais",
      "Nicolas Cou\u00ebllan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2512.05592",
    "title": "The T12 System for AudioMOS Challenge 2025: Audio Aesthetics Score Prediction System Using KAN- and VERSA-based Models",
    "abstract": "           We propose an audio aesthetics score (AES) prediction system by CyberAgent (AESCA) for AudioMOS Challenge 2025 (AMC25) Track 2. The AESCA comprises a Kolmogorov--Arnold Network (KAN)-based audiobox aesthetics and a predictor from the metric scores using the VERSA toolkit. In the KAN-based predictor, we replaced each multi-layer perceptron layer in the baseline model with a group-rational KAN and trained the model with labeled and pseudo-labeled audio samples. The VERSA-based predictor was designed as a regression model using extreme gradient boosting, incorporating outputs from existing metrics. Both the KAN- and VERSA-based models predicted the AES, including the four evaluation axes. The final AES values were calculated using an ensemble model that combined four KAN-based models and a VERSA-based model. Our proposed T12 system yielded the best correlations among the submitted systems, in three axes at the utterance level, two axes at the system level, and the overall average.         ",
    "url": "https://arxiv.org/abs/2512.05592",
    "authors": [
      "Katsuhiko Yamamoto",
      "Koichi Miyazaki",
      "Shogo Seki"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2512.05597",
    "title": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction",
    "abstract": "           Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters.         ",
    "url": "https://arxiv.org/abs/2512.05597",
    "authors": [
      "Ruihong Yin",
      "Xuepeng Shi",
      "Oleksandr Bailo",
      "Marco Manfredi",
      "Theo Gevers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05599",
    "title": "An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation",
    "abstract": "           Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources. Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities. Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets). Despite advances in optimizing detection techniques and model modifications, a fully autonomous solution capable of accurately identifying and sorting batteries across diverse WEEEs types has yet to be realized. In response to these challenges, we present our novel approach which integrates a specialized X-ray transmission dual energy imaging subsystem with advanced pre-processing algorithms, enabling high-contrast image reconstruction for effective differentiation of dense and thin materials in WEEE. Devices move along a conveyor belt through a high-resolution X-ray imaging system, where YOLO and U-Net models precisely detect and segment battery-containing items. An intelligent tracking and position estimation algorithm then guides a Delta robot equipped with a suction gripper to selectively extract and properly discard the targeted devices. The approach is validated in a photorealistic simulation environment developed in NVIDIA Isaac Sim and on the real setup.         ",
    "url": "https://arxiv.org/abs/2512.05599",
    "authors": [
      "Panagiotis Giannikos",
      "Lampis Papakostas",
      "Evangelos Katralis",
      "Panagiotis Mavridis",
      "George Chryssinas",
      "Myrto Inglezou",
      "Nikolaos Panagopoulos",
      "Antonis Porichis",
      "Athanasios Mastrogeorgiou",
      "Panagiotis Chatzakos"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2512.05623",
    "title": "Bounded Graph Clustering with Graph Neural Networks",
    "abstract": "           In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.         ",
    "url": "https://arxiv.org/abs/2512.05623",
    "authors": [
      "Kibidi Neocosmos",
      "Diego Baptista",
      "Nicole Ludwig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05651",
    "title": "Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective",
    "abstract": "           The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.         ",
    "url": "https://arxiv.org/abs/2512.05651",
    "authors": [
      "Nan Zhong",
      "Mian Zou",
      "Yiran Xu",
      "Zhenxing Qian",
      "Xinpeng Zhang",
      "Baoyuan Wu",
      "Kede Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05653",
    "title": "Executing Discrete/Continuous Declarative Process Specifications via Complex Event Processing",
    "abstract": "           Traditional Business Process Management (BPM) focuses on discrete events and fails to incorporate critical continuous sensor data in cyber-physical environments. Hybrid declarative specifications, utilizing Signal Temporal Logic (STL), address this limitation by allowing constraints over both discrete events and real-valued signals. However, existing work has been limited to monitoring and post-hoc conformance checking. This paper introduces a novel Complex Event Processing (CEP)-based execution architecture that enables the real-time execution and enforcement of hybrid declarative models. Our three-layer approach integrates STL-inspired predicates into the execution flow, allowing the system to actively trigger activities and enforce process boundaries based on continuous sensor behavior. This approach bridges the gap between hybrid specification and operational control.         ",
    "url": "https://arxiv.org/abs/2512.05653",
    "authors": [
      "Stefan Sch\u00f6nig",
      "Leo Poss",
      "Fabrizio Maria Maggi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Logic in Computer Science (cs.LO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2512.05663",
    "title": "LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection",
    "abstract": "           Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.         ",
    "url": "https://arxiv.org/abs/2512.05663",
    "authors": [
      "Johannes Meier",
      "Jonathan Michel",
      "Oussema Dhaouadi",
      "Yung-Hsu Yang",
      "Christoph Reich",
      "Zuria Bauer",
      "Stefan Roth",
      "Marc Pollefeys",
      "Jacques Kaiser",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05673",
    "title": "Inexact Uzawa-Double Deep Ritz Method for Weak Adversarial Neural Networks",
    "abstract": "           The emergence of deep learning has stimulated a new class of PDE solvers in which the unknown solution is represented by a neural network. Within this framework, residual minimization in dual norms -- central to weak adversarial neural network approaches -- naturally leads to saddle-point problems whose stability depends on the underlying iterative scheme. Motivated by this structure, we develop an inexact Uzawa methodology in which both trial and test functions are represented by neural networks and updated only approximately. We introduce the Uzawa Deep Double Ritz method, a mesh-free deep PDE solver equipped with a continuous level convergence showing that the overall iteration remains stable and convergent provided the inexact inner updates move in the correct descent direction. Numerical experiments validate the theoretical findings and demonstrate the practical robustness and accuracy of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2512.05673",
    "authors": [
      "Emin Benny-Chacko",
      "Ignacio Brevis",
      "Luis Espath",
      "Kristoffer G. van der Zee"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2512.05674",
    "title": "Hyperspectral Unmixing with 3D Convolutional Sparse Coding and Projected Simplex Volume Maximization",
    "abstract": "           Hyperspectral unmixing (HSU) aims to separate each pixel into its constituent endmembers and estimate their corresponding abundance fractions. This work presents an algorithm-unrolling-based network for the HSU task, named the 3D Convolutional Sparse Coding Network (3D-CSCNet), built upon a 3D CSC model. Unlike existing unrolling-based networks, our 3D-CSCNet is designed within the powerful autoencoder (AE) framework. Specifically, to solve the 3D CSC problem, we propose a 3D CSC block (3D-CSCB) derived through deep algorithm unrolling. Given a hyperspectral image (HSI), 3D-CSCNet employs the 3D-CSCB to estimate the abundance matrix. The use of 3D CSC enables joint learning of spectral and spatial relationships in the 3D HSI data cube. The estimated abundance matrix is then passed to the AE decoder to reconstruct the HSI, and the decoder weights are extracted as the endmember matrix. Additionally, we propose a projected simplex volume maximization (PSVM) algorithm for endmember estimation, and the resulting endmembers are used to initialize the decoder weights of 3D-CSCNet. Extensive experiments on three real datasets and one simulated dataset with three different signal-to-noise ratio (SNR) levels demonstrate that our 3D-CSCNet outperforms state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2512.05674",
    "authors": [
      "Gargi Panda",
      "Soumitra Kundu",
      "Saumik Bhattacharya",
      "Aurobinda Routray"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05679",
    "title": "The Power of Network Pluralism: Multi-Perspective Modeling of Heterogeneous Legal Document Networks",
    "abstract": "           Insights are relative - influenced by a range of factors such as assumptions, scopes, or methods that together define a research perspective. In normative and empirical fields alike, this insight has led to the conclusion that no single perspective can generate complete knowledge. As a response, epistemological pluralism mandates that researchers consider multiple perspectives simultaneously to obtain a holistic understanding of their phenomenon under study. Translating this mandate to network science, our work introduces Network Pluralism as a conceptual framework that leverages multi-perspectivity to yield more complete, meaningful, and robust results. We develop and demonstrate the benefits of this approach via a hands-on analysis of complex legal systems, constructing a network space from references across documents from different branches of government as well as including organizational hierarchy above and fine-grained structure below the document level. Leveraging the resulting heterogeneity in a multi-network analysis, we show how complementing perspectives can help contextualize otherwise high-level findings, how contrasting several networks derived from the same data enables researchers to learn by difference, and how relating metrics to perspectives may increase the transparency and robustness of network-analytical results. To analyze a space of networks as perspectives, researchers need to map dimensions of variation in a given domain to network-modeling decisions and network-metric parameters. While this remains a challenging and inherently interdisciplinary task, our work acts as a blueprint to facilitate the broader adoption of Network Pluralism in domain-driven network research.         ",
    "url": "https://arxiv.org/abs/2512.05679",
    "authors": [
      "Titus P\u00fcnder",
      "Corinna Coupette"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2512.05680",
    "title": "Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks",
    "abstract": "           Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \\textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.         ",
    "url": "https://arxiv.org/abs/2512.05680",
    "authors": [
      "Alexander Mattick",
      "George Yammine",
      "Georgios Kontes",
      "Setareh Maghsudi",
      "Christopher Mutschler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05681",
    "title": "Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods",
    "abstract": "           Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.         ",
    "url": "https://arxiv.org/abs/2512.05681",
    "authors": [
      "Tereza Novotna",
      "Jakub Harasta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05682",
    "title": "Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees",
    "abstract": "           Reliable uncertainty quantification in trajectory prediction is crucial for safety-critical autonomous driving systems, yet existing deep learning predictors lack uncertainty-aware frameworks adaptable to heterogeneous real-world scenarios. To bridge this gap, we propose a novel scenario-aware uncertainty quantification framework to provide the predicted trajectories with prediction intervals and reliability assessment. To begin with, predicted trajectories from the trained predictor and their ground truth are projected onto the map-derived reference routes within the Frenet coordinate system. We then employ CopulaCPTS as the conformal calibration method to generate temporal prediction intervals for distinct scenarios as the uncertainty measure. Building upon this, within the proposed trajectory reliability discriminator (TRD), mean error and calibrated confidence intervals are synergistically analyzed to establish reliability models for different scenarios. Subsequently, the risk-aware discriminator leverages a joint risk model that integrates longitudinal and lateral prediction intervals within the Frenet coordinate to identify critical points. This enables segmentation of trajectories into reliable and unreliable segments, holding the advantage of informing downstream planning modules with actionable reliability results. We evaluated our framework using the real-world nuPlan dataset, demonstrating its effectiveness in scenario-aware uncertainty quantification and reliability assessment across diverse driving contexts.         ",
    "url": "https://arxiv.org/abs/2512.05682",
    "authors": [
      "Yiming Shu",
      "Jiahui Xu",
      "Linghuan Kong",
      "Fangni Zhang",
      "Guodong Yin",
      "Chen Sun"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2512.05683",
    "title": "Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction",
    "abstract": "           Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2512.05683",
    "authors": [
      "Yong En Kok",
      "Bowen Deng",
      "Alexander Bentley",
      "Andrew J. Parkes",
      "Michael G. Somekh",
      "Amanda J. Wright",
      "Michael P. Pound"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2512.05698",
    "title": "OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning",
    "abstract": "           Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.         ",
    "url": "https://arxiv.org/abs/2512.05698",
    "authors": [
      "Xusheng Guo",
      "Wanfa Zhang",
      "Shijia Zhao",
      "Qiming Xia",
      "Xiaolong Xie",
      "Mingming Wang",
      "Hai Wu",
      "Chenglu Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05721",
    "title": "BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language",
    "abstract": "           We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\\times$ variation in service quality, making it well suited for intelligent RAN deployments.         ",
    "url": "https://arxiv.org/abs/2512.05721",
    "authors": [
      "Nitin Priyadarshini Shankar",
      "Vaibhav Singh",
      "Sheetal Kalyani",
      "Christian Maciocco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05729",
    "title": "Informed Consent: We Can Do Better to Defend Privacy",
    "abstract": "           We need to rethink our approach to defend privacy on the internet. Currently, policymakers focus heavily on the idea of informed consent as a means to defend privacy. For instance, in many countries the law requires firms to obtain an individual's consent before they use data about her; with such informed consent requirements, the law aims to empower people to make privacy choices in their best interests. But behavioural studies cast doubt on this approach's effectiveness, as people tend to click OK to almost any request they see on their screens. To improve privacy protection, this article argues for a combined approach of protecting and empowering the individual. This article discusses practical problems with informed consent as a means to protect privacy, and illustrates the problems with current data privacy rules regarding behavioural targeting. First, the privacy problems of behavioural targeting, and the central role of informed consent in privacy law are discussed. Following that, practical problems with informed consent are highlighted. Then, the article argues that policymakers should give more attention to rules that protect, rather than empower, people.         ",
    "url": "https://arxiv.org/abs/2512.05729",
    "authors": [
      "Frederik Zuiderveen Borgesius"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2512.05760",
    "title": "Evolutionary System 2 Reasoning: An Empirical Proof",
    "abstract": "           Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at this https URL for reproduction needs.         ",
    "url": "https://arxiv.org/abs/2512.05760",
    "authors": [
      "Zeyuan Ma",
      "Wenqi Huang",
      "Guo-Huan Song",
      "Hongshu Guo",
      "Sijie Ma",
      "Zhiguang Cao",
      "Yue-Jiao Gong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05762",
    "title": "FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators",
    "abstract": "           We present FNOpt, a self-supervised cloth simulation framework that formulates time integration as an optimization problem and trains a resolution-agnostic neural optimizer parameterized by a Fourier neural operator (FNO). Prior neural simulators often rely on extensive ground truth data or sacrifice fine-scale detail, and generalize poorly across resolutions and motion patterns. In contrast, FNOpt learns to simulate physically plausible cloth dynamics and achieves stable and accurate rollouts across diverse mesh resolutions and motion patterns without retraining. Trained only on a coarse grid with physics-based losses, FNOpt generalizes to finer resolutions, capturing fine-scale wrinkles and preserving rollout stability. Extensive evaluations on a benchmark cloth simulation dataset demonstrate that FNOpt outperforms prior learning-based approaches in out-of-distribution settings in both accuracy and robustness. These results position FNO-based meta-optimization as a compelling alternative to previous neural simulators for cloth, thus reducing the need for curated data and improving cross-resolution reliability.         ",
    "url": "https://arxiv.org/abs/2512.05762",
    "authors": [
      "Ruochen Chen",
      "Thuy Tran",
      "Shaifali Parashar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2512.05764",
    "title": "Towards agent-based-model informed neural networks",
    "abstract": "           In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.         ",
    "url": "https://arxiv.org/abs/2512.05764",
    "authors": [
      "Nino Antulov-Fantulin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2512.05790",
    "title": "Learnability Window in Gated Recurrent Neural Networks",
    "abstract": "           We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \\emph{effective learning rates} $\\mu_{t,\\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\\ell$ satisfies $N(\\ell)\\propto f(\\ell)^{-\\alpha}$, where $f(\\ell)=\\|\\mu_{t,\\ell}\\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.         ",
    "url": "https://arxiv.org/abs/2512.05790",
    "authors": [
      "Lorenzo Livi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2512.05812",
    "title": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation",
    "abstract": "           Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.         ",
    "url": "https://arxiv.org/abs/2512.05812",
    "authors": [
      "Fabian Konstantinidis",
      "Moritz Sackmann",
      "Ulrich Hofmann",
      "Christoph Stiller"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05814",
    "title": "UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection",
    "abstract": "           Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.         ",
    "url": "https://arxiv.org/abs/2512.05814",
    "authors": [
      "Fubao Zhu",
      "Zhanyuan Jia",
      "Zhiguo Wang",
      "Huan Huang",
      "Danyang Sun",
      "Chuang Han",
      "Yanting Li",
      "Jiaofen Nan",
      "Chen Zhao",
      "Weihua Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05823",
    "title": "3D-ICE 4.0: Accurate and efficient thermal modeling for 2.5D/3D heterogeneous chiplet systems",
    "abstract": "           The increasing power densities and intricate heat dissipation paths in advanced 2.5D/3D chiplet systems necessitate thermal modeling frameworks that deliver detailed thermal maps with high computational efficiency. Traditional compact thermal models (CTMs) often struggle to scale with the complexity and heterogeneity of modern architectures. This work introduces 3D-ICE 4.0, designed for heterogeneous chip-based systems. Key innovations include: (i) preservation of material heterogeneity and anisotropy directly from industrial layouts, integrated with OpenMP and SuperLU MT-based parallel solvers for scalable performance, (ii) adaptive vertical layer partitioning to accurately model vertical heat conduction, and (iii) temperature-aware non-uniform grid generation. The results with different benchmarks demonstrate that 3D-ICE 4.0 achieves speedups ranging from 3.61x-6.46x over state-of-the-art tools, while reducing grid complexity by more than 23.3% without compromising accuracy. Compared to the commercial software COMSOL, 3D-ICE 4.0 effectively captures both lateral and vertical heat flows, validating its precision and robustness. These advances demonstrate that 3D-ICE 4.0 is an efficient solution for thermal modeling in emerging heterogeneous 2.5D/3D integrated systems.         ",
    "url": "https://arxiv.org/abs/2512.05823",
    "authors": [
      "Kai Zhu",
      "Darong Huang",
      "Luis Costero",
      "David Atienza"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2512.05824",
    "title": "Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma",
    "abstract": "           Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.         ",
    "url": "https://arxiv.org/abs/2512.05824",
    "authors": [
      "Hafsa Akebli",
      "Adam Shephard",
      "Vincenzo Della Mea",
      "Nasir Rajpoot"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05830",
    "title": "Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning",
    "abstract": "           This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: this https URL.         ",
    "url": "https://arxiv.org/abs/2512.05830",
    "authors": [
      "Muhammet Cagri Yeke",
      "Samil Sirin",
      "Kivilcim Yuksel",
      "Abdurrahman Gumus"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05831",
    "title": "Dissecting Embedding Bag Performance in DLRM Inference",
    "abstract": "           As the size of DLRMs gets larger, the models must be partitioned across multiple GPUs or nodes of GPUs due to the size limitation of total HBM memory that can be packaged in a GPU. This partitioning adds communication and synchronization overhead of sending and receiving data across GPUs. We use the NCCL and NVSHMEM libraries to measure the performance of an Embedding Bag kernel implemented on H100 GPUs. We compare its performance across diOerent batch sizes, number of tables, table sizes, pooling factors, and embedding dimensions. For a large embedding table that spans multiple GPUs, we project the performance slowdown from distributing an embedding table across multiple GPUs.         ",
    "url": "https://arxiv.org/abs/2512.05831",
    "authors": [
      "Chandrish Ambati",
      "Jing Ding",
      "Trung Diep"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2512.05836",
    "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions",
    "abstract": "           Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.         ",
    "url": "https://arxiv.org/abs/2512.05836",
    "authors": [
      "Clarissa W. Ong",
      "Hiba Arnaout",
      "Kate Sheehan",
      "Estella Fox",
      "Eugen Owtscharow",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05837",
    "title": "Wasserstein Evolution : Evolutionary Optimization as Phase Transition",
    "abstract": "           This paper establishes a novel connection between evolutionary computation and statistical physics by formalizing evolutionary optimization as a phase transition process. We introduce Wasserstein Evolution (WE), a principled optimization framework that implements the Wasserstein gradient flow of a free energy functional, mathematically bridging evolutionary dynamics with thermodynamics. WE directly translates the physical competition between potential gradient forces (exploitation) and entropic forces (exploration) into algorithmic dynamics, providing an adaptive, theoretically grounded mechanism for balancing exploration and exploitation. Experiments on challenging benchmark functions demonstrate that WE achieves competitive convergence performance while maintaining dramatically higher population diversity than classical methods (GA, DE, CMA-ES).This superior entropy preservation enables effective navigation of multi-modal landscapes without premature convergence, validating the physical interpretation of optimization as a disorder-to-order transition. Our work provides not only an effective optimization algorithm but also a new paradigm for understanding evolutionary computation through statistical physics.         ",
    "url": "https://arxiv.org/abs/2512.05837",
    "authors": [
      "Kaichen Ouyang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2512.05853",
    "title": "VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack",
    "abstract": "           Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.         ",
    "url": "https://arxiv.org/abs/2512.05853",
    "authors": [
      "Shiji Zhao",
      "Shukun Xiong",
      "Yao Huang",
      "Yan Jin",
      "Zhenyu Wu",
      "Jiyang Guan",
      "Ranjie Duan",
      "Jialing Tao",
      "Hui Xue",
      "Xingxing Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.05868",
    "title": "Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks",
    "abstract": "           Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.         ",
    "url": "https://arxiv.org/abs/2512.05868",
    "authors": [
      "Brian Ezinwoke",
      "Oliver Rhodes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ]
  },
  {
    "id": "arXiv:2512.05878",
    "title": "Complex Bounded Operators in Isabelle/HOL",
    "abstract": "           Functional analysis, especially the theory of Hilbert spaces and of operators on these, form an important area in mathematics. We formalized the Isabelle/HOL library Complex_Bounded_Operators containing a large amount of theorems about complex Hilbert spaces and (bounded) operators on these. Specifically, we formalize the properties complex vector spaces, inner product (and Hilbert) spaces, one-dimensional spaces, bounded operators, adjoints, unitaries, projections, extensions of bounded operators (BLT-theorem), positive operators, square-summable sequences and much more. Additionally, we provide support for code generation in the finite-dimensional case.         ",
    "url": "https://arxiv.org/abs/2512.05878",
    "authors": [
      "Dominique Unruh",
      "Jos\u00e9 Manuel Rodr\u00edguez Caballero"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2512.05880",
    "title": "Neural Coherence : Find higher performance to out-of-distribution tasks from few samples",
    "abstract": "           To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.         ",
    "url": "https://arxiv.org/abs/2512.05880",
    "authors": [
      "Simon Guiroy",
      "Mats Richter",
      "Sarath Chandar",
      "Christopher Pal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05881",
    "title": "DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints",
    "abstract": "           Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2512.05881",
    "authors": [
      "Rahul Golder",
      "Bimol Nath Roy",
      "M. M. Faruque Hasan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05893",
    "title": "NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process",
    "abstract": "           In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $\\mu >0$ and $\\beta \\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.         ",
    "url": "https://arxiv.org/abs/2512.05893",
    "authors": [
      "Neha Gupta",
      "Aditya Maheshwari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2512.05915",
    "title": "LDLT $\\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction",
    "abstract": "           Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\\%-13\\% accuracy gains over SLL Layers on 121 UCI data sets.         ",
    "url": "https://arxiv.org/abs/2512.05915",
    "authors": [
      "Marius F.R. Juston",
      "Ramavarapu S. Sreenivas",
      "Dustin Nottage",
      "Ahmet Soylemezoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2512.05920",
    "title": "NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction",
    "abstract": "           Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.         ",
    "url": "https://arxiv.org/abs/2512.05920",
    "authors": [
      "Jiawen Yang",
      "Yihui Cao",
      "Xuanyu Tian",
      "Yuyao Zhang",
      "Hongjiang Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05950",
    "title": "Impugan: Learning Conditional Generative Models for Robust Data Imputation",
    "abstract": "           Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: this http URL ",
    "url": "https://arxiv.org/abs/2512.05950",
    "authors": [
      "Zalish Mahmud",
      "Anantaa Kotal",
      "Aritran Piplai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05954",
    "title": "SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code",
    "abstract": "           We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems         ",
    "url": "https://arxiv.org/abs/2512.05954",
    "authors": [
      "Shima Imani",
      "Seungwhan Moon",
      "Adel Ahmadyan",
      "Lu Zhang",
      "Kirmani Ahmed",
      "Babak Damavandi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05960",
    "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement",
    "abstract": "           Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.         ",
    "url": "https://arxiv.org/abs/2512.05960",
    "authors": [
      "Munsif Ali",
      "Najmul Hassan",
      "Lucia Ventura",
      "Davide Di Bari",
      "Simonepietro Canese"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.05127",
    "title": "Bayesian Optimization and Convolutional Neural Networks for Zernike-Based Wavefront Correction in High Harmonic Generation",
    "abstract": "           High harmonic generation (HHG) is a nonlinear process that enables table-top generation of tunable, high-energy, coherent, ultrashort radiation pulses in the extreme ultraviolet (EUV) to soft X-ray range. These pulses find applications in photoemission spectroscopy in condensed matter physics, pump-probe spectroscopy for high-energy-density plasmas, and attosecond science. However, optical aberrations in the high-power laser systems required for HHG degrade beam quality and reduce efficiency. We present a machine learning approach to optimize aberration correction using a spatial light modulator. We implemented and compared Bayesian optimization and convolutional neural network (CNN) methods to predict optimal Zernike polynomial coefficients for wavefront correction. Our CNN achieved promising results with 80.39% accuracy on test data, demonstrating the potential for automated aberration correction in HHG systems.         ",
    "url": "https://arxiv.org/abs/2512.05127",
    "authors": [
      "Guilherme Grancho D. Fernandes",
      "Duarte Alexandrino",
      "Eduardo Silva",
      "Jo\u00e3o Matias",
      "Joaquim Pereira"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05144",
    "title": "Job Satisfaction Through the Lens of Social Media: Rural--Urban Patterns in the U.S",
    "abstract": "           We analyze a novel large-scale social-media-based measure of U.S. job satisfaction, constructed by applying a fine-tuned large language model to 2.6 billion georeferenced tweets, and link it to county-level labor market conditions (2013-2023). Logistic regressions show that rural counties consistently report lower job satisfaction sentiment than urban ones, but this gap decreases under tight labor markets. In contrast to widening rural-urban income disparities, perceived job quality converges when unemployment is low, suggesting that labor market slack, not income alone, drives spatial inequality in subjective work-related well-being.         ",
    "url": "https://arxiv.org/abs/2512.05144",
    "authors": [
      "Stefano M Iacus",
      "Giuseppe Porro"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2512.05245",
    "title": "STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings",
    "abstract": "           Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2512.05245",
    "authors": [
      "Mehmet Efe Ak\u00e7a",
      "G\u00f6k\u00e7e Uludo\u011fan",
      "Arzucan \u00d6zg\u00fcr",
      "\u0130nci M. Bayta\u015f"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.05357",
    "title": "Universality of asymptotic graph homomorphism",
    "abstract": "           The Shannon capacity of graphs, introduced by Shannon in 1956 to model zero-error communication, asks for determining the rate of growth of independent sets in strong powers of graphs. Much is still unknown about this parameter, for instance whether it is computable. Recent work has established a dual characterization of the Shannon capacity in terms of the asymptotic spectrum of graphs. A core step in this duality theory is to shift focus from Shannon capacity itself to studying the asymptotic relations between graphs, that is, the asymptotic cohomomorphisms. Towards understanding the structure of Shannon capacity, we study the \"combinatorial complexity\" of asymptotic cohomomorphism. As our main result, we prove that the asymptotic cohomomorphism order is universal for all countable preorders. That is, we prove that any countable preorder can be order-embedded into the asymptotic cohomomorphism order (i.e. appears as a suborder). Previously this was only known for (non-asymptotic) cohomomorphism. Our proof is based on techniques from asymptotic spectrum duality and convex structure of the asymptotic spectrum of graphs. Our approach in fact leads to a new proof of the universality of (non-asymptotic) cohomomorphism.         ",
    "url": "https://arxiv.org/abs/2512.05357",
    "authors": [
      "Anna Luchnikov",
      "Jim Wittebol",
      "Jeroen Zuiddam"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2512.05480",
    "title": "On the Word-Representability of 5-Regular Circulant Graphs",
    "abstract": "           A graph $G = (V, E)$ is word-representable if there exists a word $w$ over the alphabet $V$ such that, for any two distinct vertices $x, y \\in V$, $xy \\in E$ if and only if $x$ and $y$ alternate in $w$. Two letters $x$ and $y$ are said to alternate in $w$ if, after removing all other letters from $w$, the resulting word is of the form $xyxy\\dots$ or $yxyx\\dots$ (of even or odd length). For a given set $R = \\{r_1, r_2, \\dots, r_k\\}$ of jump elements, an undirected circulant graph $C_n(R)$ on $n$ vertices has vertex set $\\{0, 1, \\dots, n-1\\}$ and edge set $ E = \\left\\{ \\{i,j\\} \\;\\middle|\\; |i - j| \\bmod n \\in \\{r_1, r_2, \\dots, r_k\\} \\right\\}, $ where $0 < r_1 < r_2 < \\dots < r_k < \\frac{n}{2}$. Recently, Kitaev and Pyatkin proved that every 4-regular circulant graph is word-representable. Srinivasan and Hariharasubramanian further investigated circulant graphs and obtained bounds on the representation number for $k$-regular circulant graphs with $2 \\le k \\le 4$. In addition to these positive results, their work also presents examples of non-word-representable circulant graphs. In this work, we study word-representability and the representation number of 5-regular circulant graphs via techniques from elementary number theory and group theory, as well as graph coloring, graph factorization and morphisms.         ",
    "url": "https://arxiv.org/abs/2512.05480",
    "authors": [
      "Suchanda Roy",
      "Ramesh Hariharasubramanian"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2512.05727",
    "title": "Modified global finite-time quasi-continuous second-order robust feedback control",
    "abstract": "           A non-overshooting quasi-continuous sliding mode control with sub-optimal damping was recently introduced in Ruderman and Efimov (2025) for perturbed second-order systems. The present work proposes an essential modification of the nonlinear control law which (i) allows for a parameterizable control amplitude limitation in a large subset of the initial values, (ii) admits an entire state-space R2 (that was not given in Ruderman and Efimov (2025)) for the finite-time control, and finally (iii) enables for the found analytic solution of the state trajectories in the unperturbed case. The latter allows also for an exact estimation of the finite convergence time, and open an avenue for other potentially interesting analysis of the control properties in the future. For a perturbed case, the solution-based and Lyapunov function-based approaches are developed to show the uniform global asymptotic stability. The proposed robustness and convergence analysis are accompanied by several illustrative numerical examples.         ",
    "url": "https://arxiv.org/abs/2512.05727",
    "authors": [
      "Michael Ruderman",
      "Denis Efimov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2512.05751",
    "title": "Exoplanet formation inference using conditional invertible neural networks",
    "abstract": "           The interpretation of the origin of observed exoplanets is usually done only qualitatively due to uncertainties of key parameters in planet formation models. To allow a quantitative methodology which traces back in time to the planet birth locations, we train recently developed conditional invertible neural networks (cINN) on synthetic data from a global planet formation model which tracks growth from dust grains to evolved final giant planets. In addition to deterministic single planet formation runs, we also include gravitationally interacting planets in multiplanetary systems, which include some measure of chaos. For the latter case, we treat them as individual planets or choose the two or three planets most likely to be discovered by telescopes. We find that training on multiplanetary data, each planet treated as individual point, is promising. The single-planet data only covers a small range of planets and does not extrapolate well to planet properties not included in the training data. Extension to planetary systems will require more training data due to the higher dimensionality of the problem.         ",
    "url": "https://arxiv.org/abs/2512.05751",
    "authors": [
      "Remo Burn",
      "Victor F. Ksoll",
      "Hubert Klahr",
      "Thomas Henning"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2512.05940",
    "title": "Designing an Optimal Sensor Network via Minimizing Information Loss",
    "abstract": "           Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that \"minimize information loss\" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.         ",
    "url": "https://arxiv.org/abs/2512.05940",
    "authors": [
      "Daniel Waxman",
      "Fernando Llorente",
      "Katia Lamer",
      "Petar M. Djuri\u0107"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2205.04491",
    "title": "Statistical Guarantees for Approximate Stationary Points of Shallow Neural Networks",
    "abstract": "           Since statistical guarantees for neural networks are usually restricted to global optima of intricate objective functions, it is unclear whether these theories explain the performances of actual outputs of neural network pipelines. The goal of this paper is, therefore, to bring statistical theory closer to practice. We develop statistical guarantees for shallow linear neural networks that coincide up to logarithmic factors with the global optima but apply to stationary points and the points nearby. These results support the common notion that neural networks do not necessarily need to be optimized globally from a mathematical perspective. We then extend our statistical guarantees to shallow ReLU neural networks, assuming the first layer weight matrices are nearly identical for the stationary network and the target. More generally, despite being limited to shallow neural networks for now, our theories make an important step forward in describing the practical properties of neural networks in mathematical terms.         ",
    "url": "https://arxiv.org/abs/2205.04491",
    "authors": [
      "Mahsa Taheri",
      "Fang Xie",
      "Johannes Lederer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2310.02993",
    "title": "Finding coherent node groups in directed graphs",
    "abstract": "           Grouping the nodes of a graph into clusters is a standard technique for studying networks. We study a problem where we are given a directed network and are asked to partition the graph into a sequence of coherent groups. We assume that nodes in the network have features, and we measure the group coherence by comparing these features. Furthermore, we incorporate the cross edges by penalizing the forward cross edges and backward cross edges with different weights. If the weights are set to 0, then the problem is equivalent to clustering. However, if we penalize the backward edges, the order of discovered groups matters, and we can view our problem as a generalization of a classic segmentation problem. We consider a common iterative approach where we solve the groups given the centroids, and then find the centroids given the groups. We show that, unlike in clustering, the first subproblem is NP-hard. However, we show that we can solve the subproblem exactly if the underlying graph is a tree or if the number of groups is 2. For a general case, we propose an approximation algorithm based on linear programming. We propose 3 additional heuristics: (1) optimizing each pair of groups separately while keeping the remaining groups intact, (2) computing a spanning tree and then optimizing using only the edges in that, and (3) a greedy search moving nodes between the groups while optimizing the overall loss. We demonstrate with our experiments that the algorithms are practical and yield interpretable results.         ",
    "url": "https://arxiv.org/abs/2310.02993",
    "authors": [
      "Iiro Kumpulainen",
      "Nikolaj Tatti"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2403.06084",
    "title": "pETNNs: Partial Evolutionary Tensor Neural Networks for Solving Time-dependent Partial Differential Equations",
    "abstract": "           We present the partial evolutionary tensor neural networks (pETNNs), a novel framework for solving time-dependent partial differential equations with high accuracy and capable of handling high-dimensional problems. Our architecture incorporates tensor neural networks and evolutionary parametric approximation. A posterior error bounded is proposed to support the extrapolation capabilities. In the numerical implementations, we adopt a partial update strategy to achieve a significant reduction in computational cost while maintaining precision and robustness. Notably, as a low-rank approximation method of complex dynamical systems, pETNNs enhance the accuracy of evolutionary deep neural networks and empower computational abilities to address high-dimensional problems. Numerical experiments demonstrate the superior performance of the pETNNs in solving time-dependent complex equations, including the incompressible Navier-Stokes equations, high-dimensional heat equations, high-dimensional transport equations, and dispersive equations of higher-order derivatives.         ",
    "url": "https://arxiv.org/abs/2403.06084",
    "authors": [
      "Tunan Kao",
      "He Zhang",
      "Lei Zhang",
      "Jin Zhao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2405.11352",
    "title": "Hierarchical Reinforcement Learning Empowered Task Offloading in V2I Networks",
    "abstract": "           Edge computing plays an essential role in the vehicle-to-infrastructure (V2I) networks, where vehicles offload their intensive computation tasks to the road-side units for saving energy and reduce the latency. This paper designs the optimal task offloading policy to address the concerns involving processing delay, energy consumption and edge computing cost. Each computation task consisting of some interdependent sub-tasks is characterized as a directed acyclic graph (DAG). In such dynamic networks, a novel hierarchical Offloading scheme is proposed by leveraging deep reinforcement learning (DRL). The inter-dependencies among the DAGs of the computation tasks are extracted using a graph neural network with attention mechanism. A parameterized DRL algorithm is developed to deal with the hierarchical action space containing both discrete and continuous actions. Simulation results with a real-world car speed dataset demonstrate that the proposed scheme can effectively reduce the system overhead.         ",
    "url": "https://arxiv.org/abs/2405.11352",
    "authors": [
      "Xinyu You",
      "Haojie Yan",
      "Yuedong Xu",
      "Lifeng Wang",
      "Liangui Dai"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2405.11359",
    "title": "Optimizing Layerwise Microservice Management in Heterogeneous Wireless Networks",
    "abstract": "           Small cells with edge computing are densely deployed in 5G mobile networks to provide high throughput communication and low-latency computation. The flexibility of edge computation is empowered by the deployment of lightweight container-based microservices. In this paper, we take the first step toward optimizing the microservice management in small-cell networks. The prominent feature is that each microservice consists of multiple image layers and different microservices may share some basic layers, thus bringing deep coupling in their placement and service provision. Our objective is to minimize the expected total latency of microservice requests under the storage, communication and computing constraints of the sparsely interconnected small cell nodes. We formulate a binary quadratic program (BQP) with the multi-dimensional strategy of the image layer placement, the access selection and the task assignment. The BQP problem is then transformed into an ILP problem, and is solved by use of a novel sphere-box alternating direction multipliers method (ADMM) with reasonable complexity $O(q^{4})$, where $q$ is the number of variables in the transformed problem. Trace-driven experiments show that the gap between our proposed algorithm and the optimal is reduced by 35$\\%$ compared with benchmark algorithms.         ",
    "url": "https://arxiv.org/abs/2405.11359",
    "authors": [
      "Haojie Yan",
      "Yuedong Xu",
      "Lianggui Dai"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2405.18574",
    "title": "SpecTra: Enhancing the Code Translation Ability of Language Models by Generating Multi-Modal Specifications",
    "abstract": "           Large language models (LLMs) are increasingly being used for the task of automated code translation, which has important real-world applications. However, most existing approaches use only the source code of a program as an input to an LLM, and do not consider the different kinds of specifications that can be extracted from a program. In this paper, we propose SpecTra, a multi-stage approach that uses a novel self-consistency filter to first generate high-quality static specifications, test cases, and natural language descriptions from a given program, and then uses these along with the source code to improve the quality of LLM-generated translations. We evaluate SpecTra on three code translation tasks - C to Rust, C to Go, and JavaScript to TypeScript - and show that it can enhance the performance of six popular LLMs on these tasks by up to a relative improvement of 46%. We also present a case study on extending this approach to handle translation of a full C project to Rust. Our research suggests that generating high-quality specifications could be a promising and efficient way to improve the performance of LLMs for code translation.         ",
    "url": "https://arxiv.org/abs/2405.18574",
    "authors": [
      "Vikram Nitin",
      "Rahul Krishna",
      "Baishakhi Ray"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.02037",
    "title": "Multi-Scale Direction-Aware Network for Infrared Small Target Detection",
    "abstract": "           Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on edge and shape features, but ignore the richer structural differences and detailed information embedded in high-frequency components from different directions, thereby failing to fully exploit the value of high-frequency directional features in target perception. To address this limitation, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, to fully mine the high-frequency directional features, on the one hand, a high-frequency direction injection (HFDI) module without trainable parameters is constructed to inject the high-frequency directional information of the original image into the network. On the other hand, a multi-scale direction-aware (MSDA) module is constructed, which promotes the full extraction of local relations at different scales and the full perception of key features in different directions. In addition, considering the characteristics of infrared small targets, we construct a feature aggregation (FA) structure to address target disappearance in high-level feature maps, and a feature calibration fusion (FCF) module to alleviate feature bias during cross-layer feature fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on multiple public datasets. The code can be available at this https URL ",
    "url": "https://arxiv.org/abs/2406.02037",
    "authors": [
      "Jinmiao Zhao",
      "Zelin Shi",
      "Chuang Yu",
      "Yunpeng Liu",
      "Xinyi Ying",
      "Yimian Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.01826",
    "title": "GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer",
    "abstract": "           Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.         ",
    "url": "https://arxiv.org/abs/2408.01826",
    "authors": [
      "Yihong Lin",
      "Zhaoxin Fan",
      "Xianjia Wu",
      "Lingyu Xiong",
      "Liang Peng",
      "Xiandong Li",
      "Wenxiong Kang",
      "Songju Lei",
      "Huang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.08236",
    "title": "Derivatives on Graphs for the Positive Calculus of Relations with Transitive Closure",
    "abstract": "           We prove that the equational theory of the positive calculus of relations with transitive closure (PCoR*) is EXPSPACE-complete. Here, PCoR* terms consist of the following standard operators on binary relations: identity, empty, universality, union, intersection, composition, converse, and reflexive transitive closure (so, PCoR* terms subsume Kleene algebra and allegory terms as fragments). Additionally, we show that the equational theory of PCoR* extended with tests and nominals (in hybrid logic) is still EXPSPACE-complete; moreover, it is PSPACE-complete for its intersection-free fragment. To this end, we design derivatives on graphs by extending derivatives on words for regular expressions. The derivatives give a finite automata construction on path decompositions, like those on words. Because the equational theory has a linearly bounded pathwidth model property, we can decide the equational theory of PCoR* using these automata.         ",
    "url": "https://arxiv.org/abs/2408.08236",
    "authors": [
      "Yoshiki Nakamura"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2408.09449",
    "title": "From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification",
    "abstract": "           In whole slide images (WSIs) analysis, attention-based multi-instance learning (MIL) models are susceptible to spurious correlations and degrade under domain shift. These methods may assign high attention weights to non-tumor regions, such as staining biases or artifacts, leading to unreliable tumor region localization. In this paper, we revisit max-pooling-based MIL methods from a causal perspective. Under mild assumptions, our theoretical results demonstrate that max-pooling encourages the model to focus on causal factors while ignoring bias-related factors. Furthermore, we discover that existing max-pooling-based methods may overfit the training set through rote memorization of instance features and fail to learn meaningful patterns. To address these issues, we propose FocusMIL, which couples max-pooling with an instance-level variational information bottleneck (VIB) to learn compact, predictive latent representations, and employs a multi-bag mini-batch scheme to stabilize optimization. We conduct comprehensive experiments on three real-world datasets and one semi-synthetic dataset. The results show that, by capturing causal factors, FocusMIL exhibits significant advantages in out-of-distribution scenarios and instance-level tumor region localization tasks.         ",
    "url": "https://arxiv.org/abs/2408.09449",
    "authors": [
      "Xin Liu",
      "Weijia Zhang",
      "Wei Tang",
      "Thuc Duy Le",
      "Jiuyong Li",
      "Lin Liu",
      "Min-Ling Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.14985",
    "title": "Image-Guided Semantic Pseudo-LiDAR Point Generation for 3D Object Detection",
    "abstract": "           In autonomous driving scenarios, accurate perception is becoming an even more critical task for safe navigation. While LiDAR provides precise spatial data, its inherent sparsity makes it difficult to detect small or distant objects. Existing methods try to address this by generating additional points within a Region of Interest (RoI), but relying on LiDAR alone often leads to false positives and a failure to recover meaningful structures. To address these limitations, we propose Image-Guided Semantic Pseudo-LiDAR Point Generation model, called ImagePG, a novel framework that leverages rich RGB image features to generate dense and semantically meaningful 3D points. Our framework includes an Image-Guided RoI Points Generation (IG-RPG) module, which creates pseudo-points guided by image features, and an Image-Aware Occupancy Prediction Network (I-OPN), which provides spatial priors to guide point placement. A multi-stage refinement (MR) module further enhances point quality and detection robustness. To the best of our knowledge, ImagePG is the first method to directly leverage image features for point generation. Extensive experiments on the KITTI and Waymo datasets demonstrate that ImagePG significantly improves the detection of small and distant objects like pedestrians and cyclists, reducing false positives by nearly 50%. On the KITTI benchmark, our framework improves mAP by +1.38%p (car), +7.91%p (pedestrian), and +5.21%p (cyclist) on the test set over the baseline, achieving state-of-the-art cyclist performance on the KITTI leaderboard. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2409.14985",
    "authors": [
      "Minseung Lee",
      "Seokha Moon",
      "Seung Joon Lee",
      "Reza Mahjourian",
      "Jinkyu Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.02031",
    "title": "Neural Eulerian Scene Flow Fields",
    "abstract": "           We reframe scene flow as the task of estimating a continuous space-time ODE that describes motion for an entire observation sequence, represented with a neural prior. Our method, EulerFlow, optimizes this neural prior estimate against several multi-observation reconstruction objectives, enabling high quality scene flow estimation via pure self-supervision on real-world data. EulerFlow works out-of-the-box without tuning across multiple domains, including large-scale autonomous driving scenes and dynamic tabletop settings. Remarkably, EulerFlow produces high quality flow estimates on small, fast moving objects like birds and tennis balls, and exhibits emergent 3D point tracking behavior by solving its estimated ODE over long-time horizons. On the Argoverse 2 2024 Scene Flow Challenge, EulerFlow outperforms all prior art, surpassing the next-best unsupervised method by more than 2.5x, and even exceeding the next-best supervised method by over 10%.         ",
    "url": "https://arxiv.org/abs/2410.02031",
    "authors": [
      "Kyle Vedder",
      "Neehar Peri",
      "Ishan Khatri",
      "Siyi Li",
      "Eric Eaton",
      "Mehmet Kocamaz",
      "Yue Wang",
      "Zhiding Yu",
      "Deva Ramanan",
      "Joachim Pehserl"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.06807",
    "title": "Wavehax: Aliasing-Free Neural Waveform Synthesis Based on 2D Convolution and Harmonic Prior for Reliable Complex Spectrogram Estimation",
    "abstract": "           Neural vocoders often struggle with aliasing in latent feature spaces, caused by time-domain nonlinear operations and resampling layers. Aliasing folds high-frequency components into the low-frequency range, making aliased and original frequency components indistinguishable and introducing two practical issues. First, aliasing complicates the waveform generation process, as the subsequent layers must address these aliasing effects, increasing the computational complexity. Second, it limits extrapolation performance, particularly in handling high fundamental frequencies, which degrades the perceptual quality of generated speech waveforms. This paper demonstrates that 1) time-domain nonlinear operations inevitably introduce aliasing but provide a strong inductive bias for harmonic generation, and 2) time-frequency-domain processing can achieve aliasing-free waveform synthesis but lacks the inductive bias for effective harmonic generation. Building on this insight, we propose Wavehax, an aliasing-free neural WAVEform generator that integrates 2D convolution and a HArmonic prior for reliable Complex Spectrogram estimation. Experimental results show that Wavehax achieves speech quality comparable to existing high-fidelity neural vocoders and exhibits exceptional robustness in scenarios requiring high fundamental frequency extrapolation, where aliasing effects become typically severe. Moreover, Wavehax requires less than 5% of the multiply-accumulate operations and model parameters compared to HiFi-GAN V1, while achieving over four times faster CPU inference speed.         ",
    "url": "https://arxiv.org/abs/2411.06807",
    "authors": [
      "Reo Yoneyama",
      "Atsushi Miyashita",
      "Ryuichi Yamamoto",
      "Tomoki Toda"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2411.10500",
    "title": "Edge-Only Universal Adversarial Attacks in Distributed Learning",
    "abstract": "           Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems, but may also introduce new vulnerabilities to evasion attacks, often in the form of adversarial perturbations. In this work, we present a new threat model that explores the feasibility of generating universal adversarial perturbations (UAPs) when the attacker has access only to the edge portion of the model, consisting of its initial network layers. Unlike traditional attacks that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud component by manipulating key feature representations at the edge. Following the proposed threat model, we introduce both edge-only untargeted and targeted formulations of UAPs designed to control intermediate features before the split point. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part, and we compare the proposed method with classical white-box and black-box techniques, highlighting its effectiveness. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effects with edge-only knowledge, revealing intriguing behaviors across multiple networks. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area.         ",
    "url": "https://arxiv.org/abs/2411.10500",
    "authors": [
      "Giulio Rossolini",
      "Tommaso Baldi",
      "Alessandro Biondi",
      "Giorgio Buttazzo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.14857",
    "title": "Image resizing by neural network operators and their convergence rate with respect to the $L^p$-norm and the dissimilarity index defined through the continuous SSIM",
    "abstract": "           In literature, several algorithms for imaging based on interpolation or approximation methods are available. The implementation of theoretical processes highlighted the necessity of providing theoretical frameworks for the convergence and error estimate analysis to support the experimental setups. In this paper, we establish new techniques for deriving quantitative estimates for the order of approximation for multivariate linear operators of the pointwise-type, with respect to the $L^p$-norm and to the so-called dissimilarity index defined through the continuous SSIM. In particular, we consider a family of approximation operators known as neural network (NN) operators, that have been widely studied in the last years in view of their connection with the theory of artificial neural networks. For these operators, we first establish sharp estimates in case of $C^1$ and piecewise (everywhere defined) $C^1$-functions. Then, the case of functions modeling digital images is considered, and specific quantitative estimates are achieved, including those with respect to the mentioned dissimilarity index. Moreover, the above analysis has also been extended to $L^p$-spaces, using a new constructive technique, in which the multivariate averaged modulus of smoothness has been employed. Finally, numerical experiments of image resizing have been given to support the theoretical results. The accuracy of the proposed algorithm has been evaluated through similarity indexes such as SSIM, likelihood index (S-index) and PSNR, and compared with other rescaling methods, including bilinear, bicubic, and upscaling-de la Vall\u00e9e-Poussin interpolation (u-VPI). Numerical simulations show the effectiveness of the proposed method for image processing tasks, particularly in terms of the aforementioned SSIM, and are consistent with the provided theoretical analysis.         ",
    "url": "https://arxiv.org/abs/2501.14857",
    "authors": [
      "Danilo Costarelli",
      "Mariarosaria Natale",
      "Michele Piconi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Functional Analysis (math.FA)"
    ]
  },
  {
    "id": "arXiv:2502.16205",
    "title": "A neural signed configuration distance function for path planning of picking manipulators",
    "abstract": "           Picking manipulators are task specific robots, with fewer degrees of freedom compared to general-purpose manipulators, and are heavily used in industry. The efficiency of the picking robots is highly dependent on the path planning solution, which is commonly done using sampling-based multi-query methods. The planner is robustly able to solve the problem, but its heavy use of collision-detection limits the planning capabilities for online use. We approach this problem by presenting a novel implicit obstacle representation for path planning, a neural signed configuration distance function (nSCDF), which allows us to form collision-free balls in the configuration space. We use the ball representation to re-formulate a state of the art multi-query path planner, i.e., instead of points, we use balls in the graph. Our planner returns a collision-free corridor, which allows us to use convex programming to produce optimized paths. From our numerical experiments, we observe that our planner produces paths that are close to those from an asymptotically optimal path planner, in significantly less time.         ",
    "url": "https://arxiv.org/abs/2502.16205",
    "authors": [
      "Bernhard Wullt",
      "Mikael Norrl\u00f6f",
      "Per Mattsson",
      "Thomas B. Sch\u00f6n"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2503.04504",
    "title": "AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM",
    "abstract": "           Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive results on VAD benchmarks, achieving state-of-the-art performance on UBnormal and UCF-Crime and surpassing other methods in generalization across all datasets. Our code is available online at this http URL.         ",
    "url": "https://arxiv.org/abs/2503.04504",
    "authors": [
      "Sunghyun Ahn",
      "Youngwan Jo",
      "Kijung Lee",
      "Sein Kwon",
      "Inpyo Hong",
      "Sanghyun Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.08485",
    "title": "Test-Time 3D Occupancy Prediction",
    "abstract": "           Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense occupancy decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and once trained, such models struggle to adapt to varying voxel resolutions or novel object categories without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-Occ. Our method incrementally constructs, optimizes and voxelizes time-aware 3D Gaussians from raw sensor streams by integrating vision foundation models (VFMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at arbitrary user-specified resolutions, while the generalization ability of VFMs enables accurate perception and open-vocabulary recognition, without any network training or fine-tuning. Specifically, TT-Occ operates in a lift-track-voxelize symphony: We first lift the geometry and semantics of surrounding-view extracted from VFMs to instantiate Gaussians at 3D space; Next, we track dynamic Gaussians while accumulating static ones to complete the scene and enforce temporal consistency; Finally, we voxelize the optimized Gaussians to generate occupancy prediction. Optionally, inherent noise in VFM predictions and tracking is mitigated by periodically smoothing neighboring Gaussians during optimization. To validate the generality and effectiveness of our framework, we offer two variants: one LiDAR-based and one vision-centric, and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying voxel resolutions.         ",
    "url": "https://arxiv.org/abs/2503.08485",
    "authors": [
      "Fengyi Zhang",
      "Xiangyu Sun",
      "Huitong Yang",
      "Zheng Zhang",
      "Zi Huang",
      "Yadan Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.14572",
    "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
    "abstract": "           The capacity of foundation models allows for their application to new, unseen tasks. The adaptation to such tasks is called transfer learning. An efficient transfer learning method that circumvents parameter optimization is imprinting. The conceptual differences between studies on imprinting form the basis of our systematic investigation. In this work, we propose the general \\texttt{IMPRINT} framework, identifying three main components: generation, normalization, and aggregation. Through the lens of this framework, we conduct an in-depth analysis and a comparison of the existing methods. Our findings reveal the benefits of representing novel data with multiple proxies in the generation step and show the importance of proper normalization. Beyond an extensive analytical grounding, our framework enables us to propose a novel variant of imprinting which outperforms previous work on transfer learning tasks by 4\\%. This variant determines proxies through clustering motivated by the neural collapse phenomenon -- a connection that we draw for the first time. We publicly release our code at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.14572",
    "authors": [
      "Justus Westerhoff",
      "Golzar Atefi",
      "Mario Koddenbrock",
      "Alexei Figueroa",
      "Alexander L\u00f6ser",
      "Erik Rodner",
      "Felix A. Gers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.16832",
    "title": "Joint Self-Supervised Video Alignment and Action Segmentation",
    "abstract": "           We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused Gromov-Wasserstein optimal transport formulation with a structural prior, which trains efficiently on GPUs and needs only a few iterations for solving the optimal transport problem. Our single-task method achieves the state-of-the-art performance on multiple video alignment benchmarks and outperforms VAVA, which relies on a traditional Kantorovich optimal transport formulation with an optimality prior. Furthermore, we extend our approach by proposing a unified optimal transport framework for joint self-supervised video alignment and action segmentation, which requires training and storing a single model and saves both time and memory consumption as compared to two different single-task models. Extensive evaluations on several video alignment and action segmentation datasets demonstrate that our multi-task method achieves comparable video alignment yet superior action segmentation results over previous methods in video alignment and action segmentation respectively. Finally, to the best of our knowledge, this is the first work to unify video alignment and action segmentation into a single model. Our code is available on our research website: this https URL.         ",
    "url": "https://arxiv.org/abs/2503.16832",
    "authors": [
      "Ali Shah Ali",
      "Syed Ahmed Mahmood",
      "Mubin Saeed",
      "Andrey Konin",
      "M. Zeeshan Zia",
      "Quoc-Huy Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.22506",
    "title": "Design and Analysis of a Robust Control System for Triple Inverted Pendulum Stabilization",
    "abstract": "           The design of robust controllers for triple inverted pendulum systems presents significant challenges due to their inherent instability and nonlinear dynamics. Furthermore, uncertainties in system parameters further complicate the control design. This paper investigates a robust control strategy for triple inverted pendulums under parameter uncertainty. Two control approaches, namely the $H_\\infty$ controller and the $\\mu$-synthesis controller, are compared in terms of their ability to achieve reference tracking and disturbance rejection. Simulation results demonstrate that the $H_\\infty$ controller provides superior transient performance, making it a promising solution for the robust stabilization of such complex systems.         ",
    "url": "https://arxiv.org/abs/2503.22506",
    "authors": [
      "Tohid Kargar Tasooji",
      "Sakineh Khodadadi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.11081",
    "title": "DPS: Design Pattern Summarisation Using Code Features",
    "abstract": "           Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been employed to generate these summaries. Summarising software design patterns is important because it helps developers quickly understand and reuse complex design concepts, thereby improving software maintainability and development efficiency. However, the generation of summaries for software design patterns has not yet been explored. Our approach utilises code features and JavaParser to parse the code and create a JSON representation. Using an NLG library on this JSON representation, we convert it into natural language text that acts as a summary of the code, capturing the contextual information of the design pattern. Our empirical results indicate that the summaries generated by our approach capture the context in which patterns are applied in the codebase. Statistical evaluations demonstrate that our summaries closely align with human-written summaries, as evident from high values in the ROUGE-L, BLEU-4, NIST, and FrugalScore metrics. A follow-up survey further shows that DPS summaries were rated as capturing context better than human-generated summaries. Additionally, a time based task activity shows that summaries increase the time of understanding of design pattern for developer better than when the summaries are not present.         ",
    "url": "https://arxiv.org/abs/2504.11081",
    "authors": [
      "Najam Nazar",
      "Sameer Sikka",
      "Christoph Treude"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.01309",
    "title": "Enhancing SPARQL Query Rewriting for Complex Ontology Alignments",
    "abstract": "           SPARQL query rewriting is a fundamental mechanism for uniformly querying heterogeneous ontologies in the Linked Data Web. However, the complexity of ontology alignments, particularly rich correspondences (c : c), makes this process challenging. Existing approaches primarily focus on simple (s : s) and partially complex ( s : c) alignments, thereby overlooking the challenges posed by more expressive alignments. Moreover, the intricate syntax of SPARQL presents a barrier for non-expert users seeking to fully exploit the knowledge encapsulated in ontologies. This article proposes an innovative approach for the automatic rewriting of SPARQL queries from a source ontology to a target ontology, based on a user's need expressed in natural language. It leverages the principles of equivalence transitivity as well as the advanced capabilities of large language models such as GPT-4. By integrating these elements, this approach stands out for its ability to efficiently handle complex alignments, particularly (c : c) correspondences , by fully exploiting their expressiveness. Additionally, it facilitates access to aligned ontologies for users unfamiliar with SPARQL, providing a flexible solution for querying heterogeneous data.         ",
    "url": "https://arxiv.org/abs/2505.01309",
    "authors": [
      "Anicet Lepetit Ondo",
      "Laurence Capus",
      "Mamadou Bousso"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.11539",
    "title": "Lur\u00e9-Postnikov Stability Analysis of Closed-Loop Control Systems with Gated Recurrent Neural Network-based Virtual Sensors",
    "abstract": "           This article addresses certification of closed-loop stability when a virtual-sensor based on a gated recurrent neural network operates in the feedback path of a nonlinear control system. The Hadamard gating used in standard GRU/LSTM cells is shown to violate the Lur\u00e9-Postnikov Lyapunov conditions of absolute-stability theory, leading to conservative analysis. To overcome this limitation, a modified architecture-termed the Lur\u00e9-Postnikov gated recurrent neural network (LP-GRNN)-is proposed; its affine update law is compatible with the Lur\u00e9-Postnikov framework while matching the prediction accuracy of vanilla GRU/LSTM models on the NASA CMAPSS benchmark. Embedding the LP-GRNN, the plant, and a saturated PI controller in a unified standard nonlinear operator form (SNOF) reduces the stability problem to a compact set of tractable linear matrix inequalities (LMIs) whose feasibility certifies global asymptotic stability. A linearized boiler case study illustrates the workflow and validates the closed-loop performance, thereby bridging modern virtual-sensor design with formal stability guarantees.         ",
    "url": "https://arxiv.org/abs/2505.11539",
    "authors": [
      "Eric Hilgert",
      "Andreas Schwung"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.15721",
    "title": "Privacy-Preserving Conformal Prediction Under Local Differential Privacy",
    "abstract": "           Conformal prediction (CP) provides sets of candidate classes with a guaranteed probability of containing the true class. However, it typically relies on a calibration set with clean labels. We address privacy-sensitive scenarios where the aggregator is untrusted and can only access a perturbed version of the true labels. We propose two complementary approaches under local differential privacy (LDP). In the first approach, users do not access the model but instead provide their input features and a perturbed label using a k-ary randomized response. In the second approach, which enforces stricter privacy constraints, users add noise to their conformity score by binary search response. This method requires access to the classification model but preserves both data and label privacy. Both approaches compute the conformal threshold directly from noisy data without accessing the true labels. We prove finite-sample coverage guarantees and demonstrate robust coverage even under severe randomization. This approach unifies strong local privacy with predictive uncertainty control, making it well-suited for sensitive applications such as medical imaging or large language model queries, regardless of whether users can (or are willing to) compute their own scores.         ",
    "url": "https://arxiv.org/abs/2505.15721",
    "authors": [
      "Coby Penso",
      "Bar Mahpud",
      "Jacob Goldberger",
      "Or Sheffet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.16188",
    "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models",
    "abstract": "           Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but controlling their behavior reliably remains challenging, especially in open-ended generation settings. This paper introduces a novel supervised steering approach that operates in sparse, interpretable representation spaces. We employ sparse autoencoders (SAEs) to obtain sparse latent representations that aim to disentangle semantic attributes from model activations. Then we train linear classifiers to identify a small subspace of task-relevant dimensions in latent representations. Finally, we learn supervised steering vectors constrained to this subspace, optimized to align with target behaviors. Experiments across sentiment, truthfulness, and political polarity steering tasks with multiple LLMs demonstrate that our supervised steering vectors achieve higher success rates with minimal degradation in generation quality compared to existing methods. Further analysis reveals that a notably small subspace is sufficient for effective steering, enabling more targeted and interpretable interventions. Our implementation is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.16188",
    "authors": [
      "Zirui He",
      "Mingyu Jin",
      "Bo Shen",
      "Ali Payani",
      "Yongfeng Zhang",
      "Mengnan Du"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.23471",
    "title": "Synthesizing Performance Constraints for Evaluating and Improving Code Efficiency",
    "abstract": "           Large Language Models (LLMs) have been increasingly used to optimize code efficiency. Evaluating their effectiveness and further suggesting optimization opportunities often rely on high-quality tests to demonstrate the performance bottlenecks presented in the program. However, existing approaches rely on a limited set of hand-curated inputs or LLM-generated uninteresting length-stressing tests, failing to reveal more nuanced optimization opportunities. We present WEDGE, a framework for generating performance-stressing input given the program under test. WEDGE synthesizes explicit performance-characterizing constraints in the form of branch conditions to partition the programs' execution space into performance-specific regions. When integrated with the coverage-guided fuzzer, reaching different regions introduces explicit rewards for test generation to explore inefficient implementations. Our evaluation shows that WEDGE introduces a significant slowdown compared to the tests in CodeContests and those claimed to be optimized by existing approaches. From the utility perspective, integrating our tests substantially improves the existing code optimization approaches that rely on test-driven execution feedback. We release PERFFORGE, the performance tests generated by WEDGE, to benchmark future approaches for efficient code generation at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.23471",
    "authors": [
      "Jun Yang",
      "Cheng-Chi Wang",
      "Bogdan Alexandru Stoica",
      "Kexin Pei"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.02738",
    "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning",
    "abstract": "           In biomedical vision-language modeling, datasets are typically mined from scientific literature, pairing compound figures with captions that are short, context-dependent, and oftern partially informative. Prior work on subfigure extraction has been limited in both dataset size and generalizability. In addition, no existing effort has incorporated rich medical context in image-text pairs. We revisit data curation as a foundational component of effective biomedical representation learning. Our data curation process integrates transformer-based subfigure detection, subcaption extraction, and contextual text enrichment derived from inline references. Our subfigure extraction model, trained on a corpus of 500,000 compound figures, achieves state-of-the-art performance on real and synthetic benchmarks. Using this process, we curate and release Open-PMC-18M, a large-scale high-fidelity biomedical dataset comprising 18 million image-text pairs, spanning radiology, microscopy, and visible light photography. We train vision-language models on our dataset and perform extensive evaluation on 6 retrieval and 19 zero-shot classification tasks across three major modalities. The models trained on our dataset set a new state-of-the-art results in medical representation learning. We release our dataset, models, and code to support reproducible benchmarks and further study into biomedical vision-language modeling and representation learning.         ",
    "url": "https://arxiv.org/abs/2506.02738",
    "authors": [
      "Negin Baghbanzadeh",
      "Mohammed Saidul Islam",
      "Sajad Ashkezari",
      "Elham Dolatabadi",
      "Arash Afkanpour"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.03761",
    "title": "Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services",
    "abstract": "           As interest in using Large Language Models for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.         ",
    "url": "https://arxiv.org/abs/2506.03761",
    "authors": [
      "Hongcheng Guo",
      "Zheyong Xie",
      "Shaosheng Cao",
      "Boyang Wang",
      "Weiting Liu",
      "Zheyu Ye",
      "Zhoujun Li",
      "Zuozhu Liu",
      "Wei Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.04713",
    "title": "Enabling Validation for Robust Few-Shot Recognition",
    "abstract": "           Few-Shot Recognition (FSR) tackles classification tasks by training with minimal task-specific labeled data. Prevailing methods adapt or finetune a pretrained Vision-Language Model (VLM) and augment the scarce training data by retrieving task-relevant but noisy samples from open data sources. The finetuned VLM generalizes decently well to the task-specific in-distribution (ID) test data but struggles with out-of-distribution (OOD) test data. This motivates our study of robust FSR with VLM finetuning. The core challenge of FSR is data scarcity, extending beyond limited training data to a complete lack of validation data. We identify a key paradox as a potential solution: repurposing the retrieved open data for validation. As such retrieved data are inherently OOD compared with the task-specific ID training data, finetuned VLMs yield degraded performance on the retrieved data. This causes the validation logic to favor the pretrained model without any finetuning, hindering improvements w.r.t generalization. To resolve this dilemma, we introduce a novel validation strategy that harmonizes performance gain and degradation on the few-shot ID data and the retrieved data, respectively. Our validation enables parameter selection for partial finetuning and checkpoint selection, mitigating overfitting and improving test-data generalization. We unify this strategy with robust learning into a cohesive framework: Validation-Enabled Stage-wise Tuning (VEST). Extensive experiments on the established ImageNet OOD benchmarks show that VEST significantly outperforms existing VLM adaptation methods, achieving state-of-the-art FSR performance on both ID and OOD data.         ",
    "url": "https://arxiv.org/abs/2506.04713",
    "authors": [
      "Hanxin Wang",
      "Tian Liu",
      "Shu Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.17162",
    "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
    "abstract": "           Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability. To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07% on baseline dataset compared to state-of-the-art PDF malware classifiers.         ",
    "url": "https://arxiv.org/abs/2506.17162",
    "authors": [
      "Side Liu",
      "Jiang Ming",
      "Guodong Zhou",
      "Xinyi Liu",
      "Jianming Fu",
      "Guojun Peng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.08355",
    "title": "scE2TM improves single-cell embedding interpretability and reveals cellular perturbation signatures",
    "abstract": "           Single-cell RNA sequencing technologies have revolutionized our understanding of cellular heterogeneity, yet computational methods often struggle to balance performance with biological interpretability. Embedded topic models have been widely used for interpretable single-cell embedding learning. However, these models suffer from the potential problem of interpretation collapse, where topics semantically collapse towards each other, resulting in redundant topics and incomplete capture of biological variation. Furthermore, the rise of single-cell foundation models creates opportunities to harness external biological knowledge for guiding model embeddings. Here, we present scE2TM, an external knowledge-guided embedded topic model that provides a high-quality cell embedding and interpretation for scRNA-seq analysis. Through embedding clustering regularization method, each topic is constrained to be the center of a separately aggregated gene cluster, enabling it to capture unique biological information. Across 20 scRNA-seq datasets, scE2TM achieves superior clustering performance compared with seven state-of-the-art methods. A comprehensive interpretability benchmark further shows that scE2TM-learned topics exhibit higher diversity and stronger consistency with underlying biological pathways. Modeling interferon-stimulated PBMCs, scE2TM simulates topic perturbations that drive control cells toward stimulated-like transcriptional states, faithfully mirroring experimental interferon responses. In melanoma, scE2TM identifies malignant-specific topics and extrapolates them to unseen patient data, revealing gene programs associated with patient survival.         ",
    "url": "https://arxiv.org/abs/2507.08355",
    "authors": [
      "Hegang Chen",
      "Yuyin Lu",
      "Yifan Zhao",
      "Zhiming Dai",
      "Fu Lee Wang",
      "Qing Li",
      "Yanghui Rao",
      "Yue Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.13326",
    "title": "A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains",
    "abstract": "           Hand-object interaction detection remains an open challenge in real-time applications, where intuitive user experiences depend on fast and accurate detection of interactions with surrounding objects. We propose an efficient approach for detecting hand-objects interactions from streaming egocentric vision that operates in real time. Our approach consists of an action recognition module and an object detection module for identifying active objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object. We implement our models in a cascaded architecture where the action recognition and object detection modules operate sequentially. When the action recognition predicts a contact state, it activates the object detection module, which in turn performs inference on the relevant frame to detect and classify the active object.         ",
    "url": "https://arxiv.org/abs/2507.13326",
    "authors": [
      "Antonio Finocchiaro",
      "Alessandro Sebastiano Catinello",
      "Michele Mazzamuto",
      "Rosario Leonardi",
      "Antonino Furnari",
      "Giovanni Maria Farinella"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.15663",
    "title": "SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models",
    "abstract": "           Background: Text-to-image generation models are widely used across numerous domains. Among these models, Stable Diffusion (SD) - an open-source text-to-image generation model - has become the most popular, producing over 12 billion images annually. However, the widespread use of these models raises concerns regarding their social and environmental sustainability. Aims: To reduce the harm that SD models may have on society and the environment, we introduce SustainDiffusion, a search-based approach designed to enhance the social and environmental sustainability of SD models. Method: SustainDiffusion searches the optimal combination of hyperparameters and prompt structures that can reduce gender and ethnic bias in generated images while also lowering the energy consumption required for image generation. Importantly, SustainDiffusion maintains image quality comparable to that of the original SD model. Results: We conduct a comprehensive empirical evaluation of SustainDiffusion, testing it against six different baselines using 56 different prompts. Our results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%, ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are consistent across multiple runs and can be generalised to various prompts. Conclusions: With SustainDiffusion, we demonstrate how enhancing the social and environmental sustainability of text-to-image generation models is possible without fine-tuning or changing the model's architecture.         ",
    "url": "https://arxiv.org/abs/2507.15663",
    "authors": [
      "Giordano d'Aloisio",
      "Tosin Fadahunsi",
      "Jay Choy",
      "Rebecca Moussa",
      "Federica Sarro"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.17665",
    "title": "Perspective-Invariant 3D Object Detection",
    "abstract": "           With the rise of robotics, LiDAR-based 3D object detection has garnered significant attention in both academia and industry. However, existing datasets and methods predominantly focus on vehicle-mounted platforms, leaving other autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET, the first benchmark featuring LiDAR data and 3D bounding box annotations collected from multiple platforms: vehicle, quadruped, and drone, thereby facilitating research in 3D object detection for non-vehicle platforms as well as cross-platform 3D detection. Based on Pi3DET, we propose a novel cross-platform adaptation framework that transfers knowledge from the well-studied vehicle platform to other platforms. This framework achieves perspective-invariant 3D detection through robust alignment at both geometric and feature levels. Additionally, we establish a benchmark to evaluate the resilience and robustness of current 3D detectors in cross-platform scenarios, providing valuable insights for developing adaptive 3D perception systems. Extensive experiments validate the effectiveness of our approach on challenging cross-platform tasks, demonstrating substantial gains over existing adaptation methods. We hope this work paves the way for generalizable and unified 3D perception systems across diverse and complex environments. Our Pi3DET dataset, cross-platform benchmark suite, and annotation toolkit have been made publicly available.         ",
    "url": "https://arxiv.org/abs/2507.17665",
    "authors": [
      "Ao Liang",
      "Lingdong Kong",
      "Dongyue Lu",
      "Youquan Liu",
      "Jian Fang",
      "Huaici Zhao",
      "Wei Tsang Ooi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.20423",
    "title": "CodeNER: Code Prompting for Named Entity Recognition",
    "abstract": "           Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.         ",
    "url": "https://arxiv.org/abs/2507.20423",
    "authors": [
      "Sungwoo Han",
      "Jingun Kwon",
      "Hidetaka Kamigaito",
      "Manabu Okumura"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.03136",
    "title": "Distributionally Robust Markov Games with Average Reward",
    "abstract": "           We study distributionally robust Markov games (DR-MGs) with the average-reward criterion, a framework for multi-agent decision-making under uncertainty over extended horizons. In average reward DR-MGs, agents aim to maximize their worst-case infinite-horizon average reward, to ensure satisfactory performance under environment uncertainties and opponent actions. We first establish a connection between the best-response policies and the optimal policies for the induced single-agent problems. Under a standard irreducible assumption, we derive a correspondence between the optimal policies and the solutions of the robust Bellman equation, and derive the existence of stationary Nash Equilibrium (NE) based on these results. We further study DR-MGs under the weakly communicating setting, where we construct a set-valued map and show its value is a subset of the best-response policies, convex and upper hemi-continuous, and derive the existence of NE. We then explore algorithmic solutions, by first proposing a Robust Nash-Iteration algorithm and providing convergence guarantees under some additional assumptions and a NE computing oracle. We further develop a temporal-difference based algorithm for DR-MGs, and provide convergence guarantees without any additional oracle or assumptions. Finally, we connect average-reward robust NE to discounted ones, showing that the average reward robust NE can be approximated by the discounted ones under a large discount factor. Our studies provide a comprehensive theoretical and algorithmic foundation for decision-making in complex, uncertain, and long-running multi-player environments.         ",
    "url": "https://arxiv.org/abs/2508.03136",
    "authors": [
      "Zachary Roch",
      "Yue Wang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.20934",
    "title": "Finding happiness by evolutionary algorithms",
    "abstract": "           A recent line of research concerns the problem of soft happy colouring (SHC), which requires that a partially coloured graph be extended to a complete colouring to maximise local agreements, so that as many vertices as possible end up surrounded by enough same-coloured neighbours. It is already known that SHC is NP-hard, and its solutions have a direct relationship with the community structure of networks; thus, it has immense applications in security and resilience. Past studies have shown that local search approaches can be fast and effective to an extent on the SHC; however, they often get stuck in local optima. Regarding the related problem of maximising happy vertices, evolutionary approaches have been proven effective; hence, this study develops a customised memetic algorithm that is a hybrid of genetic algorithms and local search. The experimental evaluation on a range of graphs in the stochastic block model shows that the memetic algorithm can achieve excellent results in search for an optimised solution to SHC compared to the local search approaches and standard genetic algorithms. Moreover, learning and evolution in the memetic algorithm allow diversification of solutions generated by fast, effective local search approaches, which prove superior for the challenging problem of community detection.         ",
    "url": "https://arxiv.org/abs/2508.20934",
    "authors": [
      "Mohammad Hadi Shekarriz",
      "Dhananjay Thiruvady",
      "Asef Nazari"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2509.01183",
    "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
    "abstract": "           High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2509.01183",
    "authors": [
      "Bingnan Yang",
      "Mi Zhang",
      "Zhili Zhang",
      "Zhan Zhang",
      "Yuanxin Zhao",
      "Xiangyun Hu",
      "Jianya Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.05835",
    "title": "Yours or Mine? Overwriting Attacks Against Neural Audio Watermarking",
    "abstract": "           As generative audio models are rapidly evolving, AI-generated audios increasingly raise concerns about copyright infringement and misinformation spread. Audio watermarking, as a proactive defense, can embed secret messages into audio for copyright protection and source verification. However, current neural audio watermarking methods focus primarily on the imperceptibility and robustness of watermarking, while ignoring its vulnerability to security attacks. In this paper, we develop a simple yet powerful attack: the overwriting attack that overwrites the legitimate audio watermark with a forged one and makes the original legitimate watermark undetectable. Based on the audio watermarking information that the adversary has, we propose three categories of overwriting attacks, i.e., white-box, gray-box, and black-box attacks. We also thoroughly evaluate the proposed attacks on state-of-the-art neural audio watermarking methods. Experimental results demonstrate that the proposed overwriting attacks can effectively compromise existing watermarking schemes across various settings and achieve a nearly 100% attack success rate. The practicality and effectiveness of the proposed overwriting attacks expose security flaws in existing neural audio watermarking systems, underscoring the need to enhance security in future audio watermarking designs.         ",
    "url": "https://arxiv.org/abs/2509.05835",
    "authors": [
      "Lingfeng Yao",
      "Chenpei Huang",
      "Shengyao Wang",
      "Junpei Xue",
      "Hanqing Guo",
      "Jiang Liu",
      "Phone Lin",
      "Tomoaki Ohtsuki",
      "Miao Pan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2509.11536",
    "title": "HARP: Hallucination Detection via Reasoning Subspace Projection",
    "abstract": "           Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.         ",
    "url": "https://arxiv.org/abs/2509.11536",
    "authors": [
      "Junjie Hu",
      "Gang Tu",
      "ShengYu Cheng",
      "Jinxin Li",
      "Jinting Wang",
      "Rui Chen",
      "Zhilong Zhou",
      "Dongbo Shan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2509.15239",
    "title": "KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems",
    "abstract": "           Neural algorithmic reasoning (NAR) is a growing field that aims to embed algorithmic logic into neural networks by imitating classical algorithms. In this extended abstract, we detail our attempt to build a neural algorithmic reasoner that can solve Knapsack, a pseudo-polynomial problem bridging classical algorithms and combinatorial optimisation, but omitted in standard NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow the two-phase pipeline for the Knapsack problem, which involves first constructing the dynamic programming table and then reconstructing the solution from it. The approach, which models intermediate states through dynamic programming supervision, achieves better generalization to larger problem instances than a direct-prediction baseline that attempts to select the optimal subset only from the problem inputs.         ",
    "url": "https://arxiv.org/abs/2509.15239",
    "authors": [
      "Stjepan Po\u017egaj",
      "Dobrik Georgiev",
      "Marin \u0160ili\u0107",
      "Petar Veli\u010dkovi\u0107"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2509.16054",
    "title": "Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model",
    "abstract": "           Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level <ACT> token and multiple cluster-specific <GROUP> tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP> tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the <ACT> token's ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM's hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.         ",
    "url": "https://arxiv.org/abs/2509.16054",
    "authors": [
      "Jihua Peng",
      "Qianxiong Xu",
      "Yichen Liu",
      "Chenxi Liu",
      "Cheng Long",
      "Rui Zhao",
      "Ziyue Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2509.16158",
    "title": "Designing Culturally Aligned AI Systems For Social Good in Non-Western Contexts",
    "abstract": "           AI technologies are increasingly deployed in high-stakes domains such as education, healthcare, law, and agriculture to address complex challenges in non-Western contexts. This paper examines eight real-world deployments spanning seven countries and 18 languages, combining 17 interviews with AI developers and domain experts with secondary research. Our findings identify six cross-cutting factors - Language, Institution, Safety, Task, End-User Demography, and Domain - that structured how systems were designed and deployed. These factors were shaped by Sociocultural (diversity, practices), Institutional (resources, policies), and Technological (capabilities, limits) influences. We find that building effective AI systems required extensive collaboration between AI developers and domain experts, with human resources proving more critical to achieving safe and effective outcomes in high-stakes domains than technological expertise alone. Additionally, we present 12 guidelines synthesizing these dynamics for designing AI for social good systems that are culturally grounded, equitable, and responsive to the needs of non-Western contexts.         ",
    "url": "https://arxiv.org/abs/2509.16158",
    "authors": [
      "Deepak Varuvel Dennison",
      "Mohit Jain",
      "Tanuja Ganu",
      "Aditya Vashistha"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2509.16625",
    "title": "Self-Supervised Learning of Graph Representations for Network Intrusion Detection",
    "abstract": "           Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.         ",
    "url": "https://arxiv.org/abs/2509.16625",
    "authors": [
      "Lorenzo Guerra",
      "Thomas Chapuis",
      "Guillaume Duc",
      "Pavlo Mozharovskyi",
      "Van-Tam Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2509.22161",
    "title": "Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization",
    "abstract": "           Vector quantization, which discretizes a continuous vector space into a finite set of representative vectors (a codebook), has been widely adopted in modern machine learning. Despite its effectiveness, vector quantization poses a fundamental challenge: the non-differentiable quantization step blocks gradient backpropagation. Smoothed vector quantization addresses this issue by relaxing the hard assignment of a codebook vector into a weighted combination of codebook entries, represented as the matrix product of a simplex vector and the codebook. Effective smoothing requires two properties: (1) smoothed quantizers should remain close to a onehot vector, ensuring tight approximation, and (2) all codebook entries should be utilized, preventing code collapse. Existing methods typically address these desiderata separately. By contrast, the present study introduces a simple and intuitive regularization that promotes both simultaneously by minimizing the distance between each simplex vertex and its $K$-nearest smoothed quantizers. Experiments on representative benchmarks, including discrete image autoencoding and contrastive speech representation learning, demonstrate that the proposed method achieves more reliable codebook utilization and improves performance compared to prior approaches.         ",
    "url": "https://arxiv.org/abs/2509.22161",
    "authors": [
      "Takashi Morita"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00080",
    "title": "SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction",
    "abstract": "           Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00080",
    "authors": [
      "Hanze Guo",
      "Yijun Ma",
      "Xiao Zhou"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2510.00769",
    "title": "ZQBA: Zero Query Black-box Adversarial Attack",
    "abstract": "           Current black-box adversarial attacks either require multiple queries or diffusion models to produce adversarial samples that can impair the target model performance. However, these methods require training a surrogate loss or diffusion models to produce adversarial samples, which limits their applicability in real-world settings. Thus, we propose a Zero Query Black-box Adversarial (ZQBA) attack that exploits the representations of Deep Neural Networks (DNNs) to fool other networks. Instead of requiring thousands of queries to produce deceiving adversarial samples, we use the feature maps obtained from a DNN and add them to clean images to impair the classification of a target model. The results suggest that ZQBA can transfer the adversarial samples to different models and across various datasets, namely CIFAR and Tiny ImageNet. The experiments also show that ZQBA is more effective than state-of-the-art black-box attacks with a single query, while maintaining the imperceptibility of perturbations, evaluated both quantitatively (SSIM) and qualitatively, emphasizing the vulnerabilities of employing DNNs in real-world contexts. All the source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2510.00769",
    "authors": [
      "Joana C. Costa",
      "Tiago Roxo",
      "Hugo Proen\u00e7a",
      "Pedro R. M. In\u00e1cio"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2510.01472",
    "title": "LLM-NAS: LLM-driven Hardware-Aware Neural Architecture Search",
    "abstract": "           Hardware-Aware Neural Architecture Search (HW-NAS) requires joint optimization of accuracy and latency under device constraints. Traditional supernet-based methods require multiple GPU days per dataset. Large Language Model (LLM)-driven approaches avoid training a large supernet and can provide quick feedback, but we observe an exploration bias: the LLM repeatedly proposes neural network designs within limited search space and fails to discover architectures across different latency ranges in the entire search space. To address this issue, we propose LLM-NAS: an LLM-driven Neural Architecture Search that can generate neural networks with high accuracy and low latency with reduced search cost. Our proposed LLM-NAS has three key components: 1) a complexity-driven partitioning engine that divides the search space by complexity to enforce diversity and mitigate exploration bias; 2) an LLM-powered architecture prompt co-evolution operator, in which the LLM first updates a knowledge base of design heuristics based on results from the previous round, then performs a guided evolution algorithm on architectures with prompts that incorporate this knowledge base. Prompts and designs improve together across rounds which avoids random guesswork and improve efficiency; 3) a zero-cost predictor to avoid training a large number of candidates from scratch. Experimental results show that on HW-NAS-Bench, LLM-NAS can achieve overall higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy. Meanwhile, the search cost drops from days to minutes compared with traditional supernet baselines.         ",
    "url": "https://arxiv.org/abs/2510.01472",
    "authors": [
      "Hengyi Zhu",
      "Grace Li Zhang",
      "Shaoyi Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.09114",
    "title": "Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks",
    "abstract": "           Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.         ",
    "url": "https://arxiv.org/abs/2511.09114",
    "authors": [
      "Tim Dudman",
      "Martyn Bull"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.11928",
    "title": "Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks",
    "abstract": "           Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.         ",
    "url": "https://arxiv.org/abs/2511.11928",
    "authors": [
      "Ziyao Cui",
      "Edric Tam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.12208",
    "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Reasoning Framework for Incomplete Knowledge Graph Question Answering",
    "abstract": "           Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2511.12208",
    "authors": [
      "Jilong Liu",
      "Pengyang Shao",
      "Wei Qin",
      "Fei Liu",
      "Yonghui Yang",
      "Richang Hong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.14084",
    "title": "Observational Auditing of Label Privacy",
    "abstract": "           Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.         ",
    "url": "https://arxiv.org/abs/2511.14084",
    "authors": [
      "Iden Kalemaj",
      "Luca Melis",
      "Maxime Boucher",
      "Ilya Mironov",
      "Saeed Mahloujifar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2511.14174",
    "title": "A Longitudinal Study on the Attitudes of Gay Men in Beijing Towards Gay Social Media Platforms: Lonely Souls in the Digital Concrete Jungle",
    "abstract": "           Over the past decade, specialized social networking applications have become a cornerstone of life for many gay men in China. This paper employs a longitudinal mixed-methods approach to investigate how Chinese men who have sex with men (MSM) have shifted their attitudes toward these platforms between approximately 2013 and 2023. Drawing on archival analysis of online discourses, a quantitative survey of 412 participants, and in-depth semi-structured interviews with 32 participants, we trace the complex trajectory of this evolution. Our findings reveal a clear pattern: from the initial embrace of these applications as revolutionary tools for community building and identity affirmation (2014--2017), to a period of growing ambivalence and critique centered on commercialization, ``hookup culture,'' and multiple forms of discrimination (2017--2020), and finally to the present era (2020--2023), characterized by pragmatic, fragmented, yet simultaneously critical and reconstructive uses. Today, users strategically employ a repertoire of applications -- including global platforms (e.g., Grindr and Tinder), domestic mainstream platforms (e.g., Blued), and niche alternatives (e.g., Aloha) -- to fulfill differentiated needs. We develop a detailed temporal framework to capture this attitudinal evolution and discuss its design implications for creating more supportive, secure, and community-oriented digital environments for marginalized groups.         ",
    "url": "https://arxiv.org/abs/2511.14174",
    "authors": [
      "Yibo Meng",
      "Xiaolan Ding",
      "Lyumanshan Ye",
      "Zhiming Liu",
      "Yan Guan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2511.18322",
    "title": "Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video",
    "abstract": "           Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.         ",
    "url": "https://arxiv.org/abs/2511.18322",
    "authors": [
      "Henrik Krauss",
      "Johann Licher",
      "Naoya Takeishi",
      "Annika Raatz",
      "Takehisa Yairi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.19907",
    "title": "MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition",
    "abstract": "           This paper employs a multimodal approach for continuous sign recognition by first using ML for detecting the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then by recognizing the segmented signs. For improved robustness we use 3D skeletal features extracted from sign language videos to take into account the convergence of sign properties and their dynamics that tend to cluster at sign boundaries. Another focus of this paper is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for detection of 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing-as such signs often differ a bit in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.         ",
    "url": "https://arxiv.org/abs/2511.19907",
    "authors": [
      "Mingyu Zhao",
      "Zhanfu Yang",
      "Yang Zhou",
      "Zhaoyang Xia",
      "Can Jin",
      "Xiaoxiao He",
      "Dimitris N. Metaxas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2511.21740",
    "title": "Decoding inner speech with an end-to-end brain-to-text neural interface",
    "abstract": "           Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.         ",
    "url": "https://arxiv.org/abs/2511.21740",
    "authors": [
      "Yizi Zhang",
      "Linyang He",
      "Chaofei Fan",
      "Tingkai Liu",
      "Han Yu",
      "Trung Le",
      "Jingyuan Li",
      "Scott Linderman",
      "Lea Duncker",
      "Francis R Willett",
      "Nima Mesgarani",
      "Liam Paninski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2511.21962",
    "title": "Communication-Aware Dissipative Control for Networks of Heterogeneous Nonlinear Agents",
    "abstract": "           Communication-aware control is essential to reduce costs and complexity in large-scale networks. However, it is challenging to simultaneously determine a sparse communication topology and achieve high performance and robustness. This work achieves all three objectives through dissipativity-based, sparsity-promoting controller synthesis. The approach identifies an optimal sparse structure using either weighted l1 penalties or alternating direction methods of multipliers (ADMM) with a cardinality term, and iteratively solves a convexified version of the NP hard structured optimal control problem. The proposed methods are demonstrated on heterogeneous networks with uncertain and unstable agents.         ",
    "url": "https://arxiv.org/abs/2511.21962",
    "authors": [
      "Ingyu Jang",
      "Leila J. Bridgeman"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2512.00713",
    "title": "Concept-Guided Backdoor Attack on Vision Language Models",
    "abstract": "           Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing \"cat\" with \"dog\"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.         ",
    "url": "https://arxiv.org/abs/2512.00713",
    "authors": [
      "Haoyu Shen",
      "Weimin Lyu",
      "Haotian Xu",
      "Tengfei Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2512.01030",
    "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
    "abstract": "           Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.         ",
    "url": "https://arxiv.org/abs/2512.01030",
    "authors": [
      "Jing He",
      "Haodong Li",
      "Mingzhi Sheng",
      "Ying-Cong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2512.01678",
    "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
    "abstract": "           Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.         ",
    "url": "https://arxiv.org/abs/2512.01678",
    "authors": [
      "Anubhab",
      "Rupesh Nasre"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2512.02002",
    "title": "LLM-Driven Corrective Robot Operation Code Generation with Static Text-Based Simulation",
    "abstract": "           Recent advances in Large language models (LLMs) have demonstrated their promising capabilities of generating robot operation code to enable LLM-driven robots. To enhance the reliability of operation code generated by LLMs, corrective designs with feedback from the observation of executing code have been increasingly adopted in existing research. However, the code execution in these designs relies on either a physical experiment or a customized simulation environment, which limits their deployment due to the high configuration effort of the environment and the potential long execution time. In this paper, we explore the possibility of directly leveraging LLM to enable static simulation of robot operation code, and then leverage it to design a new reliable LLM-driven corrective robot operation code generation framework. Our framework configures the LLM as a static simulator with enhanced capabilities that reliably simulate robot code execution by interpreting actions, reasoning over state transitions, analyzing execution outcomes, and generating semantic observations that accurately capture trajectory dynamics. To validate the performance of our framework, we performed experiments on various operation tasks for different robots, including UAVs and small ground vehicles. The experiment results not only demonstrated the high accuracy of our static text-based simulation but also the reliable code generation of our LLM-driven corrective framework, which achieves a comparable performance with state-of-the-art research while does not rely on dynamic code execution using physical experiments or simulators.         ",
    "url": "https://arxiv.org/abs/2512.02002",
    "authors": [
      "Wenhao Wang",
      "Yi Rong",
      "Yanyan Li",
      "Long Jiao",
      "Jiawei Yuan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2512.03555",
    "title": "Accelerating shape optimization by deep neural networks with on-the-fly determined architecture",
    "abstract": "           In component shape optimization, the component properties are often evaluated by computationally expensive simulations. Such optimization becomes unfeasible when it is focused on a global search requiring thousands of simulations to be evaluated. Here, we present a viable global shape optimization methodology based on multi-objective evolutionary algorithms accelerated by deep neural networks (DNNs). Our methodology alternates between evaluating simulations and utilizing the generated data to train DNNs with various architectures. When a suitable DNN architecture is identified, the DNN replaces the simulation in the rest of the global search. Our methodology was tested on five ZDT benchmark functions, showing itself at the level of and sometimes more flexible than other state-of-the-art acceleration approaches. Then, it was applied to a real-life optimization problem, namely the shape optimization of a single-phase ejector. Compared with a non-accelerated methodology, ours was able to save weeks of CPU time in solving this problem. To experimentally confirm the performance of the optimized ejector shapes, four of them were 3D printed and tested on the lab scale confirming the predicted performance. This suggests that our methodology could be used for acceleration of other real-life shape optimization problems.         ",
    "url": "https://arxiv.org/abs/2512.03555",
    "authors": [
      "Lucie Kub\u00ed\u010dkov\u00e1",
      "On\u0159ej Gebousk\u00fd",
      "Jan Haidl",
      "Martin Isoz"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2512.04596",
    "title": "QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction",
    "abstract": "           Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. Such reliance not only leads to the intractability of explicit graph construction in large-scale scenarios but also limits the modeling of implicit topological relationships and exacerbates susceptibility to environmental noise and outliers. To address these challenges, this paper introduces \\emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against observational noise.         ",
    "url": "https://arxiv.org/abs/2512.04596",
    "authors": [
      "Guanchen Du",
      "Jianlong Xu",
      "Wei Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2512.04751",
    "title": "NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students",
    "abstract": "           Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.         ",
    "url": "https://arxiv.org/abs/2512.04751",
    "authors": [
      "Junhao Wei",
      "Yanzhao Gu",
      "Ran Zhang",
      "Mingjing Huang",
      "Jinhong Song",
      "Yanxiao Li",
      "Wenxuan Zhu",
      "Yapeng Wang",
      "Zikun Li",
      "Zhiwen Wang",
      "Xu Yang",
      "Ngai Cheong"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2512.04888",
    "title": "You Only Train Once (YOTO): A Retraining-Free Object Detection Framework",
    "abstract": "           Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.         ",
    "url": "https://arxiv.org/abs/2512.04888",
    "authors": [
      "Priyanto Hidayatullah",
      "Nurjannah Syakrani",
      "Yudi Widhiyasana",
      "Muhammad Rizqi Sholahuddin",
      "Refdinal Tubagus",
      "Zahri Al Adzani Hidayat",
      "Hanri Fajar Ramadhan",
      "Dafa Alfarizki Pratama",
      "Farhan Muhammad Yasin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.21287",
    "title": "On Supports for graphs of bounded genus",
    "abstract": "           Let $(X,\\mathcal{E})$ be a hypergraph. A support is a graph $Q$ on $X$ such that for each $E\\in\\mathcal{E}$, the subgraph of $Q$ induced on the elements in $E$ is connected. We consider the problem of constructing a support for hypergraphs defined by connected subgraphs of a host graph. For a graph $G=(V,E)$, let $\\mathcal{H}$ be a set of connected subgraphs of $G$. Let the vertices of $G$ be partitioned into two sets the \\emph{terminals} $\\mathbf{b}(V)$ and the \\emph{non-terminals} $\\mathbf{r}(V)$. We define a hypergraph on $\\mathbf{b}(V)$, where each $H\\in\\mathcal{H}$ defines a hyperedge consisting of the vertices of $\\mathbf{b}(V)$ in $H$. We also consider the problem of constructing a support for the \\emph{dual hypergraph} - a hypergraph on $\\mathcal{H}$ where each $v\\in \\mathbf{b}(V)$ defines a hyperedge consisting of the subgraphs in $\\mathcal{H}$ containing $v$. In fact, we construct supports for a common generalization of the primal and dual settings called the \\emph{intersection hypergraph}. As our main result, we show that if the host graph $G$ has bounded genus and the subgraphs in $\\mathcal{H}$ satisfy a condition of being \\emph{cross-free}, then there exists a support that also has bounded genus. Our results are a generalization of the results of Raman and Ray (Rajiv Raman, Saurabh Ray: Constructing Planar Support for Non-Piercing Regions. Discret. Comput. Geom. 64(3): 1098-1122 (2020)). Our techniques imply a unified analysis for packing and covering problems for hypergraphs defined on surfaces of bounded genus. We also describe applications of our results for hypergraph colorings.         ",
    "url": "https://arxiv.org/abs/2503.21287",
    "authors": [
      "Rajiv Raman",
      "Karamjeet Singh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2505.05261",
    "title": "ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming",
    "abstract": "           Two-stage stochastic programming (2SP) offers a basic framework for modelling decision-making under uncertainty, yet scalability remains a challenge due to the computational complexity of recourse function evaluation. Existing learning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP) employ neural networks (NNs) as recourse function surrogates but rely on computationally intensive mixed-integer programming (MIP) formulations. We propose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks (ICNNs) to exploit linear programming (LP) representability in convex 2SP problems. By architecturally enforcing convexity and enabling exact inference through LP, our approach eliminates the need for integer variables inherent to the conventional MIP-based formulation while retaining an exact embedding of the ICNN surrogate within the 2SP framework. This results in a more computationally efficient alternative, and we show that good solution quality can be maintained. Comprehensive experiments reveal that ICNNs incur only marginally longer training times while achieving validation accuracy on par with their standard NN counterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits considerably faster solution times than the MIP-based formulations while preserving solution quality, with these advantages becoming significantly more pronounced as problem scale increases. For the most challenging instances, the method achieves speedups of up to 100$\\times$ and solution quality superior to MIP-based formulations.         ",
    "url": "https://arxiv.org/abs/2505.05261",
    "authors": [
      "Yu Liu",
      "Fabricio Oliveira",
      "Jan Kronqvist"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.11437",
    "title": "Social Networks: Enumerating Maximal Community Patterns in $c$-Closed Graphs",
    "abstract": "           Jacob Fox, C. Seshadhri, Tim Roughgarden, Fan Wei, and Nicole Wein introduced the model of $c$-closed graphs--a distribution-free model motivated by triadic closure, one of the most pervasive structural signatures of social networks. While enumerating maximal cliques in general graphs can take exponential time, it is known that in $c$-closed graphs, maximal cliques and maximal complete bipartite subgraphs can always be enumerated in polynomial time. These structures correspond to blow-ups of simple patterns: a single vertex or a single edge, with some vertices required to form cliques. In this work, we explore a natural extension: we study maximal blow-ups of arbitrary finite graphs $H$ in $c$-closed graphs. We prove that for any fixed graph $H$, the number of maximal blow-ups of $H$ in an $n$-vertex $c$-closed graph is always bounded by a polynomial in $n$. We further investigate the case of induced blow-ups and provide a precise characterization of the graphs $H$ for which the number of maximal induced blow-ups is also polynomially bounded in $n$. Finally, we study the analogue questions when $H$ ranges over an infinite family of graphs.         ",
    "url": "https://arxiv.org/abs/2506.11437",
    "authors": [
      "Gabriela Bourla",
      "Kaixin Wang",
      "Fan Wei",
      "Runtian Zhou"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2507.04254",
    "title": "On Modular Edge Colourings of Graphs",
    "abstract": "           Given a graph $G$ and an integer $k\\geq 2$, let $\\chi'_k(G)$ denote the minimum number of colours required to colour the edges of $G$ such that, in each colour class, the subgraph induced by the edges of that colour has all non-zero degrees congruent to $1$ modulo $k$. In 1992, Pyber proved that $\\chi'_2(G) \\leq 4$ for every graph $G$, and posed the question of whether $\\chi'_k(G)$ can be bounded solely in terms of $k$ for every $k\\geq 3$. This question was answered in 1997 by Scott, who showed that $\\chi'_k(G)\\leq5k^2\\log k$, and further asked whether $\\chi'_k(G) = O(k)$. Recently, Botler, Colucci, and Kohayakawa (2023) answered Scott's question affirmatively proving that $\\chi'_k(G) \\leq 198k - 101$, and conjectured that the multiplicative constant could be reduced to $1$. A step towards this latter conjecture was made in 2024 by Nweit and Yang, who improved the bound to $\\chi'_k(G) \\leq 177k - 93$. In this paper, we further improve the multiplicative constant to $9$. More specifically, we prove that there is a function $f\\in o(k)$ for which $\\chi'_k(G) \\leq 7k + f(k)$ if $k$ is odd, and $\\chi'_k(G) \\leq 9k + f(k)$ if $k$ is even. In doing so, we prove that $\\chi'_k(G) \\leq k + O(d)$ for every $d$-degenerate graph $G$, which plays a central role in our proof.         ",
    "url": "https://arxiv.org/abs/2507.04254",
    "authors": [
      "Ga\u00e9tan Berthe",
      "Marthe Bonamy",
      "F\u00e1bio Botler",
      "Gaia Carenini",
      "Lucas Colucci",
      "Arthur Dumas",
      "Fatemeh Ghasemi",
      "Pedro Mariano Viana Neto"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2509.22467",
    "title": "CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks",
    "abstract": "           Deep neural networks achieve state-of-the-art performance in estimating heterogeneous treatment effects, but their opacity limits trust and adoption in sensitive domains such as medicine, economics, and public policy. Building on well-established and high-performing causal neural architectures, we propose causalKANs, a framework that transforms neural estimators of conditional average treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By incorporating pruning and symbolic simplification, causalKANs yields interpretable closed-form formulas while preserving predictive accuracy. Experiments on benchmark datasets demonstrate that causalKANs perform on par with neural baselines in CATE error metrics, and that even simple KAN variants achieve competitive performance, offering a favorable accuracy--interpretability trade-off. By combining reliability with analytic accessibility, causalKANs provide auditable estimators supported by closed-form expressions and interpretable plots, enabling trustworthy individualized decision-making in high-stakes settings. We release the code for reproducibility at this https URL .         ",
    "url": "https://arxiv.org/abs/2509.22467",
    "authors": [
      "Alejandro Almod\u00f3var",
      "Patricia A. Apell\u00e1niz",
      "Santiago Zazo",
      "Juan Parras"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2511.14048",
    "title": "Wasserstein Distributionally Robust Nash Equilibrium Seeking with Heterogeneous Data: A Lagrangian Approach",
    "abstract": "           We study a class of distributionally robust games where agents are allowed to heterogeneously choose their risk aversion with respect to distributional shifts of the uncertainty. In our formulation, heterogeneous Wasserstein ball constraints on each distribution are enforced through a penalty function leveraging a Lagrangian formulation. We then formulate the distributionally robust Nash equilibrium problem and show that under certain assumptions it is equivalent to a finite-dimensional variational inequality problem with a strongly monotone mapping. We then design an approximate Nash equilibrium seeking algorithm and prove convergence of the average regret to a quantity that diminishes with the number of iterations, thus learning the desired equilibrium up to an a priori specified accuracy. Numerical simulations corroborate our theoretical findings.         ",
    "url": "https://arxiv.org/abs/2511.14048",
    "authors": [
      "Zifan Wang",
      "Georgios Pantazis",
      "Sergio Grammatico",
      "Michael M. Zavlanos",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2511.19114",
    "title": "Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation",
    "abstract": "           As artificial intelligence emerges as a transformative enabler for fusion energy commercialization, fast and accurate solvers become increasingly critical. In magnetic confinement nuclear fusion, rapid and accurate solution of the Grad-Shafranov equation (GSE) is essential for real-time plasma control and analysis. Traditional numerical solvers achieve high precision but are computationally prohibitive, while data-driven surrogates infer quickly but fail to enforce physical laws and generalize poorly beyond training distributions. To address this challenge, we present a Physics-Informed Neural Operator (PINO) that directly learns the GSE solution operator, mapping shape parameters of last closed flux surface to equilibrium solutions for realistic nonlinear current profiles. Comprehensive benchmarking of five neural architectures identifies the novel Transformer-KAN (Kolmogorov-Arnold Network) Neural Operator (TKNO) as achieving highest accuracy (0.25% mean L2 relative error) under supervised training (only data-driven). However, all data-driven models exhibit large physics residuals, indicating poor physical consistency. Our unsupervised training can reduce the residuals by nearly four orders of magnitude through embedding physics-based loss terms without labeled data. Critically, semi-supervised learning--integrating sparse labeled data (100 interior points) with physics constraints--achieves optimal balance: 0.48% interpolation error and the most robust extrapolation performance (4.76% error, 8.9x degradation factor vs 39.8x for supervised models). Accelerated by TensorRT optimization, our models enable millisecond-level inference, establishing PINO as a promising pathway for next-generation fusion control systems.         ",
    "url": "https://arxiv.org/abs/2511.19114",
    "authors": [
      "Siqi Ding",
      "Zitong Zhang",
      "Guoyang Shi",
      "Xingyu Li",
      "Xiang Gu",
      "Yanan Xu",
      "Huasheng Xie",
      "Hanyue Zhao",
      "Yuejiang Shi",
      "Tianyuan Liu"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Artificial Intelligence (cs.AI)"
    ]
  }
]