[
  {
    "id": "arXiv:2212.08661",
    "title": "EffMulti: Efficiently Modeling Complex Multimodal Interactions for  Emotion Analysis",
    "abstract": "Humans are skilled in reading the interlocutor's emotion from multimodal signals, including spoken words, simultaneous speech, and facial expressions. It is still a challenge to effectively decode emotions from the complex interactions of multimodal signals. In this paper, we design three kinds of multimodal latent representations to refine the emotion analysis process and capture complex multimodal interactions from different views, including a intact three-modal integrating representation, a modality-shared representation, and three modality-individual representations. Then, a modality-semantic hierarchical fusion is proposed to reasonably incorporate these representations into a comprehensive interaction representation. The experimental results demonstrate that our EffMulti outperforms the state-of-the-art methods. The compelling performance benefits from its well-designed framework with ease of implementation, lower computing complexity, and less trainable parameters. ",
    "url": "https://arxiv.org/abs/2212.08661",
    "authors": [
      "Feng Qiu",
      "Chengyang Xie",
      "Yu Ding",
      "Wanzeng Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.08663",
    "title": "Randomized Quantization for Data Agnostic Representation Learning",
    "abstract": "Self-supervised representation learning follows a paradigm of withholding some part of the data and tasking the network to predict it from the remaining part. Towards this end, masking has emerged as a generic and powerful tool where content is withheld along the sequential dimension, e.g., spatial in images, temporal in audio, and syntactic in language. In this paper, we explore the orthogonal channel dimension for generic data augmentation. The data for each channel is quantized through a non-uniform quantizer, with the quantized value sampled randomly within randomly sampled quantization bins. From another perspective, quantization is analogous to channel-wise masking, as it removes the information within each bin, but preserves the information across bins. We apply the randomized quantization in conjunction with sequential augmentations on self-supervised contrastive models. This generic approach achieves results on par with modality-specific augmentation on vision tasks, and state-of-the-art results on 3D point clouds as well as on audio. We also demonstrate this method to be applicable for augmenting intermediate embeddings in a deep neural network on the comprehensive DABS benchmark which is comprised of various data modalities. Code is availabel at this http URL ",
    "url": "https://arxiv.org/abs/2212.08663",
    "authors": [
      "Huimin Wu",
      "Chenyang Lei",
      "Xiao Sun",
      "Peng-Shuai Wang",
      "Qifeng Chen",
      "Kwang-Ting Cheng",
      "Stephen Lin",
      "Zhirong Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.08665",
    "title": "Hard Sample Aware Network for Contrastive Deep Graph Clustering",
    "abstract": "Contrastive deep graph clustering, which aims to divide nodes into disjoint groups via contrastive mechanisms, is a challenging research spot. Among the recent works, hard sample mining-based algorithms have achieved great attention for their promising performance. However, we find that the existing hard sample mining methods have two problems as follows. 1) In the hardness measurement, the important structural information is overlooked for similarity calculation, degrading the representativeness of the selected hard negative samples. 2) Previous works merely focus on the hard negative sample pairs while neglecting the hard positive sample pairs. Nevertheless, samples within the same cluster but with low similarity should also be carefully learned. To solve the problems, we propose a novel contrastive deep graph clustering method dubbed Hard Sample Aware Network (HSAN) by introducing a comprehensive similarity measure criterion and a general dynamic sample weighing strategy. Concretely, in our algorithm, the similarities between samples are calculated by considering both the attribute embeddings and the structure embeddings, better revealing sample relationships and assisting hardness measurement. Moreover, under the guidance of the carefully collected high-confidence clustering information, our proposed weight modulating function will first recognize the positive and negative samples and then dynamically up-weight the hard sample pairs while down-weighting the easy ones. In this way, our method can mine not only the hard negative samples but also the hard positive sample, thus improving the discriminative capability of the samples further. Extensive experiments and analyses demonstrate the superiority and effectiveness of our proposed method. ",
    "url": "https://arxiv.org/abs/2212.08665",
    "authors": [
      "Yue Liu",
      "Xihong Yang",
      "Sihang Zhou",
      "Xinwang Liu",
      "Zhen Wang",
      "Ke Liang",
      "Wenxuan Tu",
      "Liang Li",
      "Jingcan Duan",
      "Cancan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08689",
    "title": "TopoImb: Toward Topology-level Imbalance in Learning from Graphs",
    "abstract": "Graph serves as a powerful tool for modeling data that has an underlying structure in non-Euclidean space, by encoding relations as edges and entities as nodes. Despite developments in learning from graph-structured data over the years, one obstacle persists: graph imbalance. Although several attempts have been made to target this problem, they are limited to considering only class-level imbalance. In this work, we argue that for graphs, the imbalance is likely to exist at the sub-class topology group level. Due to the flexibility of topology structures, graphs could be highly diverse, and learning a generalizable classification boundary would be difficult. Therefore, several majority topology groups may dominate the learning process, rendering others under-represented. To address this problem, we propose a new framework {\\method} and design (1 a topology extractor, which automatically identifies the topology group for each instance with explicit memory cells, (2 a training modulator, which modulates the learning process of the target GNN model to prevent the case of topology-group-wise under-representation. {\\method} can be used as a key component in GNN models to improve their performances under the data imbalance setting. Analyses on both topology-level imbalance and the proposed {\\method} are provided theoretically, and we empirically verify its effectiveness with both node-level and graph-level classification as the target tasks. ",
    "url": "https://arxiv.org/abs/2212.08689",
    "authors": [
      "Tianxiang Zhao",
      "Dongsheng Luo",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08710",
    "title": "JFP: Joint Future Prediction with Interactive Multi-Agent Modeling for  Autonomous Driving",
    "abstract": "We propose JFP, a Joint Future Prediction model that can learn to generate accurate and consistent multi-agent future trajectories. For this task, many different methods have been proposed to capture social interactions in the encoding part of the model, however, considerably less focus has been placed on representing interactions in the decoder and output stages. As a result, the predicted trajectories are not necessarily consistent with each other, and often result in unrealistic trajectory overlaps. In contrast, we propose an end-to-end trainable model that learns directly the interaction between pairs of agents in a structured, graphical model formulation in order to generate consistent future trajectories. It sets new state-of-the-art results on Waymo Open Motion Dataset (WOMD) for the interactive setting. We also investigate a more complex multi-agent setting for both WOMD and a larger internal dataset, where our approach improves significantly on the trajectory overlap metrics while obtaining on-par or better performance on single-agent trajectory metrics. ",
    "url": "https://arxiv.org/abs/2212.08710",
    "authors": [
      "Wenjie Luo",
      "Cheolho Park",
      "Andre Cornman",
      "Benjamin Sapp",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2212.08712",
    "title": "Towards Causal Temporal Reasoning for Markov Decision Processes",
    "abstract": "We introduce a new probabilistic temporal logic for the verification of Markov Decision Processes (MDP). Our logic is the first to include operators for causal reasoning, allowing us to express interventional and counterfactual queries. Given a path formula $\\phi$, an interventional property is concerned with the satisfaction probability of $\\phi$ if we apply a particular change $I$ to the MDP (e.g., switching to a different policy); a counterfactual allows us to compute, given an observed MDP path $\\tau$, what the outcome of $\\phi$ would have been had we applied $I$ in the past. For its ability to reason about different configurations of the MDP, our approach represents a departure from existing probabilistic temporal logics that can only reason about a fixed system configuration. From a syntactic viewpoint, we introduce a generalized counterfactual operator that subsumes both interventional and counterfactual probabilities as well as the traditional probabilistic operator found in e.g., PCTL. From a semantics viewpoint, our logic is interpreted over a structural causal model (SCM) translation of the MDP, which gives us a representation amenable to counterfactual reasoning. We provide a proof-of-concept evaluation of our logic on a reach-avoid task in a grid-world model. ",
    "url": "https://arxiv.org/abs/2212.08712",
    "authors": [
      "Milad Kazemi",
      "Nicola Paoletti"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2212.08718",
    "title": "Neural Story Planning",
    "abstract": "Automated plot generation is the challenge of generating a sequence of events that will be perceived by readers as the plot of a coherent story. Traditional symbolic planners plan a story from a goal state and guarantee logical causal plot coherence but rely on a library of hand-crafted actions with their preconditions and effects. This closed world setting limits the length and diversity of what symbolic planners can generate. On the other hand, pre-trained neural language models can generate stories with great diversity, while being generally incapable of ending a story in a specified manner and can have trouble maintaining coherence. In this paper, we present an approach to story plot generation that unifies causal planning with neural language models. We propose to use commonsense knowledge extracted from large language models to recursively expand a story plot in a backward chaining fashion. Specifically, our system infers the preconditions for events in the story and then events that will cause those conditions to become true. We performed automatic evaluation to measure narrative coherence as indicated by the ability to answer questions about whether different events in the story are causally related to other events. Results indicate that our proposed method produces more coherent plotlines than several strong baselines. ",
    "url": "https://arxiv.org/abs/2212.08718",
    "authors": [
      "Anbang Ye",
      "Christopher Cui",
      "Taiwei Shi",
      "Mark O. Riedl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08726",
    "title": "Learning Non-robustness using Simulation-based Testing: a Network  Traffic-shaping Case Study",
    "abstract": "An input to a system reveals a non-robust behaviour when, by making a small change in the input, the output of the system changes from acceptable (passing) to unacceptable (failing) or vice versa. Identifying inputs that lead to non-robust behaviours is important for many types of systems, e.g., cyber-physical and network systems, whose inputs are prone to perturbations. In this paper, we propose an approach that combines simulation-based testing with regression tree models to generate value ranges for inputs in response to which a system is likely to exhibit non-robust behaviours. We apply our approach to a network traffic-shaping system (NTSS) -- a novel case study from the network domain. In this case study, developed and conducted in collaboration with a network solutions provider, RabbitRun Technologies, input ranges that lead to non-robustness are of interest as a way to identify and mitigate network quality-of-service issues. We demonstrate that our approach accurately characterizes non-robust test inputs of NTSS by achieving a precision of 84% and a recall of 100%, significantly outperforming a standard baseline. In addition, we show that there is no statistically significant difference between the results obtained from our simulated testbed and a hardware testbed with identical configurations. Finally we describe lessons learned from our industrial collaboration, offering insights about how simulation helps discover unknown and undocumented behaviours as well as a new perspective on using non-robustness as a measure for system re-configuration. ",
    "url": "https://arxiv.org/abs/2212.08726",
    "authors": [
      "Baharin Aliashrafi Jodat",
      "Shiva Nejati",
      "Mehrdad Sabetzadeh",
      "Patricio Saavedra"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2212.08729",
    "title": "Distribution-aware Goal Prediction and Conformant Model-based Planning  for Safe Autonomous Driving",
    "abstract": "The feasibility of collecting a large amount of expert demonstrations has inspired growing research interests in learning-to-drive settings, where models learn by imitating the driving behaviour from experts. However, exclusively relying on imitation can limit agents' generalisability to novel scenarios that are outside the support of the training data. In this paper, we address this challenge by factorising the driving task, based on the intuition that modular architectures are more generalisable and more robust to changes in the environment compared to monolithic, end-to-end frameworks. Specifically, we draw inspiration from the trajectory forecasting community and reformulate the learning-to-drive task as obstacle-aware perception and grounding, distribution-aware goal prediction, and model-based planning. Firstly, we train the obstacle-aware perception module to extract salient representation of the visual context. Then, we learn a multi-modal goal distribution by performing conditional density-estimation using normalising flow. Finally, we ground candidate trajectory predictions road geometry, and plan the actions based on on vehicle dynamics. Under the CARLA simulator, we report state-of-the-art results on the CARNOVEL benchmark. ",
    "url": "https://arxiv.org/abs/2212.08729",
    "authors": [
      "Jonathan Francis",
      "Bingqing Chen",
      "Weiran Yao",
      "Eric Nyberg",
      "Jean Oh"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2212.08736",
    "title": "A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle  Scattering Problem",
    "abstract": "We consider the inverse acoustic obstacle problem for sound-soft star-shaped obstacles in two dimensions wherein the boundary of the obstacle is determined from measurements of the scattered field at a collection of receivers outside the object. One of the standard approaches for solving this problem is to reformulate it as an optimization problem: finding the boundary of the domain that minimizes the $L^2$ distance between computed values of the scattered field and the given measurement data. The optimization problem is computationally challenging since the local set of convexity shrinks with increasing frequency and results in an increasing number of local minima in the vicinity of the true solution. In many practical experimental settings, low frequency measurements are unavailable due to limitations of the experimental setup or the sensors used for measurement. Thus, obtaining a good initial guess for the optimization problem plays a vital role in this environment. We present a neural network warm-start approach for solving the inverse scattering problem, where an initial guess for the optimization problem is obtained using a trained neural network. We demonstrate the effectiveness of our method with several numerical examples. For high frequency problems, this approach outperforms traditional iterative methods such as Gauss-Newton initialized without any prior (i.e., initialized using a unit circle), or initialized using the solution of a direct method such as the linear sampling method. The algorithm remains robust to noise in the scattered field measurements and also converges to the true solution for limited aperture data. However, the number of training samples required to train the neural network scales exponentially in frequency and the complexity of the obstacles considered. We conclude with a discussion of this phenomenon and potential directions for future research. ",
    "url": "https://arxiv.org/abs/2212.08736",
    "authors": [
      "Mo Zhou",
      "Jiequn Han",
      "Manas Rachh",
      "Carlos Borges"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2212.08738",
    "title": "SkillFence: A Systems Approach to Practically Mitigating Voice-Based  Confusion Attacks",
    "abstract": "Voice assistants are deployed widely and provide useful functionality. However, recent work has shown that commercial systems like Amazon Alexa and Google Home are vulnerable to voice-based confusion attacks that exploit design issues. We propose a systems-oriented defense against this class of attacks and demonstrate its functionality for Amazon Alexa. We ensure that only the skills a user intends execute in response to voice commands. Our key insight is that we can interpret a user's intentions by analyzing their activity on counterpart systems of the web and smartphones. For example, the Lyft ride-sharing Alexa skill has an Android app and a website. Our work shows how information from counterpart apps can help reduce dis-ambiguities in the skill invocation process. We build SkilIFence, a browser extension that existing voice assistant users can install to ensure that only legitimate skills run in response to their commands. Using real user data from MTurk (N = 116) and experimental trials involving synthetic and organic speech, we show that SkillFence provides a balance between usability and security by securing 90.83% of skills that a user will need with a False acceptance rate of 19.83%. ",
    "url": "https://arxiv.org/abs/2212.08738",
    "authors": [
      "Ashish Hooda",
      "Matthew Wallace",
      "Kushal Jhunjhunwalla",
      "Earlence Fernandes",
      "Kassem Fawaz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08751",
    "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
    "abstract": "While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e. ",
    "url": "https://arxiv.org/abs/2212.08751",
    "authors": [
      "Alex Nichol",
      "Heewoo Jun",
      "Prafulla Dhariwal",
      "Pamela Mishkin",
      "Mark Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08756",
    "title": "Multi-Scales Data Augmentation Approach In Natural Language Inference  For Artifacts Mitigation And Pre-Trained Model Optimization",
    "abstract": "Machine learning models can reach high performance on benchmark natural language processing (NLP) datasets but fail in more challenging settings. We study this issue when a pre-trained model learns dataset artifacts in natural language inference (NLI), the topic of studying the logical relationship between a pair of text sequences. We provide a variety of techniques for analyzing and locating dataset artifacts inside the crowdsourced Stanford Natural Language Inference (SNLI) corpus. We study the stylistic pattern of dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a unique multi-scale data augmentation technique with two distinct frameworks: a behavioral testing checklist at the sentence level and lexical synonym criteria at the word level. Specifically, our combination method enhances our model's resistance to perturbation testing, enabling it to continuously outperform the pre-trained baseline. ",
    "url": "https://arxiv.org/abs/2212.08756",
    "authors": [
      "Zhenyuan Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2212.08757",
    "title": "Short-term Prediction of Household Electricity Consumption Using  Customized LSTM and GRU Models",
    "abstract": "With the evolution of power systems as it is becoming more intelligent and interactive system while increasing in flexibility with a larger penetration of renewable energy sources, demand prediction on a short-term resolution will inevitably become more and more crucial in designing and managing the future grid, especially when it comes to an individual household level. Projecting the demand for electricity for a single energy user, as opposed to the aggregated power consumption of residential load on a wide scale, is difficult because of a considerable number of volatile and uncertain factors. This paper proposes a customized GRU (Gated Recurrent Unit) and Long Short-Term Memory (LSTM) architecture to address this challenging problem. LSTM and GRU are comparatively newer and among the most well-adopted deep learning approaches. The electricity consumption datasets were obtained from individual household smart meters. The comparison shows that the LSTM model performs better for home-level forecasting than alternative prediction techniques-GRU in this case. To compare the NN-based models with contrast to the conventional statistical technique-based model, ARIMA based model was also developed and benchmarked with LSTM and GRU model outcomes in this study to show the performance of the proposed model on the collected time series data. ",
    "url": "https://arxiv.org/abs/2212.08757",
    "authors": [
      "Saad Emshagin",
      "Wayes Koroni Halim",
      "Rasha Kashef"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2212.08765",
    "title": "Latent Variable Representation for Reinforcement Learning",
    "abstract": "Deep latent variable models have achieved significant empirical successes in model-based reinforcement learning (RL) due to their expressiveness in modeling complex transition dynamics. On the other hand, it remains unclear theoretically and empirically how latent variable models may facilitate learning, planning, and exploration to improve the sample efficiency of RL. In this paper, we provide a representation view of the latent variable models for state-action value functions, which allows both tractable variational learning algorithm and effective implementation of the optimism/pessimism principle in the face of uncertainty for exploration. In particular, we propose a computationally efficient planning algorithm with UCB exploration by incorporating kernel embeddings of latent variable models. Theoretically, we establish the sample complexity of the proposed approach in the online and offline settings. Empirically, we demonstrate superior performance over current state-of-the-art algorithms across various benchmarks. ",
    "url": "https://arxiv.org/abs/2212.08765",
    "authors": [
      "Tongzheng Ren",
      "Chenjun Xiao",
      "Tianjun Zhang",
      "Na Li",
      "Zhaoran Wang",
      "Sujay Sanghavi",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.08769",
    "title": "Improving Levenberg-Marquardt Algorithm for Neural Networks",
    "abstract": "We explore the usage of the Levenberg-Marquardt (LM) algorithm for regression (non-linear least squares) and classification (generalized Gauss-Newton methods) tasks in neural networks. We compare the performance of the LM method with other popular first-order algorithms such as SGD and Adam, as well as other second-order algorithms such as L-BFGS , Hessian-Free and KFAC. We further speed up the LM method by using adaptive momentum, learning rate line search, and uphill step acceptance. ",
    "url": "https://arxiv.org/abs/2212.08769",
    "authors": [
      "Omead Pooladzandi",
      "Yiming Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08781",
    "title": "Multi-Scale Relational Graph Convolutional Network for Multiple Instance  Learning in Histopathology Images",
    "abstract": "Graph convolutional neural networks have shown significant potential in natural and histopathology images. However, their use has only been studied in a single magnification or multi-magnification with late fusion. In order to leverage the multi-magnification information and early fusion with graph convolutional networks, we handle different embedding spaces at each magnification by introducing the Multi-Scale Relational Graph Convolutional Network (MS-RGCN) as a multiple instance learning method. We model histopathology image patches and their relation with neighboring patches and patches at other scales (i.e., magnifications) as a graph. To pass the information between different magnification embedding spaces, we define separate message-passing neural networks based on the node and edge type. We experiment on prostate cancer histopathology images to predict the grade groups based on the extracted features from patches. We also compare our MS-RGCN with multiple state-of-the-art methods with evaluations on both source and held-out datasets. Our method outperforms the state-of-the-art on both datasets and especially on the classification of grade groups 2 and 3, which are significant for clinical decisions for patient management. Through an ablation study, we test and show the value of the pertinent design features of the MS-RGCN. ",
    "url": "https://arxiv.org/abs/2212.08781",
    "authors": [
      "Roozbeh Bazargani",
      "Ladan Fazli",
      "Larry Goldenberg",
      "Martin Gleave",
      "Ali Bashashati",
      "Septimiu Salcudean"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.08792",
    "title": "Stateful Switch: Optimized Time Series Release with Local Differential  Privacy",
    "abstract": "Time series data have numerous applications in big data analytics. However, they often cause privacy issues when collected from individuals. To address this problem, most existing works perturb the values in the time series while retaining their temporal order, which may lead to significant distortion of the values. Recently, we propose TLDP model that perturbs temporal perturbation to ensure privacy guarantee while retaining original values. It has shown great promise to achieve significantly higher utility than value perturbation mechanisms in many time series analysis. However, its practicability is still undermined by two factors, namely, utility cost of extra missing or empty values, and inflexibility of privacy budget settings. To address them, in this paper we propose {\\it switch} as a new two-way operation for temporal perturbation, as opposed to the one-way {\\it dispatch} operation. The former inherently eliminates the cost of missing, empty or repeated values. Optimizing switch operation in a {\\it stateful} manner, we then propose $StaSwitch$ mechanism for time series release under TLDP. Through both analytical and empirical studies, we show that $StaSwitch$ has significantly higher utility for the published time series than any state-of-the-art temporal- or value-perturbation mechanism, while allowing any combination of privacy budget settings. ",
    "url": "https://arxiv.org/abs/2212.08792",
    "authors": [
      "Qingqing Ye",
      "Haibo Hu",
      "Kai Huang",
      "Man Ho Au",
      "Qiao Xue"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2212.08802",
    "title": "Relational Sentence Embedding for Flexible Semantic Matching",
    "abstract": "We present Relational Sentence Embedding (RSE), a new paradigm to further discover the potential of sentence embeddings. Prior work mainly models the similarity between sentences based on their embedding distance. Because of the complex semantic meanings conveyed, sentence pairs can have various relation types, including but not limited to entailment, paraphrasing, and question-answer. It poses challenges to existing embedding methods to capture such relational information. We handle the problem by learning associated relational embeddings. Specifically, a relation-wise translation operation is applied to the source sentence to infer the corresponding target sentence with a pre-trained Siamese-based encoder. The fine-grained relational similarity scores can be computed from learned embeddings. We benchmark our method on 19 datasets covering a wide range of tasks, including semantic textual similarity, transfer, and domain-specific tasks. Experimental results show that our method is effective and flexible in modeling sentence relations and outperforms a series of state-of-the-art sentence embedding methods. https://github.com/BinWang28/RSE ",
    "url": "https://arxiv.org/abs/2212.08802",
    "authors": [
      "Bin Wang",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.08815",
    "title": "FSCNN: A Fast Sparse Convolution Neural Network Inference System",
    "abstract": "Convolution neural networks (CNNs) have achieved remarkable success, but typically accompany high computation cost and numerous redundant weight parameters. To reduce the FLOPs, structure pruning is a popular approach to remove the entire hidden structures via introducing coarse-grained sparsity. Meanwhile, plentiful pruning works leverage fine-grained sparsity instead (sparsity are randomly distributed), whereas their sparse models lack special designed computing library for potential speedup. In this technical report, we study and present an efficient convolution neural network inference system to accelerate its forward pass by utilizing the fine-grained sparsity of compressed CNNs. Our developed FSCNN is established based on a set of specialized designed sparse data structures, operators and associated algorithms. Experimentally, we validate that FSCNN outperforms standard deep learning library PyTorch on popular CNN architectures such as VGG16 if sufficiently high sparsity exhibits. However, due to the contiguity issue of sparse operators, FSCNN is typically not comparable with highly optimized dense operator. Therefore, coarse-grained (structured) sparsity is our recommendation for generic model compression. ",
    "url": "https://arxiv.org/abs/2212.08815",
    "authors": [
      "Bo Ji",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08818",
    "title": "Latent Evolution Model for Change Point Detection in Time-varying  Networks",
    "abstract": "Graph-based change point detection (CPD) play an irreplaceable role in discovering anomalous graphs in the time-varying network. While several techniques have been proposed to detect change points by identifying whether there is a significant difference between the target network and successive previous ones, they neglect the natural evolution of the network. In practice, real-world graphs such as social networks, traffic networks, and rating networks are constantly evolving over time. Considering this problem, we treat the problem as a prediction task and propose a novel CPD method for dynamic graphs via a latent evolution model. Our method focuses on learning the low-dimensional representations of networks and capturing the evolving patterns of these learned latent representations simultaneously. After having the evolving patterns, a prediction of the target network can be achieved. Then, we can detect the change points by comparing the prediction and the actual network by leveraging a trade-off strategy, which balances the importance between the prediction network and the normal graph pattern extracted from previous networks. Intensive experiments conducted on both synthetic and real-world datasets show the effectiveness and superiority of our model. ",
    "url": "https://arxiv.org/abs/2212.08818",
    "authors": [
      "Yongshun Gong",
      "Xue Dong",
      "Jian Zhang",
      "Meng Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08822",
    "title": "Better Datastore, Better Translation: Generating Datastores from  Pre-Trained Models for Nearest Neural Machine Translation",
    "abstract": "Nearest Neighbor Machine Translation (kNNMT) is a simple and effective method of augmenting neural machine translation (NMT) with a token-level nearest neighbor retrieval mechanism. The effectiveness of kNNMT directly depends on the quality of retrieved neighbors. However, original kNNMT builds datastores based on representations from NMT models, which would result in poor retrieval accuracy when NMT models are not good enough, leading to sub-optimal translation performance. In this paper, we propose PRED, a framework that leverages Pre-trained models for Datastores in kNN-MT. Better representations from pre-trained models allow us to build datastores of better quality. We also design a novel contrastive alignment objective to mitigate the representation gap between the NMT model and pre-trained models, enabling the NMT model to retrieve from better datastores. We conduct extensive experiments on both bilingual and multilingual translation benchmarks, including WMT17 English $\\leftrightarrow$ Chinese, WMT14 English $\\leftrightarrow$ German, IWSLT14 German $\\leftrightarrow$ English, and IWSLT14 multilingual datasets. Empirical results demonstrate the effectiveness of PRED. ",
    "url": "https://arxiv.org/abs/2212.08822",
    "authors": [
      "Jiahuan Li",
      "Shanbo Cheng",
      "Zewei Sun",
      "Mingxuan Wang",
      "Shujian Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.08834",
    "title": "Towards Robust Handwritten Text Recognition with On-the-fly User  Participation",
    "abstract": "Long-term OCR services aim to provide high-quality output to their users at competitive costs. It is essential to upgrade the models because of the complex data loaded by the users. The service providers encourage the users who provide data where the OCR model fails by rewarding them based on data complexity, readability, and available budget. Hitherto, the OCR works include preparing the models on standard datasets without considering the end-users. We propose a strategy of consistently upgrading an existing Handwritten Hindi OCR model three times on the dataset of 15 users. We fix the budget of 4 users for each iteration. For the first iteration, the model directly trains on the dataset from the first four users. For the rest iteration, all remaining users write a page each, which service providers later analyze to select the 4 (new) best users based on the quality of predictions on the human-readable words. Selected users write 23 more pages for upgrading the model. We upgrade the model with Curriculum Learning (CL) on the data available in the current iteration and compare the subset from previous iterations. The upgraded model is tested on a held-out set of one page each from all 23 users. We provide insights into our investigations on the effect of CL, user selection, and especially the data from unseen writing styles. Our work can be used for long-term OCR services in crowd-sourcing scenarios for the service providers and end users. ",
    "url": "https://arxiv.org/abs/2212.08834",
    "authors": [
      "Ajoy Mondal",
      "Rohit saluja",
      "C. V. Jawahar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.08841",
    "title": "Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable  Augmentation with Query Extraction and Generation",
    "abstract": "Dense retrievers have made significant strides in obtaining state-of-the-art results on text retrieval and open-domain question answering (ODQA). Yet most of these achievements were made possible with the help of large annotated datasets, unsupervised learning for dense retrieval models remains an open problem. In this work, we explore two categories of methods for creating pseudo query-document pairs, named query extraction (QExt) and transferred query generation (TQGen), to augment the retriever training in an annotation-free and scalable manner. Specifically, QExt extracts pseudo queries by document structures or selecting salient random spans, and TQGen utilizes generation models trained for other NLP tasks (e.g., summarization) to produce pseudo queries. Extensive experiments show that dense retrievers trained with individual augmentation methods can perform comparably well with multiple strong baselines, and combining them leads to further improvements, achieving state-of-the-art performance of unsupervised dense retrieval on both BEIR and ODQA datasets. ",
    "url": "https://arxiv.org/abs/2212.08841",
    "authors": [
      "Rui Meng",
      "Ye Liu",
      "Semih Yavuz",
      "Divyansh Agarwal",
      "Lifu Tu",
      "Ning Yu",
      "Jianguo Zhang",
      "Meghana Bhat",
      "Yingbo Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2212.08853",
    "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden  Representation Perturbation",
    "abstract": "Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In addition, HyPe acquires negligible computational overheads, and is better than and compatible with previous state-of-the-art fine-tuning techniques. ",
    "url": "https://arxiv.org/abs/2212.08853",
    "authors": [
      "Hongyi Yuan",
      "Zheng Yuan",
      "Chuanqi Tan",
      "Fei Huang",
      "Songfang Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.08854",
    "title": "An Evolutionary Multitasking Algorithm with Multiple Filtering for  High-Dimensional Feature Selection",
    "abstract": "Recently, evolutionary multitasking (EMT) has been successfully used in the field of high-dimensional classification. However, the generation of multiple tasks in the existing EMT-based feature selection (FS) methods is relatively simple, using only the Relief-F method to collect related features with similar importance into one task, which cannot provide more diversified tasks for knowledge transfer. Thus, this paper devises a new EMT algorithm for FS in high-dimensional classification, which first adopts different filtering methods to produce multiple tasks and then modifies a competitive swarm optimizer to efficiently solve these related tasks via knowledge transfer. First, a diversified multiple task generation method is designed based on multiple filtering methods, which generates several relevant low-dimensional FS tasks by eliminating irrelevant features. In this way, useful knowledge for solving simple and relevant tasks can be transferred to simplify and speed up the solution of the original high-dimensional FS task. Then, a competitive swarm optimizer is modified to simultaneously solve these relevant FS tasks by transferring useful knowledge among them. Numerous empirical results demonstrate that the proposed EMT-based FS method can obtain a better feature subset than several state-of-the-art FS methods on eighteen high-dimensional datasets. ",
    "url": "https://arxiv.org/abs/2212.08854",
    "authors": [
      "Lingjie Li",
      "Manlin Xuan",
      "Qiuzhen Lin",
      "Min Jiang",
      "Zhong Ming",
      "Kay Chen Tan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2212.08892",
    "title": "Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud  Analysis",
    "abstract": "Point clouds are characterized by irregularity and unstructuredness, which pose challenges in efficient data exploitation and discriminative feature extraction. In this paper, we present an unsupervised deep neural architecture called Flattening-Net to represent irregular 3D point clouds of arbitrary geometry and topology as a completely regular 2D point geometry image (PGI) structure, in which coordinates of spatial points are captured in colors of image pixels. \\mr{Intuitively, Flattening-Net implicitly approximates a locally smooth 3D-to-2D surface flattening process while effectively preserving neighborhood consistency.} \\mr{As a generic representation modality, PGI inherently encodes the intrinsic property of the underlying manifold structure and facilitates surface-style point feature aggregation.} To demonstrate its potential, we construct a unified learning framework directly operating on PGIs to achieve \\mr{diverse types of high-level and low-level} downstream applications driven by specific task networks, including classification, segmentation, reconstruction, and upsampling. Extensive experiments demonstrate that our methods perform favorably against the current state-of-the-art competitors. We will make the code and data publicly available at https://github.com/keeganhk/Flattening-Net. ",
    "url": "https://arxiv.org/abs/2212.08892",
    "authors": [
      "Qijian Zhang",
      "Junhui Hou",
      "Yue Qian",
      "Yiming Zeng",
      "Juyong Zhang",
      "Ying He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.08898",
    "title": "A Unified Approach for Resilience and Causal Responsibility with Integer  Linear Programming (ILP) and LP Relaxations",
    "abstract": "Resilience is one of the key algorithmic problems underlying various forms of reverse data management (such as view maintenance, deletion propagation, and various interventions for fairness): What is the minimal number of tuples to delete from a database in order to remove all answers from a query? A long-open question is determining those conjunctive queries (CQs) for which this problem can be solved in guaranteed PTIME. We shed new light on this and the related problem of causal responsibility by proposing a unified Integer Linear Programming (ILP) formulation. It is unified in that it can solve both prior studied restrictions (e.g., self-join-free CQs under set semantics that allow a PTIME solution) and new cases (e.g., all CQs under set or bag semantics It is also unified in that all queries and all instances are treated with the same approach, and the algorithm is guaranteed to terminate in PTIME for the easy cases. We prove that, for all easy self-join-free CQs, the Linear Programming (LP) relaxation of our encoding is identical to the ILP solution and thus standard ILP solvers are guaranteed to return the solution in PTIME. Our approach opens up the door to new variants and new fine-grained analysis: 1) It also works under bag semantics and we give the first dichotomy result for bags semantics in the problem space. 2) We give a more fine-grained analysis of the complexity of causal responsibility. 3) We recover easy instances for generally hard queries, such as instances with read-once provenance and instances that become easy because of Functional Dependencies in the data. 4) We solve an open conjecture from PODS 2020. 5) Experiments confirm that our results indeed predict the asymptotic running times, and that our universal ILP encoding is at times even faster to solve for the PTIME cases than a prior proposed dedicated flow algorithm. ",
    "url": "https://arxiv.org/abs/2212.08898",
    "authors": [
      "Neha Makhija",
      "Wolfgang Gatterbauer"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2212.08900",
    "title": "Robust Predictive Output-Feedback Safety Filter for Uncertain Nonlinear  Control Systems",
    "abstract": "In real-world applications, we often require reliable decision making under dynamics uncertainties using noisy high-dimensional sensory data. Recently, we have seen an increasing number of learning-based control algorithms developed to address the challenge of decision making under dynamics uncertainties. These algorithms often make assumptions about the underlying unknown dynamics and, as a result, can provide safety guarantees. This is more challenging for other widely used learning-based decision making algorithms such as reinforcement learning. Furthermore, the majority of existing approaches assume access to state measurements, which can be restrictive in practice. In this paper, inspired by the literature on safety filters and robust output-feedback control, we present a robust predictive output-feedback safety filter (RPOF-SF) framework that provides safety certification to an arbitrary controller applied to an uncertain nonlinear control system. The proposed RPOF-SF combines a robustly stable observer that estimates the system state from noisy measurement data and a predictive safety filter that renders an arbitrary controller safe by (possibly) minimally modifying the controller input to guarantee safety. We show in theory that the proposed RPOF-SF guarantees constraint satisfaction despite disturbances applied to the system. We demonstrate the efficacy of the proposed RPOF-SF algorithm using an uncertain mass-spring-damper system. ",
    "url": "https://arxiv.org/abs/2212.08900",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Angela P. Schoellig"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2212.08905",
    "title": "IoT Device Identification Based on Network Traffic Characteristics",
    "abstract": "IoT device identification plays an important role in monitoring and improving the performance and security of IoT devices. Compared to traditional non-IoT devices, IoT devices provide us with both unique challenges and opportunities in detecting the types of IoT devices. Based on critical insights obtained in our previous work on understanding the network traffic characteristics of IoT devices, in this paper we develop an effective machine-learning based IoT device identification scheme, named iotID. In developing iotID, we extract 70 features of TCP flows from three complementary aspects: remote network servers and port numbers, packet-level traffic characteristics such as packet inter-arrival times, and flow-level traffic characteristics such as flow duration. Different from existing work, we take into account the imbalance nature of network traffic generated by various devices in both the learning and evaluation phases of iotID. Our performance studies based on network traffic collected on a typical smart home environment consisting of both IoT and non-IoT devices show that iotID can achieve a balanced accuracy score of above 99%. ",
    "url": "https://arxiv.org/abs/2212.08905",
    "authors": [
      "Md Mainuddin",
      "Zhenhai Duan",
      "Yingfei Dong",
      "Shaeke Salman",
      "Tania Taami"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2212.08909",
    "title": "Controlling Styles in Neural Machine Translation with Activation Prompt",
    "abstract": "Neural machine translation(NMT) has aroused wide attention due to its impressive quality. Beyond quality, controlling translation styles is also an important demand for many languages. Previous related studies mainly focus on controlling formality and gain some improvements. However, they still face two challenges. The first is the evaluation limitation. Style contains abundant information including lexis, syntax, etc. But only formality is well studied. The second is the heavy reliance on iterative fine-tuning when new styles are required. Correspondingly, this paper contributes in terms of the benchmark and approach. First, we re-visit this task and propose a multiway stylized machine translation (MSMT) benchmark, which includes multiple categories of styles in four language directions to push the boundary of this task. Second, we propose a method named style activation prompt (StyleAP) by retrieving prompts from stylized monolingual corpus, which needs no extra fine-tuning. Experiments show that StyleAP could effectively control the style of translation and achieve remarkable performance. All of our data and code are released at https://github.com/IvanWang0730/StyleAP. ",
    "url": "https://arxiv.org/abs/2212.08909",
    "authors": [
      "Yifan Wang",
      "Zewei Sun",
      "Shanbo Cheng",
      "Weiguo Zheng",
      "Mingxuan Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.08923",
    "title": "Analyzing the Traffic of MANETs using Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have been taking role in many areas, thanks to their expressive power on graph-structured data. On the other hand, Mobile Ad-Hoc Networks (MANETs) are gaining attention as network technologies have been taken to the 5G level. However, there is no study that evaluates the efficiency of GNNs on MANETs. In this study, we aim to fill this absence by implementing a MANET dataset in a popular GNN framework, i.e., PyTorch Geometric; and show how GNNs can be utilized to analyze the traffic of MANETs. We operate an edge prediction task on the dataset with GraphSAGE (SAG) model, where SAG model tries to predict whether there is a link between two nodes. We construe several evaluation metrics to measure the performance and efficiency of GNNs on MANETs. SAG model showed 82.1 accuracy on average in the experiments. ",
    "url": "https://arxiv.org/abs/2212.08923",
    "authors": [
      "Taha Tekdogan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2212.08924",
    "title": "Convergence Analysis for Training Stochastic Neural Networks via  Stochastic Gradient Descent",
    "abstract": "In this paper, we carry out numerical analysis to prove convergence of a novel sample-wise back-propagation method for training a class of stochastic neural networks (SNNs). The structure of the SNN is formulated as discretization of a stochastic differential equation (SDE). A stochastic optimal control framework is introduced to model the training procedure, and a sample-wise approximation scheme for the adjoint backward SDE is applied to improve the efficiency of the stochastic optimal control solver, which is equivalent to the back-propagation for training the SNN. The convergence analysis is derived with and without convexity assumption for optimization of the SNN parameters. Especially, our analysis indicates that the number of SNN training steps should be proportional to the square of the number of layers in the convex optimization case. Numerical experiments are carried out to validate the analysis results, and the performance of the sample-wise back-propagation method for training SNNs is examined by benchmark machine learning examples. ",
    "url": "https://arxiv.org/abs/2212.08924",
    "authors": [
      "Richard Archibald",
      "Feng Bao",
      "Yanzhao Cao",
      "Hui Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08950",
    "title": "Beyond the C: Retargetable Decompilation using Neural Machine  Translation",
    "abstract": "The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software. Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation. Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code. In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages. We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation. ",
    "url": "https://arxiv.org/abs/2212.08950",
    "authors": [
      "Iman Hosseini",
      "Brendan Dolan-Gavitt"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2212.08954",
    "title": "Cascaded Compositional Residual Learning for Complex Interactive  Behaviors",
    "abstract": "Real-world autonomous missions often require rich interaction with nearby objects, such as doors or switches, along with effective navigation. However, such complex behaviors are difficult to learn because they involve both high-level planning and low-level motor control. We present a novel framework, Cascaded Compositional Residual Learning (CCRL), which learns composite skills by recursively leveraging a library of previously learned control policies. Our framework learns multiplicative policy composition, task-specific residual actions, and synthetic goal information simultaneously while freezing the prerequisite policies. We further explicitly control the style of the motion by regularizing residual actions. We show that our framework learns joint-level control policies for a diverse set of motor skills ranging from basic locomotion to complex interactive navigation, including navigating around obstacles, pushing objects, crawling under a table, pushing a door open with its leg, and holding it open while walking through it. The proposed CCRL framework leads to policies with consistent styles and lower joint torques, which we successfully transfer to a real Unitree A1 robot without any additional fine-tuning. ",
    "url": "https://arxiv.org/abs/2212.08954",
    "authors": [
      "K. Niranjan Kumar",
      "Irfan Essa",
      "Sehoon Ha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08965",
    "title": "Physics-informed Neural Networks with Periodic Activation Functions for  Solute Transport in Heterogeneous Porous Media",
    "abstract": "Solute transport in porous media is relevant to a wide range of applications in hydrogeology, geothermal energy, underground CO2 storage, and a variety of chemical engineering systems. Due to the complexity of solute transport in heterogeneous porous media, traditional solvers require high resolution meshing and are therefore expensive computationally. This study explores the application of a mesh-free method based on deep learning to accelerate the simulation of solute transport. We employ Physics-informed Neural Networks (PiNN) to solve solute transport problems in homogeneous and heterogeneous porous media governed by the advection-dispersion equation. Unlike traditional neural networks that learn from large training datasets, PiNNs only leverage the strong form mathematical models to simultaneously solve for multiple dependent or independent field variables (e.g., pressure and solute concentration fields). In this study, we construct PiNN using a periodic activation function to better represent the complex physical signals (i.e., pressure) and their derivatives (i.e., velocity). Several case studies are designed with the intention of investigating the proposed PiNN's capability to handle different degrees of complexity. A manual hyperparameter tuning method is used to find the best PiNN architecture for each test case. Point-wise error and mean square error (MSE) measures are employed to assess the performance of PiNNs' predictions against the ground truth solutions obtained analytically or numerically using the finite element method. Our findings show that the predictions of PiNN are in good agreement with the ground truth solutions while reducing computational complexity and cost by, at least, three orders of magnitude. ",
    "url": "https://arxiv.org/abs/2212.08965",
    "authors": [
      "Salah A Faroughi",
      "Pingki Datta",
      "Seyed Kourosh Mahjour",
      "Shirko Faroughi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08966",
    "title": "Graph Learning: A Comprehensive Survey and Future Directions",
    "abstract": "Graph learning aims to learn complex relationships among nodes and the topological structure of graphs, such as social networks, academic networks and e-commerce networks, which are common in the real world. Those relationships make graphs special compared with traditional tabular data in which nodes are dependent on non-Euclidean space and contain rich information to explore. Graph learning developed from graph theory to graph data mining and now is empowered with representation learning, making it achieve great performances in various scenarios, even including text, image, chemistry, and biology. Due to the broad application prospects in the real world, graph learning has become a popular and promising area in machine learning. Thousands of works have been proposed to solve various kinds of problems in graph learning and is appealing more and more attention in academic community, which makes it pivotal to survey previous valuable works. Although some of the researchers have noticed this phenomenon and finished impressive surveys on graph learning. However, they failed to link related objectives, methods and applications in a more logical way and cover current ample scenarios as well as challenging problems due to the rapid expansion of the graph learning. ",
    "url": "https://arxiv.org/abs/2212.08966",
    "authors": [
      "Shaopeng Wei",
      "Yu Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08975",
    "title": "Clinical Deterioration Prediction in Brazilian Hospitals Based on  Artificial Neural Networks and Tree Decision Models",
    "abstract": "Early recognition of clinical deterioration (CD) has vital importance in patients' survival from exacerbation or death. Electronic health records (EHRs) data have been widely employed in Early Warning Scores (EWS) to measure CD risk in hospitalized patients. Recently, EHRs data have been utilized in Machine Learning (ML) models to predict mortality and CD. The ML models have shown superior performance in CD prediction compared to EWS. Since EHRs data are structured and tabular, conventional ML models are generally applied to them, and less effort is put into evaluating the artificial neural network's performance on EHRs data. Thus, in this article, an extremely boosted neural network (XBNet) is used to predict CD, and its performance is compared to eXtreme Gradient Boosting (XGBoost) and random forest (RF) models. For this purpose, 103,105 samples from thirteen Brazilian hospitals are used to generate the models. Moreover, the principal component analysis (PCA) is employed to verify whether it can improve the adopted models' performance. The performance of ML models and Modified Early Warning Score (MEWS), an EWS candidate, are evaluated in CD prediction regarding the accuracy, precision, recall, F1-score, and geometric mean (G-mean) metrics in a 10-fold cross-validation approach. According to the experiments, the XGBoost model obtained the best results in predicting CD among Brazilian hospitals' data. ",
    "url": "https://arxiv.org/abs/2212.08975",
    "authors": [
      "Hamed Yazdanpanah",
      "Augusto C. M. Silva",
      "Murilo Guedes",
      "Hugo M. P. Morales",
      "Leandro dos S. Coelho",
      "Fernando G. Moro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08979",
    "title": "Language model acceptability judgements are not always robust to context",
    "abstract": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Most targeted syntactic evaluation datasets ask models to make these judgements with just a single context-free sentence as input. This does not match language models' training regime, in which input sentences are always highly contextualized by the surrounding corpus. This mismatch raises an important question: how robust are models' syntactic judgements in different contexts? In this paper, we investigate the stability of language models' performance on targeted syntactic evaluations as we vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts. However, they are substantially unstable for contexts containing syntactic structures matching those in the critical test content. Among all tested models (GPT-2 and five variants of OPT), we significantly improve models' judgements by providing contexts with matching syntactic structures, and conversely significantly worsen them using unacceptable contexts with matching but violated syntactic structures. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by simple features matching the context and the test inputs, such as lexical overlap and dependency overlap. This sensitivity to highly specific syntactic features of the context can only be explained by the models' implicit in-context learning abilities. ",
    "url": "https://arxiv.org/abs/2212.08979",
    "authors": [
      "Koustuv Sinha",
      "Jon Gauthier",
      "Aaron Mueller",
      "Kanishka Misra",
      "Keren Fuentes",
      "Roger Levy",
      "Adina Williams"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08981",
    "title": "A Layered Architecture for Universal Causality",
    "abstract": "We propose a layered hierarchical architecture called UCLA (Universal Causality Layered Architecture), which combines multiple levels of categorical abstraction for causal inference. At the top-most level, causal interventions are modeled combinatorially using a simplicial category of ordinal numbers. At the second layer, causal models are defined by a graph-type category. The non-random ``surgical\" operations on causal structures, such as edge deletion, are captured using degeneracy and face operators from the simplicial layer above. The third categorical abstraction layer corresponds to the data layer in causal inference. The fourth homotopy layer comprises of additional structure imposed on the instance layer above, such as a topological space, which enables evaluating causal models on datasets. Functors map between every pair of layers in UCLA. Each functor between layers is characterized by a universal arrow, which defines an isomorphism between every pair of categorical layers. These universal arrows define universal elements and representations through the Yoneda Lemma, and in turn lead to a new category of elements based on a construction introduced by Grothendieck. Causal inference between each pair of layers is defined as a lifting problem, a commutative diagram whose objects are categories, and whose morphisms are functors that are characterized as different types of fibrations. We illustrate the UCLA architecture using a range of examples, including integer-valued multisets that represent a non-graphical framework for conditional independence, and causal models based on graphs and string diagrams using symmetric monoidal categories. We define causal effect in terms of the homotopy colimit of the nerve of the category of elements. ",
    "url": "https://arxiv.org/abs/2212.08981",
    "authors": [
      "Sridhar Mahadevan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Category Theory (math.CT)"
    ]
  },
  {
    "id": "arXiv:2212.08987",
    "title": "A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter  Dataset",
    "abstract": "Most recent semantic frame parsing systems for spoken language understanding (SLU) are designed based on recurrent neural networks. These systems display decent performance on benchmark SLU datasets such as ATIS or SNIPS, which contain short utterances with relatively simple patterns. However, the current semantic frame parsing models lack a mechanism to handle out-of-distribution (\\emph{OOD}) patterns and out-of-vocabulary (\\emph{OOV}) tokens. In this paper, we introduce a robust semantic frame parsing pipeline that can handle both \\emph{OOD} patterns and \\emph{OOV} tokens in conjunction with a new complex Twitter dataset that contains long tweets with more \\emph{OOD} patterns and \\emph{OOV} tokens. The new pipeline demonstrates much better results in comparison to state-of-the-art baseline SLU models on both the SNIPS dataset and the new Twitter dataset (Our new Twitter dataset can be downloaded from https://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also build an E2E application to demo the feasibility of our algorithm and show why it is useful in real application. ",
    "url": "https://arxiv.org/abs/2212.08987",
    "authors": [
      "Yu Wang",
      "Hongxia Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08990",
    "title": "Plankton-FL: Exploration of Federated Learning for Privacy-Preserving  Training of Deep Neural Networks for Phytoplankton Classification",
    "abstract": "Creating high-performance generalizable deep neural networks for phytoplankton monitoring requires utilizing large-scale data coming from diverse global water sources. A major challenge to training such networks lies in data privacy, where data collected at different facilities are often restricted from being transferred to a centralized location. A promising approach to overcome this challenge is federated learning, where training is done at site level on local data, and only the model parameters are exchanged over the network to generate a global model. In this study, we explore the feasibility of leveraging federated learning for privacy-preserving training of deep neural networks for phytoplankton classification. More specifically, we simulate two different federated learning frameworks, federated learning (FL) and mutually exclusive FL (ME-FL), and compare their performance to a traditional centralized learning (CL) framework. Experimental results from this study demonstrate the feasibility and potential of federated learning for phytoplankton monitoring. ",
    "url": "https://arxiv.org/abs/2212.08990",
    "authors": [
      "Daniel Zhang",
      "Vikram Voleti",
      "Alexander Wong",
      "Jason Deglint"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.08995",
    "title": "Impact of Sentiment Analysis in Fake Review Detection",
    "abstract": "Fake review identification is an important topic and has gained the interest of experts all around the world. Identifying fake reviews is challenging for researchers, and there are several primary challenges to fake review detection. We propose developing an initial research paper for investigating fake reviews by using sentiment analysis. Ten research papers are identified that show fake reviews, and they discuss currently available solutions for predicting or detecting fake reviews. They also show the distribution of fake and truthful reviews through the analysis of sentiment. We summarize and compare previous studies related to fake reviews. We highlight the most significant challenges in the sentiment evaluation process and demonstrate that there is a significant impact on sentiment scores used to identify fake feedback. ",
    "url": "https://arxiv.org/abs/2212.08995",
    "authors": [
      "Amira Yousif",
      "James Buckley"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.08999",
    "title": "Sentence-level Feedback Generation for English Language Learners: Does  Data Augmentation Help?",
    "abstract": "In this paper, we present strong baselines for the task of Feedback Comment Generation for Writing Learning. Given a sentence and an error span, the task is to generate a feedback comment explaining the error. Sentences and feedback comments are both in English. We experiment with LLMs and also create multiple pseudo datasets for the task, investigating how it affects the performance of our system. We present our results for the task along with extensive analysis of the generated comments with the aim of aiding future studies in feedback comment generation for English language learners. ",
    "url": "https://arxiv.org/abs/2212.08999",
    "authors": [
      "Shabnam Behzad",
      "Amir Zeldes",
      "Nathan Schneider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09000",
    "title": "Confidence-aware Training of Smoothed Classifiers for Certified  Robustness",
    "abstract": "Any classifier can be \"smoothed out\" under Gaussian noise to build a new classifier that is provably robust to $\\ell_2$-adversarial perturbations, viz., by averaging its predictions over the noise via randomized smoothing. Under the smoothed classifiers, the fundamental trade-off between accuracy and (adversarial) robustness has been well evidenced in the literature: i.e., increasing the robustness of a classifier for an input can be at the expense of decreased accuracy for some other inputs. In this paper, we propose a simple training method leveraging this trade-off to obtain robust smoothed classifiers, in particular, through a sample-wise control of robustness over the training samples. We make this control feasible by using \"accuracy under Gaussian noise\" as an easy-to-compute proxy of adversarial robustness for an input. Specifically, we differentiate the training objective depending on this proxy to filter out samples that are unlikely to benefit from the worst-case (adversarial) objective. Our experiments show that the proposed method, despite its simplicity, consistently exhibits improved certified robustness upon state-of-the-art training methods. Somewhat surprisingly, we find these improvements persist even for other notions of robustness, e.g., to various types of common corruptions. ",
    "url": "https://arxiv.org/abs/2212.09000",
    "authors": [
      "Jongheon Jeong",
      "Seojin Kim",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09008",
    "title": "Hidden State Approximation in Recurrent Neural Networks Using Continuous  Particle Filtering",
    "abstract": "Using historical data to predict future events has many applications in the real world, such as stock price prediction; the robot localization. In the past decades, the Convolutional long short-term memory (LSTM) networks have achieved extraordinary success with sequential data in the related field. However, traditional recurrent neural networks (RNNs) keep the hidden states in a deterministic way. In this paper, we use the particles to approximate the distribution of the latent state and show how it can extend into a more complex form, i.e., the Encoder-Decoder mechanism. With the proposed continuous differentiable scheme, our model is capable of adaptively extracting valuable information and updating the latent state according to the Bayes rule. Our empirical studies demonstrate the effectiveness of our method in the prediction tasks. ",
    "url": "https://arxiv.org/abs/2212.09008",
    "authors": [
      "Dexun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09013",
    "title": "Graph Neural Network based Child Activity Recognition",
    "abstract": "This paper presents an implementation on child activity recognition (CAR) with a graph convolution network (GCN) based deep learning model since prior implementations in this domain have been dominated by CNN, LSTM and other methods despite the superior performance of GCN. To the best of our knowledge, we are the first to use a GCN model in child activity recognition domain. In overcoming the challenges of having small size publicly available child action datasets, several learning methods such as feature extraction, fine-tuning and curriculum learning were implemented to improve the model performance. Inspired by the contradicting claims made on the use of transfer learning in CAR, we conducted a detailed implementation and analysis on transfer learning together with a study on negative transfer learning effect on CAR as it hasn't been addressed previously. As the principal contribution, we were able to develop a ST-GCN based CAR model which, despite the small size of the dataset, obtained around 50% accuracy on vanilla implementations. With feature extraction and fine-tuning methods, accuracy was improved by 20%-30% with the highest accuracy being 82.24%. Furthermore, the results provided on activity datasets empirically demonstrate that with careful selection of pre-train model datasets through methods such as curriculum learning could enhance the accuracy levels. Finally, we provide preliminary evidence on possible frame rate effect on the accuracy of CAR models, a direction future research can explore. ",
    "url": "https://arxiv.org/abs/2212.09013",
    "authors": [
      "Sanka Mohottala",
      "Pradeepa Samarasinghe",
      "Dharshana Kasthurirathna",
      "Charith Abhayaratne"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09017",
    "title": "Neural Rankers for Effective Screening Prioritisation in Medical  Systematic Review Literature Search",
    "abstract": "Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for ``total recall''; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant. Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance. Our results show that BERT-based rankers outperform the current state-of-the-art screening prioritisation methods. However, BERT rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction. ",
    "url": "https://arxiv.org/abs/2212.09017",
    "authors": [
      "Shuai Wang",
      "Harrisen Scells",
      "Bevan Koopman",
      "Guido Zuccon"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09028",
    "title": "Neural Coreference Resolution based on Reinforcement Learning",
    "abstract": "The target of a coreference resolution system is to cluster all mentions that refer to the same entity in a given context. All coreference resolution systems need to solve two subtasks; one task is to detect all of the potential mentions, and the other is to learn the linking of an antecedent for each possible mention. In this paper, we propose a reinforcement learning actor-critic-based neural coreference resolution system, which can achieve both mention detection and mention clustering by leveraging an actor-critic deep reinforcement learning technique and a joint training algorithm. We experiment on the BERT model to generate different input span representations. Our model with the BERT span representation achieves the state-of-the-art performance among the models on the CoNLL-2012 Shared Task English Test Set. ",
    "url": "https://arxiv.org/abs/2212.09028",
    "authors": [
      "Yu Wang",
      "Hongxia Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09034",
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by  Bridging GNNs and MLPs",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom largely attributes the success of GNNs to their advanced expressivity for learning desired functions on nodes' ego-graphs, we conjecture that this is \\emph{not} the main cause of GNNs' superiority in node prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capabilities, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, and then adopt GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts across ten benchmarks and different experimental settings, despite the fact that PMLPs share the same (trained) weights with poorly-performed MLP. This critical finding opens a door to a brand new perspective for understanding the power of GNNs, and allow bridging GNNs and MLPs for dissecting their generalization behaviors. As an initial step to analyze PMLP, we show its essential difference with MLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, though MLP and PMLP cannot extrapolate non-linear functions for extreme OOD data, PMLP has more freedom to generalize near the training support. ",
    "url": "https://arxiv.org/abs/2212.09034",
    "authors": [
      "Chenxiao Yang",
      "Qitian Wu",
      "Jiahua Wang",
      "Junchi Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09035",
    "title": "Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted  Attacks",
    "abstract": "In this work, we study the black-box targeted attack problem from the model discrepancy perspective. On the theoretical side, we present a generalization error bound for black-box targeted attacks, which gives a rigorous theoretical analysis for guaranteeing the success of the attack. We reveal that the attack error on a target model mainly depends on empirical attack error on the substitute model and the maximum model discrepancy among substitute models. On the algorithmic side, we derive a new algorithm for black-box targeted attacks based on our theoretical analysis, in which we additionally minimize the maximum model discrepancy(M3D) of the substitute models when training the generator to generate adversarial examples. In this way, our model is capable of crafting highly transferable adversarial examples that are robust to the model variation, thus improving the success rate for attacking the black-box model. We conduct extensive experiments on the ImageNet dataset with different classification models, and our proposed approach outperforms existing state-of-the-art methods by a significant margin. Our codes will be released. ",
    "url": "https://arxiv.org/abs/2212.09035",
    "authors": [
      "Anqi Zhao",
      "Tong Chu",
      "Yahao Liu",
      "Wen Li",
      "Jingjing Li",
      "Lixin Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09040",
    "title": "The Underlying Correlated Dynamics in Neural Training",
    "abstract": "Training of neural networks is a computationally intensive task. The significance of understanding and modeling the training dynamics is growing as increasingly larger networks are being trained. We propose in this work a model based on the correlation of the parameters' dynamics, which dramatically reduces the dimensionality. We refer to our algorithm as \\emph{correlation mode decomposition} (CMD). It splits the parameter space into groups of parameters (modes) which behave in a highly correlated manner through the epochs. We achieve a remarkable dimensionality reduction with this approach, where networks like ResNet-18, transformers and GANs, containing millions of parameters, can be modeled well using just a few modes. We observe each typical time profile of a mode is spread throughout the network in all layers. Moreover, our model induces regularization which yields better generalization capacity on the test set. This representation enhances the understanding of the underlying training dynamics and can pave the way for designing better acceleration techniques. ",
    "url": "https://arxiv.org/abs/2212.09040",
    "authors": [
      "Rotem Turjeman",
      "Tom Berkov",
      "Ido Cohen",
      "Guy Gilboa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2212.09044",
    "title": "Text2Struct: A Machine Learning Pipeline for Mining Structured Data from  Text",
    "abstract": "Many analysis and prediction tasks require the extraction of structured data from unstructured texts. To solve it, this paper presents an end-to-end machine learning pipeline, Text2Struct, including a text annotation scheme, training data processing, and machine learning implementation. We formulated the mining problems as the extraction of metrics and units associated with numerals in the text. Text2Struct was evaluated on an annotated text dataset collected from abstracts of medical publications regarding thrombectomy. In terms of prediction performance, a dice coefficient of 0.82 was achieved on the test dataset. By random sampling, most predicted relations between numerals and entities were well matched to the ground-truth annotations. These results showed that the Text2Struct is viable for the mining of structured data from text without special templates or patterns. It is anticipated to further improve the pipeline by expanding the dataset and investigating other machine learning models. A code demonstration can be found at: https://github.com/zcc861007/CourseProject ",
    "url": "https://arxiv.org/abs/2212.09044",
    "authors": [
      "Chaochao Zhou",
      "Bo Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09045",
    "title": "Task Preferences across Languages on Community Question Answering  Platforms",
    "abstract": "With the steady emergence of community question answering (CQA) platforms like Quora, StackExchange, and WikiHow, users now have an unprecedented access to information on various kind of queries and tasks. Moreover, the rapid proliferation and localization of these platforms spanning geographic and linguistic boundaries offer a unique opportunity to study the task requirements and preferences of users in different socio-linguistic groups. In this study, we implement an entity-embedding model trained on a large longitudinal dataset of multi-lingual and task-oriented question-answer pairs to uncover and quantify the (i) prevalence and distribution of various online tasks across linguistic communities, and (ii) emerging and receding trends in task popularity over time in these communities. Our results show that there exists substantial variance in task preference as well as popularity trends across linguistic communities on the platform. Findings from this study will help Q&A platforms better curate and personalize content for non-English users, while also offering valuable insights to businesses looking to target non-English speaking communities online. ",
    "url": "https://arxiv.org/abs/2212.09045",
    "authors": [
      "Sebastin Santy",
      "Prasanta Bhattacharya",
      "Rishabh Mehrotra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2212.09050",
    "title": "Determining Distributions of Security Means for Wireless Sensor Networks  based on the Model of a Neighbourhood Watch",
    "abstract": "Neighbourhood watch is a concept that allows a community to distribute a complex security task in between all members. Members of the community carry out individual security tasks to contribute to the overall security of it. It reduces the workload of a particular individual while securing all members and allowing them to carry out a multitude of security tasks. Wireless sensor networks (WSNs) are composed of resource-constraint independent battery driven computers as nodes communicating wirelessly. Security in WSNs is essential. Without sufficient security, an attacker is able to eavesdrop the communication, tamper monitoring results or deny critical nodes providing their service in a way to cut off larger network parts. The resource-constraint nature of sensor nodes prevents them from running full-fledged security protocols. Instead, it is necessary to assess the most significant security threats and implement specialised protocols. A neighbourhood-watch inspired distributed security scheme for WSNs has been introduced by Langend\\\"orfer. Its goal is to increase the variety of attacks a WSN can fend off. A framework of such complexity has to be designed in multiple steps. Here, we introduce an approach to determine distributions of security means on large-scale static homogeneous WSNs. Therefore, we model WSNs as undirected graphs in which two nodes connected iff they are in transmission range. The framework aims to partition the graph into $n$ distinct security means resulting in the targeted distribution. The underlying problems turn out to be NP hard and we attempt to solve them using linear programs (LPs). To evaluate the computability of the LPs, we generate large numbers of random {\\lambda}-precision unit disk graphs (UDGs) as representation of WSNs. For this purpose, we introduce a novel {\\lambda}-precision UDG generator to model WSNs with a minimal distance in between nodes. ",
    "url": "https://arxiv.org/abs/2212.09050",
    "authors": [
      "Benjamin F\u00f6rster",
      "Peter Langend\u00f6rfer",
      "Thomas Hinze"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2212.09062",
    "title": "Bort: Towards Explainable Neural Networks with Bounded Orthogonal  Constraint",
    "abstract": "Deep learning has revolutionized human society, yet the black-box nature of deep neural networks hinders further application to reliability-demanded industries. In the attempt to unpack them, many works observe or impact internal variables to improve the model's comprehensibility and transparency. However, existing methods rely on intuitive assumptions and lack mathematical guarantees. To bridge this gap, we introduce Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and transparency. We perform reconstruction and backtracking on the model representations optimized by Bort and observe an evident improvement in model explainability. Based on Bort, we are able to synthesize explainable adversarial samples without additional parameters and training. Surprisingly, we find Bort constantly improves the classification accuracy of various architectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet. ",
    "url": "https://arxiv.org/abs/2212.09062",
    "authors": [
      "Borui Zhang",
      "Wenzhao Zheng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09067",
    "title": "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks",
    "abstract": "Backdoor attacks represent one of the major threats to machine learning models. Various efforts have been made to mitigate backdoors. However, existing defenses have become increasingly complex and often require high computational resources or may also jeopardize models' utility. In this work, we show that fine-tuning, one of the most common and easy-to-adopt machine learning training operations, can effectively remove backdoors from machine learning models while maintaining high model utility. Extensive experiments over three machine learning paradigms show that fine-tuning and our newly proposed super-fine-tuning achieve strong defense performance. Furthermore, we coin a new term, namely backdoor sequela, to measure the changes in model vulnerabilities to other attacks before and after the backdoor has been removed. Empirical evaluation shows that, compared to other defense methods, super-fine-tuning leaves limited backdoor sequela. We hope our results can help machine learning model owners better protect their models from backdoor threats. Also, it calls for the design of more advanced attacks in order to comprehensively assess machine learning models' backdoor vulnerabilities. ",
    "url": "https://arxiv.org/abs/2212.09067",
    "authors": [
      "Zeyang Sha",
      "Xinlei He",
      "Pascal Berrang",
      "Mathias Humbert",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09069",
    "title": "Masked Wavelet Representation for Compact Neural Radiance Fields",
    "abstract": "Neural radiance fields (NeRF) have demonstrated the potential of coordinate-based neural representation (neural fields or implicit neural representation) in neural rendering. However, using a multi-layer perceptron (MLP) to represent a 3D scene or object requires enormous computational resources and time. There have been recent studies on how to reduce these computational inefficiencies by using additional data structures, such as grids or trees. Despite the promising performance, the explicit data structure necessitates a substantial amount of memory. In this work, we present a method to reduce the size without compromising the advantages of having additional data structures. In detail, we propose using the wavelet transform on grid-based neural fields. Grid-based neural fields are for fast convergence, and the wavelet transform, whose efficiency has been demonstrated in high-performance standard codecs, is to improve the parameter efficiency of grids. Furthermore, in order to achieve a higher sparsity of grid coefficients while maintaining reconstruction quality, we present a novel trainable masking approach. Experimental results demonstrate that non-spatial grid coefficients, such as wavelet coefficients, are capable of attaining a higher level of sparsity than spatial grid coefficients, resulting in a more compact representation. With our proposed mask and compression pipeline, we achieved state-of-the-art performance within a memory budget of 2 MB. Our code is available at https://github.com/daniel03c1/masked_wavelet_nerf. ",
    "url": "https://arxiv.org/abs/2212.09069",
    "authors": [
      "Daniel Rho",
      "Byeonghyeon Lee",
      "Seungtae Nam",
      "Joo Chan Lee",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2212.09082",
    "title": "On the Connection between Invariant Learning and Adversarial Training  for Out-of-Distribution Generalization",
    "abstract": "Despite impressive success in many tasks, deep learning models are shown to rely on spurious features, which will catastrophically fail when generalized to out-of-distribution (OOD) data. Invariant Risk Minimization (IRM) is proposed to alleviate this issue by extracting domain-invariant features for OOD generalization. Nevertheless, recent work shows that IRM is only effective for a certain type of distribution shift (e.g., correlation shift) while it fails for other cases (e.g., diversity shift). Meanwhile, another thread of method, Adversarial Training (AT), has shown better domain transfer performance, suggesting that it has the potential to be an effective candidate for extracting domain-invariant features. This paper investigates this possibility by exploring the similarity between the IRM and AT objectives. Inspired by this connection, we propose Domainwise Adversarial Training (DAT), an AT-inspired method for alleviating distribution shift by domain-specific perturbations. Extensive experiments show that our proposed DAT can effectively remove domain-varying features and improve OOD generalization under both correlation shift and diversity shift. ",
    "url": "https://arxiv.org/abs/2212.09082",
    "authors": [
      "Shiji Xin",
      "Yifei Wang",
      "Jingtong Su",
      "Yisen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09083",
    "title": "Influence-Based Mini-Batching for Graph Neural Networks",
    "abstract": "Using graph neural networks for large graphs is challenging since there is no clear way of constructing mini-batches. To solve this, previous methods have relied on sampling or graph clustering. While these approaches often lead to good training convergence, they introduce significant overhead due to expensive random data accesses and perform poorly during inference. In this work we instead focus on model behavior during inference. We theoretically model batch construction via maximizing the influence score of nodes on the outputs. This formulation leads to optimal approximation of the output when we do not have knowledge of the trained model. We call the resulting method influence-based mini-batching (IBMB). IBMB accelerates inference by up to 130x compared to previous methods that reach similar accuracy. Remarkably, with adaptive optimization and the right training schedule IBMB can also substantially accelerate training, thanks to precomputed batches and consecutive memory accesses. This results in up to 18x faster training per epoch and up to 17x faster convergence per runtime compared to previous methods. ",
    "url": "https://arxiv.org/abs/2212.09083",
    "authors": [
      "Johannes Gasteiger",
      "Chendi Qian",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.09088",
    "title": "LR-CSNet: Low-Rank Deep Unfolding Network for Image Compressive Sensing",
    "abstract": "Deep unfolding networks (DUNs) have proven to be a viable approach to compressive sensing (CS). In this work, we propose a DUN called low-rank CS network (LR-CSNet) for natural image CS. Real-world image patches are often well-represented by low-rank approximations. LR-CSNet exploits this property by adding a low-rank prior to the CS optimization task. We derive a corresponding iterative optimization procedure using variable splitting, which is then translated to a new DUN architecture. The architecture uses low-rank generation modules (LRGMs), which learn low-rank matrix factorizations, as well as gradient descent and proximal mappings (GDPMs), which are proposed to extract high-frequency features to refine image details. In addition, the deep features generated at each reconstruction stage in the DUN are transferred between stages to boost the performance. Our extensive experiments on three widely considered datasets demonstrate the promising performance of LR-CSNet compared to state-of-the-art methods in natural image CS. ",
    "url": "https://arxiv.org/abs/2212.09088",
    "authors": [
      "Tianfang Zhang",
      "Lei Li",
      "Christian Igel",
      "Stefan Oehmcke",
      "Fabian Gieseke",
      "Zhenming Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2212.09096",
    "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on  DAG-based Blockchain",
    "abstract": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency. ",
    "url": "https://arxiv.org/abs/2212.09096",
    "authors": [
      "Hechuan Guo",
      "Minghui Xu",
      "Jiahao Zhang",
      "Chunchi Liu",
      "Dongxiao Yu",
      "Schahram Dustdar",
      "Xiuzhen Cheng"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2212.09097",
    "title": "Continually Learning from Existing Models: Knowledge Accumulation for  Neural Machine Translation",
    "abstract": "Although continually extending an existing NMT model to new domains or languages has attracted intensive interest in recent years, the equally valuable problem of continually improving a given NMT model in its domain by leveraging knowledge from an unlimited number of existing NMT models is not explored yet. To facilitate the study, we propose a formal definition for the problem named knowledge accumulation for NMT (KA-NMT) with corresponding datasets and evaluation metrics and develop a novel method for KA-NMT. We investigate a novel knowledge detection algorithm to identify beneficial knowledge from existing models at token level, and propose to learn from beneficial knowledge and learn against other knowledge simultaneously to improve learning efficiency. To alleviate catastrophic forgetting, we further propose to transfer knowledge from previous to current version of the given model. Extensive experiments show that our proposed method significantly and consistently outperforms representative baselines under homogeneous, heterogeneous, and malicious model settings for different language pairs. ",
    "url": "https://arxiv.org/abs/2212.09097",
    "authors": [
      "Yuanchi Zhang",
      "Peng Li",
      "Maosong Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09107",
    "title": "A Generalized Framework for Critical Heat Flux Detection Using  Unsupervised Image-to-Image Translation",
    "abstract": "This work proposes a framework developed to generalize Critical Heat Flux (CHF) detection classification models using an Unsupervised Image-to-Image (UI2I) translation model. The framework enables a typical classification model that was trained and tested on boiling images from domain A to predict boiling images coming from domain B that was never seen by the classification model. This is done by using the UI2I model to transform the domain B images to look like domain A images that the classification model is familiar with. Although CNN was used as the classification model and Fixed-Point GAN (FP-GAN) was used as the UI2I model, the framework is model agnostic. Meaning, that the framework can generalize any image classification model type, making it applicable to a variety of similar applications and not limited to the boiling crisis detection problem. It also means that the more the UI2I models advance, the better the performance of the framework. ",
    "url": "https://arxiv.org/abs/2212.09107",
    "authors": [
      "Firas Al-Hindawi",
      "Tejaswi Soorib",
      "Han Hu",
      "Md Siddiquee",
      "Hyunsoo Yoon",
      "Teresa Wu",
      "Ying Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2212.09144",
    "title": "Performance Analysis of YOLO-based Architectures for Vehicle Detection  from Traffic Images in Bangladesh",
    "abstract": "The task of locating and classifying different types of vehicles has become a vital element in numerous applications of automation and intelligent systems ranging from traffic surveillance to vehicle identification and many more. In recent times, Deep Learning models have been dominating the field of vehicle detection. Yet, Bangladeshi vehicle detection has remained a relatively unexplored area. One of the main goals of vehicle detection is its real-time application, where `You Only Look Once' (YOLO) models have proven to be the most effective architecture. In this work, intending to find the best-suited YOLO architecture for fast and accurate vehicle detection from traffic images in Bangladesh, we have conducted a performance analysis of different variants of the YOLO-based architectures such as YOLOV3, YOLOV5s, and YOLOV5x. The models were trained on a dataset containing 7390 images belonging to 21 types of vehicles comprising samples from the DhakaAI dataset, the Poribohon-BD dataset, and our self-collected images. After thorough quantitative and qualitative analysis, we found the YOLOV5x variant to be the best-suited model, performing better than YOLOv3 and YOLOv5s models respectively by 7 & 4 percent in mAP, and 12 & 8.5 percent in terms of Accuracy. ",
    "url": "https://arxiv.org/abs/2212.09144",
    "authors": [
      "Refaat Mohammad Alamgir",
      "Ali Abir Shuvro",
      "Mueeze Al Mushabbir",
      "Mohammed Ashfaq Raiyan",
      "Nusrat Jahan Rani",
      "Md. Mushfiqur Rahman",
      "Md. Hasanul Kabir",
      "Sabbir Ahmed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09155",
    "title": "Estimating the Adversarial Robustness of Attributions in Text with  Transformers",
    "abstract": "Explanations are crucial parts of deep neural network (DNN) classifiers. In high stakes applications, faithful and robust explanations are important to understand and gain trust in DNN classifiers. However, recent work has shown that state-of-the-art attribution methods in text classifiers are susceptible to imperceptible adversarial perturbations that alter explanations significantly while maintaining the correct prediction outcome. If undetected, this can critically mislead the users of DNNs. Thus, it is crucial to understand the influence of such adversarial perturbations on the networks' explanations and their perceptibility. In this work, we establish a novel definition of attribution robustness (AR) in text classification, based on Lipschitz continuity. Crucially, it reflects both attribution change induced by adversarial input alterations and perceptibility of such alterations. Moreover, we introduce a wide set of text similarity measures to effectively capture locality between two text samples and imperceptibility of adversarial perturbations in text. We then propose our novel TransformerExplanationAttack (TEA), a strong adversary that provides a tight estimation for attribution robustness in text classification. TEA uses state-of-the-art language models to extract word substitutions that result in fluent, contextual adversarial samples. Finally, with experiments on several text classification architectures, we show that TEA consistently outperforms current state-of-the-art AR estimators, yielding perturbations that alter explanations to a greater extent while being more fluent and less perceptible. ",
    "url": "https://arxiv.org/abs/2212.09155",
    "authors": [
      "Adam Ivankay",
      "Mattia Rigotti",
      "Ivan Girardi",
      "Chiara Marchiori",
      "Pascal Frossard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09163",
    "title": "CEDCES: A Cost Effective Deadline Constrained Evolutionary Scheduler for  Task Graphs in Multi-Cloud System",
    "abstract": "Many scientific workflows can be modeled as a Directed Acyclic Graph (henceforth mentioned as DAG) where the nodes represent individual tasks and the directed edges represent data and control flow dependency between two tasks. Due to large computational resource requirements, a single cloud cannot meet the requirements of the workflow. Hence, a multi-cloud system, where multiple cloud providers pool their resources together becomes a good solution. The major objectives considered while scheduling the tasks present in a task graph include execution cost and makespan. In this paper, we present Cost Effective Deadline Constrained Evolutionary Scheduler (henceforth mentioned as CEDCES) which aims to minimize the execution cost under a given deadline constraint. CEDCES contains Particle Swarm Optimization-based (henceforth mentioned as PSO) method in its core, however includes novel initialization, crossover, and mutation schemes. Extensive simulation experiments on real-world workflows show that CEDCES outperforms the state-of-art algorithms, in particular, 60.41% on average in terms of execution cost. In cases where the deadline is violated, CEDCES gives the least overshoot in execution time and outperforming the others by 10.96% on average. ",
    "url": "https://arxiv.org/abs/2212.09163",
    "authors": [
      "Atharva Tekawade",
      "Suman Banerjee"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2212.09166",
    "title": "A Cost Effective Reliability Aware Scheduler for Task Graphs in  Multi-Cloud System",
    "abstract": "Many scientific workflows can be represented by a Directed Acyclic Graph (DAG) where each node represents a task, and there will be a directed edge between two tasks if and only if there is a dependency relationship between the two i.e. the second one can not be started unless the first one is finished. Due to the increasing computational requirements of these workflows, they are deployed on cloud computing systems. Scheduling of workflows on such systems to achieve certain goals(e.g. minimization of makespan, cost, or maximization of reliability, etc.) remains an active area of research. In this paper, we propose a scheduling algorithm for allocating the nodes of our task graph in a heterogeneous multi-cloud system. The proposed scheduler considers many practical concerns such as pricing mechanisms, discounting schemes, and reliability analysis for task execution. This is a list-based heuristic that allocates tasks based on the expected times for which VMs need to be rented for them. We have analyzed the proposed approach to understand its time requirement. We perform a large number of experiments with real-world workflows: FFT, Ligo, Epigenomics, and Random workflows and observe that the proposed scheduler outperforms the state-of-art approaches up to 12%, 11%, and 1.1% in terms of cost, makespan, and reliability, respectively. ",
    "url": "https://arxiv.org/abs/2212.09166",
    "authors": [
      "Atharva Tekawade",
      "Suman Banerjee"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2212.09170",
    "title": "On Isotropy and Learning Dynamics of Contrastive-based Sentence  Representation Learning",
    "abstract": "Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, It is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we take a closer look at contrastive sentence representation learning through the lens of isotropy and learning dynamics. We interpret its success stories through the geometry of the representation shifts. We show that contrastive learning brings isotropy, and surprisingly learns to converge tokens to similar positions in the semantic space if given the signal that they are in the same sentence. Also, what we formalize as \"spurious contextualization\" is mitigated for semantically meaningful tokens, while augmented for functional ones. The embedding space is pushed toward the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamic with different training temperatures, batch sizes and pooling methods. With these findings, we aim to shed light on future designs of sentence representation learning methods. ",
    "url": "https://arxiv.org/abs/2212.09170",
    "authors": [
      "Chenghao Xiao",
      "Yang Long",
      "Noura Al Moubayed"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09175",
    "title": "Predicting Citi Bike Demand Evolution Using Dynamic Graphs",
    "abstract": "Bike sharing systems often suffer from poor capacity management as a result of variable demand. These bike sharing systems would benefit from models to predict demand in order to moderate the number of bikes stored at each station. In this paper, we attempt to apply a graph neural network model to predict bike demand in the New York City, Citi Bike dataset. ",
    "url": "https://arxiv.org/abs/2212.09175",
    "authors": [
      "Alexander Saff",
      "Mayur Bhandary",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09184",
    "title": "Faithful Heteroscedastic Regression with Neural Networks",
    "abstract": "Heteroscedastic regression models a Gaussian variable's mean and variance as a function of covariates. Parametric methods that employ neural networks for these parameter maps can capture complex relationships in the data. Yet, optimizing network parameters via log likelihood gradients can yield suboptimal mean and uncalibrated variance estimates. Current solutions side-step this optimization problem with surrogate objectives or Bayesian treatments. Instead, we make two simple modifications to optimization. Notably, their combination produces a heteroscedastic model with mean estimates that are provably as accurate as those from its homoscedastic counterpart (i.e.~fitting the mean under squared error loss). For a wide variety of network and task complexities, we find that mean estimates from existing heteroscedastic solutions can be significantly less accurate than those from an equivalently expressive mean-only model. Our approach provably retains the accuracy of an equally flexible mean-only model while also offering best-in-class variance calibration. Lastly, we show how to leverage our method to recover the underlying heteroscedastic noise variance. ",
    "url": "https://arxiv.org/abs/2212.09184",
    "authors": [
      "Andrew Stirn",
      "Hans-Hermann Wessels",
      "Megan Schertzer",
      "Laura Pereira",
      "Neville E. Sanjana",
      "David A. Knowles"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.09225",
    "title": "An Extension of Fisher's Criterion: Theoretical Results with a Neural  Network Realization",
    "abstract": "Fisher's criterion is a widely used tool in machine learning for feature selection. For large search spaces, Fisher's criterion can provide a scalable solution to select features. A challenging limitation of Fisher's criterion, however, is that it performs poorly when mean values of class-conditional distributions are close to each other. Motivated by this challenge, we propose an extension of Fisher's criterion to overcome this limitation. The proposed extension utilizes the available heteroscedasticity of class-conditional distributions to distinguish one class from another. Additionally, we describe how our theoretical results can be casted into a neural network framework, and conduct a proof-of-concept experiment to demonstrate the viability of our approach to solve classification problems. ",
    "url": "https://arxiv.org/abs/2212.09225",
    "authors": [
      "Ibrahim Alsolami",
      "Tomoki Fukai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09248",
    "title": "Natural Language to Code Generation in Interactive Data Science  Notebooks",
    "abstract": "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1082 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. ",
    "url": "https://arxiv.org/abs/2212.09248",
    "authors": [
      "Pengcheng Yin",
      "Wen-Ding Li",
      "Kefan Xiao",
      "Abhishek Rao",
      "Yeming Wen",
      "Kensen Shi",
      "Joshua Howland",
      "Paige Bailey",
      "Michele Catasta",
      "Henryk Michalewski",
      "Alex Polozov",
      "Charles Sutton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2212.09254",
    "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven  Optimization",
    "abstract": "Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense. ",
    "url": "https://arxiv.org/abs/2212.09254",
    "authors": [
      "Bairu Hou",
      "Jinghan Jia",
      "Yihua Zhang",
      "Guanhua Zhang",
      "Yang Zhang",
      "Sijia Liu",
      "Shiyu Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09270",
    "title": "The One-Inclusion Graph Algorithm is not Always Optimal",
    "abstract": "The one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth achieves an optimal in-expectation risk bound in the standard PAC classification setup. In one of the first COLT open problems, Warmuth conjectured that this prediction strategy always implies an optimal high probability bound on the risk, and hence is also an optimal PAC algorithm. We refute this conjecture in the strongest sense: for any practically interesting Vapnik-Chervonenkis class, we provide an in-expectation optimal one-inclusion graph algorithm whose high probability risk bound cannot go beyond that implied by Markov's inequality. Our construction of these poorly performing one-inclusion graph algorithms uses Varshamov-Tenengolts error correcting codes. Our negative result has several implications. First, it shows that the same poor high-probability performance is inherited by several recent prediction strategies based on generalizations of the one-inclusion graph algorithm. Second, our analysis shows yet another statistical problem that enjoys an estimator that is provably optimal in expectation via a leave-one-out argument, but fails in the high-probability regime. This discrepancy occurs despite the boundedness of the binary loss for which arguments based on concentration inequalities often provide sharp high probability risk bounds. ",
    "url": "https://arxiv.org/abs/2212.09270",
    "authors": [
      "Ishaq Aden-Ali",
      "Yeshwanth Cherapanamjeri",
      "Abhishek Shetty",
      "Nikita Zhivotovskiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2212.09271",
    "title": "Very Large Language Model as a Unified Methodology of Text Mining",
    "abstract": "Text data mining is the process of deriving essential information from language text. Typical text mining tasks include text categorization, text clustering, topic modeling, information extraction, and text summarization. Various data sets are collected and various algorithms are designed for the different types of tasks. In this paper, I present a blue sky idea that very large language model (VLLM) will become an effective unified methodology of text mining. I discuss at least three advantages of this new methodology against conventional methods. Finally I discuss the challenges in the design and development of VLLM techniques for text mining. ",
    "url": "https://arxiv.org/abs/2212.09271",
    "authors": [
      "Meng Jiang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09273",
    "title": "Learning Object-level Point Augmentor for Semi-supervised 3D Object  Detection",
    "abstract": "Semi-supervised object detection is important for 3D scene understanding because obtaining large-scale 3D bounding box annotations on point clouds is time-consuming and labor-intensive. Existing semi-supervised methods usually employ teacher-student knowledge distillation together with an augmentation strategy to leverage unlabeled point clouds. However, these methods adopt global augmentation with scene-level transformations and hence are sub-optimal for instance-level object detection. In this work, we propose an object-level point augmentor (OPA) that performs local transformations for semi-supervised 3D object detection. In this way, the resultant augmentor is derived to emphasize object instances rather than irrelevant backgrounds, making the augmented data more useful for object detector training. Extensive experiments on the ScanNet and SUN RGB-D datasets show that the proposed OPA performs favorably against the state-of-the-art methods under various experimental settings. The source code will be available at https://github.com/nomiaro/OPA. ",
    "url": "https://arxiv.org/abs/2212.09273",
    "authors": [
      "Cheng-Ju Ho",
      "Chen-Hsuan Tai",
      "Yi-Hsuan Tsai",
      "Yen-Yu Lin",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09277",
    "title": "Building Height Prediction with Instance Segmentation",
    "abstract": "Extracting building heights from satellite images is an active research area used in many fields such as telecommunications, city planning, etc. Many studies utilize DSM (Digital Surface Models) generated with lidars or stereo images for this purpose. Predicting the height of the buildings using only RGB images is challenging due to the insufficient amount of data, low data quality, variations of building types, different angles of light and shadow, etc. In this study, we present an instance segmentation-based building height extraction method to predict building masks with their respective heights from a single RGB satellite image. We used satellite images with building height annotations of certain cities along with an open-source satellite dataset with the transfer learning approach. We reached, the bounding box mAP 59, the mask mAP 52.6, and the average accuracy value of 70% for buildings belonging to each height class in our test set. ",
    "url": "https://arxiv.org/abs/2212.09277",
    "authors": [
      "Furkan Burak Bagci",
      "Ahmet Alp Kindriroglu",
      "Metehan Yalcin",
      "Ufuk Uyan",
      "Mahiye Uluyagmur Ozturk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09289",
    "title": "Detecting Features Concerning Privacy From App Reviews",
    "abstract": "Privacy requirements not only relate to legal compliance but also influence user satisfaction. Massive rapidly increasing App reviews have been proved a valuable requirements knowledge repository. Existing studies on App reviews mining have made much effort to automatically extract various requirements related information, e.g., feature request, bug report, and user opinions. However, less attention has been paid to privacy requirements refinement based on user reviews mining, which is beneficial for addressing users' privacy concern. In this work, we aim to detect privacy related features from App reviews to facilitate software maintenance activities. To that end, we design a semi-automatic framework to identify privacy related reviews from which App features are extracted and mapped to those listed in App descriptions. Firstly, we combine information retrieval and supervised text classification to identify privacy related reviews. Then, we design a dependency parsing method to extract App features from those privacy related reviews. Finally, those automatically extracted features are matched with those manually annotated ones in App descriptions based on phrase similarity. We quantitatively evaluate the three components of our framework on the reviews of Apps from multiple categories. For privacy related reviews identification, Gradient Boosting classifier achieves the highest F1 score of 93.77\\% among other competitive algorithms including deep learning ones. On App feature extraction, our dependency parsing based method can achieve a recall of 85.63\\%, more than 20\\% higher than the baseline. For feature matching, the phrase embedding cosine similarity shows the best matching result among four types of similarity methods, obtaining an average accuracy of 57\\%. We finally discuss the potential applications of our framework in detecting feature problems that may cause privacy threats. ",
    "url": "https://arxiv.org/abs/2212.09289",
    "authors": [
      "Jianzhang Zhang",
      "Jinping Hua",
      "Yiyang Chen",
      "Nan Niu",
      "Chuang Liu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2212.09290",
    "title": "XEngine: Optimal Tensor Rematerialization for Neural Networks in  Heterogeneous Environments",
    "abstract": "Memory efficiency is crucial in training deep learning networks on resource-restricted devices. During backpropagation, forward tensors are used to calculate gradients. Despite the option of keeping those dependencies in memory until they are reused in backpropagation, some forward tensors can be discarded and recomputed later from saved tensors, so-called checkpoints. This allows, in particular, for resource-constrained heterogeneous environments to make use of all available compute devices. Unfortunately, the definition of these checkpoints is a non-trivial problem and poses a challenge to the programmer - improper or excessive recomputations negate the benefit of checkpointing. In this article, we present XEngine, an approach that schedules network operators to heterogeneous devices in low memory environments by determining checkpoints and recomputations of tensors. Our approach selects suitable resources per timestep and operator and optimizes the end-to-end time for neural networks taking the memory limitation of each device into account. For this, we formulate a mixed-integer quadratic program (MIQP) to schedule operators of deep learning networks on heterogeneous systems. We compare our MIQP solver XEngine against Checkmate, a mixed-integer linear programming (MILP) approach that solves recomputation on a single device. Our solver finds solutions that are up to 22.5 % faster than the fastest Checkmate schedule in which the network is computed exclusively on a single device. We also find valid schedules for networks making use of both central processing units and graphics processing units if memory limitations do not allow scheduling exclusively to the graphics processing unit. ",
    "url": "https://arxiv.org/abs/2212.09290",
    "authors": [
      "Manuela Schuler",
      "Richard Membarth",
      "Philipp Slusallek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09304",
    "title": "Enabling Temporal-Spectral Decoding in Pre-movement Detection",
    "abstract": "Non-invasive brain-computer interfaces help the subjects to control external devices by brain intentions. The multi-class classification of upper limb movements can provide external devices with more control commands. The onsets of the upper limb movements are located by the external limb trajectory to eliminate the delay and bias among the trials. However, the trajectories are not recorded due to the limitation of experiments. The delay cannot be avoided in the analysis of signals. The delay negatively influences the classification performance, which limits the further application of upper limb movements in the brain-computer interface. This work focuses on multi-channel brain signals analysis in the temporal-frequency approach. It proposes the two-stage-training temporal-spectral neural network (TTSNet) to decode patterns from brain signals. The TTSNet first divides the signals into various filter banks. In each filter bank, task-related component analysis is used to reduce the dimension and reject the noise of the brain. A convolutional neural network (CNN) is then used to optimize the temporal characteristic of signals and extract class-related features. Finally, these class-related features from all filter banks are fused by concatenation and classified by the fully connected layer of the CNN. The proposed method is evaluated in two public datasets. The results show that TTSNet has an improved accuracy of 0.7456$\\pm$0.1205 compared to the EEGNet of 0.6506$\\pm$0.1275 ($p<0.05$) and FBTRCA of 0.6787$\\pm$0.1260 ($p<0.1$) in the movement detection task, which classifies the movement state and the resting state. The proposed method is expected to help detect limb movements and assist in the rehabilitation of stroke patients. ",
    "url": "https://arxiv.org/abs/2212.09304",
    "authors": [
      "Hao Jia",
      "Feng Duan",
      "Yu Zhang",
      "Zhe Sun",
      "Jordi Sole-Casals"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2212.09317",
    "title": "Synthetic Data Augmentation Using GAN For Improved Automated Visual  Inspection",
    "abstract": "Quality control is a crucial activity performed by manufacturing companies to ensure their products conform to the requirements and specifications. The introduction of artificial intelligence models enables to automate the visual quality inspection, speeding up the inspection process and ensuring all products are evaluated under the same criteria. In this research, we compare supervised and unsupervised defect detection techniques and explore data augmentation techniques to mitigate the data imbalance in the context of automated visual inspection. Furthermore, we use Generative Adversarial Networks for data augmentation to enhance the classifiers' discriminative performance. Our results show that state-of-the-art unsupervised defect detection does not match the performance of supervised models but can be used to reduce the labeling workload by more than 50%. Furthermore, the best classification performance was achieved considering GAN-based data generation with AUC ROC scores equal to or higher than 0,9898, even when increasing the dataset imbalance by leaving only 25\\% of the images denoting defective products. We performed the research with real-world data provided by Philips Consumer Lifestyle BV. ",
    "url": "https://arxiv.org/abs/2212.09317",
    "authors": [
      "Jo\u017ee M. Ro\u017eanec",
      "Patrik Zajec",
      "Spyros Theodoropoulos",
      "Erik Koehorst",
      "Bla\u017e Fortuna",
      "Dunja Mladeni\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09329",
    "title": "SrTR: Self-reasoning Transformer with Visual-linguistic Knowledge for  Scene Graph Generation",
    "abstract": "Objects in a scene are not always related. The execution efficiency of the one-stage scene graph generation approaches are quite high, which infer the effective relation between entity pairs using sparse proposal sets and a few queries. However, they only focus on the relation between subject and object in triplet set subject entity, predicate entity, object entity, ignoring the relation between subject and predicate or predicate and object, and the model lacks self-reasoning ability. In addition, linguistic modality has been neglected in the one-stage method. It is necessary to mine linguistic modality knowledge to improve model reasoning ability. To address the above-mentioned shortcomings, a Self-reasoning Transformer with Visual-linguistic Knowledge (SrTR) is proposed to add flexible self-reasoning ability to the model. An encoder-decoder architecture is adopted in SrTR, and a self-reasoning decoder is developed to complete three inferences of the triplet set, s+o-p, s+p-o and p+o-s. Inspired by the large-scale pre-training image-text foundation models, visual-linguistic prior knowledge is introduced and a visual-linguistic alignment strategy is designed to project visual representations into semantic spaces with prior knowledge to aid relational reasoning. Experiments on the Visual Genome dataset demonstrate the superiority and fast inference ability of the proposed method. ",
    "url": "https://arxiv.org/abs/2212.09329",
    "authors": [
      "Yuxiang Zhang",
      "Zhenbo Liu",
      "Shuai Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09352",
    "title": "Robust Anomaly Map Assisted Multiple Defect Detection with Supervised  Classification Techniques",
    "abstract": "Industry 4.0 aims to optimize the manufacturing environment by leveraging new technological advances, such as new sensing capabilities and artificial intelligence. The DRAEM technique has shown state-of-the-art performance for unsupervised classification. The ability to create anomaly maps highlighting areas where defects probably lie can be leveraged to provide cues to supervised classification models and enhance their performance. Our research shows that the best performance is achieved when training a defect detection model by providing an image and the corresponding anomaly map as input. Furthermore, such a setting provides consistent performance when framing the defect detection as a binary or multiclass classification problem and is not affected by class balancing policies. We performed the experiments on three datasets with real-world data provided by Philips Consumer Lifestyle BV. ",
    "url": "https://arxiv.org/abs/2212.09352",
    "authors": [
      "Jo\u017ee M. Ro\u017eanec",
      "Patrik Zajec",
      "Spyros Theodoropoulos",
      "Erik Koehorst",
      "Bla\u017e Fortuna",
      "Dunja Mladeni\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09381",
    "title": "Cognitive Accident Prediction in Driving Scenes: A Multimodality  Benchmark",
    "abstract": "Traffic accident prediction in driving videos aims to provide an early warning of the accident occurrence, and supports the decision making of safe driving systems. Previous works usually concentrate on the spatial-temporal correlation of object-level context, while they do not fit the inherent long-tailed data distribution well and are vulnerable to severe environmental change. In this work, we propose a Cognitive Accident Prediction (CAP) method that explicitly leverages human-inspired cognition of text description on the visual observation and the driver attention to facilitate model training. In particular, the text description provides a dense semantic description guidance for the primary context of the traffic scene, while the driver attention provides a traction to focus on the critical region closely correlating with safe driving. CAP is formulated by an attentive text-to-vision shift fusion module, an attentive scene context transfer module, and the driver attention guided accident prediction module. We leverage the attention mechanism in these modules to explore the core semantic cues for accident prediction. In order to train CAP, we extend an existing self-collected DADA-2000 dataset (with annotated driver attention for each frame) with further factual text descriptions for the visual observations before the accidents. Besides, we construct a new large-scale benchmark consisting of 11,727 in-the-wild accident videos with over 2.19 million frames (named as CAP-DATA) together with labeled fact-effect-reason-introspection description and temporal accident frame label. Based on extensive experiments, the superiority of CAP is validated compared with state-of-the-art approaches. The code, CAP-DATA, and all results will be released in \\url{https://github.com/JWFanggit/LOTVS-CAP}. ",
    "url": "https://arxiv.org/abs/2212.09381",
    "authors": [
      "Jianwu Fang",
      "Lei-Lei Li",
      "Kuan Yang",
      "Zhedong Zheng",
      "Jianru Xue",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09385",
    "title": "Prediction of Auto Insurance Risk Based on t-SNE Dimensionality  Reduction",
    "abstract": "Correct scoring of a driver's risk is of great significance to auto insurance companies. While the current tools used in this field have been proven in practice to be quite efficient and beneficial, we argue that there is still a lot of room for development and improvement in the auto insurance risk estimation process. To this end, we develop a framework based on a combination of a neural network together with a dimensionality reduction technique t-SNE (t-distributed stochastic neighbour embedding). This enables us to visually represent the complex structure of the risk as a two-dimensional surface, while still preserving the properties of the local region in the features space. The obtained results, which are based on real insurance data, reveal a clear contrast between the high and low risk policy holders, and indeed improve upon the actual risk estimation performed by the insurer. Due to the visual accessibility of the portfolio in this approach, we argue that this framework could be advantageous to the auto insurer, both as a main risk prediction tool and as an additional validation stage in other approaches. ",
    "url": "https://arxiv.org/abs/2212.09385",
    "authors": [
      "Joseph Levitas",
      "Konstantin Yavilberg",
      "Oleg Korol",
      "Genadi Man"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Risk Management (q-fin.RM)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.09400",
    "title": "An Efficient Drug-Drug Interactions Prediction Technology for  Molecularly Intelligent Manufacturing",
    "abstract": "Drug-Drug Interactions (DDIs) prediction is an essential issue in the molecular field. Traditional methods of observing DDIs in medical experiments require plenty of resources and labor. In this paper, we present a computational model dubbed MedKGQA based on Graph Neural Networks to automatically predict the DDIs after reading multiple medical documents in the form of multi-hop machine reading comprehension. We introduced a knowledge fusion system to obtain the complete nature of drugs and proteins and exploited a graph reasoning system to infer the drugs and proteins contained in the documents. Our model significantly improves the performance compared to previous state-of-the-art models on the QANGAROO MedHop dataset, which obtained a 4.5% improvement in terms of DDIs prediction accuracy. ",
    "url": "https://arxiv.org/abs/2212.09400",
    "authors": [
      "Peng Gao",
      "Feng Gao",
      "Jian-Cheng Ni",
      "Hamido Fujita"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09408",
    "title": "Million-scale Object Detection with Large Vision Model",
    "abstract": "Over the past few years, developing a broad, universal, and general-purpose computer vision system has become a hot topic. A powerful universal system would be capable of solving diverse vision tasks simultaneously without being restricted to a specific problem or a specific data domain, which is of great importance in practical real-world computer vision applications. This study pushes the direction forward by concentrating on the million-scale multi-domain universal object detection problem. The problem is not trivial due to its complicated nature in terms of cross-dataset category label duplication, label conflicts, and the hierarchical taxonomy handling. Moreover, what is the resource-efficient way to utilize emerging large pre-trained vision models for million-scale cross-dataset object detection remains an open challenge. This paper tries to address these challenges by introducing our practices in label handling, hierarchy-aware loss design and resource-efficient model training with a pre-trained large model. Our method is ranked second in the object detection track of Robust Vision Challenge 2022 (RVC 2022). We hope our detailed study would serve as an alternative practice paradigm for similar problems in the community. The code is available at https://github.com/linfeng93/Large-UniDet. ",
    "url": "https://arxiv.org/abs/2212.09408",
    "authors": [
      "Feng Lin",
      "Wenze Hu",
      "Yaowei Wang",
      "Yonghong Tian",
      "Guangming Lu",
      "Fanglin Chen",
      "Yong Xu",
      "Xiaoyu Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09412",
    "title": "Difformer: Empowering Diffusion Model on Embedding Space for Text  Generation",
    "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on visual and audio tasks, and recent works adapt them to textual data by diffusing on the embedding space. But the difference between the continuous data space and the embedding space raises challenges to the diffusion model, which have not been carefully explored. In this paper, we conduct systematic studies and analyze the challenges threefold. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the loss function. Secondly, as the norm of embedding varies between popular and rare words, adding the same noise scale will lead to sub-optimal results. In addition, we find that noises sampled from a standard Gaussian distribution may distract the diffusion process. To solve the above challenges, we propose Difformer, a denoising diffusion probabilistic model based on Transformer, which consists of three techniques including utilizing an anchor loss function, a layer normalization module for embeddings, and a norm factor to the Gaussian noise. All techniques are complementary to each other and critical to boosting the model performance together. Experiments are conducted on benchmark datasets over two seminal text generation tasks including machine translation and text summarization. The results show that Difformer significantly outperforms the embedding diffusion baselines, while achieving competitive results with strong autoregressive baselines. ",
    "url": "https://arxiv.org/abs/2212.09412",
    "authors": [
      "Zhujin Gao",
      "Junliang Guo",
      "Xu Tan",
      "Yongxin Zhu",
      "Fang Zhang",
      "Jiang Bian",
      "Linli Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09415",
    "title": "Training Lightweight Graph Convolutional Networks with Phase-field  Models",
    "abstract": "In this paper, we design lightweight graph convolutional networks (GCNs) using a particular class of regularizers, dubbed as phase-field models (PFMs). PFMs exhibit a bi-phase behavior using a particular ultra-local term that allows training both the topology and the weight parameters of GCNs as a part of a single \"end-to-end\" optimization problem. Our proposed solution also relies on a reparametrization that pushes the mask of the topology towards binary values leading to effective topology selection and high generalization while implementing any targeted pruning rate. Both masks and weights share the same set of latent variables and this further enhances the generalization power of the resulting lightweight GCNs. Extensive experiments conducted on the challenging task of skeleton-based recognition show the outperformance of PFMs against other staple regularizers as well as related lightweight design methods. ",
    "url": "https://arxiv.org/abs/2212.09415",
    "authors": [
      "Hichem Sahbi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09420",
    "title": "When Neural Model Meets NL2Code: A Survey",
    "abstract": "Given a natural language that describes the user's demands, the NL2Code task aims to generate code that addresses the demands. This is a critical but challenging task that mirrors the capabilities of AI-powered programming. The NL2Code task is inherently versatile, diverse and complex. For example, a demand can be described in different languages, in different formats, and at different levels of granularity. This inspired us to do this survey for NL2Code. In this survey, we focus on how does neural network (NN) solves NL2Code. We first propose a comprehensive framework, which is able to cover all studies in this field. Then, we in-depth parse the existing studies into this framework. We create an online website to record the parsing results, which tracks existing and recent NL2Code progress. In addition, we summarize the current challenges of NL2Code as well as its future directions. We hope that this survey can foster the evolution of this field. ",
    "url": "https://arxiv.org/abs/2212.09420",
    "authors": [
      "Daoguang Zan",
      "Bei Chen",
      "Fengji Zhang",
      "Dianjie Lu",
      "Bingchao Wu",
      "Bei Guan",
      "Yongji Wang",
      "Jian-Guang Lou"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2212.09426",
    "title": "Multistep Multiappliance Load Prediction",
    "abstract": "A well-performing prediction model is vital for a recommendation system suggesting actions for energy-efficient consumer behavior. However, reliable and accurate predictions depend on informative features and a suitable model design to perform well and robustly across different households and appliances. Moreover, customers' unjustifiably high expectations of accurate predictions may discourage them from using the system in the long term. In this paper, we design a three-step forecasting framework to assess predictability, engineering features, and deep learning architectures to forecast 24 hourly load values. First, our predictability analysis provides a tool for expectation management to cushion customers' anticipations. Second, we design several new weather-, time- and appliance-related parameters for the modeling procedure and test their contribution to the model's prediction performance. Third, we examine six deep learning techniques and compare them to tree- and support vector regression benchmarks. We develop a robust and accurate model for the appliance-level load prediction based on four datasets from four different regions (US, UK, Austria, and Canada) with an equal set of appliances. The empirical results show that cyclical encoding of time features and weather indicators alongside a long-short term memory (LSTM) model offer the optimal performance. ",
    "url": "https://arxiv.org/abs/2212.09426",
    "authors": [
      "Alona Zharova",
      "Antonia Scherz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09429",
    "title": "On the Complexity of Representation Learning in Contextual Linear  Bandits",
    "abstract": "In contextual linear bandits, the reward function is assumed to be a linear combination of an unknown reward vector and a given embedding of context-arm pairs. In practice, the embedding is often learned at the same time as the reward vector, thus leading to an online representation learning problem. Existing approaches to representation learning in contextual bandits are either very generic (e.g., model-selection techniques or algorithms for learning with arbitrary function classes) or specialized to particular structures (e.g., nested features or representations with certain spectral properties). As a result, the understanding of the cost of representation learning in contextual linear bandit is still limited. In this paper, we take a systematic approach to the problem and provide a comprehensive study through an instance-dependent perspective. We show that representation learning is fundamentally more complex than linear bandits (i.e., learning with a given representation). In particular, learning with a given set of representations is never simpler than learning with the worst realizable representation in the set, while we show cases where it can be arbitrarily harder. We complement this result with an extensive discussion of how it relates to existing literature and we illustrate positive instances where representation learning is as complex as learning with a fixed representation and where sub-logarithmic regret is achievable. ",
    "url": "https://arxiv.org/abs/2212.09429",
    "authors": [
      "Andrea Tirinzoni",
      "Matteo Pirotta",
      "Alessandro Lazaric"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.09460",
    "title": "Hardware Acceleration of Lane Detection Algorithm: A GPU Versus FPGA  Comparison",
    "abstract": "A Complete Computer vision system can be divided into two main categories: detection and classification. The Lane detection algorithm is a part of the computer vision detection category and has been applied in autonomous driving and smart vehicle systems. The lane detection system is responsible for lane marking in a complex road environment. At the same time, lane detection plays a crucial role in the warning system for a car when departs the lane. The implemented lane detection algorithm is mainly divided into two steps: edge detection and line detection. In this paper, we will compare the state-of-the-art implementation performance obtained with both FPGA and GPU to evaluate the trade-off for latency, power consumption, and utilization. Our comparison emphasises the advantages and disadvantages of the two systems. ",
    "url": "https://arxiv.org/abs/2212.09460",
    "authors": [
      "Mohamed Alshemi",
      "Sherif Saif",
      "Mohamed Taher"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2212.09465",
    "title": "Improving the Generalizability of Text-Based Emotion Detection by  Leveraging Transformers with Psycholinguistic Features",
    "abstract": "In recent years, there has been increased interest in building predictive models that harness natural language processing and machine learning techniques to detect emotions from various text sources, including social media posts, micro-blogs or news articles. Yet, deployment of such models in real-world sentiment and emotion applications faces challenges, in particular poor out-of-domain generalizability. This is likely due to domain-specific differences (e.g., topics, communicative goals, and annotation schemes) that make transfer between different models of emotion recognition difficult. In this work we propose approaches for text-based emotion detection that leverage transformer models (BERT and RoBERTa) in combination with Bidirectional Long Short-Term Memory (BiLSTM) networks trained on a comprehensive set of psycholinguistic features. First, we evaluate the performance of our models within-domain on two benchmark datasets: GoEmotion and ISEAR. Second, we conduct transfer learning experiments on six datasets from the Unified Emotion Dataset to evaluate their out-of-domain robustness. We find that the proposed hybrid models improve the ability to generalize to out-of-distribution data compared to a standard transformer-based approach. Moreover, we observe that these models perform competitively on in-domain data. ",
    "url": "https://arxiv.org/abs/2212.09465",
    "authors": [
      "Sourabh Zanwar",
      "Daniel Wiechmann",
      "Yu Qiao",
      "Elma Kerz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09473",
    "title": "Graph theoretical models and algorithms of portfolio compression",
    "abstract": "In portfolio compression, market participants (banks, organizations, companies, financial agents) sign contracts, creating liabilities between each other, which increases the systemic risk. Large, dense markets commonly can be compressed by reducing obligations without lowering the net notional of each participant (an example is if liabilities make a cycle between agents, then it is possible to reduce each of them without any net notional changing), and our target is to eliminate as much excess notional as possible in practice (excess is defined as the difference between gross and net notional). A limiting factor that may reduce the effectiveness of the compression can be the preferences and priorities of compression participants, who may individually define conditions for the compression, which must be considered when designing the clearing process, otherwise, a participant may bail out, resulting in the designed clearing process to be impossible to execute. These markets can be well-represented with edge-weighted graphs. In this paper, I examine cases when preferences of participants on behalf of clearing are given, e.g., in what order would they pay back their liabilities (a key factor can be the rate of interest) and I show a clearing algorithm for these problems. On top of that, since it is a common goal for the compression coordinating authority to maximize the compressed amount, I also show a method to compute the maximum volume conservative compression in a network. I further evaluate the possibility of combining the two models. Examples and program code of the model are also shown, also a0 pseudo-code of the clearing algorithms. ",
    "url": "https://arxiv.org/abs/2212.09473",
    "authors": [
      "Mih\u00e1ly P\u00e9ter Hanics"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Discrete Mathematics (cs.DM)",
      "Portfolio Management (q-fin.PM)"
    ]
  },
  {
    "id": "arXiv:2212.09476",
    "title": "Boosting Extra-functional Code Reusability in Cyber-physical Production  Systems: The Error Handling Case Study",
    "abstract": "Cyber-Physical Production Systems (CPPS) are long-living and mechatronic systems, which include mechanics, electrics/electronics and software. The interdisciplinary nature combined with challenges and trends in the context of Industry 4.0 such as a high degree of customization, small lot sizes and evolution cause a high amount of variability. Mastering the variability of functional control software, e.g., different control variants of an actuator type, is itself a challenge in developing and reusing CPPS software. This task becomes even more complex when considering extra-functional software such as operating modes, diagnosis and error handling. These software parts have high interdependencies with functional software, often involving the human-machine interface (HMI) to enable the intervention of operators. This paper illustrates the challenges in documenting the dependencies of these software parts including their variability using family models. A procedural and an object-oriented concept for implementing error handling, which represents an extra-functional task with high dependencies to functional software and the HMI, are proposed. The suitability of both concepts to increase the software's reusability and, thus, its flexibility in the context of Industry 4.0 is discussed. Their comparison confirms the high potential of the object-oriented extension of IEC 61131-3 to handle planned reuse of extra-functional CPPS software successfully. ",
    "url": "https://arxiv.org/abs/2212.09476",
    "authors": [
      "Birgit Vogel-Heuser",
      "Juliane Fischer",
      "Dieter Hess",
      "Eva-Maria Neumann",
      "Marcus Wuerr"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2212.09500",
    "title": "Exact Error Backpropagation Through Spikes for Precise Training of  Spiking Neural Networks",
    "abstract": "Event-based simulations of Spiking Neural Networks (SNNs) are fast and accurate. However, they are rarely used in the context of event-based gradient descent because their implementations on GPUs are difficult. Discretization with the forward Euler method is instead often used with gradient descent techniques but has the disadvantage of being computationally expensive. Moreover, the lack of precision of discretized simulations can create mismatches between the simulated models and analog neuromorphic hardware. In this work, we propose a new exact error-backpropagation through spikes method for SNNs, extending Fast \\& Deep to multiple spikes per neuron. We show that our method can be efficiently implemented on GPUs in a fully event-based manner, making it fast to compute and precise enough for analog neuromorphic hardware. Compared to the original Fast \\& Deep and the current state-of-the-art event-based gradient-descent algorithms, we demonstrate increased performance on several benchmark datasets with both feedforward and convolutional SNNs. In particular, we show that multi-spike SNNs can have advantages over single-spike networks in terms of convergence, sparsity, classification latency and sensitivity to the dead neuron problem. ",
    "url": "https://arxiv.org/abs/2212.09500",
    "authors": [
      "Florian Bacho",
      "Dominique Chu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09503",
    "title": "Measuring Annotator Agreement Generally across Complex Structured,  Multi-object, and Free-text Annotation Tasks",
    "abstract": "When annotators label data, a key metric for quality assurance is inter-annotator agreement (IAA): the extent to which annotators agree on their labels. Though many IAA measures exist for simple categorical and ordinal labeling tasks, relatively little work has considered more complex labeling tasks, such as structured, multi-object, and free-text annotations. Krippendorff's alpha, best known for use with simpler labeling tasks, does have a distance-based formulation with broader applicability, but little work has studied its efficacy and consistency across complex annotation tasks. We investigate the design and evaluation of IAA measures for complex annotation tasks, with evaluation spanning seven diverse tasks: image bounding boxes, image keypoints, text sequence tagging, ranked lists, free text translations, numeric vectors, and syntax trees. We identify the difficulty of interpretability and the complexity of choosing a distance function as key obstacles in applying Krippendorff's alpha generally across these tasks. We propose two novel, more interpretable measures, showing they yield more consistent IAA measures across tasks and annotation distance functions. ",
    "url": "https://arxiv.org/abs/2212.09503",
    "authors": [
      "Alexander Braylan",
      "Omar Alonso",
      "Matthew Lease"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09507",
    "title": "VC dimensions of group convolutional neural networks",
    "abstract": "We study the generalization capacity of group convolutional neural networks. We identify precise estimates for the VC dimensions of simple sets of group convolutional neural networks. In particular, we find that for infinite groups and appropriately chosen convolutional kernels, already two-parameter families of convolutional neural networks have an infinite VC dimension, despite being invariant to the action of an infinite group. ",
    "url": "https://arxiv.org/abs/2212.09507",
    "authors": [
      "Philipp Christian Petersen",
      "Anna Sepliarskaia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2212.09508",
    "title": "A note on the smallest eigenvalue of the empirical covariance of causal  Gaussian processes",
    "abstract": "We present a simple proof for bounding the smallest eigenvalue of the empirical covariance in a causal Gaussian process. Along the way, we establish a one-sided tail inequality for Gaussian quadratic forms using a causal decomposition. Our proof only uses elementary facts about the Gaussian distribution and the union bound. ",
    "url": "https://arxiv.org/abs/2212.09508",
    "authors": [
      "Ingvar Ziemann"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2212.09518",
    "title": "FedTADBench: Federated Time-Series Anomaly Detection Benchmark",
    "abstract": "Time series anomaly detection strives to uncover potential abnormal behaviors and patterns from temporal data, and has fundamental significance in diverse application scenarios. Constructing an effective detection model usually requires adequate training data stored in a centralized manner, however, this requirement sometimes could not be satisfied in realistic scenarios. As a prevailing approach to address the above problem, federated learning has demonstrated its power to cooperate with the distributed data available while protecting the privacy of data providers. However, it is still unclear that how existing time series anomaly detection algorithms perform with decentralized data storage and privacy protection through federated learning. To study this, we conduct a federated time series anomaly detection benchmark, named FedTADBench, which involves five representative time series anomaly detection algorithms and four popular federated learning methods. We would like to answer the following questions: (1)How is the performance of time series anomaly detection algorithms when meeting federated learning? (2) Which federated learning method is the most appropriate one for time series anomaly detection? (3) How do federated time series anomaly detection approaches perform on different partitions of data in clients? Numbers of results as well as corresponding analysis are provided from extensive experiments with various settings. The source code of our benchmark is publicly available at https://github.com/fanxingliu2020/FedTADBench. ",
    "url": "https://arxiv.org/abs/2212.09518",
    "authors": [
      "Fanxing Liu",
      "Cheng Zeng",
      "Le Zhang",
      "Yingjie Zhou",
      "Qing Mu",
      "Yanru Zhang",
      "Ling Zhang",
      "Ce Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09567",
    "title": "Answering Complex Logical Queries on Knowledge Graphs via Query Tree  Optimization",
    "abstract": "Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query tree. In particular, QTO utilizes the independence encoded in the query tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains state-of-the-art performance on complex query answering, outperforming previous best results by an average of 22%. Moreover, QTO can interpret the intermediate solutions for each of the one-hop atoms in the query with over 90% accuracy. ",
    "url": "https://arxiv.org/abs/2212.09567",
    "authors": [
      "Yushi Bai",
      "Xin Lv",
      "Juanzi Li",
      "Lei Hou"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2212.09573",
    "title": "Privacy Adhering Machine Un-learning in NLP",
    "abstract": "Regulations introduced by General Data Protection Regulation (GDPR) in the EU or California Consumer Privacy Act (CCPA) in the US have included provisions on the \\textit{right to be forgotten} that mandates industry applications to remove data related to an individual from their systems. In several real world industry applications that use Machine Learning to build models on user data, such mandates require significant effort both in terms of data cleansing as well as model retraining while ensuring the models do not deteriorate in prediction quality due to removal of data. As a result, continuous removal of data and model retraining steps do not scale if these applications receive such requests at a very high frequency. Recently, a few researchers proposed the idea of \\textit{Machine Unlearning} to tackle this challenge. Despite the significant importance of this task, the area of Machine Unlearning is under-explored in Natural Language Processing (NLP) tasks. In this paper, we explore the Unlearning framework on various GLUE tasks \\cite{Wang:18}, such as, QQP, SST and MNLI. We propose computationally efficient approaches (SISA-FC and SISA-A) to perform \\textit{guaranteed} Unlearning that provides significant reduction in terms of both memory (90-95\\%), time (100x) and space consumption (99\\%) in comparison to the baselines while keeping model performance constant. ",
    "url": "https://arxiv.org/abs/2212.09573",
    "authors": [
      "Vinayshekhar Bannihatti Kumar",
      "Rashmi Gangadharaiah",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09606",
    "title": "Discrimination, calibration, and point estimate accuracy of  GRU-D-Weibull architecture for real-time individualized endpoint prediction",
    "abstract": "Real-time individual endpoint prediction has always been a challenging task but of great clinic utility for both patients and healthcare providers. With 6,879 chronic kidney disease stage 4 (CKD4) patients as a use case, we explored the feasibility and performance of gated recurrent units with decay that models Weibull probability density function (GRU-D-Weibull) as a semi-parametric longitudinal model for real-time individual endpoint prediction. GRU-D-Weibull has a maximum C-index of 0.77 at 4.3 years of follow-up, compared to 0.68 achieved by competing models. The L1-loss of GRU-D-Weibull is ~66% of XGB(AFT), ~60% of MTLR, and ~30% of AFT model at CKD4 index date. The average absolute L1-loss of GRU-D-Weibull is around one year, with a minimum of 40% Parkes serious error after index date. GRU-D-Weibull is not calibrated and significantly underestimates true survival probability. Feature importance tests indicate blood pressure becomes increasingly important during follow-up, while eGFR and blood albumin are less important. Most continuous features have non-linear/parabola impact on predicted survival time, and the results are generally consistent with existing knowledge. GRU-D-Weibull as a semi-parametric temporal model shows advantages in built-in parameterization of missing, native support for asynchronously arrived measurement, capability of output both probability and point estimates at arbitrary time point for arbitrary prediction horizon, improved discrimination and point estimate accuracy after incorporating newly arrived data. Further research on its performance with more comprehensive input features, in-process or post-process calibration are warranted to benefit CKD4 or alike terminally-ill patients. ",
    "url": "https://arxiv.org/abs/2212.09606",
    "authors": [
      "Xiaoyang Ruan",
      "Liwei Wang",
      "Michelle Mai",
      "Charat Thongprayoon",
      "Wisit Cheungpasitporn",
      "Hongfang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2212.09631",
    "title": "Optimal Transport for Unsupervised Hallucination Detection in Neural  Machine Translation",
    "abstract": "Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement effective preventive strategies to guarantee their proper functioning. In this paper, we address the problem of hallucination detection in NMT by following a simple intuition: as hallucinations are detached from the source content, they exhibit encoder-decoder attention patterns that are statistically different from those of good quality translations. We frame this problem with an optimal transport formulation and propose a fully unsupervised, plug-in detector that can be used with any attention-based NMT model. Experimental results show that our detector not only outperforms all previous model-based detectors, but is also competitive with detectors that employ large models trained on millions of samples. ",
    "url": "https://arxiv.org/abs/2212.09631",
    "authors": [
      "Nuno M. Guerreiro",
      "Pierre Colombo",
      "Pablo Piantanida",
      "Andr\u00e9 F. T. Martins"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09637",
    "title": "A Sequential Concept Drift Detection Method for On-Device Learning on  Low-End Edge Devices",
    "abstract": "A practical issue of edge AI systems is that data distributions of trained dataset and deployed environment may differ due to noise and environmental changes over time. Such a phenomenon is known as a concept drift, and this gap degrades the performance of edge AI systems and may introduce system failures. To address this gap, a retraining of neural network models triggered by concept drift detection is a practical approach. However, since available compute resources are strictly limited in edge devices, in this paper we propose a lightweight concept drift detection method in cooperation with a recently proposed on-device learning technique of neural networks. In this case, both the neural network retraining and the proposed concept drift detection are done by sequential computation only to reduce computation cost and memory utilization. Evaluation results of the proposed approach shows that while the accuracy is decreased by 3.8%-4.3% compared to existing batch-based detection methods, it decreases the memory size by 88.9%-96.4% and the execution time by 1.3%-83.8%. As a result, the combination of the neural network retraining and the proposed concept drift detection method is demonstrated on Raspberry Pi Pico that has 264kB memory. ",
    "url": "https://arxiv.org/abs/2212.09637",
    "authors": [
      "Takeya Yamada",
      "Hiroki Matsutani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09656",
    "title": "Visconde: Multi-document QA with GPT-3 and Neural Reranking",
    "abstract": "This paper proposes a question-answering system that can answer questions whose supporting evidence is spread over multiple (potentially long) documents. The system, called Visconde, uses a three-step pipeline to perform the task: decompose, retrieve, and aggregate. The first step decomposes the question into simpler questions using a few-shot large language model (LLM). Then, a state-of-the-art search engine is used to retrieve candidate passages from a large collection for each decomposed question. In the final step, we use the LLM in a few-shot setting to aggregate the contents of the passages into the final answer. The system is evaluated on three datasets: IIRC, Qasper, and StrategyQA. Results suggest that current retrievers are the main bottleneck and that readers are already performing at the human level as long as relevant passages are provided. The system is also shown to be more effective when the model is induced to give explanations before answering a question. Code is available at \\url{https://github.com/neuralmind-ai/visconde}. ",
    "url": "https://arxiv.org/abs/2212.09656",
    "authors": [
      "Jayr Pereira",
      "Robson Fidalgo",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2212.09657",
    "title": "Grafting Laplace and Gaussian distributions: A new noise mechanism for  differential privacy",
    "abstract": "The framework of Differential privacy protects an individual's privacy while publishing query responses on congregated data. In this work, a new noise addition mechanism for differential privacy is introduced where the noise added is sampled from a hybrid density that resembles Laplace in the centre and Gaussian in the tail. With a sharper centre and light, sub-Gaussian tail, this density has the best characteristics of both distributions. We theoretically analyse the proposed mechanism and we derive the necessary and sufficient condition in one dimension and a sufficient condition in high dimensions for the mechanism to guarantee (${\\epsilon}$,${\\delta}$)-differential privacy. Numerical simulations corroborate the efficacy of the proposed mechanism compared to other existing mechanisms in achieving better trade-off for privacy and accuracy. ",
    "url": "https://arxiv.org/abs/2212.09657",
    "authors": [
      "Gokularam Muthukrishnan",
      "Sheetal Kalyani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2212.09663",
    "title": "Norm of word embedding encodes information gain",
    "abstract": "Distributed representations of words encode lexical semantic information, but how is that information encoded in word embeddings? Focusing on the skip-gram with negative-sampling method, we show theoretically and experimentally that the squared norm of word embedding encodes the information gain defined by the Kullback-Leibler divergence of the co-occurrence distribution of a word to the unigram distribution of the corpus. Furthermore, through experiments on tasks of keyword extraction, hypernym prediction, and part-of-speech discrimination, we confirmed that the KL divergence and the squared norm of embedding work as a measure of the informativeness of a word provided that the bias caused by word frequency is adequately corrected. ",
    "url": "https://arxiv.org/abs/2212.09663",
    "authors": [
      "Momose Oyama",
      "Sho Yokoi",
      "Hidetoshi Shimodaira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09666",
    "title": "MultiCoder: Multi-Programming-Lingual Pre-Training for Low-Resource Code  Completion",
    "abstract": "Code completion is a valuable topic in both academia and industry. Recently, large-scale mono-programming-lingual (MonoPL) pre-training models have been proposed to boost the performance of code completion. However, the code completion on low-resource programming languages (PL) is difficult for the data-driven paradigm, while there are plenty of developers using low-resource PLs. On the other hand, there are few studies exploring the effects of multi-programming-lingual (MultiPL) pre-training for the code completion, especially the impact on low-resource programming languages. To this end, we propose the MultiCoder to enhance the low-resource code completion via MultiPL pre-training and MultiPL Mixture-of-Experts (MoE) layers. We further propose a novel PL-level MoE routing strategy (PL-MoE) for improving the code completion on all PLs. Experimental results on CodeXGLUE and MultiCC demonstrate that 1) the proposed MultiCoder significantly outperforms the MonoPL baselines on low-resource programming languages, and 2) the PL-MoE module further boosts the performance on six programming languages. In addition, we analyze the effects of the proposed method in details and explore the effectiveness of our method in a variety of scenarios. ",
    "url": "https://arxiv.org/abs/2212.09666",
    "authors": [
      "Zi Gong",
      "Yinpeng Guo",
      "Pingyi Zhou",
      "Cuiyun Gao",
      "Yasheng Wang",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2212.09724",
    "title": "A Retrieve-and-Read Framework for Knowledge Graph Link Prediction",
    "abstract": "Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to over-smoothing of representations and also limits their scalability. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the reader, which incorporates graph-based attention structure and cross-attention between query and context for deep fusion. This design enables the model to focus on salient context information relevant to the query. Empirical results on two standard KG link prediction datasets demonstrate the competitive performance of the proposed method. ",
    "url": "https://arxiv.org/abs/2212.09724",
    "authors": [
      "Vardaan Pahuja",
      "Boshi Wang",
      "Hugo Latapie",
      "Jayanth Srinivasa",
      "Yu Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.09730",
    "title": "Speaking Style Conversion With Discrete Self-Supervised Units",
    "abstract": "Voice Conversion (VC) is the task of making a spoken utterance by one speaker sound as if uttered by a different speaker, while keeping other aspects like content unchanged. Current VC methods, focus primarily on spectral features like timbre, while ignoring the unique speaking style of people which often impacts prosody. In this study, we introduce a method for converting not only the timbre, but also prosodic information (i.e., rhythm and pitch changes) to those of the target speaker. The proposed approach is based on a pretrained, self-supervised, model for encoding speech to discrete units, which make it simple, effective, and easy to optimise. We consider the many-to-many setting with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate the proposed approach is significantly superior to the evaluated baselines. Code and samples can be found under https://pages.cs.huji.ac.il/adiyoss-lab/dissc/ . ",
    "url": "https://arxiv.org/abs/2212.09730",
    "authors": [
      "Gallil Maimon",
      "Yossi Adi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2212.08674",
    "title": "An unfolding method based on conditional Invertible Neural Networks  (cINN) using iterative training",
    "abstract": "The unfolding of detector effects is crucial for the comparison of data to theory predictions. While traditional methods are limited to representing the data in a low number of dimensions, machine learning has enabled new unfolding techniques while retaining the full dimensionality. Generative networks like invertible neural networks~(INN) enable a probabilistic unfolding, which map individual events to their corresponding unfolded probability distribution. The accuracy of such methods is however limited by how well simulated training samples model the actual data that is unfolded. We introduce the iterative conditional INN~(IcINN) for unfolding that adjusts for deviations between simulated training samples and data. The IcINN unfolding is first validated on toy data and then applied to pseudo-data for the $pp \\to Z \\gamma \\gamma$ process. ",
    "url": "https://arxiv.org/abs/2212.08674",
    "authors": [
      "Mathias Backes",
      "Anja Butter",
      "Monica Dunford",
      "Bogdan Malaescu"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2212.08740",
    "title": "Lateral Strain Imaging using Self-supervised and Physically Inspired  Constraints in Unsupervised Regularized Elastography",
    "abstract": "Convolutional Neural Networks (CNN) have shown promising results for displacement estimation in UltraSound Elastography (USE). Many modifications have been proposed to improve the displacement estimation of CNNs for USE in the axial direction. However, the lateral strain, which is essential in several downstream tasks such as the inverse problem of elasticity imaging, remains a challenge. The lateral strain estimation is complicated since the motion and the sampling frequency in this direction are substantially lower than the axial one, and a lack of carrier signal in this direction. In computer vision applications, the axial and the lateral motions are independent. In contrast, the tissue motion pattern in USE is governed by laws of physics which link the axial and lateral displacements. In this paper, inspired by Hooke's law, we first propose Physically Inspired ConsTraint for Unsupervised Regularized Elastography (PICTURE), where we impose a constraint on the Effective Poisson's ratio (EPR) to improve the lateral strain estimation. In the next step, we propose self-supervised PICTURE (sPICTURE) to further enhance the strain image estimation. Extensive experiments on simulation, experimental phantom and in vivo data demonstrate that the proposed methods estimate accurate axial and lateral strain maps. ",
    "url": "https://arxiv.org/abs/2212.08740",
    "authors": [
      "Ali K. Z. Tehrani",
      "Md Ashikuzzaman",
      "Hassan Rivaz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.08826",
    "title": "Molecule optimization via multi-objective evolutionary in implicit  chemical space",
    "abstract": "Machine learning methods have been used to accelerate the molecule optimization process. However, efficient search for optimized molecules satisfying several properties with scarce labeled data remains a challenge for machine learning molecule optimization. In this study, we propose MOMO, a multi-objective molecule optimization framework to address the challenge by combining learning of chemical knowledge with Pareto-based multi-objective evolutionary search. To learn chemistry, it employs a self-supervised codec to construct an implicit chemical space and acquire the continues representation of molecules. To explore the established chemical space, MOMO uses multi-objective evolution to comprehensively and efficiently search for similar molecules with multiple desirable properties. We demonstrate the high performance of MOMO on four multi-objective property and similarity optimization tasks, and illustrate the search capability of MOMO through case studies. Remarkably, our approach significantly outperforms previous approaches in optimizing three objectives simultaneously. The results show the optimization capability of MOMO, suggesting to improve the success rate of lead molecule optimization. ",
    "url": "https://arxiv.org/abs/2212.08826",
    "authors": [
      "Xin Xia",
      "Yansen Su",
      "Chunhou Zheng",
      "Xiangxiang Zeng"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2212.09060",
    "title": "Parsing as a lifting problem and the Chomsky-Sch\u00fctzenberger  representation theorem",
    "abstract": "We begin by explaining how any context-free grammar encodes a functor of operads from a freely generated operad into a certain \"operad of spliced words\". This motivates a more general notion of CFG over any category $C$, defined as a finite species $S$ equipped with a color denoting the start symbol and a functor of operads $p : Free[S] \\to W[C]$ into the operad of spliced arrows in $C$. We show that many standard properties of CFGs can be formulated within this framework, and that usual closure properties of CF languages generalize to CF languages of arrows. We also discuss a dual fibrational perspective on the functor $p$ via the notion of \"displayed\" operad, corresponding to a lax functor of operads $W[C] \\to Span(Set)$. We then turn to the Chomsky-Sch\\\"utzenberger Representation Theorem. We describe how a non-deterministic finite state automaton can be seen as a category $Q$ equipped with a pair of objects denoting initial and accepting states and a functor of categories $Q \\to C$ satisfying the unique lifting of factorizations property and the finite fiber property. Then, we explain how to extend this notion of automaton to functors of operads, which generalize tree automata, allowing us to lift an automaton over a category to an automaton over its operad of spliced arrows. We show that every CFG over a category can be pulled back along a ND finite state automaton over the same category, and hence that CF languages are closed under intersection with regular languages. The last important ingredient is the identification of a left adjoint $C[-] : Operad \\to Cat$ to the operad of spliced arrows functor, building the \"contour category\" of an operad. Using this, we generalize the C-S representation theorem, proving that any context-free language of arrows over a category $C$ is the functorial image of the intersection of a $C$-chromatic tree contour language and a regular language. ",
    "url": "https://arxiv.org/abs/2212.09060",
    "authors": [
      "Paul-Andr\u00e9 Melli\u00e8s",
      "Noam Zeilberger"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2212.09276",
    "title": "COVID-19 Detection Based on Self-Supervised Transfer Learning Using  Chest X-Ray Images",
    "abstract": "Purpose: Considering several patients screened due to COVID-19 pandemic, computer-aided detection has strong potential in assisting clinical workflow efficiency and reducing the incidence of infections among radiologists and healthcare providers. Since many confirmed COVID-19 cases present radiological findings of pneumonia, radiologic examinations can be useful for fast detection. Therefore, chest radiography can be used to fast screen COVID-19 during the patient triage, thereby determining the priority of patient's care to help saturated medical facilities in a pandemic situation. Methods: In this paper, we propose a new learning scheme called self-supervised transfer learning for detecting COVID-19 from chest X-ray (CXR) images. We compared six self-supervised learning (SSL) methods (Cross, BYOL, SimSiam, SimCLR, PIRL-jigsaw, and PIRL-rotation) with the proposed method. Additionally, we compared six pretrained DCNNs (ResNet18, ResNet50, ResNet101, CheXNet, DenseNet201, and InceptionV3) with the proposed method. We provide quantitative evaluation on the largest open COVID-19 CXR dataset and qualitative results for visual inspection. Results: Our method achieved a harmonic mean (HM) score of 0.985, AUC of 0.999, and four-class accuracy of 0.953. We also used the visualization technique Grad-CAM++ to generate visual explanations of different classes of CXR images with the proposed method to increase the interpretability. Conclusions: Our method shows that the knowledge learned from natural images using transfer learning is beneficial for SSL of the CXR images and boosts the performance of representation learning for COVID-19 detection. Our method promises to reduce the incidence of infections among radiologists and healthcare providers. ",
    "url": "https://arxiv.org/abs/2212.09276",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09281",
    "title": "Boosting Automatic COVID-19 Detection Performance with Self-Supervised  Learning and Batch Knowledge Ensembling",
    "abstract": "Background and objective: COVID-19 and its variants have caused significant disruptions in over 200 countries and regions worldwide, affecting the health and lives of billions of people. Detecting COVID-19 from chest X-Ray (CXR) images has become one of the fastest and easiest methods for detecting COVID-19 since the common occurrence of radiological pneumonia findings in COVID-19 patients. We present a novel high-accuracy COVID-19 detection method that uses CXR images. Methods: Our method consists of two phases. One is self-supervised learning-based pertaining; the other is batch knowledge ensembling-based fine-tuning. Self-supervised learning-based pretraining can learn distinguished representations from CXR images without manually annotated labels. On the other hand, batch knowledge ensembling-based fine-tuning can utilize category knowledge of images in a batch according to their visual feature similarities to improve detection performance. Unlike our previous implementation, we introduce batch knowledge ensembling into the fine-tuning phase, reducing the memory used in self-supervised learning and improving COVID-19 detection accuracy. Results: On two public COVID-19 CXR datasets, namely, a large dataset and an unbalanced dataset, our method exhibited promising COVID-19 detection performance. Our method maintains high detection accuracy even when annotated CXR training images are reduced significantly (e.g., using only 10% of the original dataset). In addition, our method is insensitive to changes in hyperparameters. Conclusions: The proposed method outperforms other state-of-the-art COVID-19 detection methods in different settings. Our method can reduce the workloads of healthcare providers and radiologists. ",
    "url": "https://arxiv.org/abs/2212.09281",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.09310",
    "title": "Multimodal CNN Networks for Brain Tumor Segmentation in MRI: A BraTS  2022 Challenge Solution",
    "abstract": "Automatic segmentation is essential for the brain tumor diagnosis, disease prognosis, and follow-up therapy of patients with gliomas. Still, accurate detection of gliomas and their sub-regions in multimodal MRI is very challenging due to the variety of scanners and imaging protocols. Over the last years, the BraTS Challenge has provided a large number of multi-institutional MRI scans as a benchmark for glioma segmentation algorithms. This paper describes our contribution to the BraTS 2022 Continuous Evaluation challenge. We propose a new ensemble of multiple deep learning frameworks namely, DeepSeg, nnU-Net, and DeepSCAN for automatic glioma boundaries detection in pre-operative MRI. It is worth noting that our ensemble models took first place in the final evaluation on the BraTS testing dataset with Dice scores of 0.9294, 0.8788, and 0.8803, and Hausdorf distance of 5.23, 13.54, and 12.05, for the whole tumor, tumor core, and enhancing tumor, respectively. Furthermore, the proposed ensemble method ranked first in the final ranking on another unseen test dataset, namely Sub-Saharan Africa dataset, achieving mean Dice scores of 0.9737, 0.9593, and 0.9022, and HD95 of 2.66, 1.72, 3.32 for the whole tumor, tumor core, and enhancing tumor, respectively. The docker image for the winning submission is publicly available at (https://hub.docker.com/r/razeineldin/camed22). ",
    "url": "https://arxiv.org/abs/2212.09310",
    "authors": [
      "Ramy A. Zeineldin",
      "Mohamed E. Karar",
      "Oliver Burgert",
      "Franziska Mathis-Ullrich"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.09348",
    "title": "Excluding Single-Crossing Matching Minors in Bipartite Graphs",
    "abstract": "\\noindent By a seminal result of Valiant, computing the permanent of $(0,1)$-matrices is, in general, $\\#\\mathsf{P}$-hard. In 1913 P\\'olya asked for which $(0,1)$-matrices $A$ it is possible to change some signs such that the permanent of $A$ equals the determinant of the resulting matrix. In 1975, Little showed these matrices to be exactly the biadjacency matrices of bipartite graphs excluding $K_{3,3}$ as a \\{matching minor}. This was turned into a polynomial time algorithm by McCuaig, Robertson, Seymour, and Thomas in 1999. However, the relation between the exclusion of some matching minor in a bipartite graph and the tractability of the permanent extends beyond $K_{3,3}.$ Recently it was shown that the exclusion of any planar bipartite graph as a matching minor yields a class of bipartite graphs on which the {permanent} of the corresponding $(0,1)$-matrices can be computed efficiently. In this paper we unify the two results above into a single, more general result in the style of the celebrated structure theorem for single-crossing-minor-free graphs. We identify a class of bipartite graphs strictly generalising planar bipartite graphs and $K_{3,3}$ which includes infinitely many non-Pfaffian graphs. The exclusion of any member of this class as a matching minor yields a structure that allows for the efficient evaluation of the permanent. Moreover, we show that the evaluation of the permanent remains $\\#\\mathsf{P}$-hard on bipartite graphs which exclude $K_{5,5}$ as a matching minor. This establishes a first computational lower bound for the problem of counting perfect matchings on matching minor closed classes. ",
    "url": "https://arxiv.org/abs/2212.09348",
    "authors": [
      "Archontia C. Giannopoulou",
      "Dimitrios M. Thilikos",
      "Sebastian Wiederrecht"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2212.09444",
    "title": "Voltage Gated Domain Wall Magnetic Tunnel Junction-based Spiking  Convolutional Neural Network",
    "abstract": "We propose a novel spin-orbit torque (SOT) driven and voltage-gated domain wall motion (DWM)-based MTJ device and its application in neuromorphic computing. We show that by utilizing the voltage-controlled gating effect on the DWM, the access transistor can be eliminated. The device provides more control over individual synapse writing and shows highly linear synaptic behavior. The linearity dependence on material parameters such as DMI and temperature is evaluated for real-environment performance analysis. Furthermore, using skyrmion-based leaky integrate and fire neuron model, we implement the spiking convolutional neural network for pattern recognition applications on the CIFAR-10 data set. The accuracy of the device is above 85%, proving its applicability in SNN. ",
    "url": "https://arxiv.org/abs/2212.09444",
    "authors": [
      "Aijaz H Lone",
      "Hanrui Li",
      "Nazek El-Atab",
      "Xiaohang Li",
      "Hossein Fariborzi"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2212.09624",
    "title": "Holder Recommendations using Graph Representation Learning & Link  Prediction",
    "abstract": "Lead recommendations for financial products such as funds or ETF is potentially challenging in investment space due to changing market scenarios, and difficulty in capturing financial holder's mindset and their philosophy. Current methods surface leads based on certain product categorization and attributes like returns, fees, category etc. to suggest similar product to investors which may not capture the holder's investment behavior holistically. Other reported works does subjective analysis of institutional holder's ideology. This paper proposes a comprehensive data driven framework for developing a lead recommendations system in holder's space for financial products like funds by using transactional history, asset flows and product specific attributes. The system assumes holder's interest implicitly by considering all investment transactions made and collects possible meta information to detect holder's investment profile/persona like investment anticipation and investment behavior. This paper focusses on holder recommendation component of framework which employs a bi-partite graph representation of financial holders and funds using variety of attributes and further employs GraphSage model for learning representations followed by link prediction model for ranking recommendation for future period. The performance of the proposed approach is compared with baseline model i.e., content-based filtering approach on metric hits at Top-k (50, 100, 200) recommendations. We found that the proposed graph ML solution outperform baseline by absolute 42%, 22% and 14% with a look ahead bias and by absolute 18%, 19% and 18% on completely unseen holders in terms of hit rate for top-k recommendations: 50, 100 and 200 respectively. ",
    "url": "https://arxiv.org/abs/2212.09624",
    "authors": [
      "Rachna Saxena",
      "Abhijeet Kumar",
      "Mridul Mishra"
    ],
    "subjectives": [
      "General Finance (q-fin.GN)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:1711.04211",
    "title": "Convergence of Hierarchical Clustering and Persistent Homology Methods  on Directed Networks",
    "abstract": " Comments: This paper has been withdrawn by the authors. This paper has been superseded by v3 of arXiv:1708.04727 (merged from arXiv:1708.04727 (v2), arXiv:1804.02820, and arXiv:1711.04211), which in turn has appeared in the Journal of Applied and Computational Topology ",
    "url": "https://arxiv.org/abs/1711.04211",
    "authors": [
      "Samir Chowdhury",
      "Facundo M\u00e9moli"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:1804.02820",
    "title": "The Metric Space of Networks",
    "abstract": " Comments: This paper has been withdrawn by the authors. This paper has been superseded by v3 of arXiv:1708.04727 (merged from arXiv:1708.04727 (v2), arXiv:1804.02820, and arXiv:1711.04211), which in turn has appeared in the Journal of Applied and Computational Topology ",
    "url": "https://arxiv.org/abs/1804.02820",
    "authors": [
      "Samir Chowdhury",
      "Facundo M\u00e9moli"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Metric Geometry (math.MG)"
    ]
  },
  {
    "id": "arXiv:2005.03982",
    "title": "Distributed Stochastic Constrained Composite Optimization over  Time-Varying Network with a Class of Communication Noise",
    "abstract": " Title: Distributed Stochastic Constrained Composite Optimization over  Time-Varying Network with a Class of Communication Noise ",
    "url": "https://arxiv.org/abs/2005.03982",
    "authors": [
      "Zhan Yu",
      "Daniel W. C. Ho",
      "Deming Yuan",
      "Jie Liu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2007.12350",
    "title": "Improving the dilation of a metric graph by adding edges",
    "abstract": " Comments: Journal version, TALG 2022 ",
    "url": "https://arxiv.org/abs/2007.12350",
    "authors": [
      "Joachim Gudmundsson",
      "Sampson Wong"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2101.09023",
    "title": "Enhanced word embeddings using multi-semantic representation through  lexical chains",
    "abstract": " Title: Enhanced word embeddings using multi-semantic representation through  lexical chains ",
    "url": "https://arxiv.org/abs/2101.09023",
    "authors": [
      "Terry Ruas",
      "Charles Henrique Porto Ferreira",
      "William Grosky",
      "Fabr\u00edcio Olivetti de Fran\u00e7a",
      "D\u00e9bora Maria Rossi Medeiros"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2101.12430",
    "title": "Subgraph nomination: Query by Example Subgraph Retrieval in Networks",
    "abstract": " Comments: 37 pages, 11 figures ",
    "url": "https://arxiv.org/abs/2101.12430",
    "authors": [
      "Al-Fahad M. Al-Qadhi",
      "Carey E. Priebe",
      "Hayden S. Helm",
      "Vince Lyzinski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2102.06847",
    "title": "Disease2Vec: Representing Alzheimer's Progression via Disease Embedding  Tree",
    "abstract": " Comments: Submitted to Information Processing in Medical Imaging (IPMI) 2023 ",
    "url": "https://arxiv.org/abs/2102.06847",
    "authors": [
      "Lu Zhang",
      "Li Wang",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2104.06728",
    "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World",
    "abstract": " Comments: accepted by TPAMI 2022 ",
    "url": "https://arxiv.org/abs/2104.06728",
    "authors": [
      "Xingxing Wei",
      "Ying Guo",
      "Jie Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2105.03939",
    "title": "Differentiable Neural Architecture Search for Extremely Lightweight  Image Super-Resolution",
    "abstract": " Comments: Accepted to IEEE Transactions on Circuits and Systems for Video Technology ",
    "url": "https://arxiv.org/abs/2105.03939",
    "authors": [
      "Han Huang",
      "Li Shen",
      "Chaoyang He",
      "Weisheng Dong",
      "Wei Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2105.04137",
    "title": "On the inversion number of oriented graphs",
    "abstract": " Title: On the inversion number of oriented graphs ",
    "url": "https://arxiv.org/abs/2105.04137",
    "authors": [
      "J\u00f8rgen Bang-Jensen",
      "Jonas Costa Ferreira da Silva",
      "Fr\u00e9d\u00e9ric Havet"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2106.04265",
    "title": "Towards Social Role-Based Interruptibility Management",
    "abstract": " Comments: 10 pages, 6 figures, submitted on December 2022, to appear in IEEE Pervasive Computing, Special Issue - Human-Centered AI ",
    "url": "https://arxiv.org/abs/2106.04265",
    "authors": [
      "Christoph Anderson",
      "Judith Simone Heinisch",
      "Shohreh Deldari",
      "Flora D. Salim",
      "Sandra Ohly",
      "Klaus David",
      "Veljko Pejovic"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2106.15098",
    "title": "Molecule Generation by Principal Subgraph Mining and Assembling",
    "abstract": " Comments: Accepted by NeurIPS 2022. Oral presentation ",
    "url": "https://arxiv.org/abs/2106.15098",
    "authors": [
      "Xiangzhe Kong",
      "Wenbing Huang",
      "Zhixing Tan",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2107.00841",
    "title": "A Heterogeneous Graph Attention Network for Multi-hop Machine Reading  Comprehension",
    "abstract": " Title: A Heterogeneous Graph Attention Network for Multi-hop Machine Reading  Comprehension ",
    "url": "https://arxiv.org/abs/2107.00841",
    "authors": [
      "Peng Gao",
      "Feng Gao",
      "Jian-Cheng Ni",
      "Hamido Fujita"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2107.02363",
    "title": "Asymptotics of Network Embeddings Learned via Subsampling",
    "abstract": " Comments: Under review at JMLR. 120 pages, 3 figures, 1 table ",
    "url": "https://arxiv.org/abs/2107.02363",
    "authors": [
      "Andrew Davison",
      "Morgane Austern"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2107.14126",
    "title": "The Complexity of Growing a Graph",
    "abstract": " Comments: 30 pages ",
    "url": "https://arxiv.org/abs/2107.14126",
    "authors": [
      "George B. Mertzios",
      "Othon Michail",
      "George Skretas",
      "Paul G. Spirakis",
      "Michail Theofilatos"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2108.06706",
    "title": "Temporal Action Segmentation with High-level Complex Activity Labels",
    "abstract": " Comments: 12 pages, 6 figures ",
    "url": "https://arxiv.org/abs/2108.06706",
    "authors": [
      "Guodong Ding",
      "Angela Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2110.01010",
    "title": "High Capacity Reversible Data Hiding in Encrypted 3D Mesh Models Based  on Multi-MSB Prediction",
    "abstract": " Comments: Published in Signal Processing ",
    "url": "https://arxiv.org/abs/2110.01010",
    "authors": [
      "Wanli Lv",
      "Lulu Cheng",
      "Zhaoxia Yin"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2110.02355",
    "title": "Robustness and sample complexity of model-based MARL for general-sum  Markov games",
    "abstract": " Title: Robustness and sample complexity of model-based MARL for general-sum  Markov games ",
    "url": "https://arxiv.org/abs/2110.02355",
    "authors": [
      "Jayakumar Subramanian",
      "Amit Sinha",
      "Aditya Mahajan"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2110.13632",
    "title": "Generative Networks for Precision Enthusiasts",
    "abstract": " Comments: 28 pages, 14 figures ",
    "url": "https://arxiv.org/abs/2110.13632",
    "authors": [
      "Anja Butter",
      "Theo Heimel",
      "Sander Hummerich",
      "Tobias Krebs",
      "Tilman Plehn",
      "Armand Rousselot",
      "Sophia Vent"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.08486",
    "title": "Neural Class Expression Synthesis",
    "abstract": " Comments: 11 pages, 4 figures, 7 tables ",
    "url": "https://arxiv.org/abs/2111.08486",
    "authors": [
      "N'Dah Jean Kouagou",
      "Stefan Heindorf",
      "Caglar Demir",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2112.11165",
    "title": "Scalable High-Rate Twin-Field Quantum Key Distribution Networks without  Constraint of Probability and Intensity",
    "abstract": " Comments: 16 pages, 6 figures, 3 tables ",
    "url": "https://arxiv.org/abs/2112.11165",
    "authors": [
      "Yuan-Mei Xie",
      "Chen-Xun Weng",
      "Yu-Shuo Lu",
      "Yao Fu",
      "Yang Wang",
      "Hua-Lei Yin",
      "Zeng-Bing Chen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2112.12310",
    "title": "Adversarial Attacks against Windows PE Malware Detection: A Survey of  the State-of-the-Art",
    "abstract": " Title: Adversarial Attacks against Windows PE Malware Detection: A Survey of  the State-of-the-Art ",
    "url": "https://arxiv.org/abs/2112.12310",
    "authors": [
      "Xiang Ling",
      "Lingfei Wu",
      "Jiangyu Zhang",
      "Zhenqing Qu",
      "Wei Deng",
      "Xiang Chen",
      "Yaguan Qian",
      "Chunming Wu",
      "Shouling Ji",
      "Tianyue Luo",
      "Jingzheng Wu",
      "Yanjun Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2201.01689",
    "title": "Asymptotics of $\\ell_2$ Regularized Network Embeddings",
    "abstract": " Comments: Accepted in Neural Information Processing Systems 2022. 44 pages, 2 figures, 2 tables ",
    "url": "https://arxiv.org/abs/2201.01689",
    "authors": [
      "Andrew Davison"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2201.04437",
    "title": "Multi-task Joint Strategies of Self-supervised Representation Learning  on Biomedical Networks for Drug Discovery",
    "abstract": " Comments: 44 pages, 11 figures ",
    "url": "https://arxiv.org/abs/2201.04437",
    "authors": [
      "Xiaoqi Wang",
      "Yingjie Cheng",
      "Yaning Yang",
      "Yue Yu",
      "Fei Li",
      "Shaoliang Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2202.03195",
    "title": "More is Better (Mostly): On the Backdoor Attacks in Federated Graph  Neural Networks",
    "abstract": " Comments: 15 pages, 14 figures ",
    "url": "https://arxiv.org/abs/2202.03195",
    "authors": [
      "Jing Xu",
      "Rui Wang",
      "Stefanos Koffas",
      "Kaitai Liang",
      "Stjepan Picek"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2202.04744",
    "title": "Robust Bayesian Inference for Simulator-based Models via the MMD  Posterior Bootstrap",
    "abstract": " Comments: Accepted for publication (with an oral presentation) at AISTATS 2022. A preliminary version of this paper was accepted in the NeurIPS 2021 workshop \"Your Model is Wrong: Robustness and misspecification in probabilistic modeling\". v2: added some references. v3: corrected small error in theorem 3 ",
    "url": "https://arxiv.org/abs/2202.04744",
    "authors": [
      "Charita Dellaporta",
      "Jeremias Knoblauch",
      "Theodoros Damoulas",
      "Fran\u00e7ois-Xavier Briol"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2202.05786",
    "title": "Multi-Modal Knowledge Graph Construction and Application: A Survey",
    "abstract": " Comments: 20 pages, 8 figures, 6 tables. Accepted by TKDE 2022 ",
    "url": "https://arxiv.org/abs/2202.05786",
    "authors": [
      "Xiangru Zhu",
      "Zhixu Li",
      "Xiaodan Wang",
      "Xueyao Jiang",
      "Penglei Sun",
      "Xuwu Wang",
      "Yanghua Xiao",
      "Nicholas Jing Yuan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2203.16462",
    "title": "Convergence of gradient descent for deep neural networks",
    "abstract": " Comments: 23 pages. The article has been majorly reorganized, by deleting some unnecessary materials. Some minor errors have been fixed. (Theorem numbers have changed in this revision.) ",
    "url": "https://arxiv.org/abs/2203.16462",
    "authors": [
      "Sourav Chatterjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2204.05422",
    "title": "SATA: Sparsity-Aware Training Accelerator for Spiking Neural Networks",
    "abstract": " Comments: 13 pages. Accepted to IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2022) ",
    "url": "https://arxiv.org/abs/2204.05422",
    "authors": [
      "Ruokai Yin",
      "Abhishek Moitra",
      "Abhiroop Bhattacharjee",
      "Youngeun Kim",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2204.05909",
    "title": "Learning Performance Graphs from Demonstrations via Task-Based  Evaluations",
    "abstract": " Comments: Published in IEEE Robotics and Automation Letters (RA-L) Vol. 8 Issue 1 ",
    "url": "https://arxiv.org/abs/2204.05909",
    "authors": [
      "Aniruddh G. Puranic",
      "Jyotirmoy V. Deshmukh",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2204.12676",
    "title": "The Multimarginal Optimal Transport Formulation of Adversarial  Multiclass Classification",
    "abstract": " Title: The Multimarginal Optimal Transport Formulation of Adversarial  Multiclass Classification ",
    "url": "https://arxiv.org/abs/2204.12676",
    "authors": [
      "Nicolas Garcia Trillos",
      "Matt Jacobs",
      "Jakwang Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2205.13496",
    "title": "Censored Quantile Regression Neural Networks for Distribution-Free  Survival Analysis",
    "abstract": " Comments: Published in NeurIPS 2022 ",
    "url": "https://arxiv.org/abs/2205.13496",
    "authors": [
      "Tim Pearce",
      "Jong-Hyeon Jeong",
      "Yichen Jia",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2205.13733",
    "title": "Towards Faithful and Consistent Explanations for Graph Neural Networks",
    "abstract": " Comments: Accepted by WSDM2023 ",
    "url": "https://arxiv.org/abs/2205.13733",
    "authors": [
      "Tianxiang Zhao",
      "Dongsheng Luo",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2205.13958",
    "title": "Machine Learning-Based User Scheduling in Integrated  Satellite-HAPS-Ground Networks",
    "abstract": " Comments: arXiv admin note: substantial text overlap with arXiv:2204.13257 ",
    "url": "https://arxiv.org/abs/2205.13958",
    "authors": [
      "Hayssam Dahrouj",
      "Shasha Liu",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2205.15772",
    "title": "The hybrid approach -- Convolutional Neural Networks and Expectation  Maximization Algorithm -- for Tomographic Reconstruction of Hyperspectral  Images",
    "abstract": " Comments: 36 pages, 13 figures and 2 tables. Supplemental material: 21 pages and 14 figures. v2: Clarifications added, analyses and argumentation updated ",
    "url": "https://arxiv.org/abs/2205.15772",
    "authors": [
      "Mads J. Ahleb\u00e6k",
      "Mads S. Peters",
      "Wei-Chih Huang",
      "Mads T. Frandsen",
      "Ren\u00e9 L. Eriksen",
      "Bjarke J\u00f8rgensen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2206.00145",
    "title": "CASSOCK: Viable Backdoor Attacks against DNN in The Wall of  Source-Specific Backdoor Defences",
    "abstract": " Comments: 13 pages,14 figures ",
    "url": "https://arxiv.org/abs/2206.00145",
    "authors": [
      "Shang Wang",
      "Yansong Gao",
      "Anmin Fu",
      "Zhi Zhang",
      "Yuqing Zhang",
      "Willy Susilo",
      "Dongxi Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.01152",
    "title": "Causal Structure Learning: a Combinatorial Perspective",
    "abstract": " Title: Causal Structure Learning: a Combinatorial Perspective ",
    "url": "https://arxiv.org/abs/2206.01152",
    "authors": [
      "Chandler Squires",
      "Caroline Uhler"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.05398",
    "title": "E2PN: Efficient SE(3)-Equivariant Point Network",
    "abstract": " Title: E2PN: Efficient SE(3)-Equivariant Point Network ",
    "url": "https://arxiv.org/abs/2206.05398",
    "authors": [
      "Minghan Zhu",
      "Maani Ghaffari",
      "William A. Clark",
      "Huei Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2206.06942",
    "title": "Boolean dimension and dim-boundedness: Planar cover graph with a zero",
    "abstract": " Title: Boolean dimension and dim-boundedness: Planar cover graph with a zero ",
    "url": "https://arxiv.org/abs/2206.06942",
    "authors": [
      "Heather Smith Blake",
      "Piotr Micek",
      "William T. Trotter"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2206.13444",
    "title": "Decomposing and Executing Serverless Applications as Resource Graphs",
    "abstract": " Title: Decomposing and Executing Serverless Applications as Resource Graphs ",
    "url": "https://arxiv.org/abs/2206.13444",
    "authors": [
      "Zhiyuan Guo",
      "Zachary Blanco",
      "Mohammad Shahrad",
      "Zerui Wei",
      "Bili Dong",
      "Jinmou Li",
      "Ishaan Pota",
      "Harry Xu",
      "Yiying Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2207.07338",
    "title": "Context-sensitive neocortical neurons transform the effectiveness and  efficiency of neural information processing",
    "abstract": " Title: Context-sensitive neocortical neurons transform the effectiveness and  efficiency of neural information processing ",
    "url": "https://arxiv.org/abs/2207.07338",
    "authors": [
      "Ahsan Adeel",
      "Mario Franco",
      "Mohsin Raza",
      "Khubaib Ahmed"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2207.10796",
    "title": "Multiple Robust Learning for Recommendation",
    "abstract": " Comments: Accepted by AAAI'23 ",
    "url": "https://arxiv.org/abs/2207.10796",
    "authors": [
      "Haoxuan Li",
      "Quanyu Dai",
      "Yuru Li",
      "Yan Lyu",
      "Zhenhua Dong",
      "Xiao-Hua Zhou",
      "Peng Wu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2208.00349",
    "title": "What Do Deep Neural Networks Find in Disordered Structures of Glasses?",
    "abstract": " Comments: 16+12 pages, 8+10 figures ",
    "url": "https://arxiv.org/abs/2208.00349",
    "authors": [
      "Norihiro Oyama",
      "Shihori Koyama",
      "Takeshi Kawasaki"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2208.05693",
    "title": "Dynamics of cold random hyperbolic graphs with link persistence",
    "abstract": " Comments: 14 pages, 9 figures. Generalizes the model in arXiv:1907.00073 ",
    "url": "https://arxiv.org/abs/2208.05693",
    "authors": [
      "Sofoclis Zambirinis",
      "Harrison Hartle",
      "Fragkiskos Papadopoulos"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2208.08227",
    "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural  Code Generation",
    "abstract": " Title: MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural  Code Generation ",
    "url": "https://arxiv.org/abs/2208.08227",
    "authors": [
      "Federico Cassano",
      "John Gouwar",
      "Daniel Nguyen",
      "Sydney Nguyen",
      "Luna Phipps-Costin",
      "Donald Pinckney",
      "Ming-Ho Yee",
      "Yangtian Zi",
      "Carolyn Jane Anderson",
      "Molly Q Feldman",
      "Arjun Guha",
      "Michael Greenberg",
      "Abhinav Jangda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2208.09478",
    "title": "Communication Size Reduction of Federated Learning based on Neural ODE  Model",
    "abstract": " Comments: PDF format error corrected ",
    "url": "https://arxiv.org/abs/2208.09478",
    "authors": [
      "Yuto Hoshino",
      "Hiroki Kawakami",
      "Hiroki Matsutani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2209.00892",
    "title": "Scalable Adversarial Attack Algorithms on Influence Maximization",
    "abstract": " Comments: 11 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2209.00892",
    "authors": [
      "Lichao Sun",
      "Xiaobin Rui",
      "Wei Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2209.06461",
    "title": "Electric Vehicle Battery Sharing Game for Mobile Energy Storage  Provision in Power Networks",
    "abstract": " Comments: 10 pages, IEEE CDC 2022 ",
    "url": "https://arxiv.org/abs/2209.06461",
    "authors": [
      "Utkarsha Agwan",
      "Junjie Qin",
      "Kameshwar Poolla",
      "Pravin Varaiya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2209.06767",
    "title": "Parameter-Efficient Finetuning for Robust Continual Multilingual  Learning",
    "abstract": " Title: Parameter-Efficient Finetuning for Robust Continual Multilingual  Learning ",
    "url": "https://arxiv.org/abs/2209.06767",
    "authors": [
      "Kartikeya Badola",
      "Shachi Dave",
      "Partha Talukdar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2209.09464",
    "title": "Rethinking Dimensionality Reduction in Grid-based 3D Object Detection",
    "abstract": " Comments: Submitted to ICRA 2023 ",
    "url": "https://arxiv.org/abs/2209.09464",
    "authors": [
      "Dihe Huang",
      "Ying Chen",
      "Yikang Ding",
      "Jinli Liao",
      "Jianlin Liu",
      "Kai Wu",
      "Qiang Nie",
      "Yong Liu",
      "Chengjie Wang",
      "Zhiheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2209.11715",
    "title": "The \"Beatrix'' Resurrections: Robust Backdoor Detection via Gram  Matrices",
    "abstract": " Comments: 18 pages, 23 figures. Accepted to NDSS 2023. Camera-ready version. Code availability: this https URL ",
    "url": "https://arxiv.org/abs/2209.11715",
    "authors": [
      "Wanlun Ma",
      "Derui Wang",
      "Ruoxi Sun",
      "Minhui Xue",
      "Sheng Wen",
      "Yang Xiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2209.13827",
    "title": "Compressing network populations with modal networks reveals structural  diversity",
    "abstract": " Title: Compressing network populations with modal networks reveals structural  diversity ",
    "url": "https://arxiv.org/abs/2209.13827",
    "authors": [
      "Alec Kirkley",
      "Alexis Rojas",
      "Martin Rosvall",
      "Jean-Gabriel Young"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2210.00379",
    "title": "NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review",
    "abstract": " Title: NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review ",
    "url": "https://arxiv.org/abs/2210.00379",
    "authors": [
      "Kyle Gao",
      "Yina Gao",
      "Hongjie He",
      "Dening Lu",
      "Linlin Xu",
      "Jonathan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2210.01126",
    "title": "Wheel Impact Test by Deep Learning: Prediction of Location and Magnitude  of Maximum Stress",
    "abstract": " Title: Wheel Impact Test by Deep Learning: Prediction of Location and Magnitude  of Maximum Stress ",
    "url": "https://arxiv.org/abs/2210.01126",
    "authors": [
      "Seungyeon Shin",
      "Ah-hyeon Jin",
      "Soyoung Yoo",
      "Sunghee Lee",
      "ChangGon Kim",
      "Sungpil Heo",
      "Namwoo Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2210.01504",
    "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
    "abstract": " Title: Knowledge Unlearning for Mitigating Privacy Risks in Language Models ",
    "url": "https://arxiv.org/abs/2210.01504",
    "authors": [
      "Joel Jang",
      "Dongkeun Yoon",
      "Sohee Yang",
      "Sungmin Cha",
      "Moontae Lee",
      "Lajanugen Logeswaran",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2210.04371",
    "title": "A Detailed Study of Interpretability of Deep Neural Network based Top  Taggers",
    "abstract": " Comments: Repository: this https URL ",
    "url": "https://arxiv.org/abs/2210.04371",
    "authors": [
      "Ayush Khot",
      "Mark S. Neubauer",
      "Avik Roy"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2210.07588",
    "title": "Distributed Distributionally Robust Optimization with Non-Convex  Objectives",
    "abstract": " Comments: Accepted to NeurIPS2022 ",
    "url": "https://arxiv.org/abs/2210.07588",
    "authors": [
      "Yang Jiao",
      "Kai Yang",
      "Dongjin Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2210.13361",
    "title": "NASA: Neural Architecture Search and Acceleration for Hardware Inspired  Hybrid Networks",
    "abstract": " Comments: Accepted to ICCAD2022 ",
    "url": "https://arxiv.org/abs/2210.13361",
    "authors": [
      "Huihong Shi",
      "Haoran You",
      "Yang Zhao",
      "Zhongfeng Wang",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2211.03622",
    "title": "Do Users Write More Insecure Code with AI Assistants?",
    "abstract": " Comments: 18 pages, 16 figures, update adds names of statistical tests and survey questions ",
    "url": "https://arxiv.org/abs/2211.03622",
    "authors": [
      "Neil Perry",
      "Megha Srivastava",
      "Deepak Kumar",
      "Dan Boneh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2211.04871",
    "title": "Graph classes equivalent to 12-representable graphs",
    "abstract": " Comments: 12 pages, 6 figures, Corrected typos, Corrected Reference [22] ",
    "url": "https://arxiv.org/abs/2211.04871",
    "authors": [
      "Asahi Takaoka"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2211.05855",
    "title": "Robust N-1 secure HV Grid Flexibility Estimation for TSO-DSO coordinated  Congestion Management with Deep Reinforcement Learning",
    "abstract": " Comments: Conference: NEIS 2022, Hamburg ",
    "url": "https://arxiv.org/abs/2211.05855",
    "authors": [
      "Zhenqi Wang",
      "Sebastian Wende-von Berg",
      "Martin Braun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2211.08604",
    "title": "PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention  Networks with Imbalanced PU Labels",
    "abstract": " Comments: Under Review, Industry Track ",
    "url": "https://arxiv.org/abs/2211.08604",
    "authors": [
      "Jiho Choi",
      "Junghoon Park",
      "Woocheol Kim",
      "Jin-Hyeok Park",
      "Yumin Suh",
      "Minchang Sung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2211.08615",
    "title": "GLFF: Global and Local Feature Fusion for Face Forgery Detection",
    "abstract": " Title: GLFF: Global and Local Feature Fusion for Face Forgery Detection ",
    "url": "https://arxiv.org/abs/2211.08615",
    "authors": [
      "Yan Ju",
      "Shan Jia",
      "Jialing Cai",
      "Haiying Guan",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2211.08804",
    "title": "Analysis and Detectability of Offline Data Poisoning Attacks on Linear  Systems",
    "abstract": " Title: Analysis and Detectability of Offline Data Poisoning Attacks on Linear  Systems ",
    "url": "https://arxiv.org/abs/2211.08804",
    "authors": [
      "Alessio Russo",
      "Alexandre Proutiere"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2211.11403",
    "title": "Time-reversal equivariant neural network potential and Hamiltonian for  magnetic materials",
    "abstract": " Comments: 17 pages,2 figures and 2 tables ",
    "url": "https://arxiv.org/abs/2211.11403",
    "authors": [
      "Hongyu Yu",
      "Yang Zhong",
      "Junyi Ji",
      "Xingao Gong",
      "Hongjun Xiang"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2211.11482",
    "title": "Applications of statistical causal inference in software engineering",
    "abstract": " Comments: 38 pages, 12 tables, 9 figures, submitted to Information and Software Technology ",
    "url": "https://arxiv.org/abs/2211.11482",
    "authors": [
      "Julien Siebert"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2211.11711",
    "title": "CLAWSAT: Towards Both Robust and Accurate Code Models",
    "abstract": " Comments: Accepted by SANER2023 ",
    "url": "https://arxiv.org/abs/2211.11711",
    "authors": [
      "Jinghan Jia",
      "Shashank Srikant",
      "Tamara Mitrovska",
      "Chuang Gan",
      "Shiyu Chang",
      "Sijia Liu",
      "Una-May O'Reilly"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2211.11801",
    "title": "Self-Supervised Pre-training of 3D Point Cloud Networks with Image Data",
    "abstract": " Comments: In Proceedings of the Conference on Robot Learning (CoRL'22) Workshop on Pre-Training Robot Learning, Auckland, New Zealand, December 15, 2022 ",
    "url": "https://arxiv.org/abs/2211.11801",
    "authors": [
      "Andrej Janda",
      "Brandon Wagstaff",
      "Edwin G. Ng",
      "Jonathan Kelly"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2211.13649",
    "title": "End-to-end Wind Turbine Wake Modelling with Deep Graph Representation  Learning",
    "abstract": " Comments: 20 pages, 12 figures ",
    "url": "https://arxiv.org/abs/2211.13649",
    "authors": [
      "Siyi Li",
      "Mingrui Zhang",
      "Matthew D. Piggott"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2211.15081",
    "title": "Flip Initial Features: Generalization of Neural Networks for  Semi-supervised Node Classification",
    "abstract": " Title: Flip Initial Features: Generalization of Neural Networks for  Semi-supervised Node Classification ",
    "url": "https://arxiv.org/abs/2211.15081",
    "authors": [
      "Yoonhyuk Choi",
      "Jiho Choi",
      "Taewook Ko",
      "Chong-Kwon Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2211.16098",
    "title": "Three-stage binarization of color document images based on discrete  wavelet transform and generative adversarial networks",
    "abstract": " Title: Three-stage binarization of color document images based on discrete  wavelet transform and generative adversarial networks ",
    "url": "https://arxiv.org/abs/2211.16098",
    "authors": [
      "Yu-Shian Lin",
      "Rui-Yang Ju",
      "Chih-Chia Chen",
      "Ting-Yu Lin",
      "Jen-Shiun Chiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.01529",
    "title": "Laplacian Convolutional Representation for Traffic Time Series  Imputation",
    "abstract": " Comments: 13 pages, 8 figures ",
    "url": "https://arxiv.org/abs/2212.01529",
    "authors": [
      "Xinyu Chen",
      "Zhanhong Cheng",
      "Nicolas Saunier",
      "Lijun Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.02340",
    "title": "CBNet: A Plug-and-Play Network for Segmentation-based Scene Text  Detection",
    "abstract": " Title: CBNet: A Plug-and-Play Network for Segmentation-based Scene Text  Detection ",
    "url": "https://arxiv.org/abs/2212.02340",
    "authors": [
      "Xi Zhao",
      "Wei Feng",
      "Zheng Zhang",
      "Jingjing Lv",
      "Xin Zhu",
      "Zhangang Lin",
      "Jinghe Hu",
      "Jingping Shao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.04064",
    "title": "On CRC-Aided, Dual-Trellis, List Decoding for High-Rate Convolutional  Codes with Short Blocklengths",
    "abstract": " Comments: arXiv admin note: substantial text overlap with arXiv:2111.07929 ",
    "url": "https://arxiv.org/abs/2212.04064",
    "authors": [
      "Wenhui Sui",
      "Brendan Towell",
      "Ava Asmani",
      "Hengjie Yang",
      "Richard D. Wesel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2212.04902",
    "title": "Self-Supervised PPG Representation Learning Shows High Inter-Subject  Variability",
    "abstract": " Comments: The current version has been accepted to be presented at ICMLT 2023; Typo corrected in the second author's name ",
    "url": "https://arxiv.org/abs/2212.04902",
    "authors": [
      "Ramin Ghorbani",
      "Marcel J.T. Reinders",
      "David M.J. Tax"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.05245",
    "title": "Joint Spatio-Temporal Modeling for the Semantic Change Detection in  Remote Sensing Images",
    "abstract": " Title: Joint Spatio-Temporal Modeling for the Semantic Change Detection in  Remote Sensing Images ",
    "url": "https://arxiv.org/abs/2212.05245",
    "authors": [
      "Lei Ding",
      "Jing Zhang",
      "Kai Zhang",
      "Haitao Guo",
      "Bing Liu",
      "Lorenzo Bruzzone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2212.05853",
    "title": "DeepCut: Unsupervised Segmentation using Graph Neural Networks  Clustering",
    "abstract": " Title: DeepCut: Unsupervised Segmentation using Graph Neural Networks  Clustering ",
    "url": "https://arxiv.org/abs/2212.05853",
    "authors": [
      "Amit Aflalo",
      "Shai Bagon",
      "Tamar Kashti",
      "Yonina Eldar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2212.07560",
    "title": "Multi-level and multi-modal feature fusion for accurate 3D object  detection in Connected and Automated Vehicles",
    "abstract": " Title: Multi-level and multi-modal feature fusion for accurate 3D object  detection in Connected and Automated Vehicles ",
    "url": "https://arxiv.org/abs/2212.07560",
    "authors": [
      "Yiming Hou",
      "Mahdi Rezaei",
      "Richard Romano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2212.07666",
    "title": "Surrogate-assisted level-based learning evolutionary search for heat  extraction optimization of enhanced geothermal system",
    "abstract": " Title: Surrogate-assisted level-based learning evolutionary search for heat  extraction optimization of enhanced geothermal system ",
    "url": "https://arxiv.org/abs/2212.07666",
    "authors": [
      "Guodong Chen",
      "Xin Luo",
      "Chuanyin Jiang",
      "Jiu Jimmy Jiao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2212.08008",
    "title": "A New Deep Boosted CNN and Ensemble Learning based IoT Malware Detection",
    "abstract": " Comments: 20 pages, 10 figures, 6 tables; Corresponding saddamhkhan@ueas.edu.pk ",
    "url": "https://arxiv.org/abs/2212.08008",
    "authors": [
      "Saddam Hussain Khan",
      "Wasi Ullah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2212.08109",
    "title": "An Empirical Study of Deep Learning Models for Vulnerability Detection",
    "abstract": " Comments: 11 pages, 14 figures. Accepted at ICSE 2023 (not camera-ready version). Corrected typos in Listing 2 ",
    "url": "https://arxiv.org/abs/2212.08109",
    "authors": [
      "Benjamin Steenhoek",
      "Md Mahbubur Rahman",
      "Richard Jiles",
      "Wei Le"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  }
]