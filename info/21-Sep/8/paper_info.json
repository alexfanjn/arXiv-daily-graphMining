[
  {
    "id": "arXiv:2109.02839",
    "title": "Self-adaptive deep neural network: Numerical approximation to functions  and PDEs",
    "abstract": "Designing an optimal deep neural network for a given task is important and challenging in many machine learning applications. To address this issue, we introduce a self-adaptive algorithm: the adaptive network enhancement (ANE) method, written as loops of the form train, estimate and enhance. Starting with a small two-layer neural network (NN), the step train is to solve the optimization problem at the current NN; the step estimate is to compute a posteriori estimator/indicators using the solution at the current NN; the step enhance is to add new neurons to the current NN. Novel network enhancement strategies based on the computed estimator/indicators are developed in this paper to determine how many new neurons and when a new layer should be added to the current NN. The ANE method provides a natural process for obtaining a good initialization in training the current NN; in addition, we introduce an advanced procedure on how to initialize newly added neurons for a better approximation. We demonstrate that the ANE method can automatically design a nearly minimal NN for learning functions exhibiting sharp transitional layers as well as discontinuous solutions of hyperbolic partial differential equations. ",
    "url": "https://arxiv.org/abs/2109.02839",
    "authors": [
      "Zhiqiang Cai",
      "Jingshuang Chen",
      "Min Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.02914",
    "title": "Scale-invariant representation of machine learning",
    "abstract": "The success of machine learning stems from its structured data representation. Similar data have close representation as compressed codes for classification or emerged labels for clustering. We observe that the frequency of the internal representation follows power laws in both supervised and unsupervised learning. The scale-invariant distribution implies that machine learning largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this study, we derive how the power laws can naturally arise in machine learning. In terms of information theory, the scale-invariant representation corresponds to a maximally uncertain data grouping among possible representations that guarantee pre-specified learning accuracy. ",
    "url": "https://arxiv.org/abs/2109.02914",
    "authors": [
      "Sungyeop Lee",
      "Junghyo Jo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2109.02937",
    "title": "GeneNet VR: Interactive visualization of large-scale biological networks  using a standalone headset",
    "abstract": "Visualizations are an essential part of biomedical analysis result interpretation. Often, interactive networks are used to visualize the data. However, the high interconnectivity, and high dimensionality of the data often results in information overload, making it hard to interpret the results. To address the information overload problem, existing solutions typically either use data reduction, reduced interactivity, or expensive hardware. We propose using the affordable Oculus Quest Virtual Reality (VR) headset for interactive visualization of large-scale biological networks. We present the design and implementation of our solution, GeneNet VR, and we evaluate its scalability and usability using large gene-to-gene interaction networks. We achieve the 72 FPS required by the Oculus performance guidelines for the largest of our networks (2693 genes) using both a GPU and the Oculus Quest standalone. We found from our interviews with biomedical researchers that GeneNet VR is innovative, interesting, and easy to use for novice VR users. We believe affordable hardware like the Oculus Quest has a big potential for biological data analysis. However, additional work is required to evaluate its benefits to improve knowledge discovery for real data analysis use cases. GeneNet VR is open-sourced: https://github.com/kolibrid/GeneNet-VR. A video demonstrating GeneNet VR used to explore large biological networks: https://youtu.be/N4QDZiZqVNY. ",
    "url": "https://arxiv.org/abs/2109.02937",
    "authors": [
      "\u00c1lvaro Mart\u00ednez Fern\u00e1ndez",
      "Lars Ailo Bongo",
      "Edvard Pedersen"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2109.03040",
    "title": "Reconfigurable co-processor architecture with limited numerical  precision to accelerate deep convolutional neural networks",
    "abstract": "Convolutional Neural Networks (CNNs) are widely used in deep learning applications, e.g. visual systems, robotics etc. However, existing software solutions are not efficient. Therefore, many hardware accelerators have been proposed optimizing performance, power and resource utilization of the implementation. Amongst existing solutions, Field Programmable Gate Array (FPGA) based architecture provides better cost-energy-performance trade-offs as well as scalability and minimizing development time. In this paper, we present a model-independent reconfigurable co-processing architecture to accelerate CNNs. Our architecture consists of parallel Multiply and Accumulate (MAC) units with caching techniques and interconnection networks to exploit maximum data parallelism. In contrast to existing solutions, we introduce limited precision 32 bit Q-format fixed point quantization for arithmetic representations and operations. As a result, our architecture achieved significant reduction in resource utilization with competitive accuracy. Furthermore, we developed an assembly-type microinstructions to access the co-processing fabric to manage layer-wise parallelism, thereby making re-use of limited resources. Finally, we have tested our architecture up to 9x9 kernel size on Xilinx Virtex 7 FPGA, achieving a throughput of up to 226.2 GOp/S for 3x3 kernel size. ",
    "url": "https://arxiv.org/abs/2109.03040",
    "authors": [
      "Sasindu Wijeratne",
      "Sandaruwan Jayaweera",
      "Mahesh Dananjaya",
      "Ajith Pasqual"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2109.03042",
    "title": "Temporal complex networks modeling applied to vehicular ad-hoc networks",
    "abstract": "VANETs solutions use aggregated graph representation to model the interaction among the vehicles and different aggregated complex network measures to quantify some topological characteristics. This modeling ignores the temporal interactions between the cars, causing loss of information or unrealistic behavior. This work proposes the use of both temporal graphs and temporal measures to model VANETs applications. To verify the viability of this model, we initially perform a comparative analysis between the temporal and aggregated modeling considering five different real datasets. This analysis shows that the aggregated model is inefficient in modeling the temporal aspects of networks. After that, we perform a network evaluation through a simulation by considering the impact of temporal modeling applied to the deployment of RSUs. First, we compare a solution based on our temporal modeling with a greedy algorithm based on an aggregated model to choose the positions of RSUs. In a scenario with 70 RSUs, we have 77% and 65% of coverage in the temporal and aggregated model (greedy algorithm), respectively. Second, we evaluate the use of aggregated and temporal measures applied as features in a genetic algorithm. The approach with temporal betweenness had the better result with 90% of the coverage area against 61% of aggregated one applied to the same scenario. ",
    "url": "https://arxiv.org/abs/2109.03042",
    "authors": [
      "Fillipe Santos",
      "Andre L. L. Aquino",
      "Edmundo R. M. Madeira",
      "Raquel S. Cabral"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2109.03084",
    "title": "Learning grounded word meaning representations on similarity graphs",
    "abstract": "This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the hierarchy models modality-specific word representations through dedicated but communicating graphs, while the higher level puts these representations together on a single graph to learn a representation jointly from both modalities. The topology of each graph models similarity relations among words, and is estimated jointly with the graph embedding. The assumption underlying this model is that words sharing similar meaning correspond to communities in an underlying similarity graph in a low-dimensional space. We named this model Hierarchical Multi-Modal Similarity Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE to simulate human similarity judgements and concept categorization, outperforming the state of the art. ",
    "url": "https://arxiv.org/abs/2109.03084",
    "authors": [
      "Mariella Dimiccoli",
      "Herwig Wendt",
      "Pau Batlle"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2109.03085",
    "title": "Blockchain mining in pools: Analyzing the trade-off between  profitability and ruin",
    "abstract": "The resource-consuming mining of blocks on a blockchain equipped with a proof of work consensus protocol bears the risk of ruin, namely when the operational costs for the mining exceed the received rewards. In this paper we investigate to what extent it is of interest to join a mining pool that reduces the variance of the return of a miner for a specified cost for participation. Using methodology from ruin theory and risk sharing in insurance, we quantitatively study the effects of pooling in this context and derive several explicit formulas for quantities of interest. The results are illustrated in numerical examples for parameters of practical relevance. ",
    "url": "https://arxiv.org/abs/2109.03085",
    "authors": [
      "Hansjoerg Albrecher",
      "Fina Finger",
      "Pierre-Olivier Goffard"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2109.03175",
    "title": "When differential privacy meets NLP: The devil is in the detail",
    "abstract": "Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users' original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder's dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny. ",
    "url": "https://arxiv.org/abs/2109.03175",
    "authors": [
      "Ivan Habernal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2109.03044",
    "title": "Diagonal degree correlations vs. epidemic threshold in scale-free  networks",
    "abstract": "We prove that the presence of a diagonal assortative degree correlation, even if small, has the effect of dramatically lowering the epidemic threshold of large scale-free networks. The correlation matrix considered is $P(h|k)=(1-r)P^U_{hk}+r\\delta_{hk}$, where $P^U$ is uncorrelated and $r$ (the Newman assortativity coefficient) can be very small. The effect is uniform in the scale exponent $\\gamma$, if the network size is measured by the largest degree $n$. We also prove that it is possible to construct, via the Porto-Weber method, correlation matrices which have the same $k_{nn}$ as the $P(h|k)$ above, but very different elements and spectrum, and thus lead to different epidemic diffusion and threshold. Moreover, we study a subset of the admissible transformations of the form $P(h|k) \\to P(h|k)+\\Phi(h,k)$ with $\\Phi(h,k)$ depending on a parameter which leave $k_{nn}$ invariant. Such transformations affect in general the epidemic threshold. We find however that this does not happen when they act between networks with constant $k_{nn}$, i.e. networks in which the average neighbor degree is independent from the degree itself (a wider class than that of strictly uncorrelated networks). ",
    "url": "https://arxiv.org/abs/2109.03044",
    "authors": [
      "M.L. Bertotti",
      "G. Modanese"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2010.07858",
    "title": "What you need to know to train recurrent neural networks to make Flip  Flops memories and more",
    "abstract": " Title: What you need to know to train recurrent neural networks to make Flip  Flops memories and more ",
    "url": "https://arxiv.org/abs/2010.07858",
    "authors": [
      "Cecilia Jarne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2012.05825",
    "title": "Novelty detection using ensembles with regularized disagreement",
    "abstract": " Title: Novelty detection using ensembles with regularized disagreement ",
    "url": "https://arxiv.org/abs/2012.05825",
    "authors": [
      "Alexandru \u0162ifrea",
      "Eric Stavarache",
      "Fanny Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2105.01913",
    "title": "Identifying hidden coalitions in the US House of Representatives by  optimally partitioning signed networks based on generalized balance",
    "abstract": " Comments: Post-peer-review version 21 pages, 7 figures, 2 tables, combined article and supplementary information ",
    "url": "https://arxiv.org/abs/2105.01913",
    "authors": [
      "Samin Aref",
      "Zachary P. Neal"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Optimization and Control (math.OC)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2109.00953",
    "title": "TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions  and U-GRUs for skeletal pedestrian crossing prediction",
    "abstract": " Comments: Accepted to IEEE International Conference on Automatic Face & Gesture Recognition 2021 (December 15 - 18, 2021) 7 pages, 2 Figures ",
    "url": "https://arxiv.org/abs/2109.00953",
    "authors": [
      "Joseph Gesnouin",
      "Steve Pechberti",
      "Bogdan Stanciulescu",
      "Fabien Moutarde"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2109.01294",
    "title": "Measurement-device-independent quantum key distribution for  nonstandalone networks",
    "abstract": " Title: Measurement-device-independent quantum key distribution for  nonstandalone networks ",
    "url": "https://arxiv.org/abs/2109.01294",
    "authors": [
      "Guan-Jie Fan-Yuan",
      "Feng-Yu Lu",
      "Shuang Wang",
      "Zhen-Qiang Yin",
      "De-Yong He",
      "Zheng Zhou",
      "Jun Teng",
      "Wei Chen",
      "Guang-Can Guo",
      "Zheng-Fu Han"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2109.01904",
    "title": "Estimating the probabilities of causation via deep monotonic twin  networks",
    "abstract": " Comments: 7 pages + appendix ",
    "url": "https://arxiv.org/abs/2109.01904",
    "authors": [
      "Athanasios Vlontzos",
      "Bernhard Kainz",
      "Ciaran M. Gilligan-Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  }
]