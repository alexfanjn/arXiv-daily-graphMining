[
  {
    "id": "arXiv:2508.00827",
    "title": "A Schema.org Mapping for Brazilian Legal Norms: Toward Interoperable Legal Graphs and Open Government Data",
    "abstract": "           Open Government Data (OGD) initiatives aim to enhance transparency and public participation by making government data openly accessible. However, structuring legal norms for machine readability remains a critical challenge for advancing Legal Tech applications such as Legal Knowledge Graphs (LKGs). Focusing on the this http URL portal initiative by the Brazilian National Congress, we propose a unified mapping of Brazilian legislation to the this http URL vocabulary via JSON-LD and Linked Data. Our approach covers both the conceptual \"Norm\" entity (mapped to sdo:Legislation) and its digital publications or manifestations (mapped to sdo:LegislationObject). We detail key properties for each type, providing concrete examples and considering URN identifiers (per the LexML standard), multilingual support, versioning in the Official Journal, and inter-norm relationships (e.g., citations and references). Our structured schema improves the quality and interoperability of Brazilian legal data, fosters integration within the global OGD ecosystem, and facilitates the creation of a wor         ",
    "url": "https://arxiv.org/abs/2508.00827",
    "authors": [
      "Hudson de Martim"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.00853",
    "title": "A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation",
    "abstract": "           This study aims to reinforce the theoretical foundation for diverse systems--including the axiomatic definition of intelligence--by introducing a mathematically rigorous and unified formal structure for the concept of 'state,' which has long been used without consensus or formal clarity. First, a 'hierarchical state grid' composed of two axes--state depth and mapping hierarchy--is proposed to provide a unified notational system applicable across mathematical, physical, and linguistic domains. Next, the 'Intermediate Meta-Universe (IMU)' is introduced to enable explicit descriptions of definers (ourselves) and the languages we use, thereby allowing conscious meta-level operations while avoiding self-reference and logical inconsistency. Building on this meta-theoretical foundation, this study expands inter-universal theory beyond mathematics to include linguistic translation and agent integration, introducing the conceptual division between macrocosm-inter-universal and microcosm-inter-universal operations for broader expressivity. Through these contributions, this paper presents a meta-formal logical framework--grounded in the principle of definition = state--that spans time, language, agents, and operations, providing a mathematically robust foundation applicable to the definition of intelligence, formal logic, and scientific theory at large.         ",
    "url": "https://arxiv.org/abs/2508.00853",
    "authors": [
      "Kei Itoh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2508.00855",
    "title": "A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks",
    "abstract": "           Nonlinear partial differential equations (PDEs) are pivotal in modeling complex physical systems, yet traditional Physics-Informed Neural Networks (PINNs) often struggle with unresolved residuals in critical spatiotemporal regions and violations of temporal causality. To address these limitations, we propose a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN). Our framework integrates a decoder-only Transformer to inherently capture temporal correlations through autoregressive processing, coupled with a residual-aware GAN that dynamically identifies and prioritizes high-residual regions. By introducing a causal penalty term and an adaptive sampling mechanism, the method enforces temporal causality while refining accuracy in problematic domains. Extensive numerical experiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate significant improvements, achieving relative MSE reductions of up to three orders of magnitude compared to baseline methods. This work bridges the gap between deep learning and physics-driven modeling, offering a robust solution for multiscale and time-dependent PDE systems.         ",
    "url": "https://arxiv.org/abs/2508.00855",
    "authors": [
      "Ziyang Zhang",
      "Feifan Zhang",
      "Weidong Tang",
      "Lei Shi",
      "Tailai Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2508.00856",
    "title": "EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities",
    "abstract": "           In biomedical science, review by a Research Ethics Committee (REC) is an indispensable way of protecting human subjects from harm. However, in social science and the humanities, mandatory ethics compliance has long been met with scepticism as biomedical models of ethics can map poorly onto methodologies involving complex socio-political and cultural considerations. As a result, tailored ethics training and support as well as access to RECs with the necessary expertise is lacking in some areas, including parts of Europe and low- and middle-income countries. This paper suggests that Generative AI can meaningfully contribute to closing these gaps, illustrating this claim by presenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics support system for social science and humanities researchers. Drawing on constitutional AI technology and a collaborative prompt development methodology, EthicAlly provides structured ethics assessment that incorporates both universal ethics principles and contextual and interpretive considerations relevant to most social science research. In supporting researchers in ethical research design and preparation for REC submission, this kind of system can also contribute to easing the burden on institutional RECs, without attempting to automate or replace human ethical oversight.         ",
    "url": "https://arxiv.org/abs/2508.00856",
    "authors": [
      "Steph Grohmann"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.00877",
    "title": "Satellite Connectivity Prediction for Fast-Moving Platforms",
    "abstract": "           Satellite connectivity is gaining increased attention as the demand for seamless internet access, especially in transportation and remote areas, continues to grow. For fast-moving objects such as aircraft, vehicles, or trains, satellite connectivity is critical due to their mobility and frequent presence in areas without terrestrial coverage. Maintaining reliable connectivity in these cases requires frequent switching between satellite beams, constellations, or orbits. To enhance user experience and address challenges like long switching times, Machine Learning (ML) algorithms can analyze historical connectivity data and predict network quality at specific locations. This allows for proactive measures, such as network switching before connectivity issues arise. In this paper, we analyze a real dataset of communication between a Geostationary Orbit (GEO) satellite and aircraft over multiple flights, using ML to predict signal quality. Our prediction model achieved an F1 score of 0.97 on the test data, demonstrating the accuracy of machine learning in predicting signal quality during flight. By enabling seamless broadband service, including roaming between different satellite constellations and providers, our model addresses the need for real-time predictions of signal quality. This approach can further be adapted to automate satellite and beam-switching mechanisms to improve overall communication efficiency. The model can also be retrained and applied to any moving object with satellite connectivity, using customized datasets, including connected vehicles and trains.         ",
    "url": "https://arxiv.org/abs/2508.00877",
    "authors": [
      "Chao Yan",
      "Babak Mafakheri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00879",
    "title": "GNN-ASE: Graph-Based Anomaly Detection and Severity Estimation in Three-Phase Induction Machines",
    "abstract": "           The diagnosis of induction machines has traditionally relied on model-based methods that require the development of complex dynamic models, making them difficult to implement and computationally expensive. To overcome these limitations, this paper proposes a model-free approach using Graph Neural Networks (GNNs) for fault diagnosis in induction machines. The focus is on detecting multiple fault types -- including eccentricity, bearing defects, and broken rotor bars -- under varying severity levels and load conditions. Unlike traditional approaches, raw current and vibration signals are used as direct inputs, eliminating the need for signal preprocessing or manual feature extraction. The proposed GNN-ASE model automatically learns and extracts relevant features from raw inputs, leveraging the graph structure to capture complex relationships between signal types and fault patterns. It is evaluated for both individual fault detection and multi-class classification of combined fault conditions. Experimental results demonstrate the effectiveness of the proposed model, achieving 92.5\\% accuracy for eccentricity defects, 91.2\\% for bearing faults, and 93.1\\% for broken rotor bar detection. These findings highlight the model's robustness and generalization capability across different operational scenarios. The proposed GNN-based framework offers a lightweight yet powerful solution that simplifies implementation while maintaining high diagnostic performance. It stands as a promising alternative to conventional model-based diagnostic techniques for real-world induction machine monitoring and predictive maintenance.         ",
    "url": "https://arxiv.org/abs/2508.00879",
    "authors": [
      "Moutaz Bellah Bentrad",
      "Adel Ghoggal",
      "Tahar Bahi",
      "Abderaouf Bahi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00880",
    "title": "Reproducibility of Machine Learning-Based Fault Detection and Diagnosis for HVAC Systems in Buildings: An Empirical Study",
    "abstract": "           Reproducibility is a cornerstone of scientific research, enabling independent verification and validation of empirical findings. The topic gained prominence in fields such as psychology and medicine, where concerns about non - replicable results sparked ongoing discussions about research practices. In recent years, the fast-growing field of Machine Learning (ML) has become part of this discourse, as it faces similar concerns about transparency and reliability. Some reproducibility issues in ML research are shared with other fields, such as limited access to data and missing methodological details. In addition, ML introduces specific challenges, including inherent nondeterminism and computational constraints. While reproducibility issues are increasingly recognized by the ML community and its major conferences, less is known about how these challenges manifest in applied disciplines. This paper contributes to closing this gap by analyzing the transparency and reproducibility standards of ML applications in building energy systems. The results indicate that nearly all articles are not reproducible due to insufficient disclosure across key dimensions of reproducibility. 72% of the articles do not specify whether the dataset used is public, proprietary, or commercially available. Only two papers share a link to their code - one of which was broken. Two-thirds of the publications were authored exclusively by academic researchers, yet no significant differences in reproducibility were observed compared to publications with industry-affiliated authors. These findings highlight the need for targeted interventions, including reproducibility guidelines, training for researchers, and policies by journals and conferences that promote transparency and reproducibility.         ",
    "url": "https://arxiv.org/abs/2508.00880",
    "authors": [
      "Adil Mukhtar",
      "Michael Hadwiger",
      "Franz Wotawa",
      "Gerald Schweiger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00881",
    "title": "Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models",
    "abstract": "           Foundation models for natural language processing have many coherent definitions of hallucination and methods for its detection and mitigation. However, analogous definitions and methods do not exist for multi-variate time-series (MVTS) foundation models. We propose new definitions for MVTS hallucination, along with new detection and mitigation methods using a diffusion model to estimate hallucination levels. We derive relational datasets from popular time-series datasets to benchmark these relational hallucination levels. Using these definitions and models, we find that open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline. The proposed mitigation method reduces this by up to 47.7% for these models. The definition and methods may improve adoption and safe usage of MVTS foundation models.         ",
    "url": "https://arxiv.org/abs/2508.00881",
    "authors": [
      "Vijja Wichitwechkarn",
      "Charles Fox",
      "Ruchi Choudhary"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.00884",
    "title": "Multi-Grained Temporal-Spatial Graph Learning for Stable Traffic Flow Forecasting",
    "abstract": "           Time-evolving traffic flow forecasting are playing a vital role in intelligent transportation systems and smart cities. However, the dynamic traffic flow forecasting is a highly nonlinear problem with complex temporal-spatial dependencies. Although the existing methods has provided great contributions to mine the temporal-spatial patterns in the complex traffic networks, they fail to encode the globally temporal-spatial patterns and are prone to overfit on the pre-defined geographical correlations, and thus hinder the model's robustness on the complex traffic environment. To tackle this issue, in this work, we proposed a multi-grained temporal-spatial graph learning framework to adaptively augment the globally temporal-spatial patterns obtained from a crafted graph transformer encoder with the local patterns from the graph convolution by a crafted gated fusion unit with residual connection techniques. Under these circumstances, our proposed model can mine the hidden global temporal-spatial relations between each monitor stations and balance the relative importance of local and global temporal-spatial patterns. Experiment results demonstrate the strong representation capability of our proposed method and our model consistently outperforms other strong baselines on various real-world traffic networks.         ",
    "url": "https://arxiv.org/abs/2508.00884",
    "authors": [
      "Zhenan Lin",
      "Yuni Lai",
      "Wai Lun Lo",
      "Richard Tai-Chiu Hsung",
      "Harris Sik-Ho Tsang",
      "Xiaoyu Xue",
      "Kai Zhou",
      "Yulin Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00888",
    "title": "A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data",
    "abstract": "           Naturalistic driving studies offer a powerful means for observing and quantifying real-world driving behaviour. One of their prominent applications in traffic safety is the continuous monitoring and classification of risky driving behaviour. However, many existing frameworks rely on fixed time windows and static thresholds for distinguishing between safe and risky behaviour - limiting their ability to respond to the stochastic nature of real-world driving. This study proposes a dynamic and individualised framework for identifying risky driving behaviour using Belgian naturalistic driving data. The approach leverages a rolling time window and bi-level optimisation to dynamically calibrate both risk thresholds and model hyperparameters, capturing subtle behavioural shifts. Two safety indicators, speed-weighted headway and harsh driving events, were evaluated using three data-driven models: Random Forest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong capability in capturing subtle changes in driving behaviour, particularly excelling in high-recall tasks, making it promising for early-stage risk detection. XGBoost provided the most balanced and stable performance across different thresholds and evaluation metrics. While random forest showed more variability, it responded sensitively to dynamic threshold adjustments, which may be advantageous during model adaptation or tuning. Speed-weighted headway emerged as a more stable and context-sensitive risk indicator than harsh driving events, likely due to its robustness to label sparsity and contextual variation. Overall, the findings support the value of adaptive, personalised risk detection approaches for enhancing real-time safety feedback and tailoring driver support in intelligent transport systems.         ",
    "url": "https://arxiv.org/abs/2508.00888",
    "authors": [
      "Amir Hossein Kalantari",
      "Eleonora Papadimitriou",
      "Amir Pooyan Afghari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2508.00890",
    "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks",
    "abstract": "           Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.         ",
    "url": "https://arxiv.org/abs/2508.00890",
    "authors": [
      "Fali Wang",
      "Hui Liu",
      "Zhenwei Dai",
      "Jingying Zeng",
      "Zhiwei Zhang",
      "Zongyu Wu",
      "Chen Luo",
      "Zhen Li",
      "Xianfeng Tang",
      "Qi He",
      "Suhang Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.00893",
    "title": "Multi-Community Spectral Clustering for Geometric Graphs",
    "abstract": "           In this paper, we consider the soft geometric block model (SGBM) with a fixed number $k \\geq 2$ of homogeneous communities in the dense regime, and we introduce a spectral clustering algorithm for community recovery on graphs generated by this model. Given such a graph, the algorithm produces an embedding into $\\mathbb{R}^{k-1}$ using the eigenvectors associated with the $k-1$ eigenvalues of the adjacency matrix of the graph that are closest to a value determined by the parameters of the model. It then applies $k$-means clustering to the embedding. We prove weak consistency and show that a simple local refinement step ensures strong consistency. A key ingredient is an application of a non-standard version of Davis-Kahan theorem to control eigenspace perturbations when eigenvalues are not simple. We also analyze the limiting spectrum of the adjacency matrix, using a combination of combinatorial and matrix techniques.         ",
    "url": "https://arxiv.org/abs/2508.00893",
    "authors": [
      "Luiz Emilio Allem",
      "Konstantin Avrachenkov",
      "Carlos Hoppen",
      "Hariprasad Manjunath",
      "Lucas Siviero Sibemberg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Spectral Theory (math.SP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.00897",
    "title": "Maximize margins for robust splicing detection",
    "abstract": "           Despite recent progress in splicing detection, deep learning-based forensic tools remain difficult to deploy in practice due to their high sensitivity to training conditions. Even mild post-processing applied to evaluation images can significantly degrade detector performance, raising concerns about their reliability in operational contexts. In this work, we show that the same deep architecture can react very differently to unseen post-processing depending on the learned weights, despite achieving similar accuracy on in-distribution test data. This variability stems from differences in the latent spaces induced by training, which affect how samples are separated internally. Our experiments reveal a strong correlation between the distribution of latent margins and a detector's ability to generalize to post-processed images. Based on this observation, we propose a practical strategy for building more robust detectors: train several variants of the same model under different conditions, and select the one that maximizes latent margins.         ",
    "url": "https://arxiv.org/abs/2508.00897",
    "authors": [
      "Julien Simon de Kergunic",
      "Rony Abecidan",
      "Patrick Bas",
      "Vincent Itier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.00909",
    "title": "NeuCoReClass AD: Redefining Self-Supervised Time Series Anomaly Detection",
    "abstract": "           Time series anomaly detection plays a critical role in a wide range of real-world applications. Among unsupervised approaches, self-supervised learning has gained traction for modeling normal behavior without the need of labeled data. However, many existing methods rely on a single proxy task, limiting their ability to capture meaningful patterns in normal data. Moreover, they often depend on handcrafted transformations tailored specific domains, hindering their generalization accross diverse problems. To address these limitations, we introduce NeuCoReClass AD, a self-supervised multi-task time series anomaly detection framework that combines contrastive, reconstruction, and classification proxy tasks. Our method employs neural transformation learning to generate augmented views that are informative, diverse, and coherent, without requiring domain-specific knowledge. We evaluate NeuCoReClass AD across a wide range of benchmarks, demonstrating that it consistently outperforms both classical baselines and most deep-learning alternatives. Furthermore, it enables the characterization of distinct anomaly profiles in a fully unsupervised manner.         ",
    "url": "https://arxiv.org/abs/2508.00909",
    "authors": [
      "Aitor S\u00e1nchez-Ferrera",
      "Usue Mori",
      "Borja Calvo",
      "Jose A. Lozano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.00913",
    "title": "TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras",
    "abstract": "           Long-term temporal information is crucial for event-based perception tasks, as raw events only encode pixel brightness changes. Recent works show that when trained from scratch, recurrent models achieve better results than feedforward models in these tasks. However, when leveraging self-supervised pre-trained weights, feedforward models can outperform their recurrent counterparts. Current self-supervised learning (SSL) methods for event-based pre-training largely mimic RGB image-based approaches. They pre-train feedforward models on raw events within a short time interval, ignoring the temporal information of events. In this work, we introduce TESPEC, a self-supervised pre-training framework tailored for learning spatio-temporal information. TESPEC is well-suited for recurrent models, as it is the first framework to leverage long event sequences during pre-training. TESPEC employs the masked image modeling paradigm with a new reconstruction target. We design a novel method to accumulate events into pseudo grayscale videos containing high-level semantic information about the underlying scene, which is robust to sensor noise and reduces motion blur. Reconstructing this target thus requires the model to reason about long-term history of events. Extensive experiments demonstrate our state-of-the-art results in downstream tasks, including object detection, semantic segmentation, and monocular depth estimation. Project webpage: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.00913",
    "authors": [
      "Mohammad Mohammadi",
      "Ziyi Wu",
      "Igor Gilitschenski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.00919",
    "title": "Translating Machine Learning Interpretability into Clinical Insights for ICU Mortality Prediction",
    "abstract": "           Current research efforts largely focus on employing at most one interpretable method to elucidate machine learning (ML) model performance. However, significant barriers remain in translating these interpretability techniques into actionable insights for clinicians, notably due to complexities such as variability across clinical settings and the Rashomon effect. In this study, we developed and rigorously evaluated two ML models along with interpretation mechanisms, utilizing data from 131,051 ICU admissions across 208 hospitals in the United States, sourced from the eICU Collaborative Research Database. We examined two datasets: one with imputed missing values (130,810 patients, 5.58% ICU mortality) and another excluding patients with missing data (5,661 patients, 23.65% ICU mortality). The random forest (RF) model demonstrated an AUROC of 0.912 with the first dataset and 0.839 with the second dataset, while the XGBoost model achieved an AUROC of 0.924 with the first dataset and 0.834 with the second dataset. Consistently identified predictors of ICU mortality across datasets, cross-validation folds, models, and explanation mechanisms included lactate levels, arterial pH, body temperature, and others. By aligning with routinely collected clinical variables, this study aims to enhance ML model interpretability for clinical use, promote greater understanding and adoption among clinicians, and ultimately contribute to improved patient outcomes.         ",
    "url": "https://arxiv.org/abs/2508.00919",
    "authors": [
      "Ling Liao",
      "Eva Aagaard"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.00926",
    "title": "Hybrid Hypergraph Networks for Multimodal Sequence Data Classification",
    "abstract": "           Modeling temporal multimodal data poses significant challenges in classification tasks, particularly in capturing long-range temporal dependencies and intricate cross-modal interactions. Audiovisual data, as a representative example, is inherently characterized by strict temporal order and diverse modalities. Effectively leveraging the temporal structure is essential for understanding both intra-modal dynamics and inter-modal correlations. However, most existing approaches treat each modality independently and rely on shallow fusion strategies, which overlook temporal dependencies and hinder the model's ability to represent complex structural relationships. To address the limitation, we propose the hybrid hypergraph network (HHN), a novel framework that models temporal multimodal data via a segmentation-first, graph-later strategy. HHN splits sequences into timestamped segments as nodes in a heterogeneous graph. Intra-modal structures are captured via hyperedges guided by a maximum entropy difference criterion, enhancing node heterogeneity and structural discrimination, followed by hypergraph convolution to extract high-order dependencies. Inter-modal links are established through temporal alignment and graph attention for semantic fusion. HHN achieves state-of-the-art (SOTA) results on four multimodal datasets, demonstrating its effectiveness in complex classification tasks.         ",
    "url": "https://arxiv.org/abs/2508.00926",
    "authors": [
      "Feng Xu",
      "Hui Wang",
      "Yuting Huang",
      "Danwei Zhang",
      "Zizhu Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.00927",
    "title": "WOCD: A Semi-Supervised Method for Overlapping Community Detection Using Weak Cliques",
    "abstract": "           Overlapping community detection (OCD) is a fundamental graph data analysis task for extracting graph patterns. Traditional OCD methods can be broadly divided into node clustering and link clustering approaches, both of which rely solely on link information to identify overlapping communities. In recent years, deep learning-based methods have made significant advancements for this task. However, existing GNN-based approaches often face difficulties in effectively integrating link, attribute, and prior information, along with challenges like limited receptive fields and over-smoothing, which hinder their performance on complex overlapping community detection. In this paper, we propose a Weak-clique based Overlapping Community Detection method, namely WOCD, which incorporates prior information and optimizes the use of link information to improve detection accuracy. Specifically, we introduce pseudo-labels within a semi-supervised framework to strengthen the generalization ability, making WOCD more versatile. Furthermore, we initialize pseudo-labels using weak cliques to fully leverage link and prior information, leading to better detection accuracy. Additionally, we employ a single-layer Graph Transformer combined with GNN, which achieves significant performance improvements while maintaining efficiency. We evaluate WOCD on eight real-world attributed datasets, and the results demonstrate that it outperforms the state-of-the-art semi-supervised OCD method by a significant margin in terms of accuracy.         ",
    "url": "https://arxiv.org/abs/2508.00927",
    "authors": [
      "Shaozhen Ma",
      "Hanchen Wang",
      "Dong Wen",
      "Wenjie Zhang",
      "Wei Huang",
      "Ying Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.00929",
    "title": "Accessibility and Social Inclusivity: A Literature Review of Music Technology for Blind and Low Vision People",
    "abstract": "           This paper presents a systematic literature review of music technology tailored for blind and low vision (BLV) individuals. Music activities can be particularly beneficial for BLV people. However, a systematic approach to organizing knowledge on designing accessible technology for BLV people has yet to be attempted. We categorize the existing studies based on the type of technology and the extent of BLV people's involvement in the research. We identify six main categories of BLV people-oriented music technology and highlight four key trends in design goals. Based on these categories, we propose four general insights focusing on (1) spatial awareness, (2) access to information, (3) (non-verbal) communication, and (4) memory. The identified trends suggest that more empirical studies involving BLV people in real-world scenarios are needed to ensure that technological advancements can enhance musical experiences and social inclusion. This research proposes collaborative music technology and inclusive real-world testing with the target group as two key areas missing in current research. They serve as a foundational step in shifting the focus from ``accessible technology'' to ``inclusive technology'' for BLV individuals within the broader field of accessibility research.         ",
    "url": "https://arxiv.org/abs/2508.00929",
    "authors": [
      "Shumeng Zhang",
      "Raul Masu",
      "Mela Bettega",
      "Mingming Fan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.00933",
    "title": "OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction",
    "abstract": "           Sea surface temperature (SST) prediction is a critical task in ocean science, supporting various applications, such as weather forecasting, fisheries management, and storm tracking. While existing data-driven methods have demonstrated significant success, they often neglect to leverage the rich domain knowledge accumulated over the past decades, limiting further advancements in prediction accuracy. The recent emergence of large language models (LLMs) has highlighted the potential of integrating domain knowledge for downstream tasks. However, the application of LLMs to SST prediction remains underexplored, primarily due to the challenge of integrating ocean domain knowledge and numerical data. To address this issue, we propose Ocean Knowledge Graph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To the best of our knowledge, this work presents the first systematic effort to construct an Ocean Knowledge Graph (OKG) specifically designed to represent diverse ocean knowledge for SST prediction. We then develop a graph embedding network to learn the comprehensive semantic and structural knowledge within the OKG, capturing both the unique characteristics of individual sea regions and the complex correlations between them. Finally, we align and fuse the learned knowledge with fine-grained numerical SST data and leverage a pre-trained LLM to model SST patterns for accurate prediction. Extensive experiments on the real-world dataset demonstrate that OKG-LLM consistently outperforms state-of-the-art methods, showcasing its effectiveness, robustness, and potential to advance SST prediction. The codes are available in the online repository.         ",
    "url": "https://arxiv.org/abs/2508.00933",
    "authors": [
      "Hanchen Yang",
      "Jiaqi Wang",
      "Jiannong Cao",
      "Wengen Li",
      "Jialun Zheng",
      "Yangning Li",
      "Chunyu Miao",
      "Jihong Guan",
      "Shuigeng Zhou",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00938",
    "title": "Trusted Routing for Blockchain-Empowered UAV Networks via Multi-Agent Deep Reinforcement Learning",
    "abstract": "           Due to the high flexibility and versatility, unmanned aerial vehicles (UAVs) are leveraged in various fields including surveillance and disaster this http URL, in UAV networks, routing is vulnerable to malicious damage due to distributed topologies and high dynamics. Hence, ensuring the routing security of UAV networks is challenging. In this paper, we characterize the routing process in a time-varying UAV network with malicious nodes. Specifically, we formulate the routing problem to minimize the total delay, which is an integer linear programming and intractable to solve. Then, to tackle the network security issue, a blockchain-based trust management mechanism (BTMM) is designed to dynamically evaluate trust values and identify low-trust UAVs. To improve traditional practical Byzantine fault tolerance algorithms in the blockchain, we propose a consensus UAV update mechanism. Besides, considering the local observability, the routing problem is reformulated into a decentralized partially observable Markov decision process. Further, a multi-agent double deep Q-network based routing algorithm is designed to minimize the total delay. Finally, simulations are conducted with attacked UAVs and numerical results show that the delay of the proposed mechanism decreases by 13.39$\\%$, 12.74$\\%$, and 16.6$\\%$ than multi-agent proximal policy optimal algorithms, multi-agent deep Q-network algorithms, and methods without BTMM, respectively.         ",
    "url": "https://arxiv.org/abs/2508.00938",
    "authors": [
      "Ziye Jia",
      "Sijie He",
      "Qiuming Zhu",
      "Wei Wang",
      "Qihui Wu",
      "Zhu Han"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.00939",
    "title": "BarlowWalk: Self-supervised Representation Learning for Legged Robot Terrain-adaptive Locomotion",
    "abstract": "           Reinforcement learning (RL), driven by data-driven methods, has become an effective solution for robot leg motion control problems. However, the mainstream RL methods for bipedal robot terrain traversal, such as teacher-student policy knowledge distillation, suffer from long training times, which limit development efficiency. To address this issue, this paper proposes BarlowWalk, an improved Proximal Policy Optimization (PPO) method integrated with self-supervised representation learning. This method employs the Barlow Twins algorithm to construct a decoupled latent space, mapping historical observation sequences into low-dimensional representations and implementing self-supervision. Meanwhile, the actor requires only proprioceptive information to achieve self-supervised learning over continuous time steps, significantly reducing the dependence on external terrain perception. Simulation experiments demonstrate that this method has significant advantages in complex terrain scenarios. To enhance the credibility of the evaluation, this study compares BarlowWalk with advanced algorithms through comparative tests, and the experimental results verify the effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2508.00939",
    "authors": [
      "Haodong Huang",
      "Shilong Sun",
      "Yuanpeng Wang",
      "Chiyao Li",
      "Hailin Huang",
      "Wenfu Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.00947",
    "title": "Service Discovery-Based Hybrid Network Middleware for Efficient Communication in Distributed Robotic Systems",
    "abstract": "           Robotic middleware is fundamental to ensuring reliable communication among system components and is crucial for intelligent robotics, autonomous vehicles, and smart manufacturing. However, existing robotic middleware often struggles to meet the diverse communication demands, optimize data transmission efficiency, and maintain scheduling determinism between Orin computing units in large-scale L4 autonomous vehicle deployments. This paper presents RIMAOS2C, a service discovery-based hybrid network communication middleware designed to tackle these challenges. By leveraging multi-level service discovery multicast, RIMAOS2C supports a wide variety of communication modes, including multiple cross-chip Ethernet protocols and PCIe communication capabilities. Its core mechanism, the Message Bridge, optimizes data flow forwarding and employs shared memory for centralized message distribution, reducing message redundancy and minimizing transmission delay uncertainty. Tested on L4 vehicles and Jetson Orin domain controllers, RIMAOS2C leverages TCP-based ZeroMQ to overcome the large-message transmission bottleneck in native CyberRT. In scenarios with two cross-chip subscribers, it eliminates message redundancy and improves large-data transmission efficiency by 36 to 40 percent while reducing callback latency variation by 42 to 906 percent. This research advances the communication capabilities of robotic operating systems and proposes a novel approach to optimizing communication in distributed computing architectures for autonomous driving.         ",
    "url": "https://arxiv.org/abs/2508.00947",
    "authors": [
      "Shiyao Sang",
      "Yinggang Ling"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.00950",
    "title": "Investigating Crossing Perception in 3D Graph Visualisation",
    "abstract": "           Human perception of graph drawings is influenced by a variety of impact factors for which quality measures are used as a proxy indicator. The investigation of those impact factors and their effects is important to evaluate and improve quality measures and drawing algorithms. The number of edge crossings in a 2D graph drawing has long been a main quality measure for drawing evaluation. The use of stereoscopic 3D graph visualisations has gained attraction over the last years, and results from several studies indicate that they can improve analysis efficiency for a range of analysis scenarios. While edge crossings can also occur in 3D, there are edge configurations in space that are not crossings but might be perceived as such from a specific viewpoint. Such configurations create crossings when projected on the corresponding 2D image plane and could impact readability similar to 2D crossings. In 3D drawings, the additional depth aspect and the subsequent impact factors of edge distance and relative edge direction in space might further influence the importance of those configurations for readability. We investigate the impact of such factors in an empirical study and report on findings of difference between major factor categories.         ",
    "url": "https://arxiv.org/abs/2508.00950",
    "authors": [
      "Ying Zhang",
      "Niklas Groene",
      "Karsten Klein",
      "Giuseppe Liotta",
      "Falk Schreiber"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2508.00955",
    "title": "From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model",
    "abstract": "           Multimodal Large Language Models (MLLMs) have emerged as a promising solution for universal embedding tasks, yet adapting their generative nature for discriminative representation learning remains a significant challenge. The dominant paradigm of large-scale contrastive pre-training suffers from critical inefficiencies, including prohibitive computational costs and a failure to leverage the intrinsic, instruction-following capabilities of MLLMs. To overcome these limitations, we propose an efficient framework for universal multimodal embeddings, which bridges this gap by centering on two synergistic components. First, our hierarchical embedding prompt template employs a two-level instruction architecture that forces the model to produce discriminative representations. Building on this strong foundation, our second component, self-aware hard negative sampling, redefines the fine-tuning process by leveraging the model's own understanding to efficiently mine challenging negatives while actively filtering out potential false negatives. Our comprehensive experiments show that our hierarchical prompt achieves zero-shot performance competitive with contrastively trained baselines and enhances the fine-tuning process by lifting a simple in-batch negative baseline by 4.8 points on the MMEB benchmark. We further boost the performance via our self-aware hard negative sampling, achieving the state-of-the-art performance without the contrative pre-training. Our work presents an effective and efficient pathway to adapt MLLMs for universal embedding tasks, significantly reducing training time.         ",
    "url": "https://arxiv.org/abs/2508.00955",
    "authors": [
      "Yeong-Joon Ju",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.00956",
    "title": "Learning Unified User Quantized Tokenizers for User Representation",
    "abstract": "           Multi-source user representation learning plays a critical role in enabling personalized services on web platforms (e.g., Alipay). While prior works have adopted late-fusion strategies to combine heterogeneous data sources, they suffer from three key limitations: lack of unified representation frameworks, scalability and storage issues in data compression, and inflexible cross-task generalization. To address these challenges, we propose U^2QT (Unified User Quantized Tokenizers), a novel framework that integrates cross-domain knowledge transfer with early fusion of heterogeneous domains. Our framework employs a two-stage architecture: first, a causal Q-Former projects domain-specific features into a shared causal representation space to preserve inter-modality dependencies; second, a multi-view RQ-VAE discretizes causal embeddings into compact tokens through shared and source-specific codebooks, enabling efficient storage while maintaining semantic coherence. Experimental results showcase U^2QT's advantages across diverse downstream tasks, outperforming task-specific baselines in future behavior prediction and recommendation tasks while achieving efficiency gains in storage and computation. The unified tokenization framework enables seamless integration with language models and supports industrial-scale applications.         ",
    "url": "https://arxiv.org/abs/2508.00956",
    "authors": [
      "Chuan He",
      "Yang Chen",
      "Wuliang Huang",
      "Tianyi Zheng",
      "Jianhu Chen",
      "Bin Dou",
      "Yice Luo",
      "Yun Zhu",
      "Baokun Wang",
      "Yongchao Liu",
      "Xing Fu",
      "Yu Cheng",
      "Chuntao Hong",
      "Weiqiang Wang",
      "Xin-Wei Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.00959",
    "title": "Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables",
    "abstract": "           Physically Guided Neural Networks with Internal Variables are SciML tools that use only observable data for training and and have the capacity to unravel internal state relations. They incorporate physical knowledge both by prescribing the model architecture and using loss regularization, thus endowing certain specific neurons with a physical meaning as internal state variables. Despite their potential, these models face challenges in scalability when applied to high-dimensional data such as fine-grid spatial fields or time-evolving systems. In this work, we propose some enhancements to the PGNNIV framework that address these scalability limitations through reduced-order modeling techniques. Specifically, we introduce alternatives to the original decoder structure using spectral decomposition, POD, and pretrained autoencoder-based mappings. These surrogate decoders offer varying trade-offs between computational efficiency, accuracy, noise tolerance, and generalization, while improving drastically the scalability. Additionally, we integrate model reuse via transfer learning and fine-tuning strategies to exploit previously acquired knowledge, supporting efficient adaptation to novel materials or configurations, and significantly reducing training time while maintaining or improving model performance. To illustrate these various techniques, we use a representative case governed by the nonlinear diffusion equation, using only observable data. Results demonstrate that the enhanced PGNNIV framework successfully identifies the underlying constitutive state equations while maintaining high predictive accuracy. It also improves robustness to noise, mitigates overfitting, and reduces computational demands. The proposed techniques can be tailored to various scenarios depending on data availability, resources, and specific modeling objectives, overcoming scalability challenges in all the scenarios.         ",
    "url": "https://arxiv.org/abs/2508.00959",
    "authors": [
      "Rub\u00e9n Mu\u00f1oz-Sierra",
      "Manuel Doblar\u00e9",
      "Jacobo Ayensa-Jim\u00e9nez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00961",
    "title": "FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph",
    "abstract": "           Individual investors are significantly outnumbered and disadvantaged in financial markets, overwhelmed by abundant information and lacking professional analysis. Equity research reports stand out as crucial resources, offering valuable insights. By leveraging these reports, large language models (LLMs) can enhance investors' decision-making capabilities and strengthen financial analysis. However, two key challenges limit their effectiveness: (1) the rapid evolution of market events often outpaces the slow update cycles of existing knowledge bases, (2) the long-form and unstructured nature of financial reports further hinders timely and context-aware integration by LLMs. To address these challenges, we tackle both data and methodological aspects. First, we introduce the Event-Enhanced Automated Construction of Financial Knowledge Graph (FinKario), a dataset comprising over 305,360 entities, 9,625 relational triples, and 19 distinct relation types. FinKario automatically integrates real-time company fundamentals and market events through prompt-driven extraction guided by professional institutional templates, providing structured and accessible financial insights for LLMs. Additionally, we propose a Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the retrieval of evolving, large-scale financial knowledge to ensure efficient and precise data access. Extensive experiments show that FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.         ",
    "url": "https://arxiv.org/abs/2508.00961",
    "authors": [
      "Xiang Li",
      "Penglei Sun",
      "Wanyun Zhou",
      "Zikai Wei",
      "Yongqi Zhang",
      "Xiaowen Chu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00965",
    "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI",
    "abstract": "           We introduce VAULT, a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval, adversarial generation, and iterative retraining. First, we perform balanced few-shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses, which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero-shot RoBERTa-base this http URL standard benchmarks, VAULT elevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from 75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%. It also consistently outperforms prior in-context adversarial methods by up to 2.0% across datasets. By automating high-quality adversarial data curation at scale, VAULT enables rapid, human-independent robustness improvements in NLI inference tasks.         ",
    "url": "https://arxiv.org/abs/2508.00965",
    "authors": [
      "Roie Kazoom",
      "Ofir Cohen",
      "Rami Puzis",
      "Asaf Shabtai",
      "Ofer Hadar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00969",
    "title": "Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles",
    "abstract": "           Self-supervised learning has driven major advances in computational pathology by enabling models to learn rich representations from hematoxylin and eosin (H&E)-stained cancer tissue. However, histopathology alone often falls short for molecular characterization and understanding clinical outcomes, as important information is contained in high-dimensional omics profiles like transcriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS, a unified transformer-based pre-training framework that encodes both histopathology and multi-omics data into a shared latent space. At its core, MORPHEUS relies on a masked modeling objective applied to randomly selected omics portions, encouraging the model to learn biologically meaningful cross-modal relationships. The same pre-trained network can be applied to histopathology alone or in combination with any subset of omics modalities, seamlessly adapting to the available inputs. Additionally, MORPHEUS enables any-to-any omics generation, enabling one or more omics profiles to be inferred from any subset of modalities, including H&E alone. Pre-trained on a large pan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods across diverse modality combinations and tasks, positioning itself as a promising framework for developing multimodal foundation models in oncology. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2508.00969",
    "authors": [
      "Lucas Robinet",
      "Ahmad Berjaoui",
      "Elizabeth Cohen-Jonathan Moyal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00975",
    "title": "Star Network Motifs on X during COVID-19",
    "abstract": "           Social network motifs are recurring patterns of small subgraphs that indicate fundamental patterns of social communication. In this work, we study the simple star network motifs that recur on X during the COVID-19 discourse. We study the profile of the manifestation of the star network among bot and human users. There are six primary patterns of the star motif, differentiating by the bots and humans being either egos and alters. We describe the presentation of each of these six patterns in our data, demonstrating how the motif patterns can inform social media behavioral analysis.         ",
    "url": "https://arxiv.org/abs/2508.00975",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Divyaansh Sinha",
      "Kathleen M. Carley"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.00998",
    "title": "Are LLM-Powered Social Media Bots Realistic?",
    "abstract": "           As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.         ",
    "url": "https://arxiv.org/abs/2508.00998",
    "authors": [
      "Lynnette Hui Xian Ng",
      "Kathleen M. Carley"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01010",
    "title": "v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning",
    "abstract": "           Conventional deep learning models embed data in Euclidean space $\\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word senses, or file systems. We introduce van der Put Neural Networks (v-PuNNs), the first architecture whose neurons are characteristic functions of p-adic balls in $\\mathbb{Z}_p$. Under our Transparent Ultrametric Representation Learning (TURL) principle every weight is itself a p-adic number, giving exact subtree semantics. A new Finite Hierarchical Approximation Theorem shows that a depth-K v-PuNN with $\\sum_{j=0}^{K-1}p^{\\,j}$ neurons universally represents any K-level tree. Because gradients vanish in this discrete space, we propose Valuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic variant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three canonical benchmarks our CPU-only implementation sets new state-of-the-art: WordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO molecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\\rho = -0.96$ with true taxonomic distance. The learned metric is perfectly ultrametric (zero triangle violations), and its fractal and information-theoretic properties are analyzed. Beyond classification we derive structural invariants for quantum systems (HiPaQ) and controllable generative codes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data.         ",
    "url": "https://arxiv.org/abs/2508.01010",
    "authors": [
      "Gnankan Landry Regis N'guessan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01045",
    "title": "Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans",
    "abstract": "           With the increasing number of CT scan examinations, there is a need for automated methods such as organ segmentation, anomaly detection and report generation to assist radiologists in managing their increasing workload. Multi-label classification of 3D CT scans remains a critical yet challenging task due to the complex spatial relationships within volumetric data and the variety of observed anomalies. Existing approaches based on 3D convolutional networks have limited abilities to model long-range dependencies while Vision Transformers suffer from high computational costs and often require extensive pre-training on large-scale datasets from the same domain to achieve competitive performance. In this work, we propose an alternative by introducing a new graph-based approach that models CT scans as structured graphs, leveraging axial slice triplets nodes processed through spectral domain convolution to enhance multi-label anomaly classification performance. Our method exhibits strong cross-dataset generalization, and competitive performance while achieving robustness to z-axis translation. An ablation study evaluates the contribution of each proposed component.         ",
    "url": "https://arxiv.org/abs/2508.01045",
    "authors": [
      "Theo Di Piazza",
      "Carole Lazarus",
      "Olivier Nempont",
      "Loic Boussel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01048",
    "title": "Explaining GNN Explanations with Edge Gradients",
    "abstract": "           In recent years, the remarkable success of graph neural networks (GNNs) on graph-structured data has prompted a surge of methods for explaining GNN predictions. However, the state-of-the-art for GNN explainability remains in flux. Different comparisons find mixed results for different methods, with many explainers struggling on more complex GNN architectures and tasks. This presents an urgent need for a more careful theoretical analysis of competing GNN explanation methods. In this work we take a closer look at GNN explanations in two different settings: input-level explanations, which produce explanatory subgraphs of the input graph, and layerwise explanations, which produce explanatory subgraphs of the computation graph. We establish the first theoretical connections between the popular perturbation-based and classical gradient-based methods, as well as point out connections between other recently proposed methods. At the input level, we demonstrate conditions under which GNNExplainer can be approximated by a simple heuristic based on the sign of the edge gradients. In the layerwise setting, we point out that edge gradients are equivalent to occlusion search for linear GNNs. Finally, we demonstrate how our theoretical results manifest in practice with experiments on both synthetic and real datasets.         ",
    "url": "https://arxiv.org/abs/2508.01048",
    "authors": [
      "Jesse He",
      "Akbar Rafiey",
      "Gal Mishne",
      "Yusu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01060",
    "title": "Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation",
    "abstract": "           Managing connectivity in integrated satellite-terrestrial vehicular networks is critical for 6G, yet is challenged by dynamic conditions and partial observability. This letter introduces the Multi-Agent Actor-Critic with Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent reinforcement learning framework that enables vehicles to autonomously manage connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure (V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the integration of a multi-head attention mechanism, which allows for robust state estimation even with fluctuating and limited information sharing among vehicles. The framework further leverages self-imitation learning (SIL) and fingerprinting to improve learning efficiency and real-time decisions. Simulation results, based on realistic SUMO traffic models and 3GPP-compliant configurations, demonstrate that MAAC-SAM outperforms state-of-the-art terrestrial and satellite-assisted baselines by up to 14% in transmission utility and maintains high estimation accuracy across varying vehicle densities and sharing levels.         ",
    "url": "https://arxiv.org/abs/2508.01060",
    "authors": [
      "Ibrahim Althamary",
      "Chen-Fu Chou",
      "Chih-Wei Huang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01062",
    "title": "CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception",
    "abstract": "           Cooperative perception (CP) enhances situational awareness of connected and autonomous vehicles by exchanging and combining messages from multiple agents. While prior work has explored adversarial integrity attacks that degrade perceptual accuracy, little is known about CP's robustness against attacks on timeliness (or availability), a safety-critical requirement for autonomous driving. In this paper, we present CP-FREEZER, the first latency attack that maximizes the computation delay of CP algorithms by injecting adversarial perturbation via V2V messages. Our attack resolves several unique challenges, including the non-differentiability of point cloud preprocessing, asynchronous knowledge of the victim's input due to transmission delays, and uses a novel loss function that effectively maximizes the execution time of the CP pipeline. Extensive experiments show that CP-FREEZER increases end-to-end CP latency by over $90\\times$, pushing per-frame processing time beyond 3 seconds with a 100% success rate on our real-world vehicle testbed. Our findings reveal a critical threat to the availability of CP systems, highlighting the urgent need for robust defenses.         ",
    "url": "https://arxiv.org/abs/2508.01062",
    "authors": [
      "Chenyi Wang",
      "Ruoyu Song",
      "Raymond Muller",
      "Jean-Philippe Monteuuis",
      "Z. Berkay Celik",
      "Jonathan Petit",
      "Ryan Gerdes",
      "Ming Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01067",
    "title": "Expressive Power of Graph Transformers via Logic",
    "abstract": "           Transformers are the basis of modern large language models, but relatively little is known about their precise expressive power on graphs. We study the expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and GPS-networks by Ramp\u00e1sek et al. (2022), both under soft-attention and average hard-attention. Our study covers two scenarios: the theoretical setting with real numbers and the more practical case with floats. With reals, we show that in restriction to vertex properties definable in first-order logic (FO), GPS-networks have the same expressive power as graded modal logic (GML) with the global modality. With floats, GPS-networks turn out to be equally expressive as GML with the counting global modality. The latter result is absolute, not restricting to properties definable in a background logic. We also obtain similar characterizations for GTs in terms of propositional logic with the global modality (for reals) and the counting global modality (for floats).         ",
    "url": "https://arxiv.org/abs/2508.01067",
    "authors": [
      "Veeti Ahvonen",
      "Maurice Funk",
      "Damian Heiman",
      "Antti Kuusisto",
      "Carsten Lutz"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01074",
    "title": "Evading Data Provenance in Deep Neural Networks",
    "abstract": "           Modern over-parameterized deep models are highly data-dependent, with large scale general-purpose and domain-specific datasets serving as the bedrock for rapid advancements. However, many datasets are proprietary or contain sensitive information, making unrestricted model training problematic. In the open world where data thefts cannot be fully prevented, Dataset Ownership Verification (DOV) has emerged as a promising method to protect copyright by detecting unauthorized model training and tracing illicit activities. Due to its diversity and superior stealth, evading DOV is considered extremely challenging. However, this paper identifies that previous studies have relied on oversimplistic evasion attacks for evaluation, leading to a false sense of security. We introduce a unified evasion framework, in which a teacher model first learns from the copyright dataset and then transfers task-relevant yet identifier-independent domain knowledge to a surrogate student using an out-of-distribution (OOD) dataset as the intermediary. Leveraging Vision-Language Models and Large Language Models, we curate the most informative and reliable subsets from the OOD gallery set as the final transfer set, and propose selectively transferring task-oriented knowledge to achieve a better trade-off between generalization and evasion effectiveness. Experiments across diverse datasets covering eleven DOV methods demonstrate our approach simultaneously eliminates all copyright identifiers and significantly outperforms nine state-of-the-art evasion attacks in both generalization and effectiveness, with moderate computational overhead. As a proof of concept, we reveal key vulnerabilities in current DOV methods, highlighting the need for long-term development to enhance practicality.         ",
    "url": "https://arxiv.org/abs/2508.01074",
    "authors": [
      "Hongyu Zhu",
      "Sichu Liang",
      "Wenwen Wang",
      "Zhuomeng Zhang",
      "Fangqi Li",
      "Shi-Lin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01077",
    "title": "The Lattice Geometry of Neural Network Quantization -- A Short Equivalence Proof of GPTQ and Babai's algorithm",
    "abstract": "           We explain how data-driven quantization of a linear unit in a neural network corresponds to solving the closest vector problem for a certain lattice generated by input data. We prove that the GPTQ algorithm is equivalent to Babai's well-known nearest-plane algorithm. We furthermore provide geometric intuition for both algorithms. Lastly, we note the consequences of these results, in particular hinting at the possibility for using lattice basis reduction for better quantization.         ",
    "url": "https://arxiv.org/abs/2508.01077",
    "authors": [
      "Johann Birnick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01081",
    "title": "Baseline-free Damage Detection and Localization on Composite Structures with Unsupervised Kolmogorov-Arnold Autoencoder and Guided Waves",
    "abstract": "           Structural health monitoring (SHM) ensures the safety and longevity of structures such as aerospace equipment and wind power installations. Developing a simple, highly flexible, and scalable SHM method that does not depend on baseline models is significant for ensuring the operational integrity of advanced composite structures. In this regard, a hybrid baseline-free damage detection and localization framework incorporating an unsupervised Kolmogorov-Arnold autoencoder (KAE) and modified probabilistic elliptical imaging algorithm (MRAPID) is proposed for damage detection and localization in composite structures. Specifically, KAE was used to process the guided wave signals (GW) without any prior feature extraction process. The KAE continuously learns and adapts to the baseline model of each structure, learning from the response characteristics of its undamaged state. Then, the predictions from KAE are processed, combined with the MRAPID to generate a damage probability map. The performance of the proposed method for damage detection and localization was verified using the simulated damage data obtained on wind turbine blades and the actual damage data obtained on composite flat plates. The results show that the proposed method can effectively detect and localize damage and can achieve multiple damage localization. In addition, the method outperforms classical damage detection algorithms and state-of-the-art baseline-free damage detection and localization methods in terms of damage localization accuracy.         ",
    "url": "https://arxiv.org/abs/2508.01081",
    "authors": [
      "Yunlai Liao",
      "Yihan Wang",
      "Chen Fang",
      "Xin Yang",
      "Xianping Zeng",
      "Dimitrios Chronopoulos",
      "Xinlin Qing"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2508.01085",
    "title": "An Unconditionally Secure Encryption Scheme for IoBT Networks",
    "abstract": "           We consider an Internet of Battlefield Things (IoBT) system consisting of multiple devices that want to securely communicate with each other during a mission in the presence of an adversary with unbounded computational power. The adversary has complete access to listen/read the ciphertext without tampering with the communication line. We provide an unconditionally secure encryption scheme to exchange messages among devices in the system. The main idea behind the scheme is to provide secret keys to exchange messages using a random binary matrix that is securely shared among all the devices, and pair-wise random secret keys established between each pair of devices attempting to communicate before the mission. The scheme is implemented by using finite group modular addition. We show that the scheme is absolutely semantically secure, i.e., the scheme guarantees that an adversary with unbounded computational power cannot get even one bit of information about a message, except for an exponentially small probability in a security parameter. Besides that, we show that even if the random binary matrix is revealed to the adversary, the provided scheme is computationally secure against the key recovery attack.         ",
    "url": "https://arxiv.org/abs/2508.01085",
    "authors": [
      "Mohammad Moltafet",
      "Hamid R. Sadjadpour",
      "Zouheir Rezki"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2508.01087",
    "title": "COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition",
    "abstract": "           Handling novelty remains a key challenge in visual recognition systems. Existing open-set recognition (OSR) methods rely on the familiarity hypothesis, detecting novelty by the absence of familiar features. We propose a novel attenuation hypothesis: small weights learned during training attenuate features and serve a dual role-differentiating known classes while discarding information useful for distinguishing known from unknown classes. To leverage this overlooked information, we present COSTARR, a novel approach that combines both the requirement of familiar features and the lack of unfamiliar ones. We provide a probabilistic interpretation of the COSTARR score, linking it to the likelihood of correct classification and belonging in a known class. To determine the individual contributions of the pre- and post-attenuated features to COSTARR's performance, we conduct ablation studies that show both pre-attenuated deep features and the underutilized post-attenuated Hadamard product features are essential for improving OSR. Also, we evaluate COSTARR in a large-scale setting using ImageNet2012-1K as known data and NINCO, iNaturalist, OpenImage-O, and other datasets as unknowns, across multiple modern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments demonstrate that COSTARR generalizes effectively across various architectures and significantly outperforms prior state-of-the-art methods by incorporating previously discarded attenuation information, advancing open-set recognition capabilities.         ",
    "url": "https://arxiv.org/abs/2508.01087",
    "authors": [
      "Ryan Rabinowitz",
      "Steve Cruz",
      "Walter Scheirer",
      "Terrance E. Boult"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01091",
    "title": "Disaggregated Health Data in LLMs: Evaluating Data Equity in the Context of Asian American Representation",
    "abstract": "           Large language models (LLMs), such as ChatGPT and Claude, have emerged as essential tools for information retrieval, often serving as alternatives to traditional search engines. However, ensuring that these models provide accurate and equitable information tailored to diverse demographic groups remains an important challenge. This study investigates the capability of LLMs to retrieve disaggregated health-related information for sub-ethnic groups within the Asian American population, such as Korean and Chinese communities. Data disaggregation has been a critical practice in health research to address inequities, making it an ideal domain for evaluating representation equity in LLM outputs. We apply a suite of statistical and machine learning tools to assess whether LLMs deliver appropriately disaggregated and equitable information. By focusing on Asian American sub-ethnic groups, a highly diverse population often aggregated in traditional analyses; we highlight how LLMs handle complex disparities in health data. Our findings contribute to ongoing discussions about responsible AI, particularly in ensuring data equity in the outputs of LLM-based systems.         ",
    "url": "https://arxiv.org/abs/2508.01091",
    "authors": [
      "Uvini Balasuriya Mudiyanselage",
      "Bharat Jayprakash",
      "Kookjin Lee",
      "K. Hazel Kwon"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.01095",
    "title": "AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions",
    "abstract": "           This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework designed for robust, real-time detection and classification of industrial smoke emissions. The framework addresses critical limitations of current monitoring systems, which often lack the specificity to distinguish smoke types and struggle with environmental variability. AURA leverages both the dynamic movement patterns and the distinct color characteristics of industrial smoke to provide enhanced accuracy and reduced false positives. This framework aims to significantly improve environmental compliance, operational safety, and public health outcomes by enabling precise, automated monitoring of industrial emissions.         ",
    "url": "https://arxiv.org/abs/2508.01095",
    "authors": [
      "Mikhail Bychkov",
      "Matey Yordanov",
      "Andrei Kuchma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01107",
    "title": "AdVAR-DNN: Adversarial Misclassification Attack on Collaborative DNN Inference",
    "abstract": "           In recent years, Deep Neural Networks (DNNs) have become increasingly integral to IoT-based environments, enabling realtime visual computing. However, the limited computational capacity of these devices has motivated the adoption of collaborative DNN inference, where the IoT device offloads part of the inference-related computation to a remote server. Such offloading often requires dynamic DNN partitioning information to be exchanged among the participants over an unsecured network or via relays/hops, leading to novel privacy vulnerabilities. In this paper, we propose AdVAR-DNN, an adversarial variational autoencoder (VAE)-based misclassification attack, leveraging classifiers to detect model information and a VAE to generate untraceable manipulated samples, specifically designed to compromise the collaborative inference process. AdVAR-DNN attack uses the sensitive information exchange vulnerability of collaborative DNN inference and is black-box in nature in terms of having no prior knowledge about the DNN model and how it is partitioned. Our evaluation using the most popular object classification DNNs on the CIFAR-100 dataset demonstrates the effectiveness of AdVAR-DNN in terms of high attack success rate with little to no probability of detection.         ",
    "url": "https://arxiv.org/abs/2508.01107",
    "authors": [
      "Shima Yousefi",
      "Motahare Mounesan",
      "Saptarshi Debroy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.01115",
    "title": "A hierarchy tree data structure for behavior-based user segment representation",
    "abstract": "           User attributes are essential in multiple stages of modern recommendation systems and are particularly important for mitigating the cold-start problem and improving the experience of new or infrequent users. We propose Behavior-based User Segmentation (BUS), a novel tree-based data structure that hierarchically segments the user universe with various users' categorical attributes based on the users' product-specific engagement behaviors. During the BUS tree construction, we use Normalized Discounted Cumulative Gain (NDCG) as the objective function to maximize the behavioral representativeness of marginal users relative to active users in the same segment. The constructed BUS tree undergoes further processing and aggregation across the leaf nodes and internal nodes, allowing the generation of popular social content and behavioral patterns for each node in the tree. To further mitigate bias and improve fairness, we use the social graph to derive the user's connection-based BUS segments, enabling the combination of behavioral patterns extracted from both the user's own segment and connection-based segments as the connection aware BUS-based recommendation. Our offline analysis shows that the BUS-based retrieval significantly outperforms traditional user cohort-based aggregation on ranking quality. We have successfully deployed our data structure and machine learning algorithm and tested it with various production traffic serving billions of users daily, achieving statistically significant improvements in the online product metrics, including music ranking and email notifications. To the best of our knowledge, our study represents the first list-wise learning-to-rank framework for tree-based recommendation that effectively integrates diverse user categorical attributes while preserving real-world semantic interpretability at a large industrial scale.         ",
    "url": "https://arxiv.org/abs/2508.01115",
    "authors": [
      "Yang Liu",
      "Xuejiao Kang",
      "Sathya Iyer",
      "Idris Malik",
      "Ruixuan Li",
      "Juan Wang",
      "Xinchen Lu",
      "Xiangxue Zhao",
      "Dayong Wang",
      "Menghan Liu",
      "Isaac Liu",
      "Feng Liang",
      "Yinzhe Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01124",
    "title": "Network Prebunking Problem: Optimizing Prebunking Targets to Suppress the Spread of Misinformation in Social Networks",
    "abstract": "           As a countermeasure against misinformation that undermines the healthy use of social media, a preventive intervention known as prebunking has recently attracted attention in the field of psychology. Prebunking aims to strengthen individuals' cognitive resistance to misinformation by presenting weakened doses of misinformation or by teaching common manipulation techniques before they encounter actual misinformation. Despite the growing body of evidence supporting its effectiveness in reducing susceptibility to misinformation at the individual level, an important open question remains: how best to identify the optimal targets for prebunking interventions to mitigate the spread of misinformation in a social network. To address this issue, we formulate a combinatorial optimization problem, called the network prebunking problem, to identify optimal prebunking targets for minimizing the spread of misinformation in a social network. We prove that this problem is NP-hard and propose an approximation algorithm, MIA-NPP, based on the Maximum Influence Arborescence (MIA) approach, which restricts influence propagation around each node to a local directed tree rooted at that node. Through numerical experiments using real-world social network datasets, we demonstrate that MIA-NPP effectively suppresses the spread of misinformation under both fully observed and uncertain model parameter settings.         ",
    "url": "https://arxiv.org/abs/2508.01124",
    "authors": [
      "Satoshi Furutani",
      "Toshiki Shibahara",
      "Mitsuaki Akiyama",
      "Masaki Aida"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.01125",
    "title": "Shooting the Messenger? Harassment and Hate Speech Directed at Journalists on Social Media",
    "abstract": "           Journalists have incorporated social networks into their work as a standard tool, enhancing their ability to produce and disseminate information and making it easier for them to connect more directly with their audiences. However, this greater presence in the digital public sphere has also increased their exposure to harassment and hate speech, particularly in the case of women journalists. This study analyzes the presence of harassment and hate speech in responses (n = 60,684) to messages that 200 journalists and media outlets posted on X (formerly Twitter) accounts during the days immediately preceding and following the July 23 (23-J) general elections held in Spain in 2023. The results indicate that the most common forms of harassment were insults and political hate, which were more frequently aimed at personal accounts than institutional ones, highlighting the significant role of political polarization-particularly during election periods-in shaping the hostility that journalists face. Moreover, although, generally speaking, the total number of harassing messages was similar for men and women, it was found that a greater number of sexist messages were aimed at women journalists, and an ideological dimension was identified in the hate speech that extremists or right-wing populists directed at them. This study corroborates that this is a minor but systemic issue, particularly from a political and gender perspective. To counteract this, the media must develop proactive policies and protective actions extending even to the individual level, where this issue usually applies.         ",
    "url": "https://arxiv.org/abs/2508.01125",
    "authors": [
      "Sim\u00f3n Pe\u00f1a-Fern\u00e1ndez",
      "Urko Pe\u00f1a-Alonso",
      "Ainara Larrondo-Ureta",
      "Jordi Morales-i-Gras"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.01128",
    "title": "Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation",
    "abstract": "           Textual reviews enrich recommender systems with fine-grained preference signals and enhanced explainability. However, in real-world scenarios, users rarely leave reviews, resulting in severe sparsity that undermines the effectiveness of existing models. A natural solution is to impute or generate missing reviews to enrich the data. However, conventional imputation techniques -- such as matrix completion and LLM-based augmentation -- either lose contextualized semantics by embedding texts into vectors, or overlook structural dependencies among user-item interactions. To address these shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual Edge Graph Representation), a unified framework that imputes missing reviews by jointly modeling semantic and structural signals. Specifically, we represent user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge attributes. To capture relational context, we construct line-graph views and employ a large language model as a graph-aware aggregator. For each interaction lacking a textual review, our model aggregates the neighborhood's natural-language representations to generate a coherent and personalized review. Experiments on the Amazon and Goodreads datasets show that TWISTER consistently outperforms traditional numeric, graph-based, and LLM baselines, delivering higher-quality imputed reviews and, more importantly, enhanced recommendation performance. In summary, TWISTER generates reviews that are more helpful, authentic, and specific, while smoothing structural signals for improved recommendations.         ",
    "url": "https://arxiv.org/abs/2508.01128",
    "authors": [
      "Leyao Wang",
      "Xutao Mao",
      "Xuhui Zhan",
      "Yuying Zhao",
      "Bo Ni",
      "Ryan A. Rossi",
      "Nesreen K. Ahmed",
      "Tyler Derr"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01136",
    "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs",
    "abstract": "           The operation and maintenance (O&M) of database systems is critical to ensuring system availability and performance, typically requiring expert experience (e.g., identifying metric-to-anomaly relations) for effective diagnosis and recovery. However, existing automatic database O&M methods, including commercial products, cannot effectively utilize expert experience. On the one hand, rule-based methods only support basic O&M tasks (e.g., metric-based anomaly detection), which are mostly numerical equations and cannot effectively incorporate literal O&M experience (e.g., troubleshooting guidance in manuals). On the other hand, LLM-based methods, which retrieve fragmented information (e.g., standard documents + RAG), often generate inaccurate or generic results. To address these limitations, we present DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a heterogeneous graph model for representing the diagnosis experience, and proposes a semi-automatic graph construction algorithm to build that graph from thousands of documents. Second, DBAIOps develops a collection of (800+) reusable anomaly models that identify both directly alerted metrics and implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps proposes a two-stage graph evolution mechanism to explore relevant diagnosis paths and identify missing relations automatically. It then leverages a reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear diagnosis reports for both DBAs and common users. Our evaluation over four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher in root cause and human evaluation accuracy, respectively.         ",
    "url": "https://arxiv.org/abs/2508.01136",
    "authors": [
      "Wei Zhou",
      "Peng Sun",
      "Xuanhe Zhou",
      "Qianglei Zang",
      "Ji Xu",
      "Tieying Zhang",
      "Guoliang Li",
      "Fan Wu"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01137",
    "title": "Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach",
    "abstract": "           To develop a domain-agnostic, semi-supervised anomaly detection framework that integrates deep reinforcement learning (DRL) to address challenges such as large-scale data, overfitting, and class imbalance, focusing on brain MRI volumes. This retrospective study used publicly available brain MRI datasets collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and 578 T2-weighted MRI volumes (from healthy subjects) for training, while the BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing (unhealthy subjects with Glioblastomas). Preprocessing included normalization, skull-stripping, and co-registering to a uniform voxel size. Experiments were conducted on both T1- and T2-weighted modalities. Additional experiments and ablation analyses were also carried out on the industrial datasets. The proposed method integrates DRL with feature representations to handle label scarcity, large-scale data and overfitting. Statistical analysis was based on several detection and segmentation metrics including AUROC and Dice score. The proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA) methods. On industrial surface datasets, the model also showed competitive performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset, indicating strong cross-domain generalization. Studies on anomaly sample size showed a monotonic increase in AUROC as more anomalies were seen, without evidence of overfitting or additional computational cost. The domain-agnostic semi-supervised approach using DRL shows significant promise for MRI anomaly detection, achieving strong performance on both medical and industrial datasets. Its robustness, generalizability and efficiency highlight its potential for real-world clinical applications.         ",
    "url": "https://arxiv.org/abs/2508.01137",
    "authors": [
      "Zeduo Zhang",
      "Yalda Mohsenzadeh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01148",
    "title": "DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging",
    "abstract": "           Model merging has emerged as an efficient and flexible paradigm for multi-task learning, with numerous methods being proposed in recent years. However, these state-of-the-art techniques are typically evaluated on benchmark suites that are highly favorable to model merging, and their robustness in more realistic settings remains largely unexplored. In this work, we first investigate the vulnerabilities of model-merging methods and pinpoint the source-model characteristics that critically underlie them. Specifically, we identify two factors that are particularly harmful to the merging process: (1) disparities in task vector norms, and (2) the low confidence of the source models. To address this issue, we propose DisTaC (Distillation for Task vector Conditioning), a novel method that pre-conditions these problematic task vectors before the merge. DisTaC leverages knowledge distillation to adjust a task vector's norm and increase source-model confidence while preserving its essential task-specific knowledge. Our extensive experiments demonstrate that by pre-conditioning task vectors with DisTaC, state-of-the-art merging techniques can successfully integrate models exhibiting the harmful traits -- where they would otherwise fail -- achieving significant performance gains.         ",
    "url": "https://arxiv.org/abs/2508.01148",
    "authors": [
      "Kotaro Yoshida",
      "Yuji Naraki",
      "Takafumi Horie",
      "Ryotaro Shimizu",
      "Hiroki Naganuma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01158",
    "title": "H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving",
    "abstract": "           Deep learning (DL) has shown state-of-the-art performance in trajectory prediction, which is critical to safe navigation in autonomous driving (AD). However, most DL-based methods suffer from catastrophic forgetting, where adapting to a new distribution may cause significant performance degradation in previously learned ones. Such inability to retain learned knowledge limits their applicability in the real world, where AD systems need to operate across varying scenarios with dynamic distributions. As revealed by neuroscience, the hippocampal circuit plays a crucial role in memory replay, effectively reconstructing learned knowledge based on limited resources. Inspired by this, we propose a hippocampal circuit-inspired continual learning method (H2C) for trajectory prediction across varying scenarios. H2C retains prior knowledge by selectively recalling a small subset of learned samples. First, two complementary strategies are developed to select the subset to represent learned knowledge. Specifically, one strategy maximizes inter-sample diversity to represent the distinctive knowledge, and the other estimates the overall knowledge by equiprobable sampling. Then, H2C updates via a memory replay loss function calculated by these selected samples to retain knowledge while learning new data. Experiments based on various scenarios from the INTERACTION dataset are designed to evaluate H2C. Experimental results show that H2C reduces catastrophic forgetting of DL baselines by 22.71% on average in a task-free manner, without relying on manually informed distributional shifts. The implementation is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01158",
    "authors": [
      "Yunlong Lin",
      "Zirui Li",
      "Guodong Du",
      "Xiaocong Zhao",
      "Cheng Gong",
      "Xinwei Wang",
      "Chao Lu",
      "Jianwei Gong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01168",
    "title": "Graph-based Interaction Augmentation Network for Robust Multimodal Sentiment Analysis",
    "abstract": "           The inevitable modality imperfection in real-world scenarios poses significant challenges for Multimodal Sentiment Analysis (MSA). While existing methods tailor reconstruction or joint representation learning strategies to restore missing semantics, they often overlook complex dependencies within and across modalities. Consequently, they fail to fully leverage available modalities to capture complementary semantics. To this end, this paper proposes a novel graph-based framework to exploit both intra- and inter-modality interactions, enabling imperfect samples to derive missing semantics from complementary parts for robust MSA. Specifically, we first devise a learnable hypergraph to model intra-modality temporal dependencies to exploit contextual information within each modality. Then, a directed graph is employed to explore inter-modality correlations based on attention mechanism, capturing complementary information across different modalities. Finally, the knowledge from perfect samples is integrated to supervise our interaction processes, guiding the model toward learning reliable and robust joint representations. Extensive experiments on MOSI and MOSEI datasets demonstrate the effectiveness of our method.         ",
    "url": "https://arxiv.org/abs/2508.01168",
    "authors": [
      "Hu Zhangfeng",
      "Shi mengxin"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2508.01171",
    "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
    "abstract": "           We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01171",
    "authors": [
      "Ranran Huang",
      "Krystian Mikolajczyk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01184",
    "title": "Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning",
    "abstract": "           A core problem of Embodied AI is to learn object manipulation from observation, as humans do. To achieve this, it is important to localize 3D object affordance areas through observation such as images (3D affordance grounding) and understand their functionalities (affordance classification). Previous attempts usually tackle these two tasks separately, leading to inconsistent predictions due to lacking proper modeling of their dependency. In addition, these methods typically only ground the incomplete affordance areas depicted in images, failing to predict the full potential affordance areas, and operate at a fixed scale, resulting in difficulty in coping with affordances significantly varying in scale with respect to the whole object. To address these issues, we propose a novel approach that learns an affordance-aware 3D representation and employs a stage-wise inference strategy leveraging the dependency between grounding and classification tasks. Specifically, we first develop a cross-modal 3D representation through efficient fusion and multi-scale geometric feature propagation, enabling inference of full potential affordance areas at a suitable regional scale. Moreover, we adopt a simple two-stage prediction mechanism, effectively coupling grounding and classification for better affordance understanding. Experiments demonstrate the effectiveness of our method, showing improved performance in both affordance grounding and classification.         ",
    "url": "https://arxiv.org/abs/2508.01184",
    "authors": [
      "Xinhang Wan",
      "Dongqiang Gou",
      "Xinwang Liu",
      "En Zhu",
      "Xuming He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01207",
    "title": "Showcasing standards and approaches for cybersecurity, safety, and privacy issues in connected and autonomous vehicles",
    "abstract": "           In the automotive industry there is a need to handle broad quality deficiencies, eg, performance, maintainability, cybersecurity, safety, and privacy, to mention a few. The idea is to prevent these issues from reaching end-users, ie, road users and inadvertently, pedestrians, aiming to potentially reduce accidents, and allow safe operation in dynamic attack surfaces, for the benefit of a host of stakeholders. This paper aims to bridge cybersecurity, safety, and privacy concerns in Connected and Autonomous Vehicles (CAV) with respect to Risk Assessment (RA) and Threat Modelling (TM) altogether. Practitioners know the vast literature on this topic given the sheer number of recommendations, standards, best practices, and existing approaches, at times impairing projects and fostering valuable and actionable threat analysis. In this paper we collate key outcomes by highlighting latest standards and approaches in RA and TM research to tackle complex attack surfaces as the ones posed by automotive settings. We aim to provide the community with a list of approaches to align expectations with stakeholders when deciding where and when to focus threat related analysis in automotive solutions.         ",
    "url": "https://arxiv.org/abs/2508.01207",
    "authors": [
      "Ricardo M. Czekster"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01208",
    "title": "Calibrated Prediction Set in Fault Detection with Risk Guarantees via Significance Tests",
    "abstract": "           Fault detection is crucial for ensuring the safety and reliability of modern industrial systems. However, a significant scientific challenge is the lack of rigorous risk control and reliable uncertainty quantification in existing diagnostic models, particularly when facing complex scenarios such as distributional shifts. To address this issue, this paper proposes a novel fault detection method that integrates significance testing with the conformal prediction framework to provide formal risk guarantees. The method transforms fault detection into a hypothesis testing task by defining a nonconformity measure based on model residuals. It then leverages a calibration dataset to compute p-values for new samples, which are used to construct prediction sets mathematically guaranteed to contain the true label with a user-specified probability, $1-\\alpha$. Fault classification is subsequently performed by analyzing the intersection of the constructed prediction set with predefined normal and fault label sets. Experimental results on cross-domain fault diagnosis tasks validate the theoretical properties of our approach. The proposed method consistently achieves an empirical coverage rate at or above the nominal level ($1-\\alpha$), demonstrating robustness even when the underlying point-prediction models perform poorly. Furthermore, the results reveal a controllable trade-off between the user-defined risk level ($\\alpha$) and efficiency, where higher risk tolerance leads to smaller average prediction set sizes. This research contributes a theoretically grounded framework for fault detection that enables explicit risk control, enhancing the trustworthiness of diagnostic systems in safety-critical applications and advancing the field from simple point predictions to informative, uncertainty-aware outputs.         ",
    "url": "https://arxiv.org/abs/2508.01208",
    "authors": [
      "Mingchen Mei",
      "Yi Li",
      "YiYao Qian",
      "Zijun Jia"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01209",
    "title": "Oldie but Goodie: Re-illuminating Label Propagation on Graphs with Partially Observed Features",
    "abstract": "           In real-world graphs, we often encounter missing feature situations where a few or the majority of node features, e.g., sensitive information, are missed. In such scenarios, directly utilizing Graph Neural Networks (GNNs) would yield sub-optimal results in downstream tasks such as node classification. Despite the emergence of a few GNN-based methods attempting to mitigate its missing situation, when only a few features are available, they rather perform worse than traditional structure-based models. To this end, we propose a novel framework that further illuminates the potential of classical Label Propagation (Oldie), taking advantage of Feature Propagation, especially when only a partial feature is available. Now called by GOODIE, it takes a hybrid approach to obtain embeddings from the Label Propagation branch and Feature Propagation branch. To do so, we first design a GNN-based decoder that enables the Label Propagation branch to output hidden embeddings that align with those of the FP branch. Then, GOODIE automatically captures the significance of structure and feature information thanks to the newly designed Structure-Feature Attention. Followed by a novel Pseudo-Label contrastive learning that differentiates the contribution of each positive pair within pseudo-labels originating from the LP branch, GOODIE outputs the final prediction for the unlabeled nodes. Through extensive experiments, we demonstrate that our proposed model, GOODIE, outperforms the existing state-of-the-art methods not only when only a few features are available but also in abundantly available situations. Source code of GOODIE is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01209",
    "authors": [
      "Sukwon Yun",
      "Xin Liu",
      "Yunhak Oh",
      "Junseok Lee",
      "Tianlong Chen",
      "Tsuyoshi Murata",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01219",
    "title": "Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis",
    "abstract": "           The remarkable success of Deep Neural Networks(DNN) is driven by gradient-based optimization, yet this process is often undermined by its tendency to produce disordered weight structures, which harms feature clarity and degrades learning dynamics. To address this fundamental representational flaw, we introduced the Eigen Neural Network (ENN), a novel architecture that reparameterizes each layer's weights in a layer-shared, learned orthonormal eigenbasis. This design enforces decorrelated, well-aligned weight dynamics axiomatically, rather than through regularization, leading to more structured and discriminative feature representations. When integrated with standard BP, ENN consistently outperforms state-of-the-art methods on large-scale image classification benchmarks, including ImageNet, and its superior representations generalize to set a new benchmark in cross-modal image-text retrieval. Furthermore, ENN's principled structure enables a highly efficient, backpropagation-free(BP-free) local learning variant, ENN-$\\ell$. This variant not only resolves BP's procedural bottlenecks to achieve over 2$\\times$ training speedup via parallelism, but also, remarkably, surpasses the accuracy of end-to-end backpropagation. ENN thus presents a new architectural paradigm that directly remedies the representational deficiencies of BP, leading to enhanced performance and enabling a more efficient, parallelizable training regime.         ",
    "url": "https://arxiv.org/abs/2508.01219",
    "authors": [
      "Anzhe Cheng",
      "Chenzhong Yin",
      "Mingxi Cheng",
      "Shukai Duan",
      "Shahin Nazarian",
      "Paul Bogdan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01223",
    "title": "ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference",
    "abstract": "           Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training by reconstructing forward activations during backpropagation, but suffer from high latency due to strictly sequential computation. To overcome this limitation, we propose ParaRevSNN, a parallel reversible SNN architecture that decouples sequential dependencies between reversible blocks while preserving reversibility. This design enables inter-block parallelism, significantly accelerating training and inference while retaining the memory-saving benefits of reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128 Gesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard RevSNNs, while reducing training time by up to 35.2\\% and inference time to 18.15\\%, making it well-suited for deployment in resource-constrained scenarios.         ",
    "url": "https://arxiv.org/abs/2508.01223",
    "authors": [
      "Changqing Xu",
      "Guoqing Sun",
      "Yi Liu",
      "Xinfang Liao",
      "Yintang Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01244",
    "title": "Effective and Efficient Conductance-based Community Search at Billion Scale",
    "abstract": "           Community search is a widely studied semi-supervised graph clustering problem, retrieving a high-quality connected subgraph containing the user-specified query vertex. However, existing methods primarily focus on cohesiveness within the community but ignore the sparsity outside the community, obtaining sub-par results. Inspired by this, we adopt the well-known conductance metric to measure the quality of a community and introduce a novel problem of conductance-based community search (CCS). CCS aims at finding a subgraph with the smallest conductance among all connected subgraphs that contain the query vertex. We prove that the CCS problem is NP-hard. To efficiently query CCS, a four-stage subgraph-conductance-based community search algorithm, SCCS, is proposed. Specifically, we first greatly reduce the entire graph using local sampling techniques. Then, a three-stage local optimization strategy is employed to continuously refine the community quality. Namely, we first utilize a seeding strategy to obtain an initial community to enhance its internal cohesiveness. Then, we iteratively add qualified vertices in the expansion stage to guarantee the internal cohesiveness and external sparsity of the community. Finally, we gradually remove unqualified vertices during the verification stage. Extensive experiments on real-world datasets containing one billion-scale graph and synthetic datasets show the effectiveness, efficiency, and scalability of our solutions.         ",
    "url": "https://arxiv.org/abs/2508.01244",
    "authors": [
      "Longlong Lin",
      "Yue He",
      "Wei Chen",
      "Pingpeng Yuan",
      "Rong-Hua Li",
      "Tao Jia"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2508.01248",
    "title": "NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection",
    "abstract": "           The rapid progress of generative models, such as GANs and diffusion models, has facilitated the creation of highly realistic images, raising growing concerns over their misuse in security-sensitive domains. While existing detectors perform well under known generative settings, they often fail to generalize to unknown generative models, especially when semantic content between real and fake images is closely aligned. In this paper, we revisit the use of CLIP features for AI-generated image detection and uncover a critical limitation: the high-level semantic information embedded in CLIP's visual features hinders effective discrimination. To address this, we propose NS-Net, a novel detection framework that leverages NULL-Space projection to decouple semantic information from CLIP's visual features, followed by contrastive learning to capture intrinsic distributional differences between real and generated images. Furthermore, we design a Patch Selection strategy to preserve fine-grained artifacts by mitigating semantic bias caused by global image structures. Extensive experiments on an open-world benchmark comprising images generated by 40 diverse generative models show that NS-Net outperforms existing state-of-the-art methods, achieving a 7.4\\% improvement in detection accuracy, thereby demonstrating strong generalization across both GAN- and diffusion-based image generation techniques.         ",
    "url": "https://arxiv.org/abs/2508.01248",
    "authors": [
      "Jiazhen Yan",
      "Fan Wang",
      "Weiwei Jiang",
      "Ziqiang Li",
      "Zhangjie Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01250",
    "title": "DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing",
    "abstract": "           Face parsing aims to segment facial images into key components such as eyes, lips, and eyebrows. While existing methods rely on dense pixel-level annotations, such annotations are expensive and labor-intensive to obtain. To reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a new task setting that performs dense facial component segmentation using only weak supervision, such as image-level labels and natural language descriptions. WSFP introduces unique challenges due to the high co-occurrence and visual similarity of facial components, which lead to ambiguous activations and degraded parsing performance. To address this, we propose DisFaceRep, a representation disentanglement framework designed to separate co-occurring facial components through both explicit and implicit mechanisms. Specifically, we introduce a co-occurring component disentanglement strategy to explicitly reduce dataset-level bias, and a text-guided component disentanglement loss to guide component separation using language supervision implicitly. Extensive experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of WSFP and the effectiveness of DisFaceRep, which significantly outperforms existing weakly supervised semantic segmentation methods. The code will be released at \\href{this https URL}{\\textcolor{cyan}{this https URL}}.         ",
    "url": "https://arxiv.org/abs/2508.01250",
    "authors": [
      "Xiaoqin Wang",
      "Xianxu Hou",
      "Meidan Ding",
      "Junliang Chen",
      "Kaijun Deng",
      "Jinheng Xie",
      "Linlin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01253",
    "title": "ODOV: Towards Open-Domain Open-Vocabulary Object Detection",
    "abstract": "           In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV) object detection, which considers the detection model's adaptability to the real world including both domain and category shifts. For this problem, we first construct a new benchmark OD-LVIS, which includes 46,949 images, covers 18 complex real-world domains and 1,203 categories, and provides a comprehensive dataset for evaluating real-world object detection. Besides, we develop a novel baseline method for ODOV this http URL proposed method first leverages large language models to generate the domain-agnostic text prompts for category embedding. It further learns the domain embedding from the given image, which, during testing, can be integrated into the category embedding to form the customized domain-specific category embedding for each test image. We provide sufficient benchmark evaluations for the proposed ODOV detection task and report the results, which verify the rationale of ODOV detection, the usefulness of our benchmark, and the superiority of the proposed method.         ",
    "url": "https://arxiv.org/abs/2508.01253",
    "authors": [
      "Yupeng Zhang",
      "Ruize Han",
      "Fangnan Zhou",
      "Song Wang",
      "Wei Feng",
      "Liang Wan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01257",
    "title": "PageRank Centrality in Directed Graphs with Bounded In-Degree",
    "abstract": "           We study the computational complexity of locally estimating a node's PageRank centrality in a directed graph $G$. For any node $t$, its PageRank centrality $\\pi(t)$ is defined as the probability that a random walk in $G$, starting from a uniformly chosen node, terminates at $t$, where each step terminates with a constant probability $\\alpha\\in(0,1)$. To obtain a multiplicative $\\big(1\\pm O(1)\\big)$-approximation of $\\pi(t)$ with probability $\\Omega(1)$, the previously best upper bound is $O(n^{1/2}\\min\\{ \\Delta_{in}^{1/2},\\Delta_{out}^{1/2},m^{1/4}\\})$ from [Wang, Wei, Wen, Yang STOC '24], where $n$ and $m$ denote the number of nodes and edges in $G$, and $\\Delta_{in}$ and $\\Delta_{out}$ upper bound the in-degrees and out-degrees of $G$, respectively. The same paper implicitly gives the previously best lower bound of $\\Omega(n^{1/2}\\min\\{\\Delta_{in}^{1/2}/n^{\\gamma},\\Delta_{out}^{1/2}/n^{\\gamma},m^{1/4}\\})$, where $\\gamma=\\frac{\\log(1/(1-\\alpha))}{4\\log\\Delta_{in}-2\\log(1/(1-\\alpha))}$ if $\\Delta_{in}>1/(1-\\alpha)$, and $\\gamma=1/2$ if $\\Delta_{in}\\le1/(1-\\alpha)$. As $\\gamma$ only depends on $\\Delta_{in}$, the known upper bound is tight if we only parameterize the complexity by $n$, $m$, and $\\Delta_{out}$. However, there remains a gap of $\\Omega(n^{\\gamma})$ when considering $\\Delta_{in}$, and this gap is large when $\\Delta_{in}$ is small. In the extreme case where $\\Delta_{in}\\le1/(1-\\alpha)$, we have $\\gamma=1/2$, leading to a gap of $\\Omega(n^{1/2})$ between the bounds $O(n^{1/2})$ and $\\Omega(1)$. In this paper, we present a new algorithm that achieves the above lower bound (up to logarithmic factors). The algorithm assumes that $n$ and the bounds $\\Delta_{in}$ and $\\Delta_{out}$ are known in advance. Our key technique is a novel randomized backwards propagation process which only propagates selectively based on Monte Carlo estimated PageRank scores.         ",
    "url": "https://arxiv.org/abs/2508.01257",
    "authors": [
      "Mikkel Thorup",
      "Hanzhi Wang",
      "Zhewei Wei",
      "Mingji Yang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2508.01259",
    "title": "SpatioTemporal Difference Network for Video Depth Super-Resolution",
    "abstract": "           Depth super-resolution has achieved impressive performance, and the incorporation of multi-frame information further enhances reconstruction quality. Nevertheless, statistical analyses reveal that video depth super-resolution remains affected by pronounced long-tailed distributions, with the long-tailed effects primarily manifesting in spatial non-smooth regions and temporal variation zones. To address these challenges, we propose a novel SpatioTemporal Difference Network (STDNet) comprising two core branches: a spatial difference branch and a temporal difference branch. In the spatial difference branch, we introduce a spatial difference mechanism to mitigate the long-tailed issues in spatial non-smooth regions. This mechanism dynamically aligns RGB features with learned spatial difference representations, enabling intra-frame RGB-D aggregation for depth calibration. In the temporal difference branch, we further design a temporal difference strategy that preferentially propagates temporal variation information from adjacent RGB and depth frames to the current depth frame, leveraging temporal difference representations to achieve precise motion compensation in temporal long-tailed areas. Extensive experimental results across multiple datasets demonstrate the effectiveness of our STDNet, outperforming existing approaches.         ",
    "url": "https://arxiv.org/abs/2508.01259",
    "authors": [
      "Zhengxue Wang",
      "Yuan Wu",
      "Xiang Li",
      "Zhiqiang Yan",
      "Jian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01268",
    "title": "Win-k: Improved Membership Inference Attacks on Small Language Models",
    "abstract": "           Small language models (SLMs) are increasingly valued for their efficiency and deployability in resource-constrained environments, making them useful for on-device, privacy-sensitive, and edge computing applications. On the other hand, membership inference attacks (MIAs), which aim to determine whether a given sample was used in a model's training, are an important threat with serious privacy and intellectual property implications. In this paper, we study MIAs on SLMs. Although MIAs were shown to be effective on large language models (LLMs), they are relatively less studied on emerging SLMs, and furthermore, their effectiveness decreases as models get smaller. Motivated by this finding, we propose a new MIA called win-k, which builds on top of a state-of-the-art attack (min-k). We experimentally evaluate win-k by comparing it with five existing MIAs using three datasets and eight SLMs. Results show that win-k outperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR metrics, especially on smaller models.         ",
    "url": "https://arxiv.org/abs/2508.01268",
    "authors": [
      "Roya Arkhmammadova",
      "Hosein Madadi Tamar",
      "M. Emre Gursoy"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01276",
    "title": "Defending Against Beta Poisoning Attacks in Machine Learning Models",
    "abstract": "           Poisoning attacks, in which an attacker adversarially manipulates the training dataset of a machine learning (ML) model, pose a significant threat to ML security. Beta Poisoning is a recently proposed poisoning attack that disrupts model accuracy by making the training dataset linearly nonseparable. In this paper, we propose four defense strategies against Beta Poisoning attacks: kNN Proximity-Based Defense (KPB), Neighborhood Class Comparison (NCC), Clustering-Based Defense (CBD), and Mean Distance Threshold (MDT). The defenses are based on our observations regarding the characteristics of poisoning samples generated by Beta Poisoning, e.g., poisoning samples have close proximity to one another, and they are centered near the mean of the target class. Experimental evaluations using MNIST and CIFAR-10 datasets demonstrate that KPB and MDT can achieve perfect accuracy and F1 scores, while CBD and NCC also provide strong defensive capabilities. Furthermore, by analyzing performance across varying parameters, we offer practical insights regarding defenses' behaviors under varying conditions.         ",
    "url": "https://arxiv.org/abs/2508.01276",
    "authors": [
      "Nilufer Gulciftci",
      "M. Emre Gursoy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01278",
    "title": "A graph neural network based on feature network for identifying influential nodes",
    "abstract": "           Identifying influential nodes in complex networks is of great importance, and has many applications in practice. For example, finding influential nodes in e-commerce network can provide merchants with customers with strong purchase intent; identifying influential nodes in computer information system can help locating the components that cause the system break down and identifying influential nodes in these networks can accelerate the flow of information in networks. Thus, a lot of efforts have been made on the problem of indentifying influential nodes. However, previous efforts either consider only one aspect of the network structure, or using global centralities with high time consuming as node features to identify influential nodes, and the existing methods do not consider the relationships between different centralities. To solve these problems, we propose a Graph Convolutional Network Framework based on Feature Network, abbreviated as FNGCN (graph convolutional network is abbreviated as GCN in the following text). Further, to exclude noises and reduce redundency, FNGCN utilizes feature network to represent the complicated relationships among the local centralities, based on which the most suitable local centralities are determined. By taking a shallow GCN and a deep GCN into the FNGCN framework, two FNGCNs are developed. With ground truth obtained from the widely used Susceptible Infected Recovered (SIR) model, the two FNGCNs are compared with the state-of-art methods on several real-world networks. Experimental results show that the two FNGCNs can identify the influential nodes more accurately than the compared methods, indicating that the proposed framework is effective in identifying influential nodes in complex networks.         ",
    "url": "https://arxiv.org/abs/2508.01278",
    "authors": [
      "Yanmei Hu",
      "Siyuan Yin",
      "Yihang Wu",
      "Xue Yue",
      "Yue Liu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01298",
    "title": "Improving performance of content-centric networks via decentralized coded caching for multi-level popularity and access",
    "abstract": "           Content-Centric Networking (CCN) offers a novel architectural paradigm that seeks to address the inherent limitations of the prevailing Internet Protocol (IP)-based networking model. In contrast to the host-centric communication approach of IP networks, CCN prioritizes content by enabling direct addressing and routing based on content identifiers. The potential performance improvements of CCN can be further amplified through optimized management of coded data storage and transmission strategies. Decentralized Coded Caching (DCC) emerges as a promising technique that harnesses the collective caching power of distributed network elements. By strategically pre-positioning frequently accessed content closer to potential consumers during periods of low network utilization, DCC has the potential to mitigate content transfer rates during peak traffic periods. This paper proposes a series of fundamental modifications to the CCN architecture by integrating DCC. The proposed framework incorporates differentiated coding strategies tailored to user access privileges, thereby eliminating the overhead associated with queue-based searching. Additionally, the framework facilitates recoding of uncoded data encountered along the content delivery path. These combined methodologies demonstrably enhance network throughput, elevate cache hit ratios, and consequently, reduce content delivery latency compared to conventional CCN implementations.         ",
    "url": "https://arxiv.org/abs/2508.01298",
    "authors": [
      "Azadeh Sadat Miraftab",
      "Ahmadreza Montazerolghaem",
      "Behrad Mahboobi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.01302",
    "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference",
    "abstract": "           Knowledge editing aims to modify outdated knowledge in large language models (LLMs) efficiently while retaining their powerful capabilities. Most existing methods rely on either parameter-level editing or retrieval-based approaches. In this work, we propose Knowledge Editing alignment with Diverse Augmentation and Self-adaptive inference (KEDAS) to better align LLMs with knowledge editing. In the alignment phase, LLMs learn to apply in-context edited knowledge via low-rank adaptation. During editing, we design a diverse edit augmentation technique to improve the recall of edits. After that, a self-adaptive post-alignment inference mechanism is proposed, in which a filter-based smart retriever is employed to perform a dynamic selection of inference routing. Specifically, irrelevant queries will go through the original pre-alignment model directly, while relevant ones, together with their related edits, go through the model with aligned adapters activated. In experiments, KEDAS secures the highest overall performance scores in 35 out of 36 cases across four datasets with three LLMs on three settings, surpassing its strong knowledge editing alignment counterpart by about 19.8 harmonic mean scores of edit success, locality and portability and outperforming both parameter editing and retrieval-based baselines significantly. Analysis of computational cost and performance on general tasks further validates the robustness and efficiency of KEDAS, indicating that it presents an ideal paradigm of knowledge editing alignment.         ",
    "url": "https://arxiv.org/abs/2508.01302",
    "authors": [
      "Chenming Tang",
      "Yutong Yang",
      "Yunfang Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01303",
    "title": "Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation",
    "abstract": "           State-of-the-art stereo matching (SM) models trained on synthetic data often fail to generalize to real data domains due to domain differences, such as color, illumination, contrast, and texture. To address this challenge, we leverage data augmentation to expand the training domain, encouraging the model to acquire robust cross-domain feature representations instead of domain-dependent shortcuts. This paper proposes an uncertainty-guided data augmentation (UgDA) method, which argues that the image statistics in RGB space (mean and standard deviation) carry the domain characteristics. Thus, samples in unseen domains can be generated by properly perturbing these statistics. Furthermore, to simulate more potential domains, Gaussian distributions founded on batch-level statistics are poposed to model the unceratinty of perturbation direction and intensity. Additionally, we further enforce feature consistency between original and augmented data for the same scene, encouraging the model to learn structure aware, shortcuts-invariant feature representations. Our approach is simple, architecture-agnostic, and can be integrated into any SM networks. Extensive experiments on several challenging benchmarks have demonstrated that our method can significantly improve the generalization performance of existing SM networks.         ",
    "url": "https://arxiv.org/abs/2508.01303",
    "authors": [
      "Shuangli Du",
      "Jing Wang",
      "Minghua Zhao",
      "Zhenyu Xu",
      "Jie Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01310",
    "title": "GraphVSSM: Graph Variational State-Space Model for Probabilistic Spatiotemporal Inference of Dynamic Exposure and Vulnerability for Regional Disaster Resilience Assessment",
    "abstract": "           Regional disaster resilience quantifies the changing nature of physical risks to inform policy instruments ranging from local immediate recovery to international sustainable development. While many existing state-of-practice methods have greatly advanced the dynamic mapping of exposure and hazard, our understanding of large-scale physical vulnerability has remained static, costly, limited, region-specific, coarse-grained, overly aggregated, and inadequately calibrated. With the significant growth in the availability of time-series satellite imagery and derived products for exposure and hazard, we focus our work on the equally important yet challenging element of the risk equation: physical vulnerability. We leverage machine learning methods that flexibly capture spatial contextual relationships, limited temporal observations, and uncertainty in a unified probabilistic spatiotemporal inference framework. We therefore introduce Graph Variational State-Space Model (GraphVSSM), a novel modular spatiotemporal approach that uniquely integrates graph deep learning, state-space modeling, and variational inference using time-series data and prior expert belief systems in a weakly supervised or coarse-to-fine-grained manner. We present three major results: a city-wide demonstration in Quezon City, Philippines; an investigation of sudden changes in the cyclone-impacted coastal Khurushkul community (Bangladesh) and mudslide-affected Freetown (Sierra Leone); and an open geospatial dataset, METEOR 2.5D, that spatiotemporally enhances the existing global static dataset for UN Least Developed Countries (2020). Beyond advancing regional disaster resilience assessment and improving our understanding global disaster risk reduction progress, our method also offers a probabilistic deep learning approach, contributing to broader urban studies that require compositional data analysis in weak supervision.         ",
    "url": "https://arxiv.org/abs/2508.01310",
    "authors": [
      "Joshua Dimasaka",
      "Christian Gei\u00df",
      "Emily So"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01311",
    "title": "C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor",
    "abstract": "           3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-precision industrial products. However, existing methods are typically trained in a class-specific manner and also lack the capability of learning from emerging classes. In this study, we proposed a continual learning framework named Continual 3D Anomaly Detection (C3D-AD), which can not only learn generalized representations for multi-class point clouds but also handle new classes emerging over this http URL, in the feature extraction module, to extract generalized local features from diverse product types of different tasks efficiently, Kernel Attention with random feature Layer (KAL) is introduced, which normalizes the feature space. Then, to reconstruct data correctly and continually, an efficient Kernel Attention with learnable Advisor (KAA) mechanism is proposed, which learns the information from new categories while discarding redundant old information within both the encoder and decoder. Finally, to keep the representation consistency over tasks, a Reconstruction with Parameter Perturbation (RPP) module is proposed by designing a representation rehearsal loss function, which ensures that the model remembers previous category information and returns category-adaptive this http URL experiments on three public datasets demonstrate the effectiveness of the proposed method, achieving an average performance of 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD, respectively.         ",
    "url": "https://arxiv.org/abs/2508.01311",
    "authors": [
      "Haoquan Lu",
      "Hanzhe Liang",
      "Jie Zhang",
      "Chenxi Hu",
      "Jinbao Wang",
      "Can Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01314",
    "title": "Physics-Informed Neural Network Approaches for Sparse Data Flow Reconstruction of Unsteady Flow Around Complex Geometries",
    "abstract": "           The utilization of Deep Neural Networks (DNNs) in physical science and engineering applications has gained traction due to their capacity to learn intricate functions. While large datasets are crucial for training DNN models in fields like computer vision and natural language processing, obtaining such datasets for engineering applications is prohibitively expensive. Physics-Informed Neural Networks (PINNs), a branch of Physics-Informed Machine Learning (PIML), tackle this challenge by embedding physical principles within neural network architectures. PINNs have been extensively explored for solving diverse forward and inverse problems in fluid mechanics. Nonetheless, there is limited research on employing PINNs for flow reconstruction from sparse data under constrained computational resources. Earlier studies were focused on forward problems with well-defined data. The present study attempts to develop models capable of reconstructing the flow field data from sparse datasets mirroring real-world scenarios. This study focuses on two cases: (a) two-dimensional (2D) unsteady laminar flow past a circular cylinder and (b) three-dimensional (3D) unsteady turbulent flow past an ultra-large container ship (ULCS). The first case compares the effectiveness of training methods like Standard PINN and Backward Compatible PINN (BC-PINN) and explores the performance enhancements through systematic relaxation of physics constraints and dynamic weighting of loss function components. The second case highlights the capability of PINN-based models to learn underlying physics from sparse data while accurately reconstructing the flow field for a highly turbulent flow.         ",
    "url": "https://arxiv.org/abs/2508.01314",
    "authors": [
      "Vamsi Sai Krishna Malineni",
      "Suresh Rajendran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2508.01331",
    "title": "Referring Remote Sensing Image Segmentation with Cross-view Semantics Interaction Network",
    "abstract": "           Recently, Referring Remote Sensing Image Segmentation (RRSIS) has aroused wide attention. To handle drastic scale variation of remote targets, existing methods only use the full image as input and nest the saliency-preferring techniques of cross-scale information interaction into traditional single-view structure. Although effective for visually salient targets, they still struggle in handling tiny, ambiguous ones in lots of real scenarios. In this work, we instead propose a paralleled yet unified segmentation framework Cross-view Semantics Interaction Network (CSINet) to solve the limitations. Motivated by human behavior in observing targets of interest, the network orchestrates visual cues from remote and close distances to conduct synergistic prediction. In its every encoding stage, a Cross-View Window-attention module (CVWin) is utilized to supplement global and local semantics into close-view and remote-view branch features, finally promoting the unified representation of feature in every encoding stage. In addition, we develop a Collaboratively Dilated Attention enhanced Decoder (CDAD) to mine the orientation property of target and meanwhile integrate cross-view multiscale features. The proposed network seamlessly enhances the exploitation of global and local semantics, achieving significant improvements over others while maintaining satisfactory speed.         ",
    "url": "https://arxiv.org/abs/2508.01331",
    "authors": [
      "Jiaxing Yang",
      "Lihe Zhang",
      "Huchuan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01343",
    "title": "UEChecker: Detecting Unchecked External Call Vulnerabilities in DApps via Graph Analysis",
    "abstract": "           The increasing number of attacks on the contract layer of DApps has resulted in economic losses amounting to $66 billion. Vulnerabilities arise when contracts interact with external protocols without verifying the results of the calls, leading to exploit entry points such as flash loan attacks and reentrancy attacks. In this paper, we propose UEChecker, a deep learning-based tool that utilizes a call graph and a Graph Convolutional Network to detect unchecked external call vulnerabilities. We design the following components: An edge prediction module that reconstructs the feature representation of nodes and edges in the call graph; A node aggregation module that captures structural information from both the node itself and its neighbors, thereby enhancing feature representation between nodes and improving the model's understanding of the global graph structure; A Conformer Block module that integrates multi-head attention, convolutional modules, and feedforward neural networks to more effectively capture dependencies of different scales within the call graph, extending beyond immediate neighbors and enhancing the performance of vulnerability detection. Finally, we combine these modules with Graph Convolutional Network to detect unchecked external call vulnerabilities. By auditing the smart contracts of 608 DApps, our results show that our tool achieves an accuracy of 87.59% in detecting unchecked external call vulnerabilities. Furthermore, we compare our tool with GAT, LSTM, and GCN baselines, and in the comparison experiments, UEChecker consistently outperforms these models in terms of accuracy.         ",
    "url": "https://arxiv.org/abs/2508.01343",
    "authors": [
      "Dechao Kong",
      "Xiaoqi Li",
      "Wenkai Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01357",
    "title": "HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection",
    "abstract": "           Code clone detection is a critical task in software engineering, aimed at identifying duplicated or similar code fragments within or across software systems. Traditional methods often fail to capture functional equivalence, particularly for semantic clones (Type 4), where code fragments implement identical functionality despite differing syntactic structures. Recent advances in large language models (LLMs) have shown promise in understanding code semantics. However, directly applying LLMs to code clone detection yields suboptimal results due to their sensitivity to syntactic differences. To address these challenges, we propose a novel two-stage framework that combines LLM-based screening with execution-based validation for detecting semantic clones in Python programs. In the first stage, an LLM evaluates code pairs to filter out obvious non-clones based on semantic analysis. For pairs not identified as clones, the second stage employs an execution-based validation approach, utilizing LLM-generated test inputs to assess functional equivalence through cross-execution validation. Our experimental evaluation demonstrates significant improvements in precision, recall, and F1-score compared to direct LLM-based detection, highlighting the framework's effectiveness in identifying semantic clones. Future work includes exploring cross-language clone detection and optimizing the framework for large-scale applications.         ",
    "url": "https://arxiv.org/abs/2508.01357",
    "authors": [
      "Yunhao Liang",
      "Ruixuan Ying",
      "Takuya Taniguchi",
      "Guwen Lyu",
      "Zhe Cui"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.01365",
    "title": "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models",
    "abstract": "           Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.         ",
    "url": "https://arxiv.org/abs/2508.01365",
    "authors": [
      "Zihan Wang",
      "Rui Zhang",
      "Hongwei Li",
      "Wenshu Fan",
      "Wenbo Jiang",
      "Qingchuan Zhao",
      "Guowen Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01368",
    "title": "Relation-Aware LNN-Transformer for Intersection-Centric Next-Step Prediction",
    "abstract": "           Next-step location prediction plays a pivotal role in modeling human mobility, underpinning applications from personalized navigation to strategic urban planning. However, approaches that assume a closed world - restricting choices to a predefined set of points of interest (POIs) - often fail to capture exploratory or target-agnostic behavior and the topological constraints of urban road networks. Hence, we introduce a road-node-centric framework that represents road-user trajectories on the city's road-intersection graph, thereby relaxing the closed-world constraint and supporting next-step forecasting beyond fixed POI sets. To encode environmental context, we introduce a sector-wise directional POI aggregation that produces compact features capturing distance, bearing, density and presence cues. By combining these cues with structural graph embeddings, we obtain semantically grounded node representations. For sequence modeling, we integrate a Relation-Aware LNN-Transformer - a hybrid of a Continuous-time Forgetting Cell CfC-LNN and a bearing-biased self-attention module - to capture both fine-grained temporal dynamics and long-range spatial dependencies. Evaluated on city-scale road-user trajectories, our model outperforms six state-of-the-art baselines by up to 17 percentage points in accuracy at one hop and 10 percentage points in MRR, and maintains high resilience under noise, losing only 2.4 percentage points in accuracy at one under 50 meter GPS perturbation and 8.9 percentage points in accuracy at one hop under 25 percent POI noise.         ",
    "url": "https://arxiv.org/abs/2508.01368",
    "authors": [
      "Zhehong Ren",
      "Tianluo Zhang",
      "Yiheng Lu",
      "Yushen Liang",
      "Promethee Spathis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01382",
    "title": "A Full-Stage Refined Proposal Algorithm for Suppressing False Positives in Two-Stage CNN-Based Detection Methods",
    "abstract": "           False positives in pedestrian detection remain a challenge that has yet to be effectively resolved. To address this issue, this paper proposes a Full-stage Refined Proposal (FRP) algorithm aimed at eliminating these false positives within a two-stage CNN-based pedestrian detection framework. The main innovation of this work lies in employing various pedestrian feature re-evaluation strategies to filter out low-quality pedestrian proposals during both the training and testing stages. Specifically, in the training phase, the Training mode FRP algorithm (TFRP) introduces a novel approach for validating pedestrian proposals to effectively guide the model training process, thereby constructing a model with strong capabilities for false positive suppression. During the inference phase, two innovative strategies are implemented: the Classifier-guided FRP (CFRP) algorithm integrates a pedestrian classifier into the proposal generation pipeline to yield high-quality proposals through pedestrian feature evaluation, and the Split-proposal FRP (SFRP) algorithm vertically divides all proposals, sending both the original and the sub-region proposals to the subsequent subnetwork to evaluate their confidence scores, filtering out those with lower sub-region pedestrian confidence scores. As a result, the proposed algorithm enhances the model's ability to suppress pedestrian false positives across all stages. Various experiments conducted on multiple benchmarks and the SY-Metro datasets demonstrate that the model, supported by different combinations of the FRP algorithm, can effectively eliminate false positives to varying extents. Furthermore, experiments conducted on embedded platforms underscore the algorithm's effectiveness in enhancing the comprehensive pedestrian detection capabilities of the small pedestrian detector in resource-constrained edge devices.         ",
    "url": "https://arxiv.org/abs/2508.01382",
    "authors": [
      "Qiang Guo",
      "Rubo Zhang",
      "Bingbing Zhang",
      "Junjie Liu",
      "Jianqing Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01385",
    "title": "Lightweight Backbone Networks Only Require Adaptive Lightweight Self-Attention Mechanisms",
    "abstract": "           Currently, lightweight hybrid backbone networks have partially alleviated the issue of computational saturation, but the imbalance in computational efficiencys between convolutional neural networks (CNNs) and attention mechanisms is becoming increasingly apparent. Specifically, although linear attention mechanisms and their variants have made progress in lightweight design, they still fail to meet the demands of hybrid models for long-sequence modeling. On the other hand, existing lightweight SoftMax attention computations typically reduce the feature map to a fixed size to decrease the number of sequences, thereby compressing the computational scale. However, the process of determining the feature map reduction ratio is cumbersome, and computational saturation issues still persist. To address this issue, this paper proposes a lightweight SoftMax attention mechanism with adaptive feature map sizes, named Fast Window Attention (FWA), which generates a small number of key sequences (Key and Value) through window aggregation for attention computation. Additionally, it explains the rationality of using ReLU to simulate SoftMax operations in lightweight global attention mechanisms. Finally, the paper designs a global-local feature fusion mechanism and combines it with GhostNet to propose a lightweight hybrid backbone network, LOLViT. Through visual tasks such as classification (ImageNet 1K), detection (COCO 2017), and segmentation (BDD100K), along with extensive ablation studies, it is demonstrated that LOLViT outperforms CNN models of the same level in both inference speed and model accuracy. Notably, the inference speed of LOLViT-X is 5x that of MobileViT-X.         ",
    "url": "https://arxiv.org/abs/2508.01385",
    "authors": [
      "Fengyun Li",
      "Chao Zheng",
      "Yangyang Fang",
      "Jialiang Lan",
      "Jianhua Liang",
      "Luhao Zhang",
      "Fa Si"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01386",
    "title": "Construction of Digital Terrain Maps from Multi-view Satellite Imagery using Neural Volume Rendering",
    "abstract": "           Digital terrain maps (DTMs) are an important part of planetary exploration, enabling operations such as terrain relative navigation during entry, descent, and landing for spacecraft and aiding in navigation on the ground. As robotic exploration missions become more ambitious, the need for high quality DTMs will only increase. However, producing DTMs via multi-view stereo pipelines for satellite imagery, the current state-of-the-art, can be cumbersome and require significant manual image preprocessing to produce satisfactory results. In this work, we seek to address these shortcomings by adapting neural volume rendering techniques to learn textured digital terrain maps directly from satellite imagery. Our method, neural terrain maps (NTM), only requires the locus for each image pixel and does not rely on depth or any other structural priors. We demonstrate our method on both synthetic and real satellite data from Earth and Mars encompassing scenes on the order of $100 \\textrm{km}^2$. We evaluate the accuracy of our output terrain maps by comparing with existing high-quality DTMs produced using traditional multi-view stereo pipelines. Our method shows promising results, with the precision of terrain prediction almost equal to the resolution of the satellite images even in the presence of imperfect camera intrinsics and extrinsics.         ",
    "url": "https://arxiv.org/abs/2508.01386",
    "authors": [
      "Josef X. Biberstein",
      "Guilherme Cavalheiro",
      "Juyeop Han",
      "Sertac Karaman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01396",
    "title": "Spatial-Frequency Aware for Object Detection in RAW Image",
    "abstract": "           Direct RAW-based object detection offers great promise by utilizing RAW data (unprocessed sensor data), but faces inherent challenges due to its wide dynamic range and linear response, which tends to suppress crucial object details. In particular, existing enhancement methods are almost all performed in the spatial domain, making it difficult to effectively recover these suppressed details from the skewed pixel distribution of RAW images. To address this limitation, we turn to the frequency domain, where features, such as object contours and textures, can be naturally separated based on frequency. In this paper, we propose Space-Frequency Aware RAW Image Object Detection Enhancer (SFAE), a novel framework that synergizes spatial and frequency representations. Our contribution is threefold. The first lies in the ``spatialization\" of frequency bands. Different from the traditional paradigm of directly manipulating abstract spectra in deep networks, our method inversely transforms individual frequency bands back into tangible spatial maps, thus preserving direct physical intuition. Then the cross-domain fusion attention module is developed to enable deep multimodal interactions between these maps and the original spatial features. Finally, the framework performs adaptive nonlinear adjustments by predicting and applying different gamma parameters for the two domains.         ",
    "url": "https://arxiv.org/abs/2508.01396",
    "authors": [
      "Zhuohua Ye",
      "Liming Zhang",
      "Hongru Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01402",
    "title": "ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models",
    "abstract": "           Advances in generative models have led to AI-generated images visually indistinguishable from authentic ones. Despite numerous studies on detecting AI-generated images with classifiers, a gap persists between such methods and human cognitive forensic analysis. We present ForenX, a novel method that not only identifies the authenticity of images but also provides explanations that resonate with human thoughts. ForenX employs the powerful multimodal large language models (MLLMs) to analyze and interpret forensic cues. Furthermore, we overcome the limitations of standard MLLMs in detecting forgeries by incorporating a specialized forensic prompt that directs the MLLMs attention to forgery-indicative attributes. This approach not only enhance the generalization of forgery detection but also empowers the MLLMs to provide explanations that are accurate, relevant, and comprehensive. Additionally, we introduce ForgReason, a dataset dedicated to descriptions of forgery evidences in AI-generated images. Curated through collaboration between an LLM-based agent and a team of human annotators, this process provides refined data that further enhances our model's performance. We demonstrate that even limited manual annotations significantly improve explanation quality. We evaluate the effectiveness of ForenX on two major benchmarks. The model's explainability is verified by comprehensive subjective evaluations.         ",
    "url": "https://arxiv.org/abs/2508.01402",
    "authors": [
      "Chuangchuang Tan",
      "Jinglu Wang",
      "Xiang Ming",
      "Renshuai Tao",
      "Yunchao Wei",
      "Yao Zhao",
      "Yan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01419",
    "title": "Cryptocurrency Price Forecasting Using Machine Learning: Building Intelligent Financial Prediction Models",
    "abstract": "           Cryptocurrency markets are experiencing rapid growth, but this expansion comes with significant challenges, particularly in predicting cryptocurrency prices for traders in the U.S. In this study, we explore how deep learning and machine learning models can be used to forecast the closing prices of the XRP/USDT trading pair. While many existing cryptocurrency prediction models focus solely on price and volume patterns, they often overlook market liquidity, a crucial factor in price predictability. To address this, we introduce two important liquidity proxy metrics: the Volume-To-Volatility Ratio (VVR) and the Volume-Weighted Average Price (VWAP). These metrics provide a clearer understanding of market stability and liquidity, ultimately enhancing the accuracy of our price predictions. We developed four machine learning models, Linear Regression, Random Forest, XGBoost, and LSTM neural networks, using historical data without incorporating the liquidity proxy metrics, and evaluated their performance. We then retrained the models, including the liquidity proxy metrics, and reassessed their performance. In both cases (with and without the liquidity proxies), the LSTM model consistently outperformed the others. These results underscore the importance of considering market liquidity when predicting cryptocurrency closing prices. Therefore, incorporating these liquidity metrics is essential for more accurate forecasting models. Our findings offer valuable insights for traders and developers seeking to create smarter and more risk-aware strategies in the U.S. digital assets market.         ",
    "url": "https://arxiv.org/abs/2508.01419",
    "authors": [
      "Md Zahidul Islam",
      "Md Shafiqur Rahman",
      "Md Sumsuzoha",
      "Babul Sarker",
      "Md Rafiqul Islam",
      "Mahfuz Alam",
      "Sanjib Kumar Shil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01422",
    "title": "AI-Driven Cybersecurity Threat Detection: Building Resilient Defense Systems Using Predictive Analytics",
    "abstract": "           This study examines how Artificial Intelligence can aid in identifying and mitigating cyber threats in the U.S. across four key areas: intrusion detection, malware classification, phishing detection, and insider threat analysis. Each of these problems has its quirks, meaning there needs to be different approaches to each, so we matched the models to the shape of the problem. For intrusion detection, catching things like unauthorized access, we tested unsupervised anomaly detection methods. Isolation forests and deep autoencoders both gave us useful signals by picking up odd patterns in network traffic. When it came to malware detection, we leaned on ensemble models like Random Forest and XGBoost, trained on features pulled from files and traffic logs. Phishing was more straightforward. We fed standard classifiers (logistic regression, Random Forest, XGBoost) a mix of email and web-based features. These models handled the task surprisingly well. Phishing turned out to be the easiest problem to crack, at least with the data we had. There was a different story. We utilized an LSTM autoencoder to identify behavioral anomalies in user activity logs. It caught every suspicious behavior but flagged a lot of harmless ones too. That kind of model makes sense when the cost of missing a threat is high and you are willing to sift through some noise. What we saw across the board is that performance was not about stacking the most complex model. What mattered was how well the models structure matched the way the data behaved. When signals were strong and obvious, simple models worked fine. But for messier, more subtle threats, we needed something more adaptive, sequence models and anomaly detectors, though they brought their trade offs. The takeaway here is clear in cybersecurity, context drives the solution.         ",
    "url": "https://arxiv.org/abs/2508.01422",
    "authors": [
      "Biswajit Chandra Das",
      "M Saif Sartaz",
      "Syed Ali Reza",
      "Arat Hossain",
      "Md Nasiruddin",
      "Kanchon Kumar Bishnu",
      "Kazi Sharmin Sultana",
      "Sadia Sharmeen Shatyi",
      "MD Azam Khan",
      "Joynal Abed"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01423",
    "title": "3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks",
    "abstract": "           RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint estimation, still suffer from scarce, expensive annotations and a thin augmentation toolbox, since most image transforms, including resize and rotation, disrupt geometric consistency. In this paper, we introduce 3DRot, a plug-and-play augmentation that rotates and mirrors images about the camera's optical center while synchronously updating RGB images, camera intrinsics, object poses, and 3D annotations to preserve projective geometry-achieving geometry-consistent rotations and reflections without relying on any scene depth. We validate 3DRot with a classical 3D task, monocular 3D detection. On SUN RGB-D dataset, 3DRot raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation error (ROT) from 22.91$^\\circ$ to 20.93$^\\circ$, and boosts $mAP_{0.5}$ from 35.70 to 38.11. As a comparison, Cube R-CNN adds 3 other datasets together with SUN RGB-D for monocular 3D estimation, with a similar mechanism and test dataset, increases $IoU_{3D}$ from 36.2 to 37.8, boosts $mAP_{0.5}$ from 34.7 to 35.4. Because it operates purely through camera-space transforms, 3DRot is readily transferable to other 3D tasks.         ",
    "url": "https://arxiv.org/abs/2508.01423",
    "authors": [
      "Shitian Yang",
      "Deyu Li",
      "Xiaoke Jiang",
      "Lei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.01427",
    "title": "Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification",
    "abstract": "           In this paper, we propose SPECTRUM, a temporal-frequency synergistic model that unlocks the untapped potential of multi-domain representation learning for online handwriting verification (OHV). SPECTRUM comprises three core components: (1) a multi-scale interactor that finely combines temporal and frequency features through dual-modal sequence interaction and multi-scale aggregation, (2) a self-gated fusion module that dynamically integrates global temporal and frequency features via self-driven balancing. These two components work synergistically to achieve micro-to-macro spectral-temporal integration. (3) A multi-domain distance-based verifier then utilizes both temporal and frequency representations to improve discrimination between genuine and forged handwriting, surpassing conventional temporal-only approaches. Extensive experiments demonstrate SPECTRUM's superior performance over existing OHV methods, underscoring the effectiveness of temporal-frequency multi-domain learning. Furthermore, we reveal that incorporating multiple handwritten biometrics fundamentally enhances the discriminative power of handwriting representations and facilitates verification. These findings not only validate the efficacy of multi-domain learning in OHV but also pave the way for future research in multi-domain approaches across both feature and biometric domains. Code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01427",
    "authors": [
      "Peirong Zhang",
      "Kai Ding",
      "Lianwen Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01442",
    "title": "Physically-based Lighting Augmentation for Robotic Manipulation",
    "abstract": "           Despite advances in data augmentation, policies trained via imitation learning still struggle to generalize across environmental variations such as lighting changes. To address this, we propose the first framework that leverages physically-based inverse rendering for lighting augmentation on real-world human demonstrations. Specifically, inverse rendering decomposes the first frame in each demonstration into geometric (surface normal, depth) and material (albedo, roughness, metallic) properties, which are then used to render appearance changes under different lighting. To ensure consistent augmentation across each demonstration, we fine-tune Stable Video Diffusion on robot execution videos for temporal lighting propagation. We evaluate our framework by measuring the structural and temporal consistency of the augmented sequences, and by assessing its effectiveness in reducing the behavior cloning generalization gap (40.1%) on a 7-DoF robot across 6 lighting conditions using 720 real-world evaluations. We further showcase three downstream applications enabled by the proposed framework.         ",
    "url": "https://arxiv.org/abs/2508.01442",
    "authors": [
      "Shutong Jin",
      "Lezhong Wang",
      "Ben Temming",
      "Florian T. Pokorny"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.01443",
    "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective",
    "abstract": "           There is a growing interest in leveraging large language models (LLMs) for automated code optimization. However, industrial platforms deploying multiple LLMs face a critical challenge: prompts optimized for one LLM often fail with others, requiring expensive model-specific prompt engineering. This cross-model prompt engineering bottleneck severely limits the practical deployment of multi-LLM optimization systems in production environments. To address this, we introduce Meta-Prompted Code Optimization (MPCO), a framework that automatically generates high-quality, task-specific prompts across diverse LLMs while maintaining industrial efficiency requirements. MPCO leverages meta-prompting to dynamically synthesize context-aware optimization prompts by integrating project metadata, task requirements, and LLM-specific contexts, and it seamlessly deploys on the ARTEMIS industrial platform for automated validation and scaling. Our comprehensive evaluation on five real-world codebases with 366 hours of runtime benchmarking demonstrates MPCO's effectiveness: it achieves overall performance improvements up to 19.06% with the best statistical rank across all systems compared to baseline methods. Analysis shows that 96% of the top-performing optimizations stem from meaningful edits. Through systematic ablation studies and meta-prompter sensitivity analysis, we identify that comprehensive context integration is essential for effective meta-prompting, and that all three major LLMs can serve effectively as meta-prompters, providing actionable insights for industrial practitioners.         ",
    "url": "https://arxiv.org/abs/2508.01443",
    "authors": [
      "Jingzhi Gong",
      "Rafail Giavrimis",
      "Paul Brookes",
      "Vardan Voskanyan",
      "Fan Wu",
      "Mari Ashiga",
      "Matthew Truscott",
      "Mike Basios",
      "Leslie Kanthan",
      "Jie Xu",
      "Zheng Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01445",
    "title": "Multi-Agent Inverse Learning for Sensor Networks: Identifying Coordination in UAV Networks",
    "abstract": "           Suppose there is an adversarial UAV network being tracked by a radar. How can the radar determine whether the UAVs are coordinating, in some well-defined sense? How can the radar infer the objectives of the individual UAVs and the network as a whole? We present an abstract interpretation of such a strategic interaction, allowing us to conceptualize coordination as a linearly constrained multi-objective optimization problem. Then, we present some tools from microeconomic theory that allow us to detect coordination and reconstruct individual UAV objective functions, from radar tracking signals. This corresponds to performing inverse multi-objective optimization. We present details for how the abstract microeconomic interpretation corresponds to, and naturally arises from, physical-layer radar waveform modulation and multi-target filtering. This article serves as a tutorial, bringing together concepts from several established research contributions in an expository style.         ",
    "url": "https://arxiv.org/abs/2508.01445",
    "authors": [
      "Luke Snow",
      "Vikram Krishnamurthy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.01453",
    "title": "Kernel-Based Sparse Additive Nonlinear Model Structure Detection through a Linearization Approach",
    "abstract": "           The choice of parameterization in Nonlinear (NL) system models greatly affects the quality of the estimated model. Overly complex models can be impractical and hard to interpret, necessitating data-driven methods for simpler and more accurate representations. In this paper, we propose a data-driven approach to simplify a class of continuous-time NL system models using linear approximations around varying operating points. Specifically, for sparse additive NL models, our method identifies the number of NL subterms and their corresponding input spaces. Under small-signal operation, we approximate the unknown NL system as a trajectory-scheduled Linear Parameter-Varying (LPV) system, with LPV coefficients representing the gradient of the NL function and indicating input sensitivity. Using this sensitivity measure, we determine the NL system's structure through LPV model reduction by identifying non-zero LPV coefficients and selecting scheduling parameters. We introduce two sparse estimators within a vector-valued Reproducing Kernel Hilbert Space (RKHS) framework to estimate the LPV coefficients while preserving their structural relationships. The structure of the sparse additive NL model is then determined by detecting non-zero elements in the gradient vector (LPV coefficients) and the Hessian matrix (Jacobian of the LPV coefficients). We propose two computationally tractable RKHS-based estimators for this purpose. The sparsified Hessian matrix reveals the NL model's structure, with numerical simulations confirming the approach's effectiveness.         ",
    "url": "https://arxiv.org/abs/2508.01453",
    "authors": [
      "Sadegh Ebrahimkhani",
      "John Lataire"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.01455",
    "title": "Regression Augmentation With Data-Driven Segmentation",
    "abstract": "           Imbalanced regression arises when the target distribution is skewed, causing models to focus on dense regions and struggle with underrepresented (minority) samples. Despite its relevance across many applications, few methods have been designed specifically for this challenge. Existing approaches often rely on fixed, ad hoc thresholds to label samples as rare or common, overlooking the continuous complexity of the joint feature-target space and fail to represent the true underlying rare regions. To address these limitations, we propose a fully data-driven GAN-based augmentation framework that uses Mahalanobis-Gaussian Mixture Modeling (GMM) to automatically identify minority samples and employs deterministic nearest-neighbour matching to enrich sparse regions. Rather than preset thresholds, our method lets the data determine which observations are truly rare. Evaluation on 32 benchmark imbalanced regression datasets demonstrates that our approach consistently outperforms state-of-the-art data augmentation methods.         ",
    "url": "https://arxiv.org/abs/2508.01455",
    "authors": [
      "Shayan Alahyari",
      "Shiva Mehdipour Ghobadlou",
      "Mike Domaratzki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01459",
    "title": "Fast and scalable retrosynthetic planning with a transformer neural network and speculative beam search",
    "abstract": "           AI-based computer-aided synthesis planning (CASP) systems are in demand as components of AI-driven drug discovery workflows. However, the high latency of such CASP systems limits their utility for high-throughput synthesizability screening in de novo drug design. We propose a method for accelerating multi-step synthesis planning systems that rely on SMILES-to-SMILES transformers as single-step retrosynthesis models. Our approach reduces the latency of SMILES-to-SMILES transformers powering multi-step synthesis planning in AiZynthFinder through speculative beam search combined with a scalable drafting strategy called Medusa. Replacing standard beam search with our approach allows the CASP system to solve 26\\% to 86\\% more molecules under the same time constraints of several seconds. Our method brings AI-based CASP systems closer to meeting the strict latency requirements of high-throughput synthesizability screening and improving general user experience.         ",
    "url": "https://arxiv.org/abs/2508.01459",
    "authors": [
      "Mikhail Andronov",
      "Natalia Andronova",
      "Michael Wand",
      "J\u00fcrgen Schmidhuber",
      "Djork-Arn\u00e9 Clevert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01460",
    "title": "Uncertainty-Aware Segmentation Quality Prediction via Deep Learning Bayesian Modeling: Comprehensive Evaluation and Interpretation on Skin Cancer and Liver Segmentation",
    "abstract": "           Image segmentation is a critical step in computational biomedical image analysis, typically evaluated using metrics like the Dice coefficient during training and validation. However, in clinical settings without manual annotations, assessing segmentation quality becomes challenging, and models lacking reliability indicators face adoption barriers. To address this gap, we propose a novel framework for predicting segmentation quality without requiring ground truth annotations during test time. Our approach introduces two complementary frameworks: one leveraging predicted segmentation and uncertainty maps, and another integrating the original input image, uncertainty maps, and predicted segmentation maps. We present Bayesian adaptations of two benchmark segmentation models-SwinUNet and Feature Pyramid Network with ResNet50-using Monte Carlo Dropout, Ensemble, and Test Time Augmentation to quantify uncertainty. We evaluate four uncertainty estimates: confidence map, entropy, mutual information, and expected pairwise Kullback-Leibler divergence on 2D skin lesion and 3D liver segmentation datasets, analyzing their correlation with segmentation quality metrics. Our framework achieves an R2 score of 93.25 and Pearson correlation of 96.58 on the HAM10000 dataset, outperforming previous segmentation quality assessment methods. For 3D liver segmentation, Test Time Augmentation with entropy achieves an R2 score of 85.03 and a Pearson correlation of 65.02, demonstrating cross-modality robustness. Additionally, we propose an aggregation strategy that combines multiple uncertainty estimates into a single score per image, offering a more robust and comprehensive assessment of segmentation quality. Finally, we use Grad-CAM and UMAP-based embedding analysis to interpret the model's behavior and reliability, highlighting the impact of uncertainty integration.         ",
    "url": "https://arxiv.org/abs/2508.01460",
    "authors": [
      "Sikha O K",
      "Meritxell Riera-Mar\u00edn",
      "Adrian Galdran",
      "Javier Garc\u00eda Lopez",
      "Julia Rodr\u00edguez-Comas",
      "Gemma Piella",
      "Miguel A. Gonz\u00e1lez Ballester"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01463",
    "title": "Extended Interface Physics-Informed Neural Networks Method for Moving Interface Problems",
    "abstract": "           Physics-Informed Neural Networks (PINNs) have emerged as a powerful class of mesh-free numerical methods for solving partial differential equations (PDEs), particularly those involving complex geometries. In this work, we present an innovative Extended Interface Physics-Informed Neural Network (XI-PINN) framework specifically designed to solve parabolic moving interface problems. The proposed approach incorporates a level set function to characterize the interface, which can be obtained either directly or through a neural network solution. We conduct a rigorous a priori error analysis for the XI-PINN method, providing error bounds for the approximation. Leveraging the Neural Tangent Kernel (NTK) theory, we further demonstrate that XI-PINN achieves a faster training convergence rate compared to conventional PINN approaches. The method's versatility is further demonstrated by its application to the Oseen equations. We perform comprehensive numerical experiments to validate the efficacy, accuracy, and robustness of the proposed framework.         ",
    "url": "https://arxiv.org/abs/2508.01463",
    "authors": [
      "Ran Bi",
      "Weibing Deng",
      "Yameng Zhu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.01473",
    "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs",
    "abstract": "           Recent advances in diffusion-based language models have opened new possibilities for controllable and bidirectional sequence generation. These models provide an alternative to traditional autoregressive approaches by framing text generation as an iterative denoising process. However, applying diffusion models to structured domains such as source code remains a significant challenge. Programming languages differ from natural language in that they follow strict syntactic and semantic rules, with hierarchical organization that must be preserved for correctness. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code. To address this limitation, we propose a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees. This enables the model to reconstruct programs in a way that respects grammatical boundaries and captures long-range dependencies. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks.         ",
    "url": "https://arxiv.org/abs/2508.01473",
    "authors": [
      "Yiming Zeng",
      "Jinghan Cao",
      "Zexin Li",
      "Yiming Chen",
      "Tao Ren",
      "Dawei Xiang",
      "Xidong Wu",
      "Shangqian Gao",
      "Tingting Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01480",
    "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach",
    "abstract": "           Biomedical text mining and question-answering are essential yet highly demanding tasks, particularly in the face of the exponential growth of biomedical literature. In this work, we present our participation in the 13th edition of the BioASQ challenge, which involves biomedical semantic question-answering for Task 13b and biomedical question-answering for developing topics for the Synergy task. We deploy a selection of open-source large language models (LLMs) as retrieval-augmented generators to answer biomedical questions. Various models are used to process the questions. A majority voting system combines their output to determine the final answer for Yes/No questions, while for list and factoid type questions, the union of their answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring all possible model combinations to contribute to the final answer, resulting in tailored LLM pipelines for each question type. Our findings provide valuable insight into which combinations of LLMs consistently produce superior results for specific question types. In the four rounds of the 2025 BioASQ challenge, our system achieved notable results: in the Synergy task, we secured 1st place for ideal answers and 2nd place for exact answers in round 2, as well as two shared 1st places for exact answers in round 3 and 4.         ",
    "url": "https://arxiv.org/abs/2508.01480",
    "authors": [
      "Dimitra Panou",
      "Alexandros C. Dimopoulos",
      "Manolis Koubarakis",
      "Martin Reczko"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01485",
    "title": "A Parallel Algorithm for Finding Robust Spanners in Large Social Networks",
    "abstract": "           Social networks, characterized by community structures, often rely on nodes called structural hole spanners to facilitate inter-community information dissemination. However, the dynamic nature of these networks, where spanner nodes may be removed, necessitates resilient methods to maintain inter-community communication. To this end, we introduce robust spanners (RS) as nodes uniquely equipped to bridge communities despite disruptions, such as node or edge removals. We propose a novel scoring technique to identify RS nodes and present a parallel algorithm with a CUDA implementation for efficient RS detection in large networks. Empirical analysis of real-world social networks reveals that high-scoring nodes exhibit a spanning capacity comparable to those identified by benchmark spanner detection algorithms while offering superior robustness. Our implementation on Nvidia GPUs achieves an average speedup of 244X over traditional spanner detection techniques, demonstrating its efficacy to identify RS in large social networks.         ",
    "url": "https://arxiv.org/abs/2508.01485",
    "authors": [
      "Arindam Khanda",
      "Satyaki Roy",
      "Prithwiraj Roy",
      "Sajal K. Das"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.01488",
    "title": "PESTO: Real-Time Pitch Estimation with Self-supervised Transposition-equivariant Objective",
    "abstract": "           In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTO's practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our model's low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications.         ",
    "url": "https://arxiv.org/abs/2508.01488",
    "authors": [
      "Alain Riou",
      "Bernardo Torres",
      "Ben Hayes",
      "Stefan Lattner",
      "Ga\u00ebtan Hadjeres",
      "Ga\u00ebl Richard",
      "Geoffroy Peeters"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.01493",
    "title": "Translation-Equivariant Self-Supervised Learning for Pitch Estimation with Optimal Transport",
    "abstract": "           In this paper, we propose an Optimal Transport objective for learning one-dimensional translation-equivariant systems and demonstrate its applicability to single pitch estimation. Our method provides a theoretically grounded, more numerically stable, and simpler alternative for training state-of-the-art self-supervised pitch estimators.         ",
    "url": "https://arxiv.org/abs/2508.01493",
    "authors": [
      "Bernardo Torres",
      "Alain Riou",
      "Ga\u00ebl Richard",
      "Geoffroy Peeters"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.01505",
    "title": "ESM: A Framework for Building Effective Surrogate Models for Hardware-Aware Neural Architecture Search",
    "abstract": "           Hardware-aware Neural Architecture Search (NAS) is one of the most promising techniques for designing efficient Deep Neural Networks (DNNs) for resource-constrained devices. Surrogate models play a crucial role in hardware-aware NAS as they enable efficient prediction of performance characteristics (e.g., inference latency and energy consumption) of different candidate models on the target hardware device. In this paper, we focus on building hardware-aware latency prediction models. We study different types of surrogate models and highlight their strengths and weaknesses. We perform a systematic analysis to understand the impact of different factors that can influence the prediction accuracy of these models, aiming to assess the importance of each stage involved in the model designing process and identify methods and policies necessary for designing/training an effective estimation model, specifically for GPU-powered devices. Based on the insights gained from the analysis, we present a holistic framework that enables reliable dataset generation and efficient model generation, considering the overall costs of different stages of the model generation pipeline.         ",
    "url": "https://arxiv.org/abs/2508.01505",
    "authors": [
      "Azaz-Ur-Rehman Nasir",
      "Samroz Ahmad Shoaib",
      "Muhammad Abdullah Hanif",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01519",
    "title": "The Vanishing Gradient Problem for Stiff Neural Differential Equations",
    "abstract": "           Gradient-based optimization of neural differential equations and other parameterized dynamical systems fundamentally relies on the ability to differentiate numerical solutions with respect to model parameters. In stiff systems, it has been observed that sensitivities to parameters controlling fast-decaying modes become vanishingly small during training, leading to optimization difficulties. In this paper, we show that this vanishing gradient phenomenon is not an artifact of any particular method, but a universal feature of all A-stable and L-stable stiff numerical integration schemes. We analyze the rational stability function for general stiff integration schemes and demonstrate that the relevant parameter sensitivities, governed by the derivative of the stability function, decay to zero for large stiffness. Explicit formulas for common stiff integration schemes are provided, which illustrate the mechanism in detail. Finally, we rigorously prove that the slowest possible rate of decay for the derivative of the stability function is $O(|z|^{-1})$, revealing a fundamental limitation: all A-stable time-stepping methods inevitably suppress parameter gradients in stiff regimes, posing a significant barrier for training and parameter identification in stiff neural ODEs.         ",
    "url": "https://arxiv.org/abs/2508.01519",
    "authors": [
      "Colby Fronk",
      "Linda Petzold"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.01523",
    "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification",
    "abstract": "           This paper presents a study of using large language models (LLMs) in modifying existing code. While LLMs for generating code have been widely studied, their role in code modification remains less understood. Although \"prompting\" serves as the primary interface for developers to communicate intents to LLMs, constructing effective prompts for code modification introduces challenges different from generation. Prior work suggests that natural language summaries may help scaffold this process, yet such approaches have been validated primarily in narrow domains like SQL rewriting. This study investigates two prompting strategies for LLM-assisted code modification: Direct Instruction Prompting, where developers describe changes explicitly in free-form language, and Summary-Mediated Prompting, where changes are made by editing the generated summaries of the code. We conducted an exploratory study with 15 developers who completed modification tasks using both techniques across multiple scenarios. Our findings suggest that developers followed an iterative workflow: understanding the code, localizing the edit, and validating outputs through execution or semantic reasoning. Each prompting strategy presented trade-offs: direct instruction prompting was more flexible and easier to specify, while summary-mediated prompting supported comprehension, prompt scaffolding, and control. Developers' choice of strategy was shaped by task goals and context, including urgency, maintainability, learning intent, and code familiarity. These findings highlight the need for more usable prompt interactions, including adjustable summary granularity, reliable summary-code traceability, and consistency in generated summaries.         ",
    "url": "https://arxiv.org/abs/2508.01523",
    "authors": [
      "Ningzhi Tang",
      "Emory Smith",
      "Yu Huang",
      "Collin McMillan",
      "Toby Jia-Jun Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.01525",
    "title": "MiraGe: Multimodal Discriminative Representation Learning for Generalizable AI-Generated Image Detection",
    "abstract": "           Recent advances in generative models have highlighted the need for robust detectors capable of distinguishing real images from AI-generated images. While existing methods perform well on known generators, their performance often declines when tested with newly emerging or unseen generative models due to overlapping feature embeddings that hinder accurate cross-generator classification. In this paper, we propose Multimodal Discriminative Representation Learning for Generalizable AI-generated Image Detection (MiraGe), a method designed to learn generator-invariant features. Motivated by theoretical insights on intra-class variation minimization and inter-class separation, MiraGe tightly aligns features within the same class while maximizing separation between classes, enhancing feature discriminability. Moreover, we apply multimodal prompt learning to further refine these principles into CLIP, leveraging text embeddings as semantic anchors for effective discriminative representation learning, thereby improving generalizability. Comprehensive experiments across multiple benchmarks show that MiraGe achieves state-of-the-art performance, maintaining robustness even against unseen generators like Sora.         ",
    "url": "https://arxiv.org/abs/2508.01525",
    "authors": [
      "Kuo Shi",
      "Jie Lu",
      "Shanshan Ye",
      "Guangquan Zhang",
      "Zhen Fang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01542",
    "title": "Leveraging Machine Learning for Botnet Attack Detection in Edge-Computing Assisted IoT Networks",
    "abstract": "           The increase of IoT devices, driven by advancements in hardware technologies, has led to widespread deployment in large-scale networks that process massive amounts of data daily. However, the reliance on Edge Computing to manage these devices has introduced significant security vulnerabilities, as attackers can compromise entire networks by targeting a single IoT device. In light of escalating cybersecurity threats, particularly botnet attacks, this paper investigates the application of machine learning techniques to enhance security in Edge-Computing-Assisted IoT environments. Specifically, it presents a comparative analysis of Random Forest, XGBoost, and LightGBM -- three advanced ensemble learning algorithms -- to address the dynamic and complex nature of botnet threats. Utilizing a widely recognized IoT network traffic dataset comprising benign and malicious instances, the models were trained, tested, and evaluated for their accuracy in detecting and classifying botnet activities. Furthermore, the study explores the feasibility of deploying these models in resource-constrained edge and IoT devices, demonstrating their practical applicability in real-world scenarios. The results highlight the potential of machine learning to fortify IoT networks against emerging cybersecurity challenges.         ",
    "url": "https://arxiv.org/abs/2508.01542",
    "authors": [
      "Dulana Rupanetti",
      "Naima Kaabouch"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01552",
    "title": "Social Media Information Operations",
    "abstract": "           The battlefield of information warfare has moved to online social networks, where influence campaigns operate at unprecedented speed and scale. As with any strategic domain, success requires understanding the terrain, modeling adversaries, and executing interventions. This tutorial introduces a formal optimization framework for social media information operations (IO), where the objective is to shape opinions through targeted actions. This framework is parameterized by quantities such as network structure, user opinions, and activity levels - all of which must be estimated or inferred from data. We discuss analytic tools that support this process, including centrality measures for identifying influential users, clustering algorithms for detecting community structure, and sentiment analysis for gauging public opinion. These tools either feed directly into the optimization pipeline or help defense analysts interpret the information environment. With the landscape mapped, we highlight threats such as coordinated bot networks, extremist recruitment, and viral misinformation. Countermeasures range from content-level interventions to mathematically optimized influence strategies. Finally, the emergence of generative AI transforms both offense and defense, democratizing persuasive capabilities while enabling scalable defenses. This shift calls for algorithmic innovation, policy reform, and ethical vigilance to protect the integrity of our digital public sphere.         ",
    "url": "https://arxiv.org/abs/2508.01552",
    "authors": [
      "Tauhid Zaman",
      "Yen-Shao Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.01554",
    "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models",
    "abstract": "           Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01554",
    "authors": [
      "Yujia Zheng",
      "Tianhao Li",
      "Haotian Huang",
      "Tianyu Zeng",
      "Jingyu Lu",
      "Chuangxin Chu",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Qian Xiong",
      "Yuyao Ge",
      "Mingyang Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01558",
    "title": "EvoVLMA: Evolutionary Vision-Language Model Adaptation",
    "abstract": "           Pre-trained Vision-Language Models (VLMs) have been exploited in various Computer Vision tasks (e.g., few-shot recognition) via model adaptation, such as prompt tuning and adapters. However, existing adaptation methods are designed by human experts, requiring significant time cost and experience. Inspired by recent advances in Large Language Models (LLMs) based code generation, we propose an Evolutionary Vision-Language Model Adaptation (EvoVLMA) method to automatically search training-free efficient adaptation algorithms for VLMs. We recognize feature selection and logits computation as the key functions in training-free VLM adaptation, and propose a two-stage LLM-assisted evolutionary algorithm for optimizing these parts in a sequential manner, effectively addressing the challenge posed by the expansive search space through a divide-and-conquer strategy. Besides, to enhance the stability and efficiency of searching process, we propose low-precision code conversion, web based code execution and process monitoring, leading to a highly effective automatic algorithm design system. Extensive experiments demonstrate that the algorithms found by EvoVLMA can obtain promising results compared to previous manually-designed ones. More specifically, in the 8-shot image classification setting, the classical APE algorithm can be improved by 1.91 points in recognition accuracy. This research opens new possibilities for automating the optimization of adaptation algorithms of pre-trained multimodal models. Code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2508.01558",
    "authors": [
      "Kun Ding",
      "Ying Wang",
      "Shiming Xiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01562",
    "title": "Adaptive LiDAR Scanning: Harnessing Temporal Cues for Efficient 3D Object Detection via Multi-Modal Fusion",
    "abstract": "           Multi-sensor fusion using LiDAR and RGB cameras significantly enhances 3D object detection task. However, conventional LiDAR sensors perform dense, stateless scans, ignoring the strong temporal continuity in real-world scenes. This leads to substantial sensing redundancy and excessive power consumption, limiting their practicality on resource-constrained platforms. To address this inefficiency, we propose a predictive, history-aware adaptive scanning framework that anticipates informative regions of interest (ROI) based on past observations. Our approach introduces a lightweight predictor network that distills historical spatial and temporal contexts into refined query embeddings. These embeddings guide a differentiable Mask Generator network, which leverages Gumbel-Softmax sampling to produce binary masks identifying critical ROIs for the upcoming frame. Our method significantly reduces unnecessary data acquisition by concentrating dense LiDAR scanning only within these ROIs and sparsely sampling elsewhere. Experiments on nuScenes and Lyft benchmarks demonstrate that our adaptive scanning strategy reduces LiDAR energy consumption by over 65% while maintaining competitive or even superior 3D object detection performance compared to traditional LiDAR-camera fusion methods with dense LiDAR scanning.         ",
    "url": "https://arxiv.org/abs/2508.01562",
    "authors": [
      "Sara Shoouri",
      "Morteza Tavakoli Taba",
      "Hun-Seok Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01580",
    "title": "Dynamic Clustering for Personalized Federated Learning on Heterogeneous Edge Devices",
    "abstract": "           Federated Learning (FL) enables edge devices to collaboratively learn a global model, but it may not perform well when clients have high data heterogeneity. In this paper, we propose a dynamic clustering algorithm for personalized federated learning system (DC-PFL) to address the problem of data heterogeneity. DC-PFL starts with all clients training a global model and gradually groups the clients into smaller clusters for model personalization based on their data similarities. To address the challenge of estimating data heterogeneity without exposing raw data, we introduce a discrepancy metric called model discrepancy, which approximates data heterogeneity solely based on the model weights received by the server. We demonstrate that model discrepancy is strongly and positively correlated with data heterogeneity and can serve as a reliable indicator of data heterogeneity. To determine when and how to change grouping structures, we propose an algorithm based on the rapid decrease period of the training loss curve. Moreover, we propose a layer-wise aggregation mechanism that aggregates the low-discrepancy layers at a lower frequency to reduce the amount of transmitted data and communication costs. We conduct extensive experiments on various datasets to evaluate our proposed algorithm, and our results show that DC-PFL significantly reduces total training time and improves model accuracy compared to baselines.         ",
    "url": "https://arxiv.org/abs/2508.01580",
    "authors": [
      "Heting Liu",
      "Junzhe Huang",
      "Fang He",
      "Guohong Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01585",
    "title": "A Spatio-temporal Continuous Network for Stochastic 3D Human Motion Prediction",
    "abstract": "           Stochastic Human Motion Prediction (HMP) has received increasing attention due to its wide applications. Despite the rapid progress in generative fields, existing methods often face challenges in learning continuous temporal dynamics and predicting stochastic motion sequences. They tend to overlook the flexibility inherent in complex human motions and are prone to mode collapse. To alleviate these issues, we propose a novel method called STCN, for stochastic and continuous human motion prediction, which consists of two stages. Specifically, in the first stage, we propose a spatio-temporal continuous network to generate smoother human motion sequences. In addition, the anchor set is innovatively introduced into the stochastic HMP task to prevent mode collapse, which refers to the potential human motion patterns. In the second stage, STCN endeavors to acquire the Gaussian mixture distribution (GMM) of observed motion sequences with the aid of the anchor set. It also focuses on the probability associated with each anchor, and employs the strategy of sampling multiple sequences from each anchor to alleviate intra-class differences in human motions. Experimental results on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.         ",
    "url": "https://arxiv.org/abs/2508.01585",
    "authors": [
      "Hua Yu",
      "Yaqing Hou",
      "Xu Gui",
      "Shanshan Feng",
      "Dongsheng Zhou",
      "Qiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01586",
    "title": "Diffusion Models for Future Networks and Communications: A Comprehensive Survey",
    "abstract": "           The rise of Generative AI (GenAI) in recent years has catalyzed transformative advances in wireless communications and networks. Among the members of the GenAI family, Diffusion Models (DMs) have risen to prominence as a powerful option, capable of handling complex, high-dimensional data distribution, as well as consistent, noise-robust performance. In this survey, we aim to provide a comprehensive overview of the theoretical foundations and practical applications of DMs across future communication systems. We first provide an extensive tutorial of DMs and demonstrate how they can be applied to enhance optimizers, reinforcement learning and incentive mechanisms, which are popular approaches for problems in wireless networks. Then, we review and discuss the DM-based methods proposed for emerging issues in future networks and communications, including channel modeling and estimation, signal detection and data reconstruction, integrated sensing and communication, resource management in edge computing networks, semantic communications and other notable issues. We conclude the survey with highlighting technical limitations of DMs and their applications, as well as discussing future research directions.         ",
    "url": "https://arxiv.org/abs/2508.01586",
    "authors": [
      "Nguyen Cong Luong",
      "Nguyen Duc Hai",
      "Duc Van Le",
      "Huy T. Nguyen",
      "Thai-Hoc Vu",
      "Thien Huynh-The",
      "Ruichen Zhang",
      "Nguyen Duc Duy Anh",
      "Dusit Niyato",
      "Marco Di Renzo",
      "Dong In Kim",
      "Quoc-Viet Pham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.01591",
    "title": "Self-Navigated Residual Mamba for Universal Industrial Anomaly Detection",
    "abstract": "           In this paper, we propose Self-Navigated Residual Mamba (SNARM), a novel framework for universal industrial anomaly detection that leverages ``self-referential learning'' within test images to enhance anomaly discrimination. Unlike conventional methods that depend solely on pre-trained features from normal training data, SNARM dynamically refines anomaly detection by iteratively comparing test patches against adaptively selected in-image references. Specifically, we first compute the ``inter-residuals'' features by contrasting test image patches with the training feature bank. Patches exhibiting small-norm residuals (indicating high normality) are then utilized as self-generated reference patches to compute ``intra-residuals'', amplifying discriminative signals. These inter- and intra-residual features are concatenated and fed into a novel Mamba module with multiple heads, which are dynamically navigated by residual properties to focus on anomalous regions. Finally, AD results are obtained by aggregating the outputs of a self-navigated Mamba in an ensemble learning paradigm. Extensive experiments on MVTec AD, MVTec 3D, and VisA benchmarks demonstrate that SNARM achieves state-of-the-art (SOTA) performance, with notable improvements in all metrics, including Image-AUROC, Pixel-AURC, PRO, and AP.         ",
    "url": "https://arxiv.org/abs/2508.01591",
    "authors": [
      "Hanxi Li",
      "Jingqi Wu",
      "Lin Yuanbo Wu",
      "Mingliang Li",
      "Deyin Liu",
      "Jialie Shen",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01595",
    "title": "BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator",
    "abstract": "           Although existing backdoor defenses have gained success in mitigating backdoor attacks, they still face substantial challenges. In particular, most of them rely on large amounts of clean data to weaken the backdoor mapping but generally struggle with residual trigger effects, resulting in persistently high attack success rates (ASR). Therefore, in this paper, we propose a novel Backdoor defense method based on Directional mapping module and adversarial Knowledge Distillation (BeDKD), which balances the trade-off between defense effectiveness and model performance using a small amount of clean and poisoned data. We first introduce a directional mapping module to identify poisoned data, which destroys clean mapping while keeping backdoor mapping on a small set of flipped clean data. Then, the adversarial knowledge distillation is designed to reinforce clean mapping and suppress backdoor mapping through a cycle iteration mechanism between trust and punish distillations using clean and identified poisoned data. We conduct experiments to mitigate mainstream attacks on three datasets, and experimental results demonstrate that BeDKD surpasses the state-of-the-art defenses and reduces the ASR by 98% without significantly reducing the CACC. Our code are available in this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01595",
    "authors": [
      "Zhengxian Wu",
      "Juan Wen",
      "Wanli Peng",
      "Yinghan Zhou",
      "Changtong dou",
      "Yiming Xue"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01598",
    "title": "Drift-aware Collaborative Assistance Mixture of Experts for Heterogeneous Multistream Learning",
    "abstract": "           Learning from multiple data streams in real-world scenarios is fundamentally challenging due to intrinsic heterogeneity and unpredictable concept drifts. Existing methods typically assume homogeneous streams and employ static architectures with indiscriminate knowledge fusion, limiting generalizability in complex dynamic environments. To tackle this gap, we propose CAMEL, a dynamic \\textbf{C}ollaborative \\textbf{A}ssistance \\textbf{M}ixture of \\textbf{E}xperts \\textbf{L}earning framework. It addresses heterogeneity by assigning each stream an independent system with a dedicated feature extractor and task-specific head. Meanwhile, a dynamic pool of specialized private experts captures stream-specific idiosyncratic patterns. Crucially, collaboration across these heterogeneous streams is enabled by a dedicated assistance expert. This expert employs a multi-head attention mechanism to distill and integrate relevant context autonomously from all other concurrent streams. It facilitates targeted knowledge transfer while inherently mitigating negative transfer from irrelevant sources. Furthermore, we propose an Autonomous Expert Tuner (AET) strategy, which dynamically manages expert lifecycles in response to drift. It instantiates new experts for emerging concepts (freezing prior ones to prevent catastrophic forgetting) and prunes obsolete ones. This expert-level plasticity provides a robust and efficient mechanism for online model capacity adaptation. Extensive experiments demonstrate CAMEL's superior generalizability across diverse multistreams and exceptional resilience against complex concept drifts.         ",
    "url": "https://arxiv.org/abs/2508.01598",
    "authors": [
      "En Yu",
      "Jie Lu",
      "Kun Wang",
      "Xiaoyu Yang",
      "Guangquan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01603",
    "title": "Towards Generalizable AI-Generated Image Detection via Image-Adaptive Prompt Learning",
    "abstract": "           A major struggle for AI-generated image detection is identifying fake images from unseen generators. Existing cutting-edge methods typically customize pre-trained foundation models to this task via partial-parameter fine-tuning. However, these parameters trained on a narrow range of generators may fail to generalize to unknown sources. In light of this, we propose a novel framework named Image-Adaptive Prompt Learning (IAPL), which enhances flexibility in processing diverse testing images. It consists of two adaptive modules, i.e., the Conditional Information Learner and the Confidence-Driven Adaptive Prediction. The former employs CNN-based feature extractors to learn forgery-specific and image-specific conditions, which are then propagated to learnable tokens via a gated mechanism. The latter optimizes the shallowest learnable tokens based on a single test sample and selects the cropped view with the highest prediction confidence for final detection. These two modules enable the prompts fed into the foundation model to be automatically adjusted based on the input image, rather than being fixed after training, thereby enhancing the model's adaptability to various forged images. Extensive experiments show that IAPL achieves state-of-the-art performance, with 95.61% and 96.7% mean accuracy on two widely used UniversalFakeDetect and GenImage datasets, respectively.         ",
    "url": "https://arxiv.org/abs/2508.01603",
    "authors": [
      "Yiheng Li",
      "Zichang Tan",
      "Zhen Lei",
      "Xu Zhou",
      "Yang Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01605",
    "title": "Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models",
    "abstract": "           Text-to-image diffusion models (T2I DMs) have achieved remarkable success in generating high-quality and diverse images from text prompts, yet recent studies have revealed their vulnerability to backdoor attacks. Existing attack methods suffer from critical limitations: 1) they rely on unnatural adversarial prompts that lack human readability and require massive poisoned data; 2) their effectiveness is typically restricted to specific models, lacking generalizability; and 3) they can be mitigated by recent backdoor defenses. To overcome these challenges, we propose a novel backdoor attack framework that achieves three key properties: 1) \\emph{Practicality}: Our attack requires only a few stealthy backdoor samples to generate arbitrary attacker-chosen target images, as well as ensuring high-quality image generation in benign scenarios. 2) \\emph{Generalizability:} The attack is applicable across multiple T2I DMs without requiring model-specific redesign. 3) \\emph{Robustness:} The attack remains effective against existing backdoor defenses and adaptive defenses. Our extensive experimental results on multiple T2I DMs demonstrate that with only 10 carefully crafted backdoored samples, our attack method achieves $>$90\\% attack success rate with negligible degradation in benign image generation quality. We also conduct human evaluation to validate our attack effectiveness. Furthermore, recent backdoor detection and mitigation methods, as well as adaptive defense tailored to our attack are not sufficiently effective, highlighting the pressing need for more robust defense mechanisms against the proposed attack.         ",
    "url": "https://arxiv.org/abs/2508.01605",
    "authors": [
      "Haoran Dai",
      "Jiawen Wang",
      "Ruo Yang",
      "Manali Sharma",
      "Zhonghao Liao",
      "Yuan Hong",
      "Binghui Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01635",
    "title": "Learning Unified System Representations for Microservice Tail Latency Prediction",
    "abstract": "           Microservice architectures have become the de facto standard for building scalable cloud-native applications, yet their distributed nature introduces significant challenges in performance monitoring and resource management. Traditional approaches often rely on per-request latency metrics, which are highly sensitive to transient noise and fail to reflect the holistic behavior of complex, concurrent workloads. In contrast, window-level P95 tail latency provides a stable and meaningful signal that captures both system-wide trends and user-perceived performance degradation. We identify two key shortcomings in existing methods: (i) inadequate handling of heterogeneous data, where traffic-side features propagate across service dependencies and resource-side signals reflect localized bottlenecks, and (ii) the lack of principled architectural designs that effectively distinguish and integrate these complementary modalities. To address these challenges, we propose USRFNet, a deep learning network that explicitly separates and models traffic-side and resource-side features. USRFNet employs GNNs to capture service interactions and request propagation patterns, while gMLP modules independently model cluster resource dynamics. These representations are then fused into a unified system embedding to predict window-level P95 latency with high accuracy. We evaluate USRFNet on real-world microservice benchmarks under large-scale stress testing conditions, demonstrating substantial improvements in prediction accuracy over state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2508.01635",
    "authors": [
      "Wenzhuo Qian",
      "Hailiang Zhao",
      "Tianlv Chen",
      "Jiayi Chen",
      "Ziqi Wang",
      "Kingsum Chow",
      "Shuiguang Deng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2508.01641",
    "title": "Minimal High-Resolution Patches Are Sufficient for Whole Slide Image Representation via Cascaded Dual-Scale Reconstruction",
    "abstract": "           Whole-slide image (WSI) analysis remains challenging due to the gigapixel scale and sparsely distributed diagnostic regions. Multiple Instance Learning (MIL) mitigates this by modeling the WSI as bags of patches for slide-level prediction. However, most MIL approaches emphasize aggregator design while overlooking the impact of the feature extractor of the feature extraction stage, which is often pretrained on natural images. This leads to domain gap and suboptimal representations. Self-supervised learning (SSL) has shown promise in bridging domain gap via pretext tasks, but it still primarily builds upon generic backbones, thus requiring WSIs to be split into small patches. This inevitably splits histological structures and generates both redundant and interdependent patches, which in turn degrades aggregator performance and drastically increases training costs. To address this challenge, we propose a Cascaded Dual-Scale Reconstruction (CDSR) framework, demonstrating that only an average of 9 high-resolution patches per WSI are sufficient for robust slide-level representation. CDSR employs a two-stage selective sampling strategy that identifies the most informative representative regions from both model-based and semantic perspectives. These patches are then fed into a Local-to-Global Network, which reconstructs spatially coherent high-resolution WSI representations by integrating fine-grained local detail with global contextual information. Unlike existing dense-sampling or SSL pipelines, CDSR is optimized for efficiency and morphological fidelity. Experiments on Camelyon16, TCGA-NSCLC, and TCGA-RCC demonstrate that CDSR achieves improvements of 6.3% in accuracy and 5.5% in area under ROC curve on downstream classification tasks with only 7,070 (4.5% of total) high-resolution patches per dataset on average, outperforming state-of-the-art methods trained on over 10,000,000 patches.         ",
    "url": "https://arxiv.org/abs/2508.01641",
    "authors": [
      "Yujian Liu",
      "Yuechuan Lin",
      "Dongxu Shen",
      "Haoran Li",
      "Yutong Wang",
      "Xiaoli Liu",
      "Shidang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01646",
    "title": "SPARTA: Advancing Sparse Attention in Spiking Neural Networks via Spike-Timing-Based Prioritization",
    "abstract": "           Current Spiking Neural Networks (SNNs) underutilize the temporal dynamics inherent in spike-based processing, relying primarily on rate coding while overlooking precise timing information that provides rich computational cues. We propose SPARTA (Spiking Priority Attention with Resource-Adaptive Temporal Allocation), a framework that leverages heterogeneous neuron dynamics and spike-timing information to enable efficient sparse attention. SPARTA prioritizes tokens based on temporal cues, including firing patterns, spike timing, and inter-spike intervals, achieving 65.4% sparsity through competitive gating. By selecting only the most salient tokens, SPARTA reduces attention complexity from O(N^2) to O(K^2) with k << n, while maintaining high accuracy. Our method achieves state-of-the-art performance on DVS-Gesture (98.78%) and competitive results on CIFAR10-DVS (83.06%) and CIFAR-10 (95.3%), demonstrating that exploiting spike timing dynamics improves both computational efficiency and accuracy.         ",
    "url": "https://arxiv.org/abs/2508.01646",
    "authors": [
      "Minsuk Jang",
      "Changick Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.01647",
    "title": "DUP: Detection-guided Unlearning for Backdoor Purification in Language Models",
    "abstract": "           As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01647",
    "authors": [
      "Man Hu",
      "Yahui Ding",
      "Yatao Yang",
      "Liangyu Chen",
      "Yanhao Jia",
      "Shuai Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01655",
    "title": "JSidentify-V2: Leveraging Dynamic Memory Fingerprinting for Mini-Game Plagiarism Detection",
    "abstract": "           The explosive growth of mini-game platforms has led to widespread code plagiarism, where malicious users access popular games' source code and republish them with modifications. While existing static analysis tools can detect simple obfuscation techniques like variable renaming and dead code injection, they fail against sophisticated deep obfuscation methods such as encrypted code with local or cloud-based decryption keys that completely destroy code structure and render traditional Abstract Syntax Tree analysis ineffective. To address these challenges, we present JSidentify-V2, a novel dynamic analysis framework that detects mini-game plagiarism by capturing memory invariants during program execution. Our key insight is that while obfuscation can severely distort static code characteristics, runtime memory behavior patterns remain relatively stable. JSidentify-V2 employs a four-stage pipeline: (1) static pre-analysis and instrumentation to identify potential memory invariants, (2) adaptive hot object slicing to maximize execution coverage of critical code segments, (3) Memory Dependency Graph construction to represent behavioral fingerprints resilient to obfuscation, and (4) graph-based similarity analysis for plagiarism detection. We evaluate JSidentify-V2 against eight obfuscation methods on a comprehensive dataset of 1,200 mini-games ...         ",
    "url": "https://arxiv.org/abs/2508.01655",
    "authors": [
      "Zhihao Li",
      "Chaozheng Wang",
      "Zongjie Li",
      "Xinyong Peng",
      "Qun Xia",
      "Haochuan Lu",
      "Ting Xiong",
      "Shuzheng Gao",
      "Cuiyun Gao",
      "Shuai Wang",
      "Yuetang Deng",
      "Huafeng Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.01666",
    "title": "A Randomized GMsFEM with Data-Driven Predictors for Parametric Flow Problems in Multiscale Heterogeneous Media",
    "abstract": "           In this paper, we propose a randomized generalized multiscale finite element method (Randomized GMsFEM) for flow problems with parameterized inputs and high-contrast heterogeneous media. The method employs a data-driven predictor to construct multiscale basis functions in two stages: offline and online. In the offline stage, a snapshot space is generated via spectral decompositions, and a reduced matrix is obtained using SVD to predict eigenfunctions. In the online stage, these eigenfunctions are evaluated for new parameter realizations to construct the multiscale space. Furthermore, our approach addresses the complexity of multiple permeability fields with random inputs and multiple multiscale information, providing accurate and efficient approximations. Moreover, we conduct a rigorous convergence analysis for our Randomized GMsFEM. Finally, we present extensive numerical examples, demonstrating its superior performance compared to the traditional GMsFEM.         ",
    "url": "https://arxiv.org/abs/2508.01666",
    "authors": [
      "Wing Tat Leung",
      "Qiuqi Li",
      "Songwei Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.01675",
    "title": "Asynchronous Federated Learning with non-convex client objective functions and heterogeneous dataset",
    "abstract": "           Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, traditional FL suffers from communication overhead, system heterogeneity, and straggler effects. Asynchronous Federated Learning (AFL) addresses these by allowing clients to update independently, improving scalability and reducing synchronization delays. This paper extends AFL to handle non-convex objective functions and heterogeneous datasets, common in modern deep learning. We present a rigorous convergence analysis, deriving bounds on the expected gradient norm and studying the effects of staleness, variance, and heterogeneity. To mitigate stale updates, we introduce a staleness aware aggregation that prioritizes fresher updates and a dynamic learning rate schedule that adapts to client staleness and heterogeneity, improving stability and convergence. Our framework accommodates variations in computational power, data distribution, and communication delays, making it practical for real world applications. We also analyze the impact of client selection strategies-sampling with or without replacement-on variance and convergence. Implemented in PyTorch with Python's asyncio, our approach is validated through experiments demonstrating improved performance and scalability for asynchronous, heterogeneous, and non-convex FL scenarios.         ",
    "url": "https://arxiv.org/abs/2508.01675",
    "authors": [
      "Ali Forootani",
      "Raffaele Iervolino"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.01676",
    "title": "Benchmarking Adversarial Patch Selection and Location",
    "abstract": "           Adversarial patch attacks threaten the reliability of modern vision models. We present PatchMap, the first spatially exhaustive benchmark of patch placement, built by evaluating over 1.5e8 forward passes on ImageNet validation images. PatchMap reveals systematic hot-spots where small patches (as little as 2% of the image) induce confident misclassifications and large drops in model confidence. To demonstrate its utility, we propose a simple segmentation guided placement heuristic that leverages off the shelf masks to identify vulnerable regions without any gradient queries. Across five architectures-including adversarially trained ResNet50, our method boosts attack success rates by 8 to 13 percentage points compared to random or fixed placements. We publicly release PatchMap and the code implementation. The full PatchMap bench (6.5B predictions, multiple backbones) will be released soon to further accelerate research on location-aware defenses and adaptive attacks.         ",
    "url": "https://arxiv.org/abs/2508.01676",
    "authors": [
      "Shai Kimhi",
      "Avi Mendlson",
      "Moshe Kimhi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01678",
    "title": "Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models",
    "abstract": "           Vision-Language Models (VLMs) often suffer from hallucination, partly due to challenges in aligning multimodal information. We propose Prompt-in-Image, a simple method that embeds textual instructions directly into images. This removes the need for separate text inputs and forces the model to process all content through the visual channel. We evaluate this method on three popular open-source VLMs: Qwen2.5-VL, LLaVA-1.5, and InstructBLIP. The results reveal sharp differences. Prompt-in-Image improves Qwen2.5-VL's performance, increasing POPE accuracy by 4.1 percent (from 80.2 percent to 84.3 percent) and also reducing hallucination rates on MS-COCO. In contrast, LLaVA-1.5 and InstructBLIP experience a severe performance drop, with accuracy falling from around 84 percent to near-random levels. Through detailed analysis, we found that CLIP-based encoders in LLaVA and InstructBLIP exhibit excessive attention bias toward embedded text regions, disrupting visual understanding. In contrast, Qwen's vision encoder handles text-embedded images robustly. Crucially, Prompt-in-Image reduces Qwen's modality gap, enhancing cross-modal alignment by unifying information processing through a single modality.         ",
    "url": "https://arxiv.org/abs/2508.01678",
    "authors": [
      "Zhaochen Wang",
      "Yiwei Wang",
      "Yujun Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01702",
    "title": "Explicit Function-Correcting Code Constructions for Lee Metric Channels",
    "abstract": "           Function-Correcting Codes (FCCs) are a novel class of codes designed to protect function evaluations of messages against errors while minimizing redundancy. A theoretical framework for systematic FCCs to channels matched to the Lee metric has been studied recently, which introduced function-correcting Lee codes (FCLCs) and also derived upper and lower bounds on their optimal redundancy. In this paper, we propose a Plotkin-like bound for irregular Lee-distance codes, which improves an existing bound. We construct explicit FCLCs for specific classes of functions, including the Lee weight, Lee weight distribution, modular sum, and locally bounded function. For these functions, lower bounds on redundancy are obtained, and our constructions are shown to be optimal in certain cases. Finally, a comparative analysis with classical Lee error-correcting codes and codes correcting errors in function values demonstrates that FCLCs can significantly reduce redundancy while preserving function correctness.         ",
    "url": "https://arxiv.org/abs/2508.01702",
    "authors": [
      "Hareesh K.",
      "Rashid Ummer N.T.",
      "B. Sundar Rajan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2508.01712",
    "title": "HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection",
    "abstract": "           Detecting hate speech in videos remains challenging due to the complexity of multimodal content and the lack of fine-grained annotations in existing datasets. We present HateClipSeg, a large-scale multimodal dataset with both video-level and segment-level annotations, comprising over 11,714 segments labeled as Normal or across five Offensive categories: Hateful, Insulting, Sexual, Violence, Self-Harm, along with explicit target victim labels. Our three-stage annotation process yields high inter-annotator agreement (Krippendorff's alpha = 0.817). We propose three tasks to benchmark performance: (1) Trimmed Hateful Video Classification, (2) Temporal Hateful Video Localization, and (3) Online Hateful Video Classification. Results highlight substantial gaps in current models, emphasizing the need for more sophisticated multimodal and temporally aware approaches. The HateClipSeg dataset are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01712",
    "authors": [
      "Han Wang",
      "Zhuoran Wang",
      "Roy Ka-Wei Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01714",
    "title": "A Provably Secure Network Protocol for Private Communication with Analysis and Tracing Resistance",
    "abstract": "           Anonymous communication networks have emerged as crucial tools for obfuscating communication pathways and concealing user identities. However, their practical deployments face significant challenges, including susceptibility to artificial intelligence (AI)-powered metadata analysis, difficulties in decentralized architectures, and the absence of provable security guarantees. To address these issues, this paper proposes a novel decentralized anonymous routing protocol with resistance to tracing and traffic analysis. The protocol eliminates dependencies on the threshold model and trusted third-party setups, ensuring indistinguishable identity privacy even in highly adversarial environments. Different from traditional empirical security analysis of anonymous networks, this paper rigorously proves indistinguishable identity privacy for users even in extremely adversarial environments. Furthermore, simulations confirm its practical feasibility, demonstrating both security and efficiency. By achieving information sharing with privacy preservation, the proposed protocol offers a provably secure solution for privacy-preserving communication in digital environments.         ",
    "url": "https://arxiv.org/abs/2508.01714",
    "authors": [
      "Chao Ge",
      "Wei Yuan",
      "Ge Chen",
      "Yanbin Pan",
      "Yuan Shen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.01718",
    "title": "Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach",
    "abstract": "           We propose a physics-informed neural network policy iteration (PINN-PI) framework for solving stochastic optimal control problems governed by second-order Hamilton--Jacobi--Bellman (HJB) equations. At each iteration, a neural network is trained to approximate the value function by minimizing the residual of a linear PDE induced by a fixed policy. This linear structure enables systematic $L^2$ error control at each policy evaluation step, and allows us to derive explicit Lipschitz-type bounds that quantify how value gradient errors propagate to the policy updates. This interpretability provides a theoretical basis for evaluating policy quality during training. Our method extends recent deterministic PINN-based approaches to stochastic settings, inheriting the global exponential convergence guarantees of classical policy iteration under mild conditions. We demonstrate the effectiveness of our method on several benchmark problems, including stochastic cartpole, pendulum problems and high-dimensional linear quadratic regulation (LQR) problems in up to 10D.         ",
    "url": "https://arxiv.org/abs/2508.01718",
    "authors": [
      "Yeongjong Kim",
      "Yeoneung Kim",
      "Minseok Kim",
      "Namkyeong Cho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.01730",
    "title": "Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos",
    "abstract": "           Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.         ",
    "url": "https://arxiv.org/abs/2508.01730",
    "authors": [
      "Jianbo Ma",
      "Hui Luo",
      "Qi Chen",
      "Yuankai Qi",
      "Yumei Sun",
      "Amin Beheshti",
      "Jianlin Zhang",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01733",
    "title": "Topolow: Force-Directed Euclidean Embedding of Dissimilarity Data with Robustness Against Non-Metricity and Sparsity",
    "abstract": "           The problem of embedding a set of objects into a low-dimensional Euclidean space based on a matrix of pairwise dissimilarities is fundamental in data analysis, machine learning, and statistics. However, the assumptions of many standard analytical methods are violated when the input dissimilarities fail to satisfy metric or Euclidean axioms. We present the mathematical and statistical foundations of Topolow, a physics-inspired, gradient-free optimization framework for such embedding problems. Topolow is conceptually related to force-directed graph drawing algorithms but is fundamentally distinguished by its goal of quantitative metric reconstruction. It models objects as particles in a physical system, and its novel optimization scheme proceeds through sequential, stochastic pairwise interactions, which circumvents the need to compute a global gradient and provides robustness against convergence to local optima, especially for sparse data. Topolow maximizes the likelihood under a Laplace error model, robust to outliers and heterogeneous errors, and properly handles censored data. Crucially, Topolow does not require the input dissimilarities to be metric, making it a robust solution for embedding non-metric measurements into a valid Euclidean space, thereby enabling the use of standard analytical tools. We demonstrate the superior performance of Topolow compared to standard Multidimensional Scaling (MDS) methods in reconstructing the geometry of sparse and non-Euclidean data. This paper formalizes the algorithm, first introduced as Topolow in the context of antigenic mapping in (Arhami and Rohani, 2025) (open access), with emphasis on its metric embedding and mathematical properties for a broader audience. The general-purpose function Euclidify is available in the R package topolow.         ",
    "url": "https://arxiv.org/abs/2508.01733",
    "authors": [
      "Omid Arhami",
      "Pejman Rohani"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.01754",
    "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography",
    "abstract": "           The field of AI-generated text detection has evolved from supervised classification to zero-shot statistical analysis. However, current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. Our empirical analysis reveals that AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8\\% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies. On the RAID benchmark, TDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More importantly, TDT demonstrates robust performance on adversarial tasks, with 14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with only 13\\% computational overhead. Our work establishes non-stationarity as a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection.         ",
    "url": "https://arxiv.org/abs/2508.01754",
    "authors": [
      "Alva West",
      "Yixuan Weng",
      "Minjun Zhu",
      "Luodan Zhang",
      "Zhen Lin",
      "Guangsheng Bao",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01758",
    "title": "Causality and Decision-making: A Logical Framework for Systems and Security Modelling",
    "abstract": "           Causal reasoning is essential for understanding decision-making about the behaviour of complex `ecosystems' of systems that underpin modern society, with security -- including issues around correctness, safety, resilience, etc. -- typically providing critical examples. We present a theory of strategic reasoning about system modelling based on minimal structural assumptions and employing the methods of transition systems, supported by a modal logic of system states in the tradition of van Benthem, Hennessy, and Milner, and validated through equivalence theorems. Our framework introduces an intervention operator and a separating conjunction to capture actual causal relationships between component systems of the ecosystem, aligning naturally with Halpern and Pearl's counterfactual approach based on Structural Causal Models. We illustrate the applicability through examples of of decision-making about microservices in distributed systems. We discuss localized decision-making through a separating conjunction. This work unifies a formal, minimalistic notion of system behaviour with a Halpern--Pearl-compatible theory of counterfactual reasoning, providing a logical foundation for studying decision making about causality in complex interacting systems.         ",
    "url": "https://arxiv.org/abs/2508.01758",
    "authors": [
      "Pinaki Chakraborty",
      "Tristan Caulfield",
      "David Pym"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.01805",
    "title": "M3LLM: Model Context Protocol-aided Mixture of Vision Experts For Multimodal LLMs in Networks",
    "abstract": "           Current Multimodal Large Language Models (MLLMs) rely on centralized architectures and often suffer from poor alignment between the input task and their fixed visual encoding modules, which limits performance on diverse and dynamic visual tasks. With the increasing deployment of resource-efficient models on edge devices in wireless networks, a new opportunity emerges to dynamically use distributed vision experts for improved MLLM inference quality. To enable this, we propose M3LLM, where the Model Context Protocol (MCP) coordinates a mixture of vision experts to achieve distributed MLLMs. Specifically, MCP is an open protocol that structures the input task context into interpretable representations, enabling wireless network-aware coordination between the central model backbone and edge-hosted vision experts. Based on the MCP representation, M3LLM formulates vision expert routing as a joint optimization problem that balances task-expert semantic compatibility and channel performance. To solve the resulting gradient conflicts, we develop a dual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and introduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical Bayesian modeling to ensure effective routing. Experiments show that M3LLM improves task accuracy, reduces communication cost, and enhances expert routing adaptability under dynamic wireless network conditions.         ",
    "url": "https://arxiv.org/abs/2508.01805",
    "authors": [
      "Yongjie Zeng",
      "Hongyang Du"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.01815",
    "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy",
    "abstract": "           Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning.         ",
    "url": "https://arxiv.org/abs/2508.01815",
    "authors": [
      "Yang Zhao",
      "Chengxiao Dai",
      "Wei Zhuo",
      "Tan Chuan Fu",
      "Yue Xiu",
      "Dusit Niyato",
      "Jonathan Z. Low",
      "Eugene Ho Hong Zhuang",
      "Daren Zong Loong Tan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01833",
    "title": "Neural Predictive Control to Coordinate Discrete- and Continuous-Time Models for Time-Series Analysis with Control-Theoretical Improvements",
    "abstract": "           Deep sequence models have achieved notable success in time-series analysis, such as interpolation and forecasting. Recent advances move beyond discrete-time architectures like Recurrent Neural Networks (RNNs) toward continuous-time formulations such as the family of Neural Ordinary Differential Equations (Neural ODEs). Generally, they have shown that capturing the underlying dynamics is beneficial for generic tasks like interpolation, extrapolation, and classification. However, existing methods approximate the dynamics using unconstrained neural networks, which struggle to adapt reliably under distributional shifts. In this paper, we recast time-series problems as the continuous ODE-based optimal control problem. Rather than learning dynamics solely from data, we optimize control actions that steer ODE trajectories toward task objectives, bringing control-theoretical performance guarantees. To achieve this goal, we need to (1) design the appropriate control actions and (2) apply effective optimal control algorithms. As the actions should contain rich context information, we propose to employ the discrete-time model to process past sequences and generate actions, leading to a coordinate model to extract long-term temporal features to modulate short-term continuous dynamics. During training, we apply model predictive control to plan multi-step future trajectories, minimize a task-specific cost, and greedily select the optimal current action. We show that, under mild assumptions, this multi-horizon optimization leads to exponential convergence to infinite-horizon solutions, indicating that the coordinate model can gain robust and generalizable performance. Extensive experiments on diverse time-series datasets validate our method's superior generalization and adaptability compared to state-of-the-art baselines.         ",
    "url": "https://arxiv.org/abs/2508.01833",
    "authors": [
      "Haoran Li",
      "Muhao Guo",
      "Yang Weng",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01840",
    "title": "Implementing Neural Networks Over-the-Air via Reconfigurable Intelligent Surfaces",
    "abstract": "           In this paper, we investigate reconfigurable intelligent surface (RIS)-aided multiple-input-multiple-output (MIMO) OAC systems designed to emulate the fully-connected (FC) layer of a neural network (NN) via analog OAC, where the RIS and the transceivers are jointly adjusted to engineer the ambient wireless propagation environment to emulate the weights of the target FC layer. We refer to this novel computational paradigm as AirFC. We first study the case in which the precoder, combiner, and RIS phase shift matrices are jointly optimized to minimize the mismatch between the OAC system and the target FC layer. To solve this non-convex optimization problem, we propose a low-complexity alternating optimization algorithm, where semi-closed-form/closed-form solutions for all optimization variables are derived. Next, we consider training of the system parameters using two distinct learning strategies, namely centralized training and distributed training. In the centralized training approach, training is performed at either the transmitter or the receiver, whichever possesses the channel state information (CSI), and the trained parameters are provided to the other terminal. In the distributed training approach, the transmitter and receiver iteratively update their parameters through back and forth transmissions by leveraging channel reciprocity, thereby avoiding CSI acquisition and significantly reducing computational complexity. Subsequently, we extend our analysis to a multi-RIS scenario by exploiting its spatial diversity gain to enhance the system performance. Simulation results show that the AirFC system realized by the RIS-aided MIMO configuration achieves satisfactory classification accuracy.         ",
    "url": "https://arxiv.org/abs/2508.01840",
    "authors": [
      "Meng Hua",
      "Chenghong Bian",
      "Haotian Wu",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.01842",
    "title": "OmniEvent: Unified Event Representation Learning",
    "abstract": "           Event cameras have gained increasing popularity in computer vision due to their ultra-high dynamic range and temporal resolution. However, event networks heavily rely on task-specific designs due to the unstructured data distribution and spatial-temporal (S-T) inhomogeneity, making it hard to reuse existing architectures for new tasks. We propose OmniEvent, the first unified event representation learning framework that achieves SOTA performance across diverse tasks, fully removing the need of task-specific designs. Unlike previous methods that treat event data as 3D point clouds with manually tuned S-T scaling weights, OmniEvent proposes a decouple-enhance-fuse paradigm, where the local feature aggregation and enhancement is done independently on the spatial and temporal domains to avoid inhomogeneity issues. Space-filling curves are applied to enable large receptive fields while improving memory and compute efficiency. The features from individual domains are then fused by attention to learn S-T interactions. The output of OmniEvent is a grid-shaped tensor, which enables standard vision models to process event data without architecture change. With a unified framework and similar hyper-parameters, OmniEvent out-performs (tasks-specific) SOTA by up to 68.2% across 3 representative tasks and 10 datasets (Fig.1). Code will be ready in this https URL .         ",
    "url": "https://arxiv.org/abs/2508.01842",
    "authors": [
      "Weiqi Yan",
      "Chenlu Lin",
      "Youbiao Wang",
      "Zhipeng Cai",
      "Xiuhong Lin",
      "Yangyang Shi",
      "Weiquan Liu",
      "Yu Zang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01844",
    "title": "CloudAnoAgent: Anomaly Detection for Cloud Sites via LLM Agent with Neuro-Symbolic Mechanism",
    "abstract": "           Anomaly detection in cloud sites remains a critical yet challenging task. Existing approaches that rely solely on metric data often suffer from high false positive rates (FPR) due to data imbalance between normal and anomalous events, leading to significant operational overhead for system reliance engineers. Recent advances in large language models (LLMs) offer new opportunities for integrating metrics with log data, enabling more accurate and interpretable anomaly detection. In this paper, we propose CloudAnoAgent, the first neuro-symbolic LLM-based agent for anomaly detection in cloud environments. CloudAnoAgent jointly processes structured metrics and textual log data in a unified pipeline, leveraging symbolic verification to validate detection hypotheses and generate structured anomaly reports. To support systematic evaluation, we introduce CloudAnoBench, the first benchmark that provides LLM-generated paired metrics and log data with fine-grained anomaly behavior annotations, filling a critical gap in existing datasets. Experimental results demonstrate that CloudAnoAgent improves anomaly classification accuracy by 46.36% and 36.67% on average and reduces the FPR by 36.67% and 33.89% on average over traditional baselines and LLM-only baseline, with a boost on anomaly type detection accuracy by 12.8% compared to vanilla LLM prompting. These results demonstrate the strengths of our approach in improving detection accuracy, reducing false positives, and enhancing interpretability, thereby supporting practical deployment in enterprise cloud environments.         ",
    "url": "https://arxiv.org/abs/2508.01844",
    "authors": [
      "Xinkai Zou",
      "Xuan Jiang",
      "Ruikai Huang",
      "Haoze He",
      "Parv Kapoor",
      "Jiahua Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01845",
    "title": "Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems",
    "abstract": "           Adversarial attacks against computer vision systems have emerged as a critical research area that challenges the fundamental assumptions about neural network robustness and security. This comprehensive survey examines the evolving landscape of adversarial techniques, revealing their dual nature as both sophisticated security threats and valuable defensive tools. We provide a systematic analysis of adversarial attack methodologies across three primary domains: pixel-space attacks, physically realizable attacks, and latent-space attacks. Our investigation traces the technical evolution from early gradient-based methods such as FGSM and PGD to sophisticated optimization techniques incorporating momentum, adaptive step sizes, and advanced transferability mechanisms. We examine how physically realizable attacks have successfully bridged the gap between digital vulnerabilities and real-world threats through adversarial patches, 3D textures, and dynamic optical perturbations. Additionally, we explore the emergence of latent-space attacks that leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples. Beyond traditional offensive applications, we investigate the constructive use of adversarial techniques for vulnerability assessment in biometric authentication systems and protection against malicious generative models. Our analysis reveals critical research gaps, particularly in neural style transfer protection and computational efficiency requirements. This survey contributes a comprehensive taxonomy, evolution analysis, and identification of future research directions, aiming to advance understanding of adversarial vulnerabilities and inform the development of more robust and trustworthy computer vision systems.         ",
    "url": "https://arxiv.org/abs/2508.01845",
    "authors": [
      "Zhongliang Guo",
      "Yifei Qian",
      "Yanli Li",
      "Weiye Li",
      "Chun Tong Lei",
      "Shuai Zhao",
      "Lei Fang",
      "Ognjen Arandjelovi\u0107",
      "Chun Pong Lau"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.01848",
    "title": "Causal Discovery in Multivariate Time Series through Mutual Information Featurization",
    "abstract": "           Discovering causal relationships in complex multivariate time series is a fundamental scientific challenge. Traditional methods often falter, either by relying on restrictive linear assumptions or on conditional independence tests that become uninformative in the presence of intricate, non-linear dynamics. This paper proposes a new paradigm, shifting from statistical testing to pattern recognition. We hypothesize that a causal link creates a persistent and learnable asymmetry in the flow of information through a system's temporal graph, even when clear conditional independencies are obscured. We introduce Temporal Dependency to Causality (TD2C), a supervised learning framework that operationalizes this hypothesis. TD2C learns to recognize these complex causal signatures from a rich set of information-theoretic and statistical descriptors. Trained exclusively on a diverse collection of synthetic time series, TD2C demonstrates remarkable zero-shot generalization to unseen dynamics and established, realistic benchmarks. Our results show that TD2C achieves state-of-the-art performance, consistently outperforming established methods, particularly in high-dimensional and non-linear settings. By reframing the discovery problem, our work provides a robust and scalable new tool for uncovering causal structures in complex systems.         ",
    "url": "https://arxiv.org/abs/2508.01848",
    "authors": [
      "Gian Marco Paldino",
      "Gianluca Bontempi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.01860",
    "title": "Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction",
    "abstract": "           For machines to effectively assist humans in challenging visual search tasks, they must differentiate whether a human is simply glancing into a scene (navigational intent) or searching for a target object (informational intent). Previous research proposed combining electroencephalography (EEG) and eye-tracking measurements to recognize such search intents implicitly, i.e., without explicit user input. However, the applicability of these approaches to real-world scenarios suffers from two key limitations. First, previous work used fixed search times in the informational intent condition -- a stark contrast to visual search, which naturally terminates when the target is found. Second, methods incorporating EEG measurements addressed prediction scenarios that require ground truth training data from the target user, which is impractical in many use cases. We address these limitations by making the first publicly available EEG and eye-tracking dataset for navigational vs. informational intent recognition, where the user determines search times. We present the first method for cross-user prediction of search intents from EEG and eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out evaluations -- comparable to within-user prediction accuracy (85.5%) but offering much greater flexibility         ",
    "url": "https://arxiv.org/abs/2508.01860",
    "authors": [
      "Mansi Sharma",
      "Shuang Chen",
      "Philipp M\u00fcller",
      "Maurice Rekrut",
      "Antonio Kr\u00fcger"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01862",
    "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models",
    "abstract": "           Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.         ",
    "url": "https://arxiv.org/abs/2508.01862",
    "authors": [
      "Yijun Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01869",
    "title": "ProKG-Dial: Progressive Multi-Turn Dialogue Construction with Domain Knowledge Graphs",
    "abstract": "           Current large language models (LLMs) excel at general NLP tasks but often lack domain specific precision in professional settings. Building a high quality domain specific multi turn dialogue dataset is essential for developing specialized conversational systems. However, existing methods such as manual annotation, simulated human LLM interactions, and role based LLM dialogues are resource intensive or suffer from limitations in dialogue quality and domain coverage. To address these challenges, we introduce ProKG Dial, a progressive framework for constructing knowledge intensive multi turn dialogue datasets using domain specific knowledge graphs (KGs). ProKG Dial leverages the structured nature of KGs to encode complex domain knowledge and relationships, providing a solid foundation for generating meaningful and coherent dialogues. Specifically, ProKG Dial begins by applying community detection to partition the KG into semantically cohesive subgraphs. For each subgraph, the framework incrementally generates a series of questions and answers centered around a target entity, ensuring relevance and coverage. A rigorous filtering step is employed to maintain high dialogue quality. We validate ProKG Dial on a medical knowledge graph by evaluating the generated dialogues in terms of diversity, semantic coherence, and entity coverage. Furthermore, we fine tune a base LLM on the resulting dataset and benchmark it against several baselines. Both automatic metrics and human evaluations demonstrate that ProKG Dial substantially improves dialogue quality and domain specific performance, highlighting its effectiveness and practical utility.         ",
    "url": "https://arxiv.org/abs/2508.01869",
    "authors": [
      "Yuanyuan Liang",
      "Xiaoman Wang",
      "Tingyu Xie",
      "Lei Pan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01871",
    "title": "Multi-turn Natural Language to Graph Query Language Translation",
    "abstract": "           In recent years, research on transforming natural language into graph query language (NL2GQL) has been increasing. Most existing methods focus on single-turn transformation from NL to GQL. In practical applications, user interactions with graph databases are typically multi-turn, dynamic, and context-dependent. While single-turn methods can handle straightforward queries, more complex scenarios often require users to iteratively adjust their queries, investigate the connections between entities, or request additional details across multiple dialogue turns. Research focused on single-turn conversion fails to effectively address multi-turn dialogues and complex context dependencies. Additionally, the scarcity of high-quality multi-turn NL2GQL datasets further hinders the progress of this field. To address this challenge, we propose an automated method for constructing multi-turn NL2GQL datasets based on Large Language Models (LLMs) , and apply this method to develop the MTGQL dataset, which is constructed from a financial market graph database and will be publicly released for future research. Moreover, we propose three types of baseline methods to assess the effectiveness of multi-turn NL2GQL translation, thereby laying a solid foundation for future research.         ",
    "url": "https://arxiv.org/abs/2508.01871",
    "authors": [
      "Yuanyuan Liang",
      "Lei Pan",
      "Tingyu Xie",
      "Yunshi Lan",
      "Weining Qian"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2508.01873",
    "title": "DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization",
    "abstract": "           The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness.         ",
    "url": "https://arxiv.org/abs/2508.01873",
    "authors": [
      "Siran Peng",
      "Haoyuan Zhang",
      "Li Gao",
      "Tianshuo Zhang",
      "Bao Li",
      "Zhen Lei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01887",
    "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection",
    "abstract": "           AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4) % accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4 $\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.01887",
    "authors": [
      "Aldan Creo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.01897",
    "title": "Generalizable Audio Deepfake Detection via Hierarchical Structure Learning and Feature Whitening in Poincar\u00e9 sphere",
    "abstract": "           Audio deepfake detection (ADD) faces critical generalization challenges due to diverse real-world spoofing attacks and domain variations. However, existing methods primarily rely on Euclidean distances, failing to adequately capture the intrinsic hierarchical structures associated with attack categories and domain factors. To address these issues, we design a novel framework Poin-HierNet to construct domain-invariant hierarchical representations in the Poincar\u00e9 sphere. Poin-HierNet includes three key components: 1) Poincar\u00e9 Prototype Learning (PPL) with several data prototypes aligning sample features and capturing multilevel hierarchies beyond human labels; 2) Hierarchical Structure Learning (HSL) leverages top prototypes to establish a tree-like hierarchical structure from data prototypes; and 3) Poincar\u00e9 Feature Whitening (PFW) enhances domain invariance by applying feature whitening to suppress domain-sensitive features. We evaluate our approach on four datasets: ASVspoof 2019 LA, ASVspoof 2021 LA, ASVspoof 2021 DF, and In-The-Wild. Experimental results demonstrate that Poin-HierNet exceeds state-of-the-art methods in Equal Error Rate.         ",
    "url": "https://arxiv.org/abs/2508.01897",
    "authors": [
      "Mingru Yang",
      "Yanmei Gu",
      "Qianhua He",
      "Yanxiong Li",
      "Peirong Zhang",
      "Yongqiang Chen",
      "Zhiming Wang",
      "Huijia Zhu",
      "Jian Liu",
      "Weiqiang Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.01906",
    "title": "Effect of AI Performance, Risk Perception, and Trust on Human Dependence in Deepfake Detection AI system",
    "abstract": "           Synthetic images, audio, and video can now be generated and edited by Artificial Intelligence (AI). In particular, the malicious use of synthetic data has raised concerns about potential harms to cybersecurity, personal privacy, and public trust. Although AI-based detection tools exist to help identify synthetic content, their limitations often lead to user mistrust and confusion between real and fake content. This study examines the role of AI performance in influencing human trust and decision making in synthetic data identification. Through an online human subject experiment involving 400 participants, we examined how varying AI performance impacts human trust and dependence on AI in deepfake detection. Our findings indicate how participants calibrate their dependence on AI based on their perceived risk and the prediction results provided by AI. These insights contribute to the development of transparent and explainable AI systems that better support everyday users in mitigating the harms of synthetic media.         ",
    "url": "https://arxiv.org/abs/2508.01906",
    "authors": [
      "Yingfan Zhou",
      "Ester Chen",
      "Manasa Pisipati",
      "Aiping Xiong",
      "Sarah Rajtmajer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.01911",
    "title": "Machine Learning-Driven Performance Analysis of Compressed Communication in Aerial-RIS Networks for Future 6G Networks",
    "abstract": "           In the future 6G and wireless networks, particularly in dense urban environments, bandwidth exhaustion and limited capacity pose significant challenges to enhancing data rates. We introduce a novel system model designed to improve the data rate of users in next-generation multi-cell networks by integrating Unmanned Aerial Vehicle (UAV)-Assisted Reconfigurable Intelligent Surfaces (RIS), Non-Orthogonal Multiple Access (NOMA), and Coordinated Multipoint Transmission (CoMP). Optimally deploying Aerial RIS for higher data rates, employing NOMA to improve spectral efficiency, and utilizing CoMP to mitigate inter-cell interference (ICI), we significantly enhance the overall system capacity and sum rate. Furthermore, we address the challenge of feedback overhead associated with Quantized Phase Shifts (QPS) from the receiver to RIS. The feedback channel is band-limited and cannot support a large overhead of QPS for uplink communication. To ensure seamless transmission, we propose a Machine Learning Autoencoder technique for a compressed communication of QPS from the receiver to RIS, while maintaining high accuracy. Additionally, we investigate the impact of the number of Aerial RIS elements and power allocation ratio for NOMA on the individual data rate of users. Our simulation results demonstrate substantial improvements in spectral efficiency, outage probability, and bandwidth utilization, highlighting the potential of the proposed architecture to enhance network performance.         ",
    "url": "https://arxiv.org/abs/2508.01911",
    "authors": [
      "Muhammad Farhan Khan",
      "Muhammad Ahmed Mohsin",
      "Zeeshan Alam",
      "Muhammad Saad",
      "Muhammad Waqar"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.01916",
    "title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning",
    "abstract": "           Understanding internal representations of neural models is a core interest of mechanistic interpretability. Due to its large dimensionality, the representation space can encode various aspects about inputs. To what extent are different aspects organized and encoded in separate subspaces? Is it possible to find these ``natural'' subspaces in a purely unsupervised way? Somewhat surprisingly, we can indeed achieve this and find interpretable subspaces by a seemingly unrelated training objective. Our method, neighbor distance minimization (NDM), learns non-basis-aligned subspaces in an unsupervised manner. Qualitative analysis shows subspaces are interpretable in many cases, and encoded information in obtained subspaces tends to share the same abstract concept across different inputs, making such subspaces similar to ``variables'' used by the model. We also conduct quantitative experiments using known circuits in GPT-2; results show a strong connection between subspaces and circuit variables. We also provide evidence showing scalability to 2B models by finding separate subspaces mediating context and parametric knowledge routing. Viewed more broadly, our findings offer a new perspective on understanding model internals and building circuits.         ",
    "url": "https://arxiv.org/abs/2508.01916",
    "authors": [
      "Xinting Huang",
      "Michael Hahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.01922",
    "title": "Beyond Simulation: Benchmarking World Models for Planning and Causality in Autonomous Driving",
    "abstract": "           World models have become increasingly popular in acting as learned traffic simulators. Recent work has explored replacing traditional traffic simulators with world models for policy training. In this work, we explore the robustness of existing metrics to evaluate world models as traffic simulators to see if the same metrics are suitable for evaluating a world model as a pseudo-environment for policy training. Specifically, we analyze the metametric employed by the Waymo Open Sim-Agents Challenge (WOSAC) and compare world model predictions on standard scenarios where the agents are fully or partially controlled by the world model (partial replay). Furthermore, since we are interested in evaluating the ego action-conditioned world model, we extend the standard WOSAC evaluation domain to include agents that are causal to the ego vehicle. Our evaluations reveal a significant number of scenarios where top-ranking models perform well under no perturbation but fail when the ego agent is forced to replay the original trajectory. To address these cases, we propose new metrics to highlight the sensitivity of world models to uncontrollable objects and evaluate the performance of world models as pseudo-environments for policy training and analyze some state-of-the-art world models under these new metrics.         ",
    "url": "https://arxiv.org/abs/2508.01922",
    "authors": [
      "Hunter Schofield",
      "Mohammed Elmahgiubi",
      "Kasra Rezaee",
      "Jinjun Shan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.01925",
    "title": "From Binary to Continuous: Stochastic Re-Weighting for Robust Graph Explanation",
    "abstract": "           Graph Neural Networks (GNNs) have achieved remarkable performance in a wide range of graph-related learning tasks. However, explaining their predictions remains a challenging problem, especially due to the mismatch between the graphs used during training and those encountered during explanation. Most existing methods optimize soft edge masks on weighted graphs to highlight important substructures, but these graphs differ from the unweighted graphs on which GNNs are trained. This distributional shift leads to unreliable gradients and degraded explanation quality, especially when generating small, sparse subgraphs. To address this issue, we propose a novel iterative explanation framework which improves explanation robustness by aligning the model's training data distribution with the weighted graph distribution appeared during explanation. Our method alternates between two phases: explanation subgraph identification and model adaptation. It begins with a relatively large explanation subgraph where soft mask optimization is reliable. Based on this subgraph, we assign importance-aware edge weights to explanatory and non-explanatory edges, and retrain the GNN on these weighted graphs. This process is repeated with progressively smaller subgraphs, forming an iterative refinement procedure. We evaluate our method on multiple benchmark datasets using different GNN backbones and explanation methods. Experimental results show that our method consistently improves explanation quality and can be flexibly integrated with different architectures.         ",
    "url": "https://arxiv.org/abs/2508.01925",
    "authors": [
      "Zhuomin Chen",
      "Jingchao Ni",
      "Hojat Allah Salehi",
      "Xu Zheng",
      "Dongsheng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01932",
    "title": "Proactive Disentangled Modeling of Trigger-Object Pairings for Backdoor Defense",
    "abstract": "           Deep neural networks (DNNs) and generative AI (GenAI) are increasingly vulnerable to backdoor attacks, where adversaries embed triggers into inputs to cause models to misclassify or misinterpret target labels. Beyond traditional single-trigger scenarios, attackers may inject multiple triggers across various object classes, forming unseen backdoor-object configurations that evade standard detection pipelines. In this paper, we introduce DBOM (Disentangled Backdoor-Object Modeling), a proactive framework that leverages structured disentanglement to identify and neutralize both seen and unseen backdoor threats at the dataset level. Specifically, DBOM factorizes input image representations by modeling triggers and objects as independent primitives in the embedding space through the use of Vision-Language Models (VLMs). By leveraging the frozen, pre-trained encoders of VLMs, our approach decomposes the latent representations into distinct components through a learnable visual prompt repository and prompt prefix tuning, ensuring that the relationships between triggers and objects are explicitly captured. To separate trigger and object representations in the visual prompt repository, we introduce the trigger-object separation and diversity losses that aids in disentangling trigger and object visual features. Next, by aligning image features with feature decomposition and fusion, as well as learned contextual prompt tokens in a shared multimodal space, DBOM enables zero-shot generalization to novel trigger-object pairings that were unseen during training, thereby offering deeper insights into adversarial attack patterns. Experimental results on CIFAR-10 and GTSRB demonstrate that DBOM robustly detects poisoned images prior to downstream training, significantly enhancing the security of DNN training pipelines.         ",
    "url": "https://arxiv.org/abs/2508.01932",
    "authors": [
      "Kyle Stein",
      "Andrew A. Mahyari",
      "Guillermo Francia III",
      "Eman El-Sheikh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01951",
    "title": "Flow-Aware GNN for Transmission Network Reconfiguration via Substation Breaker Optimization",
    "abstract": "           This paper introduces OptiGridML, a machine learning framework for discrete topology optimization in power grids. The task involves selecting substation breaker configurations that maximize cross-region power exports, a problem typically formulated as a mixed-integer program (MIP) that is NP-hard and computationally intractable for large networks. OptiGridML replaces repeated MIP solves with a two-stage neural architecture: a line-graph neural network (LGNN) that approximates DC power flows for a given network topology, and a heterogeneous GNN (HeteroGNN) that predicts breaker states under structural and physical constraints. A physics-informed consistency loss connects these components by enforcing Kirchhoff's law on predicted flows. Experiments on synthetic networks with up to 1,000 breakers show that OptiGridML achieves power export improvements of up to 18% over baseline topologies, while reducing inference time from hours to milliseconds. These results demonstrate the potential of structured, flow-aware GNNs for accelerating combinatorial optimization in physical networked systems.         ",
    "url": "https://arxiv.org/abs/2508.01951",
    "authors": [
      "Dekang Meng",
      "Rabab Haider",
      "Pascal van Hentenryck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01956",
    "title": "Agent-Based Feature Generation from Clinical Notes for Outcome Prediction",
    "abstract": "           Electronic health records (EHRs) contain rich unstructured clinical notes that could enhance predictive modeling, yet extracting meaningful features from these notes remains challenging. Current approaches range from labor-intensive manual clinician feature generation (CFG) to fully automated representational feature generation (RFG) that lack interpretability and clinical relevance. Here we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular multi-agent system powered by large language models (LLMs) that autonomously generates structured clinical features from unstructured notes without human intervention. We evaluated SNOW against manual CFG, clinician-guided LLM approaches, and RFG methods for predicting 5-year prostate cancer recurrence in 147 patients from Stanford Healthcare. While manual CFG achieved the highest performance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without requiring any clinical expertise, significantly outperforming both baseline features alone (0.691) and all RFG approaches. The clinician-guided LLM method also performed well (0.732) but still required expert input. SNOW's specialized agents handle feature discovery, extraction, validation, post-processing, and aggregation, creating interpretable features that capture complex clinical information typically accessible only through manual review. Our findings demonstrate that autonomous LLM systems can replicate expert-level feature engineering at scale, potentially transforming how clinical ML models leverage unstructured EHR data while maintaining the interpretability essential for clinical deployment.         ",
    "url": "https://arxiv.org/abs/2508.01956",
    "authors": [
      "Jiayi Wang",
      "Jacqueline Jil Vallon",
      "Neil Panjwani",
      "Xi Ling",
      "Sushmita Vij",
      "Sandy Srinivas",
      "John Leppert",
      "Mark K. Buyyounouski",
      "Mohsen Bayati"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.01966",
    "title": "Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection",
    "abstract": "           One-stage object detectors such as the YOLO family achieve state-of-the-art performance in real-time vision applications but remain heavily reliant on large-scale labeled datasets for training. In this work, we present a systematic study of contrastive self-supervised learning (SSL) as a means to reduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeled images using the SimCLR framework. Our approach introduces a simple yet effective pipeline that adapts YOLO's convolutional backbones as encoders, employs global pooling and projection heads, and optimizes a contrastive loss using augmentations of the COCO unlabeled dataset (120k images). The pretrained backbones are then fine-tuned on a cyclist detection task with limited labeled data. Experimental results show that SSL pretraining leads to consistently higher mAP, faster convergence, and improved precision-recall performance, especially in low-label regimes. For example, our SimCLR-pretrained YOLOv8 achieves a mAP@50:95 of 0.7663, outperforming its supervised counterpart despite using no annotations during pretraining. These findings establish a strong baseline for applying contrastive SSL to one-stage detectors and highlight the potential of unlabeled data as a scalable resource for label-efficient object detection.         ",
    "url": "https://arxiv.org/abs/2508.01966",
    "authors": [
      "Manikanta Kotthapalli",
      "Reshma Bhatia",
      "Nainsi Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01970",
    "title": "Improving Hospital Risk Prediction with Knowledge-Augmented Multimodal EHR Modeling",
    "abstract": "           Accurate prediction of clinical outcomes using Electronic Health Records (EHRs) is critical for early intervention, efficient resource allocation, and improved patient care. EHRs contain multimodal data, including both structured data and unstructured clinical notes that provide rich, context-specific information. In this work, we introduce a unified framework that seamlessly integrates these diverse modalities, leveraging all relevant available information through a two-stage architecture for clinical risk prediction. In the first stage, a fine-tuned Large Language Model (LLM) extracts crucial, task-relevant information from clinical notes, which is enhanced by graph-based retrieval of external domain knowledge from sources such as a medical corpus like PubMed, grounding the LLM's understanding. The second stage combines both unstructured representations and features derived from the structured data to generate the final predictions. This approach supports a wide range of clinical tasks. Here, we demonstrate its effectiveness on 30-day readmission and in-hospital mortality prediction. Experimental results show that our framework achieves strong performance, with AUC scores of $0.84$ and $0.92$, respectively, despite these tasks involving severely imbalanced datasets, with positive rates ranging from approximately $4\\%$ to $13\\%$. Moreover, it outperforms all existing baselines and clinical practices, including established risk scoring systems. To the best of our knowledge, this is one of the first frameworks for healthcare prediction which enhances the power of an LLM-based graph-guided knowledge retrieval method by combining it with structured data for improved clinical outcome prediction.         ",
    "url": "https://arxiv.org/abs/2508.01970",
    "authors": [
      "Rituparna Datta",
      "Jiaming Cui",
      "Zihan Guan",
      "Rupesh Silwal",
      "Joshua C Eby",
      "Gregory Madden",
      "Anil Vullikanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01987",
    "title": "Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion",
    "abstract": "           Recommender systems (RSs) are now fundamental to various online platforms, but their dependence on user-contributed data leaves them vulnerable to shilling attacks that can manipulate item rankings by injecting fake users. Although widely studied, most existing attack models fail to meet two critical objectives simultaneously: achieving strong adversarial promotion of target items while maintaining realistic behavior to evade detection. As a result, the true severity of shilling threats that manage to reconcile the two objectives remains underappreciated. To expose this overlooked vulnerability, we present DLDA, a diffusion-based attack framework that can generate highly effective yet indistinguishable fake users by enabling fine-grained control over target promotion. Specifically, DLDA operates in a pre-aligned collaborative embedding space, where it employs a conditional latent diffusion process to iteratively synthesize fake user profiles with precise target item control. To evade detection, DLDA introduces a dispersive regularization mechanism that promotes variability and realism in generated behavioral patterns. Extensive experiments on three real-world datasets and five popular RS models demonstrate that, compared to prior attacks, DLDA consistently achieves stronger item promotion while remaining harder to detect. These results highlight that modern RSs are more vulnerable than previously recognized, underscoring the urgent need for more robust defenses.         ",
    "url": "https://arxiv.org/abs/2508.01987",
    "authors": [
      "Shutong Qiao",
      "Wei Yuan",
      "Junliang Yu",
      "Tong Chen",
      "Quoc Viet Hung Nguyen",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.02015",
    "title": "A Group Consensus-Driven Auction Algorithm for Cooperative Task Allocation Among Heterogeneous Multi-Agents",
    "abstract": "           In scenarios like automated warehouses, assigning tasks to robots presents a heterogeneous multi-task and multi-agent task allocation problem. However, existing task allocation study ignores the integration of multi-task and multi-attribute agent task allocation with heterogeneous task allocation. In addition, current algorithms are limited by scenario constraints and can incur significant errors in specific contexts. Therefore, this study proposes a distributed heterogeneous multi-task and multi-agent task allocation algorithm with a time window, called group consensus-based heterogeneous auction (GCBHA). Firstly, this method decomposes tasks that exceed the capability of a single Agent into subtasks that can be completed by multiple independent agents. And then groups similar or adjacent tasks through a heuristic clustering method to reduce the time required to reach a consensus. Subsequently, the task groups are allocated to agents that meet the conditions through an auction process. Furthermore, the method evaluates the task path cost distance based on the scenario, which can calculate the task cost more accurately. The experimental results demonstrate that GCBHA performs well in terms of task allocation time and solution quality, with a significant reduction in the error rate between predicted task costs and actual costs.         ",
    "url": "https://arxiv.org/abs/2508.02015",
    "authors": [
      "Gang Wang",
      "Hongfang Han",
      "Xiaowei Liu",
      "Hanfeng Jiang",
      "Ming Zhang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.02031",
    "title": "PRIME: Plasticity-Robust Incremental Model for Encrypted Traffic Classification in Dynamic Network Environments",
    "abstract": "           With the continuous development of network environments and technologies, ensuring cyber security and governance is increasingly challenging. Network traffic classification(ETC) can analyzes attributes such as application categories and malicious intent, supporting network management services like QoS optimization, intrusion detection, and targeted billing. As the prevalence of traffic encryption increases, deep learning models are relied upon for content-agnostic analysis of packet sequences. However, the emergence of new services and attack variants often leads to incremental tasks for ETC models. To ensure model effectiveness, incremental learning techniques are essential; however, recent studies indicate that neural networks experience declining plasticity as tasks increase. We identified plasticity issues in existing incremental learning methods across diverse traffic samples and proposed the PRIME framework. By observing the effective rank of model parameters and the proportion of inactive neurons, the PRIME architecture can appropriately increase the parameter scale when the model's plasticity deteriorates. Experiments show that in multiple encrypted traffic datasets and different category increment scenarios, the PRIME architecture performs significantly better than other incremental learning algorithms with minimal increase in parameter scale.         ",
    "url": "https://arxiv.org/abs/2508.02031",
    "authors": [
      "Tian Qin",
      "Guang Cheng",
      "Zihan Chen",
      "Yuyang Zhou"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.02034",
    "title": "Protego: User-Centric Pose-Invariant Privacy Protection Against Face Recognition-Induced Digital Footprint Exposure",
    "abstract": "           Face recognition (FR) technologies are increasingly used to power large-scale image retrieval systems, raising serious privacy concerns. Services like Clearview AI and PimEyes allow anyone to upload a facial photo and retrieve a large amount of online content associated with that person. This not only enables identity inference but also exposes their digital footprint, such as social media activity, private photos, and news reports, often without their consent. In response to this emerging threat, we propose Protego, a user-centric privacy protection method that safeguards facial images from such retrieval-based privacy intrusions. Protego encapsulates a user's 3D facial signatures into a pose-invariant 2D representation, which is dynamically deformed into a natural-looking 3D mask tailored to the pose and expression of any facial image of the user, and applied prior to online sharing. Motivated by a critical limitation of existing methods, Protego amplifies the sensitivity of FR models so that protected images cannot be matched even among themselves. Experiments show that Protego significantly reduces retrieval accuracy across a wide range of black-box FR models and performs at least 2x better than existing methods. It also offers unprecedented visual coherence, particularly in video settings where consistency and natural appearance are essential. Overall, Protego contributes to the fight against the misuse of FR for mass surveillance and unsolicited identity tracing.         ",
    "url": "https://arxiv.org/abs/2508.02034",
    "authors": [
      "Ziling Wang",
      "Shuya Yang",
      "Jialin Lu",
      "Ka-Ho Chow"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02043",
    "title": "Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction",
    "abstract": "           Radiotherapy treatment planning often relies on time-consuming, trial-and-error adjustments that heavily depend on the expertise of specialists, while existing deep learning methods face limitations in generalization, prediction accuracy, and clinical applicability. To tackle these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The model employs LightweightVAE3D to compress high-dimensional CT data and integrates multimodal inputs, including target and organ-at-risk (OAR) masks and beam parameters, within a progressive noise addition and denoising framework. It incorporates conditional features via a multi-head attention mechanism and utilizes a composite loss function combining MSE, conditional terms, and KL divergence to ensure both dosimetric accuracy and compliance with clinical constraints. Evaluation on a large-scale public dataset (2,877 cases) and three external institutional cohorts (450 cases in total) demonstrates that ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum dose error to within 0.1 Gy. The average plan generation time per case is reduced to 22 seconds. Ablation studies confirm that the structural encoder enhances compliance with clinical dose constraints by 28.5%. To our knowledge, this is the first study to introduce a conditional diffusion model framework for radiotherapy dose prediction, offering a generalizable and efficient solution for automated treatment planning across diverse tumor sites, with the potential to substantially reduce planning time and improve clinical workflow efficiency.         ",
    "url": "https://arxiv.org/abs/2508.02043",
    "authors": [
      "Hui Xie",
      "Haiqin Hu",
      "Lijuan Ding",
      "Qing Li",
      "Yue Sun",
      "Tao Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02044",
    "title": "Graph Unlearning via Embedding Reconstruction -- A Range-Null Space Decomposition Approach",
    "abstract": "           Graph unlearning is tailored for GNNs to handle widespread and various graph structure unlearning requests, which remain largely unexplored. The GIF (graph influence function) achieves validity under partial edge unlearning, but faces challenges in dealing with more disturbing node unlearning. To avoid the overhead of retraining and realize the model utility of unlearning, we proposed a novel node unlearning method to reverse the process of aggregation in GNN by embedding reconstruction and to adopt Range-Null Space Decomposition for the nodes' interaction learning. Experimental results on multiple representative datasets demonstrate the SOTA performance of our proposed approach.         ",
    "url": "https://arxiv.org/abs/2508.02044",
    "authors": [
      "Hang Yin",
      "Zipeng Liu",
      "Xiaoyong Peng",
      "Liyao Xiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.02049",
    "title": "Epi$^2$-Net: Advancing Epidemic Dynamics Forecasting with Physics-Inspired Neural Networks",
    "abstract": "           Advancing epidemic dynamics forecasting is vital for targeted interventions and safeguarding public health. Current approaches mainly fall into two categories: mechanism-based and data-driven models. Mechanism-based models are constrained by predefined compartmental structures and oversimplified system assumptions, limiting their ability to model complex real-world dynamics, while data-driven models focus solely on intrinsic data dependencies without physical or epidemiological constraints, risking biased or misleading representations. Although recent studies have attempted to integrate epidemiological knowledge into neural architectures, most of them fail to reconcile explicit physical priors with neural representations. To overcome these obstacles, we introduce Epi$^2$-Net, a Epidemic Forecasting Framework built upon Physics-Inspired Neural Networks. Specifically, we propose reconceptualizing epidemic transmission from the physical transport perspective, introducing the concept of neural epidemic transport. Further, we present a physic-inspired deep learning framework, and integrate physical constraints with neural modules to model spatio-temporal patterns of epidemic dynamics. Experiments on real-world datasets have demonstrated that Epi$^2$-Net outperforms state-of-the-art methods in epidemic forecasting, providing a promising solution for future epidemic containment. The code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02049",
    "authors": [
      "Rui Sun",
      "Chenghua Gong",
      "Tianjun Gu",
      "Yuhao Zheng",
      "Jie Ding",
      "Juyuan Zhang",
      "Liming Pan",
      "Linyuan L\u00fc"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02052",
    "title": "Convergence Analysis of Optimal SOR for a Class of Consistently Ordered 2-Cyclic Matrices with Complex Spectra",
    "abstract": "           Asymptotic rates of convergence of optimal SOR applied to linear systems with consistently ordered 2-cyclic matrices have been extensively studied in the case where the Jacobi eigenvalues are are real and contained in an interval centered at the origin. It is well known that as the rightmost endpoint of the interval approaches $1$ from below, optimal SOR converges an order of magnitude faster than Jacobi. We generalize this to the situation where the Jacobi spectrum is contained in a line segment in the complex plane that is symmetric about the origin. This is an important class of linear systems, which arise often in various physical applications; complex-shifted linear systems are included in this family. Optimal relaxation parameters are known in this case, but a detailed convergence analysis does not seem to exist in the literature. Using techniques of complex analysis, we derive convergence rates, finding that in the complex case they are affected not only by the distance to 1 of the right-hand endpoint of the line segment as in the real case, but also by its phase.         ",
    "url": "https://arxiv.org/abs/2508.02052",
    "authors": [
      "L. Robert Hocking",
      "Chen Greif"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.02067",
    "title": "YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges",
    "abstract": "           Over the past decade, object detection has advanced significantly, with the YOLO (You Only Look Once) family of models transforming the landscape of real-time vision applications through unified, end-to-end detection frameworks. From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each version has systematically enhanced the balance between speed, accuracy, and deployment efficiency through continuous architectural and algorithmic advancements.. Beyond core object detection, modern YOLO architectures have expanded to support tasks such as instance segmentation, pose estimation, object tracking, and domain-specific applications including medical imaging and industrial automation. This paper offers a comprehensive review of the YOLO family, highlighting architectural innovations, performance benchmarks, extended capabilities, and real-world use cases. We critically analyze the evolution of YOLO models and discuss emerging research directions that extend their impact across diverse computer vision domains.         ",
    "url": "https://arxiv.org/abs/2508.02067",
    "authors": [
      "Manikanta Kotthapalli",
      "Deepika Ravipati",
      "Reshma Bhatia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02083",
    "title": "Supervisory Control of Discrete Event Systems for Small Language Under Cyber Attacks",
    "abstract": "           Cyber attacks are unavoidable in networked discrete event systems where the plant and the supervisor communicate with each other via networks. Because of the nondeterminism in observation and control caused by cyber attacks, the language generated by the supervised system becomes nondeterministic. The small language is defined as the lower bound on all possible languages that can be generated by the supervised system, which is needed for a supervised system to perform some required tasks under cyber attacks. In this paper, we investigate supervisory control for the small language. After introducing CA-S-controllability and CA-S-observability, we prove that the supervisory control problem of achieving a required small language is solvable if and only if the given language is CA-Scontrollable and CA-S-observable. If the given language is not CA-S controllable and/or CA-S-observable, we derive conditions under which the infimal CA-S-controllable and CA-S-observable superlanguage exists and can be used to design a supervisor satisfying the given requirement.         ",
    "url": "https://arxiv.org/abs/2508.02083",
    "authors": [
      "Xiaojun Wang",
      "Shaolong Shu",
      "Feng Lin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.02089",
    "title": "Leveraging Social Media Sentiment for Predictive Algorithmic Trading Strategies",
    "abstract": "           This study investigates how social media sentiment derived from Reddit comments can be used to enhance investment decisions in a way that offers higher returns with lower risk. Using BERTweet we analyzed over 2 million Reddit comments from the subreddit r/wallstreetbets and developed a Sentiment Volume Change (SVC) metric combining sentiment and comment volume changes, which showed significantly improved correlation with next-day returns compared to sentiment alone. We then implemented two different investment strategies that relied solely on SVC to make decisions. Back testing these strategies over four years (2020-2023) our strategies significantly outperformed a comparable buy-and-hold (B&H) strategy in a bull market, achieving 70% higher returns in 2023 and 84.4% higher returns in 2021 while also mitigating losses by 4% in a declining market in 2022. Our results confirm that comment sentiment and volume data derived from Reddit can be effective in predicting short-term stock price movements and sentiment-powered strategies can offer superior risk-adjusted returns as compared to the market, implying that social media sentiment can potentially be a valuable investment tool.         ",
    "url": "https://arxiv.org/abs/2508.02089",
    "authors": [
      "Gatik Goyal",
      "Sharvil Phadke",
      "Arnav Sharma",
      "Huifang Qin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.02092",
    "title": "FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing",
    "abstract": "           Large language models represent significant investments in computation, data, and engineering expertise, making them extraordinarily valuable intellectual assets. Nevertheless, these AI assets remain vulnerable to unauthorized redistribution and commercial exploitation through fine-tuning or black-box deployment. Current fingerprinting approaches face a fundamental trade-off: intrinsic methods require full parameter access, while backdoor-based techniques employ statistically anomalous triggers easily detected and filtered by adversaries. To address these limitations, we introduce FPEdit, a novel knowledge-editing framework that injects semantically coherent natural language fingerprints by modifying a sparse subset of model weights. This ensures stealthy and precise ownership encoding without degrading the core functionality. Extensive experiments show that FPEdit achieves $95$-$100\\%$ fingerprint retention under both full-parameter fine-tuning and parameter-efficient adaptation, while preserving performance on 24 downstream benchmarks. Moreover, FPEdit remains robust under quantization, pruning, and stochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under 10 minutes using less than 32 GB of GPU memory, a $70\\%$ reduction in resource requirements compared to existing techniques. These advances establish FPEdit as the first fingerprinting approach to simultaneously achieve robustness against adaptation, resistance to detection, and preservation of model utility, providing a minimally invasive solution for reliable provenance verification of large language models in adversarial deployment scenarios.         ",
    "url": "https://arxiv.org/abs/2508.02092",
    "authors": [
      "Shida Wang",
      "Chaohu Liu",
      "Yubo Wang",
      "Linli Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02094",
    "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth",
    "abstract": "           Risk perception is subjective, and youth's understanding of toxic content differs from that of adults. Although previous research has conducted extensive studies on toxicity detection in social media, the investigation of youth's unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as youth, is ignored. To address this gap, we aim to explore: 1) What are the features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing toxicity detection techniques accurately detect these languages (RQ2). For these questions, we took Chinese youth as the research target, constructed the first Chinese ``youth-toxicity'' dataset, and then conducted extensive analysis. Our results suggest that youth's perception of these is associated with several contextual factors, like the source of an utterance and text-related features. Incorporating these meta information into current toxicity detection methods significantly improves accuracy overall. Finally, we propose several insights into future research on youth-centered toxicity detection.         ",
    "url": "https://arxiv.org/abs/2508.02094",
    "authors": [
      "Yaqiong Li",
      "Peng Zhang",
      "Lin Wang",
      "Hansu Gu",
      "Siyuan Qiao",
      "Ning Gu",
      "Tun Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.02102",
    "title": "Centralized Dynamic State Estimation Algorithm for Detecting and Distinguishing Faults and Cyber Attacks in Power Systems",
    "abstract": "           As power systems evolve with increased integration of renewable energy sources, they become more complex and vulnerable to both cyber and physical threats. This study validates a centralized Dynamic State Estimation (DSE) algorithm designed to enhance the protection of power systems, particularly focusing on microgrids with substantial renewable energy integration. The algorithm utilizing a structured hypothesis testing framework, systematically identifies and differentiates anomalies caused by cyberattacks from those resulting from physical faults. This algorithm was evaluated through four case studies: a False Data Injection Attack (FDIA) via manipulation of Current Transformer (CT) ratios, a single line-to-ground (SLG) fault, and two combined scenarios involving both anomalies. Results from real-time simulations demonstrate that the algorithm effectively distinguishes between cyber-induced anomalies and physical faults, thereby significantly enhancing the reliability and security of energy systems. This research underscores the critical role of advanced diagnostic tools in protecting power systems against the growing prevalence of cyber-physical threats, enhancing the resilience of the grid and preventing potential blackouts by avoiding the mis-operation of protection relays.         ",
    "url": "https://arxiv.org/abs/2508.02102",
    "authors": [
      "Emad Abukhousa",
      "Syed Sohail Feroz Syed Afroz",
      "Fahad Alsaeed",
      "Abdulaziz Qwbaiban",
      "A.P. Sakis Meliopoulos"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.02109",
    "title": "Real-Time Conflict Prediction for Large Truck Merging in Mixed Traffic at Work Zone Lane Closures",
    "abstract": "           Large trucks substantially contribute to work zone-related crashes, primarily due to their large size and blind spots. When approaching a work zone, large trucks often need to merge into an adjacent lane because of lane closures caused by construction activities. This study aims to enhance the safety of large truck merging maneuvers in work zones by evaluating the risk associated with merging conflicts and establishing a decision-making strategy for merging based on this risk assessment. To predict the risk of large trucks merging into a mixed traffic stream within a work zone, a Long Short-Term Memory (LSTM) neural network is employed. For a large truck intending to merge, it is critical that the immediate downstream vehicle in the target lane maintains a minimum safe gap to facilitate a safe merging process. Once a conflict-free merging opportunity is predicted, large trucks are instructed to merge in response to the lane closure. Our LSTM-based conflict prediction method is compared against baseline approaches, which include probabilistic risk-based merging, 50th percentile gap-based merging, and 85th percentile gap-based merging strategies. The results demonstrate that our method yields a lower conflict risk, as indicated by reduced Time Exposed Time-to-Collision (TET) and Time Integrated Time-to-Collision (TIT) values relative to the baseline models. Furthermore, the findings indicate that large trucks that use our method can perform early merging while still in motion, as opposed to coming to a complete stop at the end of the current lane prior to closure, which is commonly observed with the baseline approaches.         ",
    "url": "https://arxiv.org/abs/2508.02109",
    "authors": [
      "Abyad Enan",
      "Abdullah Al Mamun",
      "Gurcan Comert",
      "Debbie Aisiana Indah",
      "Judith Mwakalonge",
      "Amy W. Apon",
      "Mashrur Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02115",
    "title": "Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark",
    "abstract": "           Backdoor detection is currently the mainstream defense against backdoor attacks in federated learning (FL), where malicious clients upload poisoned updates that compromise the global model and undermine the reliability of FL deployments. Existing backdoor detection techniques fall into two categories, including passive and proactive ones, depending on whether the server proactively modifies the global model. However, both have inherent limitations in practice: passive defenses are vulnerable to common non-i.i.d. data distributions and random participation of FL clients, whereas current proactive defenses suffer inevitable out-of-distribution (OOD) bias because they rely on backdoor co-existence effects. To address these issues, we introduce a new proactive defense, dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. In general, we detect attackers by evaluating whether the server-injected, conflicting global watermark is erased during local training rather than retained. Our method preserves the advantages of proactive defenses in handling data heterogeneity (\\ie, non-i.i.d. data) while mitigating the adverse impact of OOD bias through a revised detection mechanism. Extensive experiments on benchmark datasets confirm the effectiveness of Coward and its resilience to potential adaptive attacks. The code for our method would be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02115",
    "authors": [
      "Wenjie Li",
      "Siying Gu",
      "Yiming Li",
      "Kangjie Chen",
      "Zhili Chen",
      "Tianwei Zhang",
      "Shu-Tao Xia",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02116",
    "title": "SUAD: Solid-Channel Ultrasound Injection Attack and Defense to Voice Assistants",
    "abstract": "           As a versatile AI application, voice assistants (VAs) have become increasingly popular, but are vulnerable to security threats. Attackers have proposed various inaudible attacks, but are limited by cost, distance, or LoS. Therefore, we propose \\name~Attack, a long-range, cross-barrier, and interference-free inaudible voice attack via solid channels. We begin by thoroughly analyzing the dispersion effect in solid channels, revealing its unique impact on signal propagation. To avoid distortions in voice commands, we design a modular command generation model that parameterizes attack distance, victim audio, and medium dispersion features to adapt to variations in the solid-channel state. Additionally, we propose SUAD Defense, a universal defense that uses ultrasonic perturbation signals to block inaudible voice attacks (IVAs) without impacting normal speech. Since the attack can occur at arbitrary frequencies and times, we propose a training method that randomizes both time and frequency to generate perturbation signals that break ultrasonic commands. Notably, the perturbation signal is modulated to an inaudible frequency without affecting the functionality of voice commands for VAs. Experiments on six smartphones have shown that SUAD Attack achieves activation success rates above 89.8% and SUAD Defense blocks IVAs with success rates exceeding 98%.         ",
    "url": "https://arxiv.org/abs/2508.02116",
    "authors": [
      "Chao Liu",
      "Zhezheng Zhu",
      "Hao Chen",
      "Zhe Chen",
      "Kaiwen Guo",
      "Penghao Wang",
      "Jun Luo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.02127",
    "title": "Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps",
    "abstract": "           Accurate object detection under adverse lighting conditions is critical for real-world applications such as autonomous driving. Although neuromorphic event cameras have been introduced to handle these scenarios, adverse lighting often induces distracting reflections from tunnel walls or road surfaces, which frequently lead to false obstacle detections. However, neither RGB nor event data alone is robust enough to address these complexities, and mitigating these issues without additional sensors remains underexplored. To overcome these challenges, we propose leveraging normal maps, directly predicted from monocular RGB images, as robust geometric cues to suppress false positives and enhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection framework that effectively fuses three complementary modalities: monocularly predicted surface normal maps, RGB images, and event streams. To optimize the fusion process, our framework incorporates two key modules: the Adaptive Dual-stream Fusion Module (ADFM), which integrates RGB and normal map features, and the Event-modality Aware Fusion Module (EAFM), which adapts to the high dynamic range characteristics of event data. Extensive evaluations on the DSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly outperforms state-of-the-art methods. Our approach achieves mAP50 improvements of 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing the fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by 7.1% on the PKU-DAVIS-SOD dataset.         ",
    "url": "https://arxiv.org/abs/2508.02127",
    "authors": [
      "Mingjie Liu",
      "Hanqing Liu",
      "Chuang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02131",
    "title": "A Neural Quality Metric for BRDF Models",
    "abstract": "           Accurately evaluating the quality of bidirectional reflectance distribution function (BRDF) models is essential for photo-realistic rendering. Traditional BRDF-space metrics often employ numerical error measures that fail to capture perceptual differences evident in rendered images. In this paper, we introduce the first perceptually informed neural quality metric for BRDF evaluation that operates directly in BRDF space, eliminating the need for rendering during quality assessment. Our metric is implemented as a compact multi-layer perceptron (MLP), trained on a dataset of measured BRDFs supplemented with synthetically generated data and labelled using a perceptually validated image-space metric. The network takes as input paired samples of reference and approximated BRDFs and predicts their perceptual quality in terms of just-objectionable-difference (JOD) scores. We show that our neural metric achieves significantly higher correlation with human judgments than existing BRDF-space metrics. While its performance as a loss function for BRDF fitting remains limited, the proposed metric offers a perceptually grounded alternative for evaluating BRDF models.         ",
    "url": "https://arxiv.org/abs/2508.02131",
    "authors": [
      "Behnaz Kavoosighafi",
      "Rafal K. Mantiuk",
      "Saghi Hajisharif",
      "Ehsan Miandji",
      "Jonas Unger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02136",
    "title": "FedLAD: A Linear Algebra Based Data Poisoning Defence for Federated Learning",
    "abstract": "           Sybil attacks pose a significant threat to federated learning, as malicious nodes can collaborate and gain a majority, thereby overwhelming the system. Therefore, it is essential to develop countermeasures that ensure the security of federated learning environments. We present a novel defence method against targeted data poisoning, which is one of the types of Sybil attacks, called Linear Algebra-based Detection (FedLAD). Unlike existing approaches, such as clustering and robust training, which struggle in situations where malicious nodes dominate, FedLAD models the federated learning aggregation process as a linear problem, transforming it into a linear algebra optimisation challenge. This method identifies potential attacks by extracting the independent linear combinations from the original linear combinations, effectively filtering out redundant and malicious elements. Extensive experimental evaluations demonstrate the effectiveness of FedLAD compared to five well-established defence methods: Sherpa, CONTRA, Median, Trimmed Mean, and Krum. Using tasks from both image classification and natural language processing, our experiments confirm that FedLAD is robust and not dependent on specific application settings. The results indicate that FedLAD effectively protects federated learning systems across a broad spectrum of malicious node ratios. Compared to baseline defence methods, FedLAD maintains a low attack success rate for malicious nodes when their ratio ranges from 0.2 to 0.8. Additionally, it preserves high model accuracy when the malicious node ratio is between 0.2 and 0.5. These findings underscore FedLAD's potential to enhance both the reliability and performance of federated learning systems in the face of data poisoning attacks.         ",
    "url": "https://arxiv.org/abs/2508.02136",
    "authors": [
      "Qi Xiong",
      "Hai Dong",
      "Nasrin Sohrabi",
      "Zahir Tari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02143",
    "title": "TrackletGait: A Robust Framework for Gait Recognition in the Wild",
    "abstract": "           Gait recognition aims to identify individuals based on their body shape and walking patterns. Though much progress has been achieved driven by deep learning, gait recognition in real-world surveillance scenarios remains quite challenging to current methods. Conventional approaches, which rely on periodic gait cycles and controlled environments, struggle with the non-periodic and occluded silhouette sequences encountered in the wild. In this paper, we propose a novel framework, TrackletGait, designed to address these challenges in the wild. We propose Random Tracklet Sampling, a generalization of existing sampling methods, which strikes a balance between robustness and representation in capturing diverse walking patterns. Next, we introduce Haar Wavelet-based Downsampling to preserve information during spatial downsampling. Finally, we present a Hardness Exclusion Triplet Loss, designed to exclude low-quality silhouettes by discarding hard triplet samples. TrackletGait achieves state-of-the-art results, with 77.8 and 80.4 rank-1 accuracy on the Gait3D and GREW datasets, respectively, while using only 10.3M backbone parameters. Extensive experiments are also conducted to further investigate the factors affecting gait recognition in the wild.         ",
    "url": "https://arxiv.org/abs/2508.02143",
    "authors": [
      "Shaoxiong Zhang",
      "Jinkai Zheng",
      "Shangdong Zhu",
      "Chenggang Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02148",
    "title": "Large-Scale Model Enabled Semantic Communication Based on Robust Knowledge Distillation",
    "abstract": "           Large-scale models (LSMs) can be an effective framework for semantic representation and understanding, thereby providing a suitable tool for designing semantic communication (SC) systems. However, their direct deployment is often hindered by high computational complexity and resource requirements. In this paper, a novel robust knowledge distillation based semantic communication (RKD-SC) framework is proposed to enable efficient and \\textcolor{black}{channel-noise-robust} LSM-powered SC. The framework addresses two key challenges: determining optimal compact model architectures and effectively transferring knowledge while maintaining robustness against channel noise. First, a knowledge distillation-based lightweight differentiable architecture search (KDL-DARTS) algorithm is proposed. This algorithm integrates knowledge distillation loss and a complexity penalty into the neural architecture search process to identify high-performance, lightweight semantic encoder architectures. Second, a novel two-stage robust knowledge distillation (RKD) algorithm is developed to transfer semantic capabilities from an LSM (teacher) to a compact encoder (student) and subsequently enhance system robustness. To further improve resilience to channel impairments, a channel-aware transformer (CAT) block is introduced as the channel codec, trained under diverse channel conditions with variable-length outputs. Extensive simulations on image classification tasks demonstrate that the RKD-SC framework significantly reduces model parameters while preserving a high degree of the teacher model's performance and exhibiting superior robustness compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2508.02148",
    "authors": [
      "Kuiyuan DIng",
      "Caili Guo",
      "Yang Yang",
      "Zhongtian Du",
      "Walid Saad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.02150",
    "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following",
    "abstract": "           Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02150",
    "authors": [
      "Qingyu Ren",
      "Qianyu He",
      "Bowei Zhang",
      "Jie Zeng",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02152",
    "title": "Efficient Chambolle-Pock based algorithms for Convoltional sparse representation",
    "abstract": "           Recently convolutional sparse representation (CSR), as a sparse representation technique, has attracted increasing attention in the field of image processing, due to its good characteristic of translate-invariance. The content of CSR usually consists of convolutional sparse coding (CSC) and convolutional dictionary learning (CDL), and many studies focus on how to solve the corresponding optimization problems. At present, the most efficient optimization scheme for CSC is based on the alternating direction method of multipliers (ADMM). However, the ADMM-based approach involves a penalty parameter that needs to be carefully selected, and improper parameter selection may result in either no convergence or very slow convergence. In this paper, a novel fast and efficient method using Chambolle-Pock(CP) framework is proposed, which does not require extra manual selection parameters in solving processing, and has faster convergence speed. Furthermore, we propose an anisotropic total variation penalty of the coefficient maps for CSC and apply the CP algorithm to solve it. In addition, we also apply the CP framework to solve the corresponding CDL problem. Experiments show that for noise-free image the proposed CSC algorithms can achieve rival results of the latest ADMM-based approach, while outperforms in removing noise from Gaussian noise pollution image.         ",
    "url": "https://arxiv.org/abs/2508.02152",
    "authors": [
      "Yi Liu",
      "Junjing Li",
      "Yang Chen",
      "Haowei Tang",
      "Pengcheng Zhang",
      "Tianling Lyu",
      "Zhiguo Gui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2508.02153",
    "title": "Towards High Precision: An Adaptive Self-Supervised Learning Framework for Force-Based Verification",
    "abstract": "           The automation of robotic tasks requires high precision and adaptability, particularly in force-based operations such as insertions. Traditional learning-based approaches either rely on static datasets, which limit their ability to generalize, or require frequent manual intervention to maintain good performances. As a result, ensuring long-term reliability without human supervision remains a significant challenge. To address this, we propose an adaptive self-supervised learning framework for insertion classification that continuously improves its precision over time. The framework operates in real-time, incrementally refining its classification decisions by integrating newly acquired force data. Unlike conventional methods, it does not rely on pre-collected datasets but instead evolves dynamically with each task execution. Through real-world experiments, we demonstrate how the system progressively reduces execution time while maintaining near-perfect precision as more samples are processed. This adaptability ensures long-term reliability in force-based robotic tasks while minimizing the need for manual intervention.         ",
    "url": "https://arxiv.org/abs/2508.02153",
    "authors": [
      "Zebin Duan",
      "Frederik Hagelskj\u00e6r",
      "Aljaz Kramberger",
      "Juan Heredia and",
      "Norbert Kr\u00fcger"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.02157",
    "title": "Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes",
    "abstract": "           Recognizing objects in images is a fundamental problem in computer vision. Although detecting objects in 2D images is common, many applications require determining their pose in 3D space. Traditional category-level methods rely on RGB-D inputs, which may not always be available, or employ two-stage approaches that use separate models and representations for detection and pose estimation. For the first time, we introduce a unified model that integrates detection and pose estimation into a single framework for RGB images by leveraging neural mesh models with learned features and multi-model RANSAC. Our approach achieves state-of-the-art results for RGB category-level pose estimation on REAL275, improving on the current state-of-the-art by 22.9% averaged across all scale-agnostic metrics. Finally, we demonstrate that our unified method exhibits greater robustness compared to single-stage baselines. Our code and models are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02157",
    "authors": [
      "Tom Fischer",
      "Xiaojie Zhang",
      "Eddy Ilg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02158",
    "title": "Robust Detection of Planted Subgraphs in Semi-Random Models",
    "abstract": "           Detection of planted subgraphs in Erd\u00f6s-R\u00e9nyi random graphs has been extensively studied, leading to a rich body of results characterizing both statistical and computational thresholds. However, most prior work assumes a purely random generative model, making the resulting algorithms potentially fragile in the face of real-world perturbations. In this work, we initiate the study of semi-random models for the planted subgraph detection problem, wherein an adversary is allowed to remove edges outside the planted subgraph before the graph is revealed to the statistician. Crucially, the statistician remains unaware of which edges have been removed, introducing fundamental challenges to the inference task. We establish fundamental statistical limits for detection under this semi-random model, revealing a sharp dichotomy. Specifically, for planted subgraphs with strongly sub-logarithmic maximum density detection becomes information-theoretically impossible in the presence of an adversary, despite being possible in the classical random model. In stark contrast, for subgraphs with super-logarithmic density, the statistical limits remain essentially unchanged; we prove that the optimal (albeit computationally intractable) likelihood ratio test remains robust. Beyond these statistical boundaries, we design a new computationally efficient and robust detection algorithm, and provide rigorous statistical guarantees for its performance. Our results establish the first robust framework for planted subgraph detection and open new directions in the study of semi-random models, computational-statistical trade-offs, and robustness in graph inference problems.         ",
    "url": "https://arxiv.org/abs/2508.02158",
    "authors": [
      "Dor Elimelech",
      "Wasim Huleihel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2508.02161",
    "title": "User Trajectory Prediction Unifying Global and Local Temporal Information",
    "abstract": "           Trajectory prediction is essential for formulating proactive strategies that anticipate user mobility and support advance preparation. Therefore, how to reduce the forecasting error in user trajectory prediction within an acceptable inference time arises as an interesting issue. However, trajectory data contains both global and local temporal information, complicating the extraction of the complete temporal pattern. Moreover, user behavior occurs over different time scales, increasing the difficulty of capturing behavioral patterns. To address these challenges, a trajectory prediction model based on multilayer perceptron (MLP), multi-scale convolutional neural network (MSCNN), and cross-attention (CA) is proposed. Specifically, MLP is used to extract the global temporal information of each feature. In parallel, MSCNN is employed to extract the local temporal information by modeling interactions among features within a local temporal range. Convolutional kernels with different sizes are used in MSCNN to capture temporal information at multiple resolutions, enhancing the model's adaptability to different behavioral patterns. Finally, CA is applied to fuse the global and local temporal information. Experimental results show that our model reduces mean squared error (MSE) by 5.04% and mean absolute error (MAE) by 4.35% compared with ModernTCN in 12-step prediction, while maintaining similar inference time.         ",
    "url": "https://arxiv.org/abs/2508.02161",
    "authors": [
      "Wei Hao",
      "Bin Chong",
      "Ronghua Ji",
      "Chen Hou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02172",
    "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting",
    "abstract": "           The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2508.02172",
    "authors": [
      "Lei Yao",
      "Yi Wang",
      "Yi Zhang",
      "Moyun Liu",
      "Lap-Pui Chau"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2508.02180",
    "title": "Test-Time Model Adaptation for Quantized Neural Networks",
    "abstract": "           Quantizing deep models prior to deployment is a widely adopted technique to speed up inference for various real-time applications, such as autonomous driving. However, quantized models often suffer from severe performance degradation in dynamic environments with potential domain shifts and this degradation is significantly more pronounced compared with their full-precision counterparts, as shown by our theoretical and empirical illustrations. To address the domain shift problem, test-time adaptation (TTA) has emerged as an effective solution by enabling models to learn adaptively from test data. Unfortunately, existing TTA methods are often impractical for quantized models as they typically rely on gradient backpropagation--an operation that is unsupported on quantized models due to vanishing gradients, as well as memory and latency constraints. In this paper, we focus on TTA for quantized models to improve their robustness and generalization ability efficiently. We propose a continual zeroth-order adaptation (ZOA) framework that enables efficient model adaptation using only two forward passes, eliminating the computational burden of existing methods. Moreover, we propose a domain knowledge management scheme to store and reuse different domain knowledge with negligible memory consumption, reducing the interference of different domain knowledge and fostering the knowledge accumulation during long-term adaptation. Experimental results on three classical architectures, including quantized transformer-based and CNN-based models, demonstrate the superiority of our methods for quantized model adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve a 5.0\\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02180",
    "authors": [
      "Zeshuai Deng",
      "Guohao Chen",
      "Shuaicheng Niu",
      "Hui Luo",
      "Shuhai Zhang",
      "Yifan Yang",
      "Renjie Chen",
      "Wei Luo",
      "Mingkui Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02182",
    "title": "Near-Optimal Differentially Private Graph Algorithms via the Multidimensional AboveThreshold Mechanism",
    "abstract": "           Many differentially private and classical non-private graph algorithms rely crucially on determining whether some property of each vertex meets a threshold. For example, for the $k$-core decomposition problem, the classic peeling algorithm iteratively removes a vertex if its induced degree falls below a threshold. The sparse vector technique (SVT) is generally used to transform non-private threshold queries into private ones with only a small additive loss in accuracy. However, a naive application of SVT in the graph setting leads to an amplification of the error by a factor of $n$ due to composition, as SVT is applied to every vertex. In this paper, we resolve this problem by formulating a novel generalized sparse vector technique which we call the Multidimensional AboveThreshold (MAT) Mechanism which generalizes SVT (applied to vectors with one dimension) to vectors with multiple dimensions. As an application, we solve a number of important graph problems with better bounds than previous work. We apply our MAT mechanism to obtain a set of improved bounds for a variety of problems including $k$-core decomposition, densest subgraph, low out-degree ordering, and vertex coloring. We give a tight local edge DP algorithm for $k$-core decomposition with $O(\\epsilon^{-1}\\log n)$ additive error and no multiplicative error in $O(n)$ rounds. We also give a new $(2+\\eta)$-factor multiplicative, $O(\\epsilon^{-1}\\log n)$ additive error algorithm in $O(\\log^2 n)$ rounds for any constant $\\eta > 0$. Both of these results are asymptotically tight against our new lower bound of $\\Omega(\\log n)$ for any constant-factor approximation algorithm for $k$-core decomposition. Our new algorithms for $k$-core also directly lead to new algorithms for densest subgraph and low out-degree ordering. Our novel private defective coloring algorithms uses number of colors proportional to the arboricity of the graph.         ",
    "url": "https://arxiv.org/abs/2508.02182",
    "authors": [
      "Laxman Dhulipala",
      "Monika Henzinger",
      "George Z. Li",
      "Quanquan C. Liu",
      "A. R. Sricharan",
      "Leqi Zhu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.02183",
    "title": "Multi-Treatment-DML: Causal Estimation for Multi-Dimensional Continuous Treatments with Monotonicity Constraints in Personal Loan Risk Optimization",
    "abstract": "           Optimizing credit limits, interest rates, and loan terms is crucial for managing borrower risk and lifetime value (LTV) in personal loan platform. However, counterfactual estimation of these continuous, multi-dimensional treatments faces significant challenges: randomized trials are often prohibited by risk controls and long repayment cycles, forcing reliance on biased observational data. Existing causal methods primarily handle binary/discrete treatments and struggle with continuous, multi-dimensional settings. Furthermore, financial domain knowledge mandates provably monotonic treatment-outcome relationships (e.g., risk increases with credit limit).To address these gaps, we propose Multi-Treatment-DML, a novel framework leveraging Double Machine Learning (DML) to: (i) debias observational data for causal effect estimation; (ii) handle arbitrary-dimensional continuous treatments; and (iii) enforce monotonic constraints between treatments and outcomes, guaranteeing adherence to domain this http URL experiments on public benchmarks and real-world industrial datasets demonstrate the effectiveness of our approach. Furthermore, online A/B testing conducted on a realworld personal loan platform, confirms the practical superiority of Multi-Treatment-DML in real-world loan operations.         ",
    "url": "https://arxiv.org/abs/2508.02183",
    "authors": [
      "Kexin Zhao",
      "Bo Wang",
      "Cuiying Zhao",
      "Tongyao Wan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02186",
    "title": "Failure Cases Are Better Learned But Boundary Says Sorry: Facilitating Smooth Perception Change for Accuracy-Robustness Trade-Off in Adversarial Training",
    "abstract": "           Adversarial Training (AT) is one of the most effective methods to train robust Deep Neural Networks (DNNs). However, AT creates an inherent trade-off between clean accuracy and adversarial robustness, which is commonly attributed to the more complicated decision boundary caused by the insufficient learning of hard adversarial samples. In this work, we reveal a counterintuitive fact for the first time: From the perspective of perception consistency, hard adversarial samples that can still attack the robust model after AT are already learned better than those successfully defended. Thus, different from previous views, we argue that it is rather the over-sufficient learning of hard adversarial samples that degrades the decision boundary and contributes to the trade-off problem. Specifically, the excessive pursuit of perception consistency would force the model to view the perturbations as noise and ignore the information within them, which should have been utilized to induce a smoother perception transition towards the decision boundary to support its establishment to an appropriate location. In response, we define a new AT objective named Robust Perception, encouraging the model perception to change smoothly with input perturbations, based on which we propose a novel Robust Perception Adversarial Training (RPAT) method, effectively mitigating the current accuracy-robustness trade-off. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet with ResNet-18, PreActResNet-18, and WideResNet-34-10 demonstrate the effectiveness of our method beyond four common baselines and 12 state-of-the-art (SOTA) works. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02186",
    "authors": [
      "Yanyun Wang",
      "Li Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02202",
    "title": "Self-assessment approach for resource management protocols in heterogeneous computational systems",
    "abstract": "           With an ever growing number of heterogeneous applicational services running on equally heterogeneous computational systems, the problem of resource management becomes more essential. Although current solutions consider some network and time requirements, they mostly handle a pre-defined list of resource types by design and, consequently, fail to provide an extensible solution to assess any other set of requirements or to switch strategies on its resource estimation. This work proposes an heuristics-based estimation solution to support any computational system as a self-assessment, including considerations on dynamically weighting the requirements, how to compute each node's capacity towards an admission request, and also offers the possibility to extend the list of resource types considered for assessment, which is an uncommon view in related works. This algorithm can be used by distributed and centralized resource allocation protocols to decide the best node(s) for a service intended for deployment. This approach was validated across its components and the results show that its performance is straightforward in resource estimation while allowing scalability and extensibility.         ",
    "url": "https://arxiv.org/abs/2508.02202",
    "authors": [
      "Rui Eduardo Lopes",
      "Duarte Raposo",
      "Pedro V. Teixeira",
      "Susana Sargento"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.02204",
    "title": "TacMan-Turbo: Proactive Tactile Control for Robust and Efficient Articulated Object Manipulation",
    "abstract": "           Adept manipulation of articulated objects is essential for robots to operate successfully in human environments. Such manipulation requires both effectiveness -- reliable operation despite uncertain object structures -- and efficiency -- swift execution with minimal redundant steps and smooth actions. Existing approaches struggle to achieve both objectives simultaneously: methods relying on predefined kinematic models lack effectiveness when encountering structural variations, while tactile-informed approaches achieve robust manipulation without kinematic priors but compromise efficiency through reactive, step-by-step exploration-compensation cycles. This paper introduces TacMan-Turbo, a novel proactive tactile control framework for articulated object manipulation that resolves this fundamental trade-off. Unlike previous approaches that treat tactile contact deviations merely as error signals requiring compensation, our method interprets these deviations as rich sources of local kinematic information. This new perspective enables our controller to predict optimal future interactions and make proactive adjustments, significantly enhancing manipulation efficiency. In comprehensive evaluations across 200 diverse simulated articulated objects and real-world experiments, our approach maintains a 100% success rate while significantly outperforming the previous tactile-informed method in time efficiency, action efficiency, and trajectory smoothness (all p-values < 0.0001). These results demonstrate that the long-standing trade-off between effectiveness and efficiency in articulated object manipulation can be successfully resolved without relying on prior kinematic knowledge.         ",
    "url": "https://arxiv.org/abs/2508.02204",
    "authors": [
      "Zihang Zhao",
      "Zhenghao Qi",
      "Yuyang Li",
      "Leiyao Cui",
      "Zhi Han",
      "Lecheng Ruan",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.02210",
    "title": "WhiSQA: Non-Intrusive Speech Quality Prediction Using Whisper Encoder Features",
    "abstract": "           There has been significant research effort developing neural-network-based predictors of SQ in recent years. While a primary objective has been to develop non-intrusive, i.e.~reference-free, metrics to assess the performance of SE systems, recent work has also investigated the direct inference of neural SQ predictors within the loss function of downstream speech tasks. To aid in the training of SQ predictors, several large datasets of audio with corresponding human labels of quality have been created. Recent work in this area has shown that speech representations derived from large unsupervised or semi-supervised foundational speech models are useful input feature representations for neural SQ prediction. In this work, a novel and robust SQ predictor is proposed based on feature representations extracted from an ASR model, found to be a powerful input feature for the SQ prediction task. The proposed system achieves higher correlation with human MOS ratings than recent approaches on all NISQA test sets and shows significantly better domain adaption compared to the commonly used DNSMOS metric.         ",
    "url": "https://arxiv.org/abs/2508.02210",
    "authors": [
      "George Close",
      "Kris Hong",
      "Thomas Hain",
      "Stefan Goetze"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.02216",
    "title": "Data Augmentation for Visualization Design Knowledge Bases",
    "abstract": "           Visualization knowledge bases enable computational reasoning and recommendation over a visualization design space. These systems evaluate design trade-offs using numeric weights assigned to different features (e.g., binning a variable). Feature weights can be learned automatically by fitting a model to a collection of chart pairs, in which one chart is deemed preferable to the other. To date, labeled chart pairs have been drawn from published empirical research results; however, such pairs are not comprehensive, resulting in a training corpus that lacks many design variants and fails to systematically assess potential trade-offs. To improve knowledge base coverage and accuracy, we contribute data augmentation techniques for generating and labeling chart pairs. We present methods to generate novel chart pairs based on design permutations and by identifying under-assessed features -- leading to an expanded corpus with thousands of new chart pairs, now in need of labels. Accordingly, we next compare varied methods to scale labeling efforts to annotate chart pairs, in order to learn updated feature weights. We evaluate our methods in the context of the Draco knowledge base, demonstrating improvements to both feature coverage and chart recommendation performance.         ",
    "url": "https://arxiv.org/abs/2508.02216",
    "authors": [
      "Hyeok Kim",
      "Jeffrey Heer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.02218",
    "title": "Reservoir Computing with Evolved Critical Neural Cellular Automata",
    "abstract": "           Criticality is a behavioral state in dynamical systems that is known to present the highest computation capabilities, i.e., information transmission, storage, and modification. Therefore, such systems are ideal candidates as a substrate for reservoir computing, a subfield in artificial intelligence. Our choice of a substrate is a cellular automaton (CA) governed by an artificial neural network, also known as neural cellular automaton (NCA). We apply evolution strategy to optimize the NCA to achieve criticality, demonstrated by power law distributions in structures called avalanches. With an evolved critical NCA, the substrate is tested for reservoir computing. Our evaluation of the substrate is performed with two benchmarks, 5-bit memory task and image classification of handwritten digits. The result of the 5-bit memory task achieved a perfect score and the system managed to remember all 5 bits. The result for the image classification task matched and sometimes surpassed the performance of the best elementary CA for this task. Moreover, the proposed critical NCA may operate as a self-organized critical system, due to its robustness to extreme initial conditions.         ",
    "url": "https://arxiv.org/abs/2508.02218",
    "authors": [
      "Sidney Pontes-Filho",
      "Stefano Nichele",
      "Mikkel Lepper\u00f8d"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.02230",
    "title": "FedAPTA: Federated Multi-task Learning in Computing Power Networks with Adaptive Layer-wise Pruning and Task-aware Aggregation",
    "abstract": "           Federated Learning (FL) has shown considerable promise in Computing Power Networks (CPNs) for privacy protection, efficient data utilization, and dynamic collaboration. Although it offers practical benefits, applying FL in CPNs continues to encounter a major obstacle, i.e., multi-task deployment. However, existing work mainly focuses on mitigating FL's computation and communication overhead of a single task while overlooking the computing resource wastage issue of heterogeneous devices across multiple tasks in FL under CPNs. To tackle this, we design FedAPTA, a federated multi-task learning framework in CPNs. FedAPTA alleviates computing resource wastage through the developed layer-wise model pruning technique, which reduces local model size while considering both data and device heterogeneity. To aggregate structurally heterogeneous local models of different tasks, we introduce a heterogeneous model recovery strategy and a task-aware model aggregation method that enables the aggregation through infilling local model architecture with the shared global model and clustering local models according to their specific tasks. We deploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL methods. The experimental outcomes demonstrate that the proposed FedAPTA considerably outperforms the state-of-the-art FL methods by up to 4.23%. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02230",
    "authors": [
      "Yachao Yuan",
      "Zhen Yu",
      "Jin Wang",
      "Zhipeng Cheng",
      "Jianhua Hu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.02233",
    "title": "A Methodological Framework for LLM-Based Mining of Software Repositories",
    "abstract": "           Large Language Models (LLMs) are increasingly used in software engineering research, offering new opportunities for automating repository mining tasks. However, despite their growing popularity, the methodological integration of LLMs into Mining Software Repositories (MSR) remains poorly understood. Existing studies tend to focus on specific capabilities or performance benchmarks, providing limited insight into how researchers utilize LLMs across the full research pipeline. To address this gap, we conduct a mixed-method study that combines a rapid review and questionnaire survey in the field of LLM4MSR. We investigate (1) the approaches and (2) the threats that affect the empirical rigor of researchers involved in this field. Our findings reveal 15 methodological approaches, nine main threats, and 25 mitigation strategies. Building on these findings, we present PRIMES 2.0, a refined empirical framework organized into six stages, comprising 23 methodological substeps, each mapped to specific threats and corresponding mitigation strategies, providing prescriptive and adaptive support throughout the lifecycle of LLM-based MSR studies. Our work contributes to establishing a more transparent and reproducible foundation for LLM-based MSR research.         ",
    "url": "https://arxiv.org/abs/2508.02233",
    "authors": [
      "Vincenzo De Martino",
      "Joel Casta\u00f1o",
      "Fabio Palomba",
      "Xavier Franch",
      "Silverio Mart\u00ednez-Fern\u00e1ndez"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.02235",
    "title": "Pigeon-SL: Robust Split Learning Framework for Edge Intelligence under Malicious Clients",
    "abstract": "           Recent advances in split learning (SL) have established it as a promising framework for privacy-preserving, communication-efficient distributed learning at the network edge. However, SL's sequential update process is vulnerable to even a single malicious client, which can significantly degrade model accuracy. To address this, we introduce Pigeon-SL, a novel scheme grounded in the pigeonhole principle that guarantees at least one entirely honest cluster among M clients, even when up to N of them are adversarial. In each global round, the access point partitions the clients into N+1 clusters, trains each cluster independently via vanilla SL, and evaluates their validation losses on a shared dataset. Only the cluster with the lowest loss advances, thereby isolating and discarding malicious updates. We further enhance training and communication efficiency with Pigeon-SL+, which repeats training on the selected cluster to match the update throughput of standard SL. We validate the robustness and effectiveness of our approach under three representative attack models -- label flipping, activation and gradient manipulation -- demonstrating significant improvements in accuracy and resilience over baseline SL methods in future intelligent wireless networks.         ",
    "url": "https://arxiv.org/abs/2508.02235",
    "authors": [
      "Sangjun Park",
      "Tony Q.S. Quek",
      "Hyowoon Seo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.02283",
    "title": "An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance Fraud Detection with Explainable AI",
    "abstract": "           In insurance fraud prediction, handling class imbalance remains a critical challenge. This paper presents a novel multistage focal loss function designed to enhance the performance of machine learning models in such imbalanced settings by helping to escape local minima and converge to a good solution. Building upon the foundation of the standard focal loss, our proposed approach introduces a dynamic, multi-stage convex and nonconvex mechanism that progressively adjusts the focus on hard-to-classify samples across training epochs. This strategic refinement facilitates more stable learning and improved discrimination between fraudulent and legitimate cases. Through extensive experimentation on a real-world insurance dataset, our method achieved better performance than the traditional focal loss, as measured by accuracy, precision, F1-score, recall and Area Under the Curve (AUC) metrics on the auto insurance dataset. These results demonstrate the efficacy of the multistage focal loss in boosting model robustness and predictive accuracy in highly skewed classification tasks, offering significant implications for fraud detection systems in the insurance industry. An explainable model is included to interpret the results.         ",
    "url": "https://arxiv.org/abs/2508.02283",
    "authors": [
      "Francis Boabang",
      "Samuel Asante Gyamerah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)",
      "Risk Management (q-fin.RM)"
    ]
  },
  {
    "id": "arXiv:2508.02287",
    "title": "Framework for Robust Motion Planning of Tethered Multi-Robot Systems in Marine Environments",
    "abstract": "           This paper introduces CoralGuide, a novel framework designed for path planning and trajectory optimization for tethered multi-robot systems. We focus on marine robotics, which commonly have tethered configurations of an Autonomous Surface Vehicle (ASV) and an Autonomous Underwater Vehicle (AUV). CoralGuide provides safe navigation in marine environments by enhancing the A* algorithm with specialized heuristics tailored for tethered ASV-AUV systems. Our method integrates catenary curve modelling for tether management and employs Bezier curve interpolation for smoother trajectory planning, ensuring efficient and synchronized operations without compromising safety. Through simulations and real-world experiments, we have validated CoralGuides effectiveness in improving path planning and trajectory optimization, demonstrating its potential to significantly enhance operational capabilities in marine research and infrastructure inspection.         ",
    "url": "https://arxiv.org/abs/2508.02287",
    "authors": [
      "Markus Buchholz",
      "Ignacio Carlucho",
      "Zebin Huang",
      "Michele Grimaldi",
      "Pierre Nicolay",
      "Sumer Tuncay",
      "Yvan R. Petillot"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.02288",
    "title": "Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection",
    "abstract": "           3D object detection is essential for autonomous systems, enabling precise localization and dimension estimation. While LiDAR and RGB cameras are widely used, their fixed frame rates create perception gaps in high-speed scenarios. Event cameras, with their asynchronous nature and high temporal resolution, offer a solution by capturing motion continuously. The recent approach, which integrates event cameras with conventional sensors for continuous-time detection, struggles in fast-motion scenarios due to its dependency on synchronized sensors. We propose a novel stereo 3D object detection framework that relies solely on event cameras, eliminating the need for conventional 3D sensors. To compensate for the lack of semantic and geometric information in event data, we introduce a dual filter mechanism that extracts both. Additionally, we enhance regression by aligning bounding boxes with object-centric information. Experiments show that our method outperforms prior approaches in dynamic environments, demonstrating the potential of event cameras for robust, continuous-time 3D perception. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02288",
    "authors": [
      "Jae-Young Kang",
      "Hoonhee Cho",
      "Kuk-Jin Yoon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02291",
    "title": "Flexible Automatic Identification and Removal (FAIR)-Pruner: An Efficient Neural Network Pruning Method",
    "abstract": "           Neural network pruning is a critical compression technique that facilitates the deployment of large-scale neural networks on resource-constrained edge devices, typically by identifying and eliminating redundant or insignificant parameters to reduce computational and memory overhead. This paper proposes the Flexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for neural network structured pruning. Specifically, FAIR-Pruner first evaluates the importance of each unit (e.g., neuron or channel) through the Utilization Score quantified by the Wasserstein distance. To reflect the performance degradation after unit removal, it then introduces the Reconstruction Error, which is computed via the Taylor expansion of the loss function. Finally, FAIR-Pruner identifies superfluous units with negligible impact on model performance by controlling the proposed Tolerance of Difference, which measures differences between unimportant units and those that cause performance degradation. A major advantage of FAIR-Pruner lies in its capacity to automatically determine the layer-wise pruning rates, which yields a more efficient subnetwork structure compared to applying a uniform pruning rate. Another advantage of the FAIR-Pruner is its great one-shot performance without post-pruning fine-tuning. Furthermore, with utilization scores and reconstruction errors, users can flexibly obtain pruned models under different pruning ratios. Comprehensive experimental validation on diverse benchmark datasets (e.g., ImageNet) and various neural network architectures (e.g., VGG) demonstrates that FAIR-Pruner achieves significant model compression while maintaining high accuracy.         ",
    "url": "https://arxiv.org/abs/2508.02291",
    "authors": [
      "Chenqing Lin",
      "Mostafa Hussien",
      "Chengyao Yu",
      "Mohamed Cheriet",
      "Osama Abdelrahman",
      "Ruixing Ming"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02293",
    "title": "Towards Real Unsupervised Anomaly Detection Via Confident Meta-Learning",
    "abstract": "           So-called unsupervised anomaly detection is better described as semi-supervised, as it assumes all training data are nominal. This assumption simplifies training but requires manual data curation, introducing bias and limiting adaptability. We propose Confident Meta-learning (CoMet), a novel training strategy that enables deep anomaly detection models to learn from uncurated datasets where nominal and anomalous samples coexist, eliminating the need for explicit filtering. Our approach integrates Soft Confident Learning, which assigns lower weights to low-confidence samples, and Meta-Learning, which stabilizes training by regularizing updates based on training validation loss covariance. This prevents overfitting and enhances robustness to noisy data. CoMet is model-agnostic and can be applied to any anomaly detection method trainable via gradient descent. Experiments on MVTec-AD, VIADUCT, and KSDD2 with two state-of-the-art models demonstrate the effectiveness of our approach, consistently improving over the baseline methods, remaining insensitive to anomalies in the training set, and setting a new state-of-the-art across all datasets.         ",
    "url": "https://arxiv.org/abs/2508.02293",
    "authors": [
      "Muhammad Aqeel",
      "Shakiba Sharifi",
      "Marco Cristani",
      "Francesco Setti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02296",
    "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks",
    "abstract": "           Ensuring safety and in-domain responses for Retrieval-Augmented Generation (RAG) systems is paramount in safety-critical applications, yet remains a significant challenge. To address this, we evaluate four methodologies for Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG system only responds to queries confined to the system's knowledge base. Specifically, our evaluation explores two novel dimensionality reduction and feature separation strategies: \\textit{PCA}, where top components are selected using explained variance or OOD separability, and an adaptation of \\textit{Neural Collapse Feature Separation}. We validate our approach on standard datasets (StackExchange and MSMARCO) and real-world applications (Substance Use and COVID-19), including tests against LLM-simulated and actual attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations of response correctness and relevance, we confirm that an external OOD detector is crucial for maintaining response relevance.         ",
    "url": "https://arxiv.org/abs/2508.02296",
    "authors": [
      "Ilias Triantafyllopoulos",
      "Renyi Qu",
      "Salvatore Giorgi",
      "Brenda Curtis",
      "Lyle H. Ungar",
      "Jo\u00e3o Sedoc"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.02300",
    "title": "Research Knowledge Graphs in NFDI4DataScience: Key Activities, Achievements, and Future Directions",
    "abstract": "           As research in Artificial Intelligence and Data Science continues to grow in volume and complexity, it becomes increasingly difficult to ensure transparency, reproducibility, and discoverability. To address these challenges, as research artifacts should be understandable and usable by machines, the NFDI4DataScience consortium is developing and providing Research Knowledge Graphs (RKGs). Building upon earlier works, this paper presents recent progress in creating semantically rich RKGs using standardized ontologies, shared vocabularies, and automated Information Extraction techniques. Key achievements include the development of the NFDI4DS ontology, metadata standards, tools, and services designed to support the FAIR principles, as well as community-led projects and various implementations of RKGs. Together, these efforts aim to capture and connect the complex relationships between datasets, models, software, and scientific publications.         ",
    "url": "https://arxiv.org/abs/2508.02300",
    "authors": [
      "Kanishka Silva",
      "Marcel R. Ackermann",
      "Heike Fliegl",
      "Genet-Asefa Gesese",
      "Fidan Limani",
      "Philipp Mayr",
      "Peter Mutschke",
      "Allard Oelen",
      "Muhammad Asif Suryani",
      "Sharmila Upadhyaya",
      "Benjamin Zapilko",
      "Harald Sack",
      "Stefan Dietze"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.02304",
    "title": "ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering",
    "abstract": "           Neural Radiance Fields (NeRF) offer significant promise for generating photorealistic images and videos. However, existing mainstream neural rendering models often fall short in meeting the demands for immediacy and power efficiency in practical applications. Specifically, these models frequently exhibit irregular access patterns and substantial computational overhead, leading to undesirable inference latency and high power consumption. Computing-in-memory (CIM), an emerging computational paradigm, has the potential to address these access bottlenecks and reduce the power consumption associated with model execution. To bridge the gap between model performance and real-world scene requirements, we propose an algorithm-architecture co-design approach, abbreviated as ASDR, a CIM-based accelerator supporting efficient neural rendering. At the algorithmic level, we propose two rendering optimization schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of different pixels, thus reducing access memory and computational overhead. (2) Reducing MLP overhead by decoupling and approximating the volume rendering of color and density. At the architecture level, we design an efficient ReRAM-based CIM architecture with efficient data mapping and reuse microarchitecture. Experiments demonstrate that our design can achieve up to $9.55\\times$ and $69.75\\times$ speedup over state-of-the-art NeRF accelerators and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.         ",
    "url": "https://arxiv.org/abs/2508.02304",
    "authors": [
      "Fangxin Liu",
      "Haomin Li",
      "Bowen Zhu",
      "Zongwu Wang",
      "Zhuoran Song",
      "Habing Guan",
      "Li Jiang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2508.02307",
    "title": "Whole-body Representation Learning For Competing Preclinical Disease Risk Assessment",
    "abstract": "           Reliable preclinical disease risk assessment is essential to move public healthcare from reactive treatment to proactive identification and prevention. However, image-based risk prediction algorithms often consider one condition at a time and depend on hand-crafted features obtained through segmentation tools. We propose a whole-body self-supervised representation learning method for the preclinical disease risk assessment under a competing risk modeling. This approach outperforms whole-body radiomics in multiple diseases, including cardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive pulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a preclinical screening scenario and subsequently combining with cardiac MRI, it sharpens further the prediction for CVD subgroups: ischemic heart disease (IHD), hypertensive diseases (HD), and stroke. The results indicate the translational potential of whole-body representations as a standalone screening modality and as part of a multi-modal framework within clinical workflows for early personalized risk stratification. The code is available at this https URL ",
    "url": "https://arxiv.org/abs/2508.02307",
    "authors": [
      "Dmitrii Seletkov",
      "Sophie Starck",
      "Ayhan Can Erdur",
      "Yundi Zhang",
      "Daniel Rueckert",
      "Rickmer Braren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02320",
    "title": "Zero-shot Compositional Action Recognition with Neural Logic Constraints",
    "abstract": "           Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen verb-object compositions in the videos by exploiting the learned knowledge of verb and object primitives during training. Despite compositional learning's progress in ZS-CAR, two critical challenges persist: 1) Missing compositional structure constraint, leading to spurious correlations between primitives; 2) Neglecting semantic hierarchy constraint, leading to semantic ambiguity and impairing the training process. In this paper, we argue that human-like symbolic reasoning offers a principled solution to these challenges by explicitly modeling compositional and hierarchical structured abstraction. To this end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates dual symbolic constraints: Explicit Compositional Logic and Hierarchical Primitive Logic. Specifically, the former models the restrictions within the compositions, enhancing the compositional reasoning ability of our model. The latter investigates the semantical dependencies among different primitives, empowering the models with fine-to-coarse reasoning capacity. By formalizing these constraints in first-order logic and embedding them into neural network architectures, LogicCAR systematically bridges the gap between symbolic abstraction and existing models. Extensive experiments on the Sth-com dataset demonstrate that our LogicCAR outperforms existing baseline methods, proving the effectiveness of our logic-driven constraints.         ",
    "url": "https://arxiv.org/abs/2508.02320",
    "authors": [
      "Gefan Ye",
      "Lin Li",
      "Kexin Li",
      "Jun Xiao",
      "Long chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02339",
    "title": "Correspondence-Free Fast and Robust Spherical Point Pattern Registration",
    "abstract": "           Existing methods for rotation estimation between two spherical ($\\mathbb{S}^2$) patterns typically rely on spherical cross-correlation maximization between two spherical function. However, these approaches exhibit computational complexities greater than cubic $O(n^3)$ with respect to rotation space discretization and lack extensive evaluation under significant outlier contamination. To this end, we propose a rotation estimation algorithm between two spherical patterns with linear time complexity $O(n)$. Unlike existing spherical-function-based methods, we explicitly represent spherical patterns as discrete 3D point sets on the unit sphere, reformulating rotation estimation as a spherical point-set alignment (i.e., Wahba problem for 3D unit vectors). Given the geometric nature of our formulation, our spherical pattern alignment algorithm naturally aligns with the Wahba problem framework for 3D unit vectors. Specifically, we introduce three novel algorithms: (1) SPMC (Spherical Pattern Matching by Correlation), (2) FRS (Fast Rotation Search), and (3) a hybrid approach (SPMC+FRS) that combines the advantages of the previous two methods. Our experiments demonstrate that in the $\\mathbb{S}^2$ domain and in correspondence-free settings, our algorithms are over 10x faster and over 10x more accurate than current state-of-the-art methods for the Wahba problem with outliers. We validate our approach through extensive simulations on a new dataset of spherical patterns, the ``Robust Vector Alignment Dataset. \"Furthermore, we adapt our methods to two real-world tasks: (i) Point Cloud Registration (PCR) and (ii) rotation estimation for spherical images.         ",
    "url": "https://arxiv.org/abs/2508.02339",
    "authors": [
      "Anik Sarker",
      "Alan T. Asbeck"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.02373",
    "title": "On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)",
    "abstract": "           Future networks, such as 6G, will need to support a vast and diverse range of interconnected devices and applications, each with its own set of requirements. While traditional network management approaches will suffice, an automated solutions are becoming a must. However, network automation frameworks are prone to errors, and often they employ ML-based techniques that require training to learn how the network can be optimized. In this sense, network digital twins are a useful tool that allows for the simulation, testing, and training of AI models without affecting the real-world networks and users. This paper presents an AI-based Network Digital Twin (AI-NDT) that leverages a multi-layered knowledge graph architecture and graph neural networks to predict network metrics that directly affect the quality of experience of users. An evaluation of the four most prominent Graph Neural Networks (GNN) architectures was conducted to assess their effectiveness in developing network digital twins. We trained the digital twin on publicly available measurement data from RIPE Atlas, therefore obtaining results close to what is expected in real-world applications. The results show that among the four architectures evaluated, GraphTransformer presents the best performance. However, other architectures might fit better in scenarios where shorter training time is important, while also delivering acceptable results. The results of this work are indicative of what might become common practice for proactive network management, offering a scalable and accurate solution aligned with the requirements of the next-generation networks.         ",
    "url": "https://arxiv.org/abs/2508.02373",
    "authors": [
      "Iulisloi Zacarias",
      "Oussama Ben Taarit",
      "Admela Jukan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.02379",
    "title": "USRN Discovery Pilot: Increasing the Discoverability of Open Access Content Through a National Network",
    "abstract": "           This paper presents the results of the USRN Discovery Pilot Project, a collaboration of SPARC, the Confederation of Open Access Repositories (COAR), CORE and Antleaf, to enhance the discoverability of research papers in US repositories leveraging CORE as an indexing service for USRN repositories. The project conducted actions in three strategic areas: Assessing and quantitatively measuring discoverability and barriers to it at the beginning and end of the pilot project, conducting interventions to increase discoverability, and supporting interventions by technology and guidelines (provided by CORE services), to minimise effort and maximise effect. The key results of the project include: Around three-quarters of a million research outputs held in the selected US repositories have been made discoverable (a 50% increase) compared to the year before; The project has made available the CORE Data Provider's Guide as well as a selection of new and improved tools to support repositories in increasing their discoverability. These include the CORE Reindexing Button and Index Notification modules, Fresh Finds and the USRN Desirable Characteristics for Digital Publication Repositories checking tool. The project team is now exploring ways to scale out this work to include more repositories.         ",
    "url": "https://arxiv.org/abs/2508.02379",
    "authors": [
      "Petr Knoth",
      "Paul Walk",
      "Matteo Cancellieri",
      "Micheal Upshall",
      "Halyna Torchylo",
      "Jennifer Beamer",
      "Kathleen Shearer",
      "Heather Joseph"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2508.02383",
    "title": "Graph Embedding in the Graph Fractional Fourier Transform Domain",
    "abstract": "           Spectral graph embedding plays a critical role in graph representation learning by generating low-dimensional vector representations from graph spectral information. However, the embedding space of traditional spectral embedding methods often exhibit limited expressiveness, failing to exhaustively capture latent structural features across alternative transform domains. To address this issue, we use the graph fractional Fourier transform to extend the existing state-of-the-art generalized frequency filtering embedding (GEFFE) into fractional domains, giving birth to the generalized fractional filtering embedding (GEFRFE), which enhances embedding informativeness via the graph fractional domain. The GEFRFE leverages graph fractional domain filtering and a nonlinear composition of eigenvector components derived from a fractionalized graph Laplacian. To dynamically determine the fractional order, two parallel strategies are introduced: search-based optimization and a ResNet18-based adaptive learning. Extensive experiments on six benchmark datasets demonstrate that the GEFRFE captures richer structural features and significantly enhance classification performance. Notably, the proposed method retains computational complexity comparable to GEFFE approaches.         ",
    "url": "https://arxiv.org/abs/2508.02383",
    "authors": [
      "Changjie Sheng",
      "Zhichao Zhang",
      "Wei Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.02386",
    "title": "Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection",
    "abstract": "           We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised instance segmentation and object detection. COLER first uses our developed CutOnce to generate coarse pseudo labels, then enables the detector to learn from these masks. CutOnce applies Normalized Cut only once and does not rely on any clustering methods, but it can generate multiple object masks in an image. We have designed several novel yet simple modules that not only allow CutOnce to fully leverage the object discovery capabilities of self-supervised models, but also free it from reliance on mask post-processing. During training, COLER achieves strong performance without requiring specially designed loss functions for pseudo labels, and its performance is further improved through self-training. COLER is a zero-shot unsupervised model that outperforms previous state-of-the-art methods on multiple this http URL believe our method can help advance the field of unsupervised object localization.         ",
    "url": "https://arxiv.org/abs/2508.02386",
    "authors": [
      "Xingyu Feng",
      "Hebei Gao",
      "Hong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02413",
    "title": "Improving Knowledge Graph Understanding with Contextual Views",
    "abstract": "           Navigating, visualizing, and discovery in graph data is frequently a difficult prospect. This is especially true for knowledge graphs (KGs), due to high number of possible labeled connections to other data. However, KGs are frequently equipped with an ontology as a schema. That is, it informs how the relationships between data may be constrained. This additional information can be leveraged to improve how (knowledge) graph data can be navigated, visualized, or otherwise utilized in a discovery process. In this manuscript, we introduce the Interactive Knowledge (InK) Browser. This tool specifically takes advantage ontological information (i.e., knowledge) when found in KGs. Specifically, we use modular views that provide various perspectives over the graph, including an interactive schema view, data listings based on type, neighborhood connections, and geospatial depiction (where appropriate). For this manuscript, we have evaluated the basic premise of this tool over a user group ($n= With this grown user survey, we continue to evaluate how scalable tools, including flexible views, can make KG exploration easier for a range of applications.)         ",
    "url": "https://arxiv.org/abs/2508.02413",
    "authors": [
      "Antrea Christou",
      "Cogan Shimizu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.02425",
    "title": "Multi-Class Human/Object Detection on Robot Manipulators using Proprioceptive Sensing",
    "abstract": "           In physical human-robot collaboration (pHRC) settings, humans and robots collaborate directly in shared environments. Robots must analyze interactions with objects to ensure safety and facilitate meaningful workflows. One critical aspect is human/object detection, where the contacted object is identified. Past research introduced binary machine learning classifiers to distinguish between soft and hard objects. This study improves upon those results by evaluating three-class human/object detection models, offering more detailed contact analysis. A dataset was collected using the Franka Emika Panda robot manipulator, exploring preprocessing strategies for time-series analysis. Models including LSTM, GRU, and Transformers were trained on these datasets. The best-performing model achieved 91.11\\% accuracy during real-time testing, demonstrating the feasibility of multi-class detection models. Additionally, a comparison of preprocessing strategies suggests a sliding window approach is optimal for this task.         ",
    "url": "https://arxiv.org/abs/2508.02425",
    "authors": [
      "Justin Hehli",
      "Marco Heiniger",
      "Maryam Rezayati",
      "Hans Wernher van de Venn"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02426",
    "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding",
    "abstract": "           Since knowledge graphs (KG) will continue to evolve in real scenarios, traditional KGE models are only suitable for static knowledge graphs. Therefore, continual knowledge graph embedding (CKGE) has attracted the attention of researchers. Currently, a key challenge facing CKGE is that the model is prone to \"catastrophic forgetting\", resulting in the loss of previously learned knowledge. In order to effectively alleviate this problem, we propose a new CKGE model BAKE. First, we note that the Bayesian posterior update principle provides a natural continual learning strategy that is insensitive to data order and can theoretically effectively resist the forgetting of previous knowledge during data evolution. Different from the existing CKGE method, BAKE regards each batch of new data as a Bayesian update of the model prior. Under this framework, as long as the posterior distribution of the model is maintained, the model can better preserve the knowledge of early snapshots even after evolving through multiple time snapshots. Secondly, we propose a continual clustering method for CKGE, which further directly combats knowledge forgetting by constraining the evolution difference (or change amplitude) between new and old knowledge between different snapshots. We conduct extensive experiments on BAKE on multiple datasets, and the results show that BAKE significantly outperforms existing baseline models.         ",
    "url": "https://arxiv.org/abs/2508.02426",
    "authors": [
      "Linyu Li",
      "Zhi Jin",
      "Yuanpeng He",
      "Dongming Jin",
      "Yichi Zhang",
      "Haoran Duan",
      "Nyima Tash"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02427",
    "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models",
    "abstract": "           Composable AI offers a scalable and effective paradigm for tackling complex AI tasks by decomposing them into sub-tasks and solving each sub-task using ready-to-use well-trained models. However, systematically evaluating methods under this setting remains largely unexplored. In this paper, we introduce CABENCH, the first public benchmark comprising 70 realistic composable AI tasks, along with a curated pool of 700 models across multiple modalities and domains. We also propose an evaluation framework to enable end-to-end assessment of composable AI solutions. To establish initial baselines, we provide human-designed reference solutions and compare their performance with two LLM-based approaches. Our results illustrate the promise of composable AI in addressing complex real-world problems while highlighting the need for methods that can fully unlock its potential by automatically generating effective execution pipelines.         ",
    "url": "https://arxiv.org/abs/2508.02427",
    "authors": [
      "Tung-Thuy Pham",
      "Duy-Quan Luong",
      "Minh-Quan Duong",
      "Trung-Hieu Nguyen",
      "Thu-Trang Nguyen",
      "Son Nguyen",
      "Hieu Dinh Vo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.02439",
    "title": "Glioblastoma Overall Survival Prediction With Vision Transformers",
    "abstract": "           Glioblastoma is one of the most aggressive and common brain tumors, with a median survival of 10-15 months. Predicting Overall Survival (OS) is critical for personalizing treatment strategies and aligning clinical decisions with patient outcomes. In this study, we propose a novel Artificial Intelligence (AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images, exploiting Vision Transformers (ViTs) to extract hidden features directly from MRI images, eliminating the need of tumor segmentation. Unlike traditional approaches, our method simplifies the workflow and reduces computational resource requirements. The proposed model was evaluated on the BRATS dataset, reaching an accuracy of 62.5% on the test set, comparable to the top-performing methods. Additionally, it demonstrated balanced performance across precision, recall, and F1 score, overcoming the best model in these metrics. The dataset size limits the generalization of the ViT which typically requires larger datasets compared to convolutional neural networks. This limitation in generalization is observed across all the cited studies. This work highlights the applicability of ViTs for downsampled medical imaging tasks and establishes a foundation for OS prediction models that are computationally efficient and do not rely on segmentation.         ",
    "url": "https://arxiv.org/abs/2508.02439",
    "authors": [
      "Yin Lin",
      "iccardo Barbieri",
      "Domenico Aquino",
      "Giuseppe Lauria",
      "Marina Grisoli",
      "Elena De Momi",
      "Alberto Redaelli",
      "Simona Ferrante"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02455",
    "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs",
    "abstract": "           Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.         ",
    "url": "https://arxiv.org/abs/2508.02455",
    "authors": [
      "Daniele Cipollone",
      "Egor Bogomolov",
      "Arie van Deursen",
      "Maliheh Izadi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.02460",
    "title": "InfoSyncNet: Information Synchronization Temporal Convolutional Network for Visual Speech Recognition",
    "abstract": "           Estimating spoken content from silent videos is crucial for applications in Assistive Technology (AT) and Augmented Reality (AR). However, accurately mapping lip movement sequences in videos to words poses significant challenges due to variability across sequences and the uneven distribution of information within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform sequence modeling network enhanced by tailored data augmentation techniques. Central to InfoSyncNet is a non-uniform quantization module positioned between the encoder and decoder, enabling dynamic adjustment to the network's focus and effectively handling the natural inconsistencies in visual speech data. Additionally, multiple training strategies are incorporated to enhance the model's capability to handle variations in lighting and the speaker's orientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm the superiority of InfoSyncNet, achieving new state-of-the-art accuracies of 92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).         ",
    "url": "https://arxiv.org/abs/2508.02460",
    "authors": [
      "Junxiao Xue",
      "Xiaozhen Liu",
      "Xuecheng Wu",
      "Fei Yu",
      "Jun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02477",
    "title": "Multi-class Image Anomaly Detection for Practical Applications: Requirements and Robust Solutions",
    "abstract": "           Recent advances in image anomaly detection have extended unsupervised learning-based models from single-class settings to multi-class frameworks, aiming to improve efficiency in training time and model storage. When a single model is trained to handle multiple classes, it often underperforms compared to class-specific models in terms of per-class detection accuracy. Accordingly, previous studies have primarily focused on narrowing this performance gap. However, the way class information is used, or not used, remains a relatively understudied factor that could influence how detection thresholds are defined in multi-class image anomaly detection. These thresholds, whether class-specific or class-agnostic, significantly affect detection outcomes. In this study, we identify and formalize the requirements that a multi-class image anomaly detection model must satisfy under different conditions, depending on whether class labels are available during training and evaluation. We then re-examine existing methods under these criteria. To meet these challenges, we propose Hierarchical Coreset (HierCore), a novel framework designed to satisfy all defined requirements. HierCore operates effectively even without class labels, leveraging a hierarchical memory bank to estimate class-wise decision criteria for anomaly detection. We empirically validate the applicability and robustness of existing methods and HierCore under four distinct scenarios, determined by the presence or absence of class labels in the training and evaluation phases. The experimental results demonstrate that HierCore consistently meets all requirements and maintains strong, stable performance across all settings, highlighting its practical potential for real-world multi-class anomaly detection tasks.         ",
    "url": "https://arxiv.org/abs/2508.02477",
    "authors": [
      "Jaehyuk Heo",
      "Pilsung Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02479",
    "title": "Fine-grained Multiple Supervisory Network for Multi-modal Manipulation Detecting and Grounding",
    "abstract": "           The task of Detecting and Grounding Multi-Modal Media Manipulation (DGM$^4$) is a branch of misinformation detection. Unlike traditional binary classification, it includes complex subtasks such as forgery content localization and forgery method classification. Consider that existing methods are often limited in performance due to neglecting the erroneous interference caused by unreliable unimodal data and failing to establish comprehensive forgery supervision for mining fine-grained tampering traces. In this paper, we present a Fine-grained Multiple Supervisory (FMS) network, which incorporates modality reliability supervision, unimodal internal supervision and cross-modal supervision to provide comprehensive guidance for DGM$^4$ detection. For modality reliability supervision, we propose the Multimodal Decision Supervised Correction (MDSC) module. It leverages unimodal weak supervision to correct the multi-modal decision-making process. For unimodal internal supervision, we propose the Unimodal Forgery Mining Reinforcement (UFMR) module. It amplifies the disparity between real and fake information within unimodal modality from both feature-level and sample-level perspectives. For cross-modal supervision, we propose the Multimodal Forgery Alignment Reasoning (MFAR) module. It utilizes soft-attention interactions to achieve cross-modal feature perception from both consistency and inconsistency perspectives, where we also design the interaction constraints to ensure the interaction quality. Extensive experiments demonstrate the superior performance of our FMS compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2508.02479",
    "authors": [
      "Xinquan Yu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02485",
    "title": "Federated Graph Unlearning",
    "abstract": "           The demand for data privacy has led to the development of frameworks like Federated Graph Learning (FGL), which facilitate decentralized model training. However, a significant operational challenge in such systems is adhering to the right to be forgotten. This principle necessitates robust mechanisms for two distinct types of data removal: the selective erasure of specific entities and their associated knowledge from local subgraphs and the wholesale removal of a user's entire dataset and influence. Existing methods often struggle to fully address both unlearning requirements, frequently resulting in incomplete data removal or the persistence of residual knowledge within the system. This work introduces a unified framework, conceived to provide a comprehensive solution to these challenges. The proposed framework employs a bifurcated strategy tailored to the specific unlearning request. For fine-grained Meta Unlearning, it uses prototype gradients to direct the initial local forgetting process, which is then refined by generating adversarial graphs to eliminate any remaining data traces among affected clients. In the case of complete client unlearning, the framework utilizes adversarial graph generation exclusively to purge the departed client's contributions from the remaining network. Extensive experiments on multiple benchmark datasets validate the proposed approach. The framework achieves substantial improvements in model prediction accuracy across both client and meta-unlearning scenarios when compared to existing methods. Furthermore, additional studies confirm its utility as a plug-in module, where it materially enhances the predictive capabilities and unlearning effectiveness of other established methods.         ",
    "url": "https://arxiv.org/abs/2508.02485",
    "authors": [
      "Yuming Ai",
      "Xunkai Li",
      "Jiaqi Chao",
      "Bowen Fan",
      "Zhengyu Wu",
      "Yinlin Zhu",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02510",
    "title": "On Distributional Dependent Performance of Classical and Neural Routing Solvers",
    "abstract": "           Neural Combinatorial Optimization aims to learn to solve a class of combinatorial problems through data-driven methods and notably through employing neural networks by learning the underlying distribution of problem instances. While, so far neural methods struggle to outperform highly engineered problem specific meta-heuristics, this work explores a novel approach to formulate the distribution of problem instances to learn from and, more importantly, plant a structure in the sampled problem instances. In application to routing problems, we generate large problem instances that represent custom base problem instance distributions from which training instances are sampled. The test instances to evaluate the methods on the routing task consist of unseen problems sampled from the underlying large problem instance. We evaluate representative NCO methods and specialized Operation Research meta heuristics on this novel task and demonstrate that the performance gap between neural routing solvers and highly specialized meta-heuristics decreases when learning from sub-samples drawn from a fixed base node distribution.         ",
    "url": "https://arxiv.org/abs/2508.02510",
    "authors": [
      "Daniela Thyssens",
      "Tim Dernedde",
      "Wilson Sentanoe",
      "Lars Schmidt-Thieme"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02516",
    "title": "Engagement Prediction of Short Videos with Large Multimodal Models",
    "abstract": "           The rapid proliferation of user-generated content (UGC) on short-form video platforms has made video engagement prediction increasingly important for optimizing recommendation systems and guiding content creation. However, this task remains challenging due to the complex interplay of factors such as semantic content, visual quality, audio characteristics, and user background. Prior studies have leveraged various types of features from different modalities, such as visual quality, semantic content, background sound, etc., but often struggle to effectively model their cross-feature and cross-modality interactions. In this work, we empirically investigate the potential of large multimodal models (LMMs) for video engagement prediction. We adopt two representative LMMs: VideoLLaMA2, which integrates audio, visual, and language modalities, and Qwen2.5-VL, which models only visual and language modalities. Specifically, VideoLLaMA2 jointly processes key video frames, text-based metadata, and background sound, while Qwen2.5-VL utilizes only key video frames and text-based metadata. Trained on the SnapUGC dataset, both models demonstrate competitive performance against state-of-the-art baselines, showcasing the effectiveness of LMMs in engagement prediction. Notably, VideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of audio features in engagement prediction. By ensembling two types of models, our method achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on short-form video engagement prediction. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02516",
    "authors": [
      "Wei Sun",
      "Linhan Cao",
      "Yuqin Cao",
      "Weixia Zhang",
      "Wen Wen",
      "Kaiwei Zhang",
      "Zijian Chen",
      "Fangfang Lu",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02524",
    "title": "Causality and Interpretability for Electrical Distribution System faults",
    "abstract": "           Causal analysis helps us understand variables that are responsible for system failures. This improves fault detection and makes system more reliable. In this work, we present a new method that combines causal inference with machine learning to classify faults in electrical distribution systems (EDS) using graph-based models. We first build causal graphs using transfer entropy (TE). Each fault case is represented as a graph, where the nodes are features such as voltage and current, and the edges demonstrate how these features influence each other. Then, the graphs are classified using machine learning and GraphSAGE where the model learns from both the node values and the structure of the graph to predict the type of fault. To make the predictions understandable, we further developed an integrated approach using GNNExplainer and Captums Integrated Gradients to highlight the nodes (features) that influences the most on the final prediction. This gives us clear insights into the possible causes of the fault. Our experiments show high accuracy: 99.44% on the EDS fault dataset, which is better than state of art models. By combining causal graphs with machine learning, our method not only predicts faults accurately but also helps understand their root causes. This makes it a strong and practical tool for improving system reliability.         ",
    "url": "https://arxiv.org/abs/2508.02524",
    "authors": [
      "Karthik Peddi",
      "Sai Ram Aditya Parisineni",
      "Hemanth Macharla",
      "Mayukha Pal"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02525",
    "title": "Accurate and Interpretable Postmenstrual Age Prediction via Multimodal Large Language Model",
    "abstract": "           Accurate estimation of postmenstrual age (PMA) at scan is crucial for assessing neonatal development and health. While deep learning models have achieved high accuracy in predicting PMA from brain MRI, they often function as black boxes, offering limited transparency and interpretability in clinical decision support. In this work, we address the dual challenge of accuracy and interpretability by adapting a multimodal large language model (MLLM) to perform both precise PMA prediction and clinically relevant explanation generation. We introduce a parameter-efficient fine-tuning (PEFT) strategy using instruction tuning and Low-Rank Adaptation (LoRA) applied to the Qwen2.5-VL-7B model. The model is trained on four 2D cortical surface projection maps derived from neonatal MRI scans. By employing distinct prompts for training and inference, our approach enables the MLLM to handle a regression task during training and generate clinically relevant explanations during inference. The fine-tuned model achieves a low prediction error with a 95 percent confidence interval of 0.78 to 1.52 weeks, while producing interpretable outputs grounded in developmental features, marking a significant step toward transparent and trustworthy AI systems in perinatal neuroscience.         ",
    "url": "https://arxiv.org/abs/2508.02525",
    "authors": [
      "Qifan Chen",
      "Jin Cui",
      "Cindy Duan",
      "Yushuo Han",
      "Yifei Shi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02532",
    "title": "Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction",
    "abstract": "           Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications.         ",
    "url": "https://arxiv.org/abs/2508.02532",
    "authors": [
      "Karan Reddy",
      "Mayukha Pal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02536",
    "title": "ReGate: Enabling Power Gating in Neural Processing Units",
    "abstract": "           The energy efficiency of neural processing units (NPU) is playing a critical role in developing sustainable data centers. Our study with different generations of NPU chips reveals that 30%-72% of their energy consumption is contributed by static power dissipation, due to the lack of power management support in modern NPU chips. In this paper, we present ReGate, which enables fine-grained power-gating of each hardware component in NPU chips with hardware/software co-design. Unlike conventional power-gating techniques for generic processors, enabling power-gating in NPUs faces unique challenges due to the fundamental difference in hardware architecture and program execution model. To address these challenges, we carefully investigate the power-gating opportunities in each component of NPU chips and decide the best-fit power management scheme (i.e., hardware- vs. software-managed power gating). Specifically, for systolic arrays (SAs) that have deterministic execution patterns, ReGate enables cycle-level power gating at the granularity of processing elements (PEs) following the inherent dataflow execution in SAs. For inter-chip interconnect (ICI) and HBM controllers that have long idle intervals, ReGate employs a lightweight hardware-based idle-detection mechanism. For vector units and SRAM whose idle periods vary significantly depending on workload patterns, ReGate extends the NPU ISA and allows software like compilers to manage the power gating. With implementation on a production-level NPU simulator, we show that ReGate can reduce the energy consumption of NPU chips by up to 32.8% (15.5% on average), with negligible impact on AI workload performance. The hardware implementation of power-gating logic introduces less than 3.3% overhead in NPU chips.         ",
    "url": "https://arxiv.org/abs/2508.02536",
    "authors": [
      "Yuqi Xue",
      "Jian Huang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2508.02541",
    "title": "Automatic Identification of Machine Learning-Specific Code Smells",
    "abstract": "           Machine learning (ML) has rapidly grown in popularity, becoming vital to many industries. Currently, the research on code smells in ML applications lacks tools and studies that address the identification and validity of ML-specific code smells. This work investigates suitable methods and tools to design and develop a static code analysis tool (MLpylint) based on code smell criteria. This research employed the Design Science Methodology. In the problem identification phase, a literature review was conducted to identify ML-specific code smells. In solution design, a secondary literature review and consultations with experts were performed to select methods and tools for implementing the tool. We evaluated the tool on data from 160 open-source ML applications sourced from GitHub. We also conducted a static validation through an expert survey involving 15 ML professionals. The results indicate the effectiveness and usefulness of the MLpylint. We aim to extend our current approach by investigating ways to introduce MLpylint seamlessly into development workflows, fostering a more productive and innovative developer environment.         ",
    "url": "https://arxiv.org/abs/2508.02541",
    "authors": [
      "Peter Hamfelt",
      "Ricardo Britto",
      "Lincoln Rocha",
      "Camilo Almendra"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02551",
    "title": "PrivAR: Real-Time Privacy Protection for Location-Based Augmented Reality Applications",
    "abstract": "           Location-based augmented reality (LB-AR) applications, such as Pok\u00e9mon Go, stream sub-second GPS updates to deliver responsive and immersive user experiences. However, this high-frequency location reporting introduces serious privacy risks. Protecting privacy in LB-AR is significantly more challenging than in traditional location-based services (LBS), as it demands real-time location protection with strong per-location and trajectory-level privacy guaranteed while maintaining low latency and high quality of service (QoS). Existing methods fail to meet these combined demands. To fill the gap, we present PrivAR, the first client-side privacy framework for real-time LB-AR. PrivAR introduces two lightweight mechanisms: (i) Planar Staircase Mechanism (PSM) which designs a staircase-shaped distribution to generate noisy location with strong per-location privacy and low expected error; and (ii) Thresholded Reporting with PSM (TR-PSM), a selective scheme that releases a noisy location update only when a displacement exceeds a private threshold, enabling many-to-one mappings for enhanced trace-level privacy while preserving high QoS. We present theoretical analysis, extensive experiments on two public datasets and our proprietary GeoTrace dataset, and validate PrivAR on a Pok\u00e9mon-Go-style prototype. Results show PrivAR improves QoS (Gamescore) by up to 50%, while increasing attacker error by 1.8x over baseline with an additional 0.06 milliseconds runtime overhead.         ",
    "url": "https://arxiv.org/abs/2508.02551",
    "authors": [
      "Shafizur Rahman Seeam",
      "Ye Zheng",
      "Zhengxiong Li",
      "Yidan Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.02552",
    "title": "Blockchain Epidemic Consensus for Large-Scale Networks",
    "abstract": "           Blockchain is a distributed ledger technology that has applications in many domains such as cryptocurrency, smart contracts, supply chain management, and many others. Distributed consensus is a fundamental component of blockchain systems that enables secure, precise, and tamper-proof verification of data without relying on central authorities. Existing consensus protocols, nevertheless, suffer from drawbacks, some of which are related to scalability, resource consumption, and fault tolerance. We introduce Blockchain Epidemic Consensus Protocol (BECP), a novel fully decentralised consensus protocol for blockchain networks at a large scale. BECP follows epidemic communication principles, without fixed roles like validators or leaders, and achieves probabilistic convergence, efficient message dissemination, and tolerance to message delays. We provide an extensive experimental comparison of BECP against classic protocols like PAXOS, RAFT, and PBFT, and newer epidemic-based protocols like Avalanche and Snowman. The findings indicate that BECP provides desirable gains in throughput, consensus latency, and substantial message-passing efficiency compared to existing epidemic-based approaches, validating its usability as an effective and scalable approach for next-generation blockchain systems.         ",
    "url": "https://arxiv.org/abs/2508.02552",
    "authors": [
      "Siamak Abdi",
      "Giuseppe Di Fatta",
      "Atta Badii",
      "Giancarlo Fortino"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.02553",
    "title": "CSI Obfuscation: Single-Antenna Transmitters Can Not Hide from Adversarial Multi-Antenna Radio Localization Systems",
    "abstract": "           The ability of modern telecommunication systems to locate users and objects in the radio environment raises justified privacy concerns. To prevent unauthorized localization, single-antenna transmitters can obfuscate the signal by convolving it with a randomized sequence prior to transmission, which alters the channel state information (CSI) estimated at the receiver. However, this strategy is only effective against CSI-based localization systems deploying single-antenna receivers. Inspired by the concept of blind multichannel identification, we propose a simple CSI recovery method for multi-antenna receivers to extract channel features that ensure reliable user localization regardless of the transmitted signal. We comparatively evaluate the impact of signal obfuscation and the proposed recovery method on the localization performance of CSI fingerprinting, channel charting, and classical triangulation using real-world channel measurements. This work aims to demonstrate the necessity for further efforts to protect the location privacy of users from adversarial radio-based localization systems.         ",
    "url": "https://arxiv.org/abs/2508.02553",
    "authors": [
      "Phillip Stephan",
      "Florian Euchner",
      "Stephan ten Brink"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.02556",
    "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks",
    "abstract": "           Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into overlapping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than transformer-based models, making them well-suited for real-world deployment.         ",
    "url": "https://arxiv.org/abs/2508.02556",
    "authors": [
      "Ali Noori",
      "Pratik Devkota",
      "Somya Mohanty",
      "Prashanti Manda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02583",
    "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge",
    "abstract": "           Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician (\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.         ",
    "url": "https://arxiv.org/abs/2508.02583",
    "authors": [
      "Lei Zan",
      "Keli Zhang",
      "Ruichu Cai",
      "Lujia Pan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02600",
    "title": "Adaptive Riemannian Graph Neural Networks",
    "abstract": "           Graph data often exhibits complex geometric heterogeneity, where structures with varying local curvature, such as tree-like hierarchies and dense communities, coexist within a single network. Existing geometric GNNs, which embed graphs into single fixed-curvature manifolds or discrete product spaces, struggle to capture this diversity. We introduce Adaptive Riemannian Graph Neural Networks (ARGNN), a novel framework that learns a continuous and anisotropic Riemannian metric tensor field over the graph. It allows each node to determine its optimal local geometry, enabling the model to fluidly adapt to the graph's structural landscape. Our core innovation is an efficient parameterization of the node-wise metric tensor, specializing to a learnable diagonal form that captures directional geometric information while maintaining computational tractability. To ensure geometric regularity and stable training, we integrate a Ricci flow-inspired regularization that smooths the learned manifold. Theoretically, we establish the rigorous geometric evolution convergence guarantee for ARGNN and provide a continuous generalization that unifies prior fixed or mixed-curvature GNNs. Empirically, our method demonstrates superior performance on both homophilic and heterophilic benchmark datasets with the ability to capture diverse structures adaptively. Moreover, the learned geometries both offer interpretable insights into the underlying graph structure and empirically corroborate our theoretical analysis.         ",
    "url": "https://arxiv.org/abs/2508.02600",
    "authors": [
      "Xudong Wang",
      "Tongxin Li",
      "Chris Ding",
      "Jicong Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02604",
    "title": "Periodic robust robotic rock chop via virtual model control",
    "abstract": "           Robotic cutting is a challenging contact-rich manipulation task where the robot must simultaneously negotiate unknown object mechanics, large contact forces, and precise motion requirements. We introduce a new virtual-model control scheme that enables knife rocking motion for robot manipulators, without pre-planned trajectories or precise information of the environment. Motion is generated through interconnection with virtual mechanisms, given by virtual springs, dampers, and masses arranged in a suitable way. Through analysis and experiments, we demonstrate that the controlled robot behavior settles into a periodic motion. Experiments with a Franka manipulator demonstrate robust cuts with five different vegetables, and sub-millimeter slice accuracy from 1 mm to 6 mm at nearly one cut per second. The same controller survives changes in knife shape and cutting board height, and adaptation to a different humanoid manipulator, demonstrating robustness and platform independence.         ",
    "url": "https://arxiv.org/abs/2508.02604",
    "authors": [
      "Yi Zhang",
      "Fumiya Iida",
      "Fulvio Forni"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.02609",
    "title": "Entity Representation Learning Through Onsite-Offsite Graph for Pinterset Ads",
    "abstract": "           Graph Neural Networks (GNN) have been extensively applied to industry recommendation systems, as seen in models like GraphSage\\cite{GraphSage}, TwHIM\\cite{TwHIM}, LiGNN\\cite{LiGNN} etc. In these works, graphs were constructed based on users' activities on the platforms, and various graph models were developed to effectively learn node embeddings. In addition to users' onsite activities, their offsite conversions are crucial for Ads models to capture their shopping interest. To better leverage offsite conversion data and explore the connection between onsite and offsite activities, we constructed a large-scale heterogeneous graph based on users' onsite ad interactions and opt-in offsite conversion activities. Furthermore, we introduced TransRA (TransR\\cite{TransR} with Anchors), a novel Knowledge Graph Embedding (KGE) model, to more efficiently integrate graph embeddings into Ads ranking models. However, our Ads ranking models initially struggled to directly incorporate Knowledge Graph Embeddings (KGE), and only modest gains were observed during offline experiments. To address this challenge, we employed the Large ID Embedding Table technique and innovated an attention based KGE finetuning approach within the Ads ranking models. As a result, we observed a significant AUC lift in Click-Through Rate (CTR) and Conversion Rate (CVR) prediction models. Moreover, this framework has been deployed in Pinterest's Ads Engagement Model and contributed to $2.69\\%$ CTR lift and $1.34\\%$ CPC reduction. We believe the techniques presented in this paper can be leveraged by other large-scale industrial models.         ",
    "url": "https://arxiv.org/abs/2508.02609",
    "authors": [
      "Jiayin Jin",
      "Zhimeng Pan",
      "Yang Tang",
      "Jiarui Feng",
      "Kungang Li",
      "Chongyuan Xiang",
      "Jiacheng Li",
      "Runze Su",
      "Siping Ji",
      "Han Sun",
      "Ling Leng",
      "Prathibha Deshikachar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.02611",
    "title": "Meta-RAG on Large Codebases Using Code Summarization",
    "abstract": "           Large Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (AI) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8\\%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance.         ",
    "url": "https://arxiv.org/abs/2508.02611",
    "authors": [
      "Vali Tawosia",
      "Salwa Alamir",
      "Xiaomo Liu",
      "Manuela Veloso"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.02634",
    "title": "Actionable Counterfactual Explanations Using Bayesian Networks and Path Planning with Applications to Environmental Quality Improvement",
    "abstract": "           Counterfactual explanations study what should have changed in order to get an alternative result, enabling end-users to understand machine learning mechanisms with counterexamples. Actionability is defined as the ability to transform the original case to be explained into a counterfactual one. We develop a method for actionable counterfactual explanations that, unlike predecessors, does not directly leverage training data. Rather, data is only used to learn a density estimator, creating a search landscape in which to apply path planning algorithms to solve the problem and masking the endogenous data, which can be sensitive or private. We put special focus on estimating the data density using Bayesian networks, demonstrating how their enhanced interpretability is useful in high-stakes scenarios in which fairness is raising concern. Using a synthetic benchmark comprised of 15 datasets, our proposal finds more actionable and simpler counterfactuals than the current state-of-the-art algorithms. We also test our algorithm with a real-world Environmental Protection Agency dataset, facilitating a more efficient and equitable study of policies to improve the quality of life in United States of America counties. Our proposal captures the interaction of variables, ensuring equity in decisions, as policies to improve certain domains of study (air, water quality, etc.) can be detrimental in others. In particular, the sociodemographic domain is often involved, where we find important variables related to the ongoing housing crisis that can potentially have a severe negative impact on communities.         ",
    "url": "https://arxiv.org/abs/2508.02634",
    "authors": [
      "Enrique Valero-Leal",
      "Pedro Larra\u00f1aga",
      "Concha Bielza"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02657",
    "title": "RC-Gossip: Information Freshness in Clustered Networks with Rate-Changing Gossip",
    "abstract": "           A clustered gossip network is considered in which a source updates its information over time, and end-nodes, organized in clusters through clusterheads, are keeping track of it. The goal for the nodes is to remain as fresh as possible, i.e., have the same information as the source, which we assess by the long-term average binary freshness metric. We introduce a smart mechanism of information dissemination which we coin rate-changing gossip (RC-Gossip). Its main idea is that gossiping is directed towards nodes that need it the most, and hence the rate of gossiping changes based on the number of fresh nodes in the network at a given time. While Stochastic Hybrid System (SHS) analysis has been the norm in studying freshness of gossip networks, we present an equivalent way to analyze freshness using a renewal-reward-based approach. Using that, we show that RC-gossip significantly increases freshness of nodes in different clustered networks, with optimal cluster sizes, compared to traditional gossiping techniques.         ",
    "url": "https://arxiv.org/abs/2508.02657",
    "authors": [
      "Irtiza Hasan",
      "Ahmed Arafa"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.02671",
    "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models",
    "abstract": "           For CLIP-based prompt tuning, introducing more data as additional knowledge for enhancing fine-tuning process is proved to be an effective approach. Existing data amplification strategies for prompt tuning typically rely on external knowledge (e.g., large language models or pre-structured knowledge bases), resulting in higher costs for data collection and processing, while generally ignoring further utilization of features in image modality. To address this, we propose Augmentation-driven Prompt Tuning (AugPT), a self-contained distillation-based prompt tuning approach using only internal augmentation on raw dataset to better exploit known features. Specifically, AugPT employs self-supervised augmentation on unlabeled images in the training set, and introduces a novel gating mechanism based on consensus test, reusing the pre-trained prompt tuning backbone model to spontaneously filter noisy samples, further enhancing the quality of augmented views. Extensive experiments validate that AugPT simultaneously enhances model performance and generalization capability without using appended external knowledge. The code of AugPT is available at: this https URL .         ",
    "url": "https://arxiv.org/abs/2508.02671",
    "authors": [
      "Haoyang Li",
      "Liang Wang",
      "Chao Wang",
      "Siyu Zhou",
      "Jing Jiang",
      "Yan Peng",
      "Guodong Long"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.00891",
    "title": "Accelerating multiparametric quantitative MRI using self-supervised scan-specific implicit neural representation with model reinforcement",
    "abstract": "           Purpose: To develop a self-supervised scan-specific deep learning framework for reconstructing accelerated multiparametric quantitative MRI (qMRI). Methods: We propose REFINE-MORE (REference-Free Implicit NEural representation with MOdel REinforcement), combining an implicit neural representation (INR) architecture with a model reinforcement module that incorporates MR physics constraints. The INR component enables informative learning of spatiotemporal correlations to initialize multiparametric quantitative maps, which are then further refined through an unrolled optimization scheme enforcing data consistency. To improve computational efficiency, REFINE-MORE integrates a low-rank adaptation strategy that promotes rapid model convergence. We evaluated REFINE-MORE on accelerated multiparametric quantitative magnetization transfer imaging for simultaneous estimation of free water spin-lattice relaxation, tissue macromolecular proton fraction, and magnetization exchange rate, using both phantom and in vivo brain data. Results: Under 4x and 5x accelerations on in vivo data, REFINE-MORE achieved superior reconstruction quality, demonstrating the lowest normalized root-mean-square error and highest structural similarity index compared to baseline methods and other state-of-the-art model-based and deep learning approaches. Phantom experiments further showed strong agreement with reference values, underscoring the robustness and generalizability of the proposed framework. Additionally, the model adaptation strategy improved reconstruction efficiency by approximately fivefold. Conclusion: REFINE-MORE enables accurate and efficient scan-specific multiparametric qMRI reconstruction, providing a flexible solution for high-dimensional, accelerated qMRI applications.         ",
    "url": "https://arxiv.org/abs/2508.00891",
    "authors": [
      "Ruimin Feng",
      "Albert Jang",
      "Xingxin He",
      "Fang Liu"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00907",
    "title": "Prime Factorization Equation from a Tensor Network Perspective",
    "abstract": "           This paper presents an exact and explicit equation for prime factorization, along with an algorithm for its computation. The proposed method is based on the MeLoCoToN approach, which addresses combinatorial optimization problems through classical tensor networks. The presented tensor network performs the multiplication of every pair of possible input numbers and selects those whose product is the number to be factorized. Additionally, in order to make the algorithm more efficient, the number and dimension of the tensors and their contraction scheme are optimized. Finally, a series of tests on the algorithm are conducted, contracting the tensor network both exactly and approximately using tensor train compression, and evaluating its performance.         ",
    "url": "https://arxiv.org/abs/2508.00907",
    "authors": [
      "Alejandro Mata Ali",
      "Jorge Mart\u00ednez Mart\u00edn",
      "Sergio Mu\u00f1iz Subi\u00f1as",
      "Miguel Franco Hernando",
      "Javier Sedano",
      "\u00c1ngel Miguel Garc\u00eda-Vico"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Cryptography and Security (cs.CR)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2508.00918",
    "title": "Predictive calibration for digital sun sensors using sparse submanifold convolutional neural networks",
    "abstract": "           Recent developments in AI techniques for space applications mirror the success achieved in terrestrial applications. Machine learning, which excels in data rich environments, is particularly well suited to space-based computer vision applications, such as space optical attitude sensing. Of these sensors, digital sun sensors (DSS) are one of the most common and important sensors for spacecraft attitude determination. The main challenge in using the DSS for attitude estimation are sensor errors, which limit the overall achievable estimation accuracy. However, the traditional sun sensor calibration process is costly, slow, labor-intensive and inefficient. These limitations motivate the use of AI techniques to enable more accurate and efficient DSS calibration. The objective of this work is to develop an end-to-end predictive calibration methodology for digital sun sensors to solve 2-axis state estimates utilizing a sparse submanifold convolutional neural network (SSCNN). We find that the proposed framework can achieve state-of-the-art performance on synthetic data with a mean accuracy of 0.005\u00b0 for the two sun angle estimates. Furthermore, the model is highly capable of implicitly learning complex noise patterns and handling mixed noise types, thereby greatly improving the model robustness and accuracy to real-world applications. The main contributions of this work are: (1) the first application (to our knowledge) of a CNN regression model to the problem of DSS predictive calibration, (2) the introduction of a fused end-to-end training approach for DSS calibration, (3) the creation of a publicly available physics-informed synthetic dataset and simulation for DSS training images, and (4) the evaluation of the performance of the deep learning approach for various mask configurations.         ",
    "url": "https://arxiv.org/abs/2508.00918",
    "authors": [
      "Michael Herman",
      "Olivia J. Pinon Fischer",
      "Dimitri N. Mavris"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.00996",
    "title": "Re-optimization of a deep neural network model for electron-carbon scattering using new experimental data",
    "abstract": "           We present an updated deep neural network model for inclusive electron-carbon scattering. Using the bootstrap model [Phys.Rev.C 110 (2024) 2, 025501] as a prior, we incorporate recent experimental data, as well as older measurements in the deep inelastic scattering region, to derive a re-optimized posterior model. We examine the impact of these new inputs on model predictions and associated uncertainties. Finally, we evaluate the resulting cross-section predictions in the kinematic range relevant to the Hyper-Kamiokande and DUNE experiments.         ",
    "url": "https://arxiv.org/abs/2508.00996",
    "authors": [
      "Beata E. Kowal",
      "Krzysztof M. Graczyk",
      "Artur M. Ankowski",
      "Rwik Dharmapal Banerjee",
      "Jose L. Bonilla",
      "Hemant Prasad",
      "Jan T. Sobczyk"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "Nuclear Experiment (nucl-ex)",
      "Nuclear Theory (nucl-th)"
    ]
  },
  {
    "id": "arXiv:2508.01116",
    "title": "TensoMeta-VQC: A Tensor-Train-Guided Meta-Learning Framework for Robust and Scalable Variational Quantum Computing",
    "abstract": "           Variational Quantum Computing (VQC) faces fundamental barriers in scalability, primarily due to barren plateaus and quantum noise sensitivity. To address these challenges, we introduce TensoMeta-VQC, a novel tensor-train (TT)-guided meta-learning framework designed to improve the robustness and scalability of VQC significantly. Our framework fully delegates the generation of quantum circuit parameters to a classical TT network, effectively decoupling optimization from quantum hardware. This innovative parameterization mitigates gradient vanishing, enhances noise resilience through structured low-rank representations, and facilitates efficient gradient propagation. Based on Neural Tangent Kernel and statistical learning theory, our rigorous theoretical analyses establish strong guarantees on approximation capability, optimization stability, and generalization performance. Extensive empirical results across quantum dot classification, Max-Cut optimization, and molecular quantum simulation tasks demonstrate that TensoMeta-VQC consistently achieves superior performance and robust noise tolerance, establishing it as a principled pathway toward practical and scalable VQC on near-term quantum devices.         ",
    "url": "https://arxiv.org/abs/2508.01116",
    "authors": [
      "Jun Qi",
      "Chao-Han Yang",
      "Pin-Yu Chen",
      "Min-Hsiu Hsieh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.01217",
    "title": "Uncertainty Quantification for Large-Scale Deep Networks via Post-StoNet Modeling",
    "abstract": "           Deep learning has revolutionized modern data science. However, how to accurately quantify the uncertainty of predictions from large-scale deep neural networks (DNNs) remains an unresolved issue. To address this issue, we introduce a novel post-processing approach. This approach feeds the output from the last hidden layer of a pre-trained large-scale DNN model into a stochastic neural network (StoNet), then trains the StoNet with a sparse penalty on a validation dataset and constructs prediction intervals for future observations. We establish a theoretical guarantee for the validity of this approach; in particular, the parameter estimation consistency for the sparse StoNet is essential for the success of this approach. Comprehensive experiments demonstrate that the proposed approach can construct honest confidence intervals with shorter interval lengths compared to conformal methods and achieves better calibration compared to other post-hoc calibration techniques. Additionally, we show that the StoNet formulation provides us with a platform to adapt sparse learning theory and methods from linear models to DNNs.         ",
    "url": "https://arxiv.org/abs/2508.01217",
    "authors": [
      "Yan Sun",
      "Faming Liang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01322",
    "title": "SWAN: Synergistic Wavelet-Attention Network for Infrared Small Target Detection",
    "abstract": "           Infrared small target detection (IRSTD) is thus critical in both civilian and military applications. This study addresses the challenge of precisely IRSTD in complex backgrounds. Recent methods focus fundamental reliance on conventional convolution operations, which primarily capture local spatial patterns and struggle to distinguish the unique frequency-domain characteristics of small targets from intricate background clutter. To overcome these limitations, we proposed the Synergistic Wavelet-Attention Network (SWAN), a novel framework designed to perceive targets from both spatial and frequency domains. SWAN leverages a Haar Wavelet Convolution (HWConv) for a deep, cross-domain fusion of the frequency energy and spatial details of small target. Furthermore, a Shifted Spatial Attention (SSA) mechanism efficiently models long-range spatial dependencies with linear computational complexity, enhancing contextual awareness. Finally, a Residual Dual-Channel Attention (RDCA) module adaptively calibrates channel-wise feature responses to suppress background interference while amplifying target-pertinent signals. Extensive experiments on benchmark datasets demonstrate that SWAN surpasses existing state-of-the-art methods, showing significant improvements in detection accuracy and robustness, particularly in complex challenging scenarios.         ",
    "url": "https://arxiv.org/abs/2508.01322",
    "authors": [
      "Yuxin Jing",
      "Jufeng Zhao",
      "Tianpei Zhang",
      "Yiming Zhu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01341",
    "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
    "abstract": "           Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials. However, because standard training objectives prioritize overall predictive accuracy, these predictions inherently suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy. Existing debiasing methods, such as Prediction-Powered Inference, can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. Here, we introduce and evaluate two correction methods -- linear calibration correction and Tweedie's correction -- that substantially reduce prediction bias without relying on newly collected labeled data. Linear calibration corrects bias through a straightforward linear transformation derived from held-out calibration data, whereas Tweedie's correction leverages empirical Bayes principles to directly address shrinkage-induced biases by exploiting score functions derived from the model's learning patterns. Through analytical exercises and experiments using Demographic and Health Survey data, we demonstrate that the proposed methods meet or outperform existing approaches that either require (a) adjustments to training pipelines or (b) additional labeled data. These approaches may represent a promising avenue for improving the reliability of causal inference when direct outcome measures are limited or unavailable, enabling a \"one map, many trials\" paradigm where a single upstream data creation team produces predictions usable by many downstream teams across diverse ML pipelines.         ",
    "url": "https://arxiv.org/abs/2508.01341",
    "authors": [
      "Markus Pettersson",
      "Connor T. Jerzak",
      "Adel Daoud"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01799",
    "title": "Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery",
    "abstract": "           Accurate prediction of protein-ligand interactions is essential for computer-aided drug discovery. However, existing methods often fail to capture solvent-dependent conformational changes and lack the ability to jointly learn multiple related tasks. To address these limitations, we introduce a pre-training method that incorporates ligand conformational ensembles generated under diverse solvent conditions as augmented input. This design enables the model to learn both structural flexibility and environmental context in a unified manner. The training process integrates molecular reconstruction to capture local geometry, interatomic distance prediction to model spatial relationships, and contrastive learning to build solvent-invariant molecular representations. Together, these components lead to significant improvements, including a 3.7% gain in binding affinity prediction, an 82% success rate on the PoseBusters Astex docking benchmarks, and an area under the curve of 97.1% in virtual screening. The framework supports solvent-aware, multi-task modeling and produces consistent results across benchmarks. A case study further demonstrates sub-angstrom docking accuracy with a root-mean-square deviation of 0.157 angstroms, offering atomic-level insight into binding mechanisms and advancing structure-based drug design.         ",
    "url": "https://arxiv.org/abs/2508.01799",
    "authors": [
      "Jing Lan",
      "Hexiao Ding",
      "Hongzhao Chen",
      "Yufeng Jiang",
      "Ng Nga Chun",
      "Gerald W.Y. Cheng",
      "Zongxi Li",
      "Jing Cai",
      "Liang-ting Lin",
      "Jung Sun Yoo"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01822",
    "title": "Deep Learning-Driven Prediction of Microstructure Evolution via Latent Space Interpolation",
    "abstract": "           Phase-field models accurately simulate microstructure evolution, but their dependence on solving complex differential equations makes them computationally expensive. This work achieves a significant acceleration via a novel deep learning-based framework, utilizing a Conditional Variational Autoencoder (CVAE) coupled with Cubic Spline Interpolation and Spherical Linear Interpolation (SLERP). We demonstrate the method for binary spinodal decomposition by predicting microstructure evolution for intermediate alloy compositions from a limited set of training compositions. First, using microstructures from phase-field simulations of binary spinodal decomposition, we train the CVAE, which learns compact latent representations that encode essential morphological features. Next, we use cubic spline interpolation in the latent space to predict microstructures for any unknown composition. Finally, SLERP ensures smooth morphological evolution with time that closely resembles coarsening. The predicted microstructures exhibit high visual and statistical similarity to phase-field simulations. This framework offers a scalable and efficient surrogate model for microstructure evolution, enabling accelerated materials design and composition optimization.         ",
    "url": "https://arxiv.org/abs/2508.01822",
    "authors": [
      "Sachin Gaikwad",
      "Thejas Kasilingam",
      "Owais Ahmad",
      "Rajdip Mukherjee",
      "Somnath Bhowmick"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.01831",
    "title": "Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing Network for pCR Classification in Magnetic Resonance Images",
    "abstract": "           Accurate breast tumor segmentation in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is important for downstream tasks such as pathological complete response (pCR) assessment. In this work, we address both segmentation and pCR classification using the large-scale MAMA-MIA DCE-MRI dataset. We employ a large-kernel MedNeXt architecture with a two-stage training strategy that expands the receptive field from 3x3x3 to 5x5x5 kernels using the UpKern algorithm. This approach allows stable transfer of learned features to larger kernels, improving segmentation performance on the unseen validation set. An ensemble of large-kernel models achieved a Dice score of 0.67 and a normalized Hausdorff Distance (NormHD) of 0.24. For pCR classification, we trained a self-normalizing network (SNN) on radiomic features extracted from the predicted segmentations and first post-contrast DCE-MRI, reaching an average balanced accuracy of 57\\%, and up to 75\\% in some subgroups. Our findings highlight the benefits of combining larger receptive fields and radiomics-driven classification while motivating future work on advanced ensembling and the integration of clinical variables to further improve performance and generalization. Code: this https URL ",
    "url": "https://arxiv.org/abs/2508.01831",
    "authors": [
      "Toufiq Musah"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.01865",
    "title": "Structure Maintained Representation Learning Neural Network for Causal Inference",
    "abstract": "           Recent developments in causal inference have greatly shifted the interest from estimating the average treatment effect to the individual treatment effect. In this article, we improve the predictive accuracy of representation learning and adversarial networks in estimating individual treatment effects by introducing a structure keeper which maintains the correlation between the baseline covariates and their corresponding representations in the high dimensional space. We train a discriminator at the end of representation layers to trade off representation balance and information loss. We show that the proposed discriminator minimizes an upper bound of the treatment estimation error. We can address the tradeoff between distribution balance and information loss by considering the correlations between the learned representation space and the original covariate feature space. We conduct extensive experiments with simulated and real-world observational data to show that our proposed Structure Maintained Representation Learning (SMRL) algorithm outperforms state-of-the-art methods. We also demonstrate the algorithms on real electronic health record data from the MIMIC-III database.         ",
    "url": "https://arxiv.org/abs/2508.01865",
    "authors": [
      "Yang Sun",
      "Wenbin Lu",
      "Yi-Hui Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2508.02111",
    "title": "Tackling Ill-posedness of Reversible Image Conversion with Well-posed Invertible Network",
    "abstract": "           Reversible image conversion (RIC) suffers from ill-posedness issues due to its forward conversion process being considered an underdetermined system. Despite employing invertible neural networks (INN), existing RIC methods intrinsically remain ill-posed as inevitably introducing uncertainty by incorporating randomly sampled variables. To tackle the ill-posedness dilemma, we focus on developing a reliable approximate left inverse for the underdetermined system by constructing an overdetermined system with a non-zero Gram determinant, thus ensuring a well-posed solution. Based on this principle, we propose a well-posed invertible $1\\times1$ convolution (WIC), which eliminates the reliance on random variable sampling and enables the development of well-posed invertible networks. Furthermore, we design two innovative networks, WIN-Na\u00efve and WIN, with the latter incorporating advanced skip-connections to enhance long-term memory. Our methods are evaluated across diverse RIC tasks, including reversible image hiding, image rescaling, and image decolorization, consistently achieving state-of-the-art performance. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to overcome the bottlenecks of existing RIC solutions and setting a new benchmark in the field. Codes are available in this https URL.         ",
    "url": "https://arxiv.org/abs/2508.02111",
    "authors": [
      "Yuanfei Huang",
      "Hua Huang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.02164",
    "title": "Distributed Constraint-coupled Resource Allocation: Anytime Feasibility and Violation Robustness",
    "abstract": "           This paper considers distributed resource allocation problems (DRAPs) with a coupled constraint for real-time systems. Based on primal-dual methods, we adopt a control perspective for optimization algorithm design by synthesizing a safe feedback controller using control barrier functions to enforce constraint satisfaction. On this basis, a distributed anytime-feasible resource allocation (DanyRA) algorithm is proposed. It is shown that DanyRA algorithm converges to the exact optimal solution of DRAPs while ensuring feasibility of the coupled inequality constraint at all time steps. Considering constraint violation arises from potential external interferences, a virtual queue with minimum buffer is incorporated to restore the constraint satisfaction before the pre-defined deadlines. We characterize the trade-off between convergence accuracy and violation robustness for maintaining or recovering feasibility. DanyRA algorithm is further extended to address DRAPs with a coupled equality constraint, and its linear convergence rate is theoretically established. Finally, a numerical example is provided for verification.         ",
    "url": "https://arxiv.org/abs/2508.02164",
    "authors": [
      "Wenwen Wu",
      "Shanying Zhu",
      "Cailian Chen",
      "Xinping Guan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.02250",
    "title": "Solving Sudoku Using Oscillatory Neural Networks",
    "abstract": "           This paper explores the application of Oscillatory Neural Networks (ONNs) to solving Sudoku puzzles, presenting a biologically inspired approach based on phase synchronization. Each cell is represented by an oscillator whose phase encodes a digit, and the synchronization is governed by the Kuramoto model. The system dynamically evolves towards a valid solution by having the puzzle constraints encoded into the weight matrix of the network, and through a proposed novel phase mapping of the Sudoku digits. Experimental results show that ONNs achieve high performance for puzzles with moderate difficulty and outperform Hopfield Neural Networks, particularly in cases with up to 20 initially unknown values. Although the performance decreases with increased ambiguity, ONNs still produce correct solutions in some of the iterations, cases in which the baseline Hopfield Neural Network algorithm fails. The findings support ONNs as a viable alternative for solving constraint optimization problems and reinforce their relevance within emerging non-von Neumann computing paradigms.         ",
    "url": "https://arxiv.org/abs/2508.02250",
    "authors": [
      "Stefan Porfir",
      "Bram F. Haverkort",
      "Federico Sbravati",
      "Aida Todri-Sanial"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2508.02295",
    "title": "Reference-free Adversarial Sex Obfuscation in Speech",
    "abstract": "           Sex conversion in speech involves privacy risks from data collection and often leaves residual sex-specific cues in outputs, even when target speaker references are unavailable. We introduce RASO for Reference-free Adversarial Sex Obfuscation. Innovations include a sex-conditional adversarial learning framework to disentangle linguistic content from sex-related acoustic markers and explicit regularisation to align fundamental frequency distributions and formant trajectories with sex-neutral characteristics learned from sex-balanced training data. RASO preserves linguistic content and, even when assessed under a semi-informed attack model, it significantly outperforms a competing approach to sex obfuscation.         ",
    "url": "https://arxiv.org/abs/2508.02295",
    "authors": [
      "Yangyang Qu",
      "Michele Panariello",
      "Massimiliano Todisco",
      "Nicholas Evans"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2508.02422",
    "title": "Superior resilience to poisoning and amenability to unlearning in quantum machine learning",
    "abstract": "           The reliability of artificial intelligence hinges on the integrity of its training data, a foundation often compromised by noise and corruption. Here, through a comparative study of classical and quantum neural networks on both classical and quantum data, we reveal a fundamental difference in their response to data corruption. We find that classical models exhibit brittle memorization, leading to a failure in generalization. In contrast, quantum models demonstrate remarkable resilience, which is underscored by a phase transition-like response to increasing label noise, revealing a critical point beyond which the model's performance changes qualitatively. We further establish and investigate the field of quantum machine unlearning, the process of efficiently forcing a trained model to forget corrupting influences. We show that the brittle nature of the classical model forms rigid, stubborn memories of erroneous data, making efficient unlearning challenging, while the quantum model is significantly more amenable to efficient forgetting with approximate unlearning methods. Our findings establish that quantum machine learning can possess a dual advantage of intrinsic resilience and efficient adaptability, providing a promising paradigm for the trustworthy and robust artificial intelligence of the future.         ",
    "url": "https://arxiv.org/abs/2508.02422",
    "authors": [
      "Yu-Qin Chen",
      "Shi-Xin Zhang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02641",
    "title": "FastCSP: Accelerated Molecular Crystal Structure Prediction with Universal Model for Atoms",
    "abstract": "           Crystal Structure Prediction (CSP) of molecular crystals plays a central role in applications, such as pharmaceuticals and organic electronics. CSP is challenging and computationally expensive due to the need to explore a large search space with sufficient accuracy to capture energy differences of a few kJ/mol between polymorphs. Dispersion-inclusive density functional theory (DFT) provides the required accuracy but its computational cost is impractical for a large number of putative structures. We introduce FastCSP, an open-source, high-throughput CSP workflow based on machine learning interatomic potentials (MLIPs). FastCSP combines random structure generation using Genarris 3.0 with geometry relaxation and free energy calculations powered entirely by the Universal Model for Atoms (UMA) MLIP. We benchmark FastCSP on a curated set of 28 mostly rigid molecules, demonstrating that our workflow consistently generates known experimental structures and ranks them within 5 kJ/mol per molecule of the global minimum. Our results demonstrate that universal MLIPs can be used across diverse compounds without requiring system-specific tuning. Moreover, the speed and accuracy afforded by UMA eliminate the need for classical force fields in the early stages of CSP and for final re-ranking with DFT. The open-source release of the entire FastCSP workflow significantly lowers the barrier to accessing CSP. CSP results for a single system can be obtained within hours on tens of modern GPUs, making high-throughput crystal structure prediction feasible for a broad range of scientific applications.         ",
    "url": "https://arxiv.org/abs/2508.02641",
    "authors": [
      "Vahe Gharakhanyan",
      "Yi Yang",
      "Luis Barroso-Luque",
      "Muhammed Shuaibi",
      "Daniel S. Levine",
      "Kyle Michel",
      "Viachaslau Bernat",
      "Misko Dzamba",
      "Xiang Fu",
      "Meng Gao",
      "Xingyu Liu",
      "Keian Noori",
      "Lafe J. Purvis",
      "Tingling Rao",
      "Brandon M. Wood",
      "Ammar Rizvi",
      "Matt Uyttendaele",
      "Andrew J. Ouderkirk",
      "Chiara Daraio",
      "C. Lawrence Zitnick",
      "Arman Boromand",
      "Noa Marom",
      "Zachary W. Ulissi",
      "Anuroop Sriram"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:1810.12277",
    "title": "A Maximum Linear Arrangement Problem on Directed Graphs",
    "abstract": "           We propose a new arrangement problem on directed graphs, Maximum Directed Linear Arrangement (MaxDLA). This is a directed variant of a similar problem for undirected graphs, in which however one seeks maximum and not minimum; this problem known as the Minimum Linear Arrangement Problem (MinLA) has been much studied in the literature. We establish a number of theorems illustrating the behavior and complexity of MaxDLA. First, we relate MaxDLA to Maximum Directed Cut (MaxDiCut) by proving that every simple digraph $D$ on $n$ vertices satisfies $\\frac{n}{2}$$maxDiCut(D) \\leq MaxDLA(D) \\leq (n-1)MaxDiCut(D)$. Next, we prove that MaxDiCut is NP-Hard for planar digraphs (even with the added restriction of maximum degree 15); it follows from the above bounds that MaxDLA is also NP-Hard for planar digraphs. In contrast, Hadlock (1975) and Dorfman and Orlova (1972) showed that the undirected Maximum Cut problem is solvable in polynomial time on planar graphs. On the positive side, we present a polynomial-time algorithm for solving MaxDLA on orientations of trees with degree bounded by a constant, which translates to a polynomial-time algorithm for solving MinLA on the complements of those trees. This pairs with results by Goldberg and Klipker (1976), Shiloach (1979) and Chung (1984) solving MinLA in polynomial time on trees. Finally, analogues of Harper's famous isoperimetric inequality for the hypercube, in the setting of MaxDLA, are shown for tournaments, orientations of graphs with degree at most two, and transitive acyclic digraphs.         ",
    "url": "https://arxiv.org/abs/1810.12277",
    "authors": [
      "Matt DeVos",
      "Kathryn Nurse"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2007.03545",
    "title": "Network Embedding with Completely-imbalanced Labels",
    "abstract": "           Network embedding, aiming to project a network into a low-dimensional space, is increasingly becoming a focus of network research. Semi-supervised network embedding takes advantage of labeled data, and has shown promising performance. However, existing semi-supervised methods would get unappealing results in the completely-imbalanced label setting where some classes have no labeled nodes at all. To alleviate this, we propose two novel semi-supervised network embedding methods. The first one is a shallow method named RSDNE. Specifically, to benefit from the completely-imbalanced labels, RSDNE guarantees both intra-class similarity and inter-class dissimilarity in an approximate way. The other method is RECT which is a new class of graph neural networks. Different from RSDNE, to benefit from the completely-imbalanced labels, RECT explores the class-semantic knowledge. This enables RECT to handle networks with node features and multi-label setting. Experimental results on several real-world datasets demonstrate the superiority of the proposed methods. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2007.03545",
    "authors": [
      "Zheng Wang",
      "Xiaojun Ye",
      "Chaokun Wang",
      "Jian Cui",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.09736",
    "title": "Geo-NI: Geometry-aware Neural Interpolation for Light Field Rendering",
    "abstract": "           In this paper, we present a Geometry-aware Neural Interpolation (Geo-NI) framework for light field rendering. Previous learning-based approaches either rely on the capability of neural networks to perform direct interpolation, which we dubbed Neural Interpolation (NI), or explore scene geometry for novel view synthesis, also known as Depth Image-Based Rendering (DIBR). Instead, we incorporate the ideas behind these two kinds of approaches by launching the NI with a novel DIBR pipeline. Specifically, the proposed Geo-NI first performs NI using input light field sheared by a set of depth hypotheses. Then the DIBR is implemented by assigning the sheared light fields with a novel reconstruction cost volume according to the reconstruction quality under different depth hypotheses. The reconstruction cost is interpreted as a blending weight to render the final output light field by blending the reconstructed light fields along the dimension of depth hypothesis. By combining the superiorities of NI and DIBR, the proposed Geo-NI is able to render views with large disparity with the help of scene geometry while also reconstruct non-Lambertian effect when depth is prone to be ambiguous. Extensive experiments on various datasets demonstrate the superior performance of the proposed geometry-aware light field rendering framework.         ",
    "url": "https://arxiv.org/abs/2206.09736",
    "authors": [
      "Gaochang Wu",
      "Yuemei Zhou",
      "Yebin Liu",
      "Lu Fang",
      "Tianyou Chai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2208.08083",
    "title": "Two Heads are Better than One: Robust Learning Meets Multi-branch Models",
    "abstract": "           Deep neural networks (DNNs) are vulnerable to adversarial examples, in which DNNs are misled to false outputs due to inputs containing imperceptible perturbations. Adversarial training, a reliable and effective method of defense, may significantly reduce the vulnerability of neural networks and becomes the de facto standard for robust learning. While many recent works practice the data-centric philosophy, such as how to generate better adversarial examples or use generative models to produce additional training data, we look back to the models themselves and revisit the adversarial robustness from the perspective of deep feature distribution as an insightful complementarity. In this paper, we propose \\textit{Branch Orthogonality adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely the original dataset for adversarial training. To practice our design idea of integrating multiple orthogonal solution spaces, we leverage a simple and straightforward multi-branch neural network that eclipses adversarial attacks with no increase in inference time. We heuristically propose a corresponding loss function, branch-orthogonal loss, to make each solution space of the multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100 and SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\\epsilon = 8/255$, respectively. Exhaustive experiments are conducted to show that our method goes beyond all state-of-the-art methods without any tricks. Compared to all methods that do not use additional data for training, our models achieve 67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the state-of-the-art by +7.23\\% and +9.07\\%). We also outperform methods using a training set with a far larger scale than ours.         ",
    "url": "https://arxiv.org/abs/2208.08083",
    "authors": [
      "Zongyuan Zhang",
      "Qingwen Bu",
      "Tianyang Duan",
      "Zheng Lin",
      "Yuhao Qing",
      "Zihan Fang",
      "Heming Cui",
      "Dong Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.13922",
    "title": "Equivariant Map and Agent Geometry for Autonomous Driving Motion Prediction",
    "abstract": "           In autonomous driving, deep learning enabled motion prediction is a popular topic. A critical gap in traditional motion prediction methodologies lies in ensuring equivariance under Euclidean geometric transformations and maintaining invariant interaction relationships. This research introduces a groundbreaking solution by employing EqMotion, a theoretically geometric equivariant and interaction invariant motion prediction model for particles and humans, plus integrating agent-equivariant high-definition (HD) map features for context aware motion prediction in autonomous driving. The use of EqMotion as backbone marks a significant departure from existing methods by rigorously ensuring motion equivariance and interaction invariance. Equivariance here implies that an output motion must be equally transformed under the same Euclidean transformation as an input motion, while interaction invariance preserves the manner in which agents interact despite transformations. These properties make the network robust to arbitrary Euclidean transformations and contribute to more accurate prediction. In addition, we introduce an equivariant method to process the HD map to enrich the spatial understanding of the network while preserving the overall network equivariance property. By applying these technologies, our model is able to achieve high prediction accuracy while maintain a lightweight design and efficient data utilization.         ",
    "url": "https://arxiv.org/abs/2310.13922",
    "authors": [
      "Yuping Wang",
      "Jier Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.01130",
    "title": "Joint Generative Modeling of Grounded Scene Graphs and Images via Diffusion Models",
    "abstract": "           We introduce a framework for joint grounded scene graph - image generation, a challenging task involving high-dimensional, multi-modal structured data. To effectively model this complex joint distribution, we adopt a factorized approach: first generating a grounded scene graph, followed by image generation conditioned on the generated grounded scene graph. While conditional image generation has been widely explored in the literature, our primary focus is on the generation of grounded scene graphs from noise, which provides efficient and interpretable control over the image generation process. This task requires generating plausible grounded scene graphs with heterogeneous attributes for both nodes (objects) and edges (relations among objects), encompassing continuous attributes (e.g., object bounding boxes) and discrete attributes (e.g., object and relation categories). To address this challenge, we introduce DiffuseSG, a novel diffusion model that jointly models the heterogeneous node and edge attributes. We explore different encoding strategies to effectively handle the categorical data. Leveraging a graph transformer as the denoiser, DiffuseSG progressively refines grounded scene graph representations in a continuous space before discretizing them to generate structured outputs. Additionally, we introduce an IoU-based regularization term to enhance empirical performance. Our model outperforms existing methods in grounded scene graph generation on the VG and COCO-Stuff datasets, excelling in both standard and newly introduced metrics that more accurately capture the task's complexity. Furthermore, we demonstrate the broader applicability of DiffuseSG in two important downstream tasks: 1) achieving superior results in a range of grounded scene graph completion tasks, and 2) enhancing grounded scene graph detection models by leveraging additional training samples generated by DiffuseSG.         ",
    "url": "https://arxiv.org/abs/2401.01130",
    "authors": [
      "Bicheng Xu",
      "Qi Yan",
      "Renjie Liao",
      "Lele Wang",
      "Leonid Sigal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.09711",
    "title": "Node Duplication Improves Cold-start Link Prediction",
    "abstract": "           Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing GNNs with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to GNNs and state-of-the-art cold-start methods.         ",
    "url": "https://arxiv.org/abs/2402.09711",
    "authors": [
      "Zhichun Guo",
      "Tong Zhao",
      "Yozen Liu",
      "Kaiwen Dong",
      "William Shiao",
      "Mingxuan Ju",
      "Neil Shah",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.00830",
    "title": "Collaborative Novel Object Discovery and Box-Guided Cross-Modal Alignment for Open-Vocabulary 3D Object Detection",
    "abstract": "           Open-vocabulary 3D Object Detection (OV-3DDet) addresses the detection of objects from an arbitrary list of novel categories in 3D scenes, which remains a very challenging problem. In this work, we propose CoDAv2, a unified framework designed to innovatively tackle both the localization and classification of novel 3D objects, under the condition of limited base categories. For localization, the proposed 3D Novel Object Discovery (3D-NOD) strategy utilizes 3D geometries and 2D open-vocabulary semantic priors to discover pseudo labels for novel objects during training. 3D-NOD is further extended with an Enrichment strategy that significantly enriches the novel object distribution in the training scenes, and then enhances the model's ability to localize more novel objects. The 3D-NOD with Enrichment is termed 3D-NODE. For classification, the Discovery-driven Cross-modal Alignment (DCMA) module aligns features from 3D point clouds and 2D/textual modalities, employing both class-agnostic and class-specific alignments that are iteratively refined to handle the expanding vocabulary of objects. Besides, 2D box guidance boosts the classification accuracy against complex background noises, which is coined as Box-DCMA. Extensive evaluation demonstrates the superiority of CoDAv2. CoDAv2 outperforms the best-performing method by a large margin (AP_Novel of 9.17 vs. 3.61 on SUN-RGBD and 9.12 vs. 3.74 on ScanNetv2). Source code and pre-trained models are available at the GitHub project page.         ",
    "url": "https://arxiv.org/abs/2406.00830",
    "authors": [
      "Yang Cao",
      "Yihan Zeng",
      "Hang Xu",
      "Dan Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.05358",
    "title": "Reinforcement Learning for Intensity Control: An Application to Choice-Based Network Revenue Management",
    "abstract": "           Intensity control is a type of continuous-time dynamic optimization problems with many important applications in Operations Research including queueing and revenue management. In this study, we adapt the reinforcement learning framework to intensity control using choice-based network revenue management as a case study, which is a classical problem in revenue management that features a large state space, a large action space and a continuous time horizon. We show that by utilizing the inherent discretization of the sample paths created by the jump points, a unique and defining feature of intensity control, one does not need to discretize the time horizon in advance, which was believed to be necessary because most reinforcement learning algorithms are designed for discrete-time problems. As a result, the computation can be facilitated and the discretization error is significantly reduced. We lay the theoretical foundation for the Monte Carlo and temporal difference learning algorithms for policy evaluation and develop policy-gradient-based actor-critic algorithms for intensity control. Via a comprehensive numerical study, we demonstrate the benefit of our approach versus other state-of-the-art benchmarks.         ",
    "url": "https://arxiv.org/abs/2406.05358",
    "authors": [
      "Huiling Meng",
      "Ningyuan Chen",
      "Xuefeng Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2406.05376",
    "title": "Adversarial flows: A gradient flow characterization of adversarial attacks",
    "abstract": "           A popular method to perform adversarial attacks on neuronal networks is the so-called fast gradient sign method and its iterative variant. In this paper, we interpret this method as an explicit Euler discretization of a differential inclusion, where we also show convergence of the discretization to the associated gradient flow. To do so, we consider the concept of p-curves of maximal slope in the case $p=\\infty$. We prove existence of $\\infty$-curves of maximum slope and derive an alternative characterization via differential inclusions. Furthermore, we also consider Wasserstein gradient flows for potential energies, where we show that curves in the Wasserstein space can be characterized by a representing measure on the space of curves in the underlying Banach space, which fulfill the differential inclusion. The application of our theory to the finite-dimensional setting is twofold: On the one hand, we show that a whole class of normalized gradient descent methods (in particular signed gradient descent) converge, up to subsequences, to the flow, when sending the step size to zero. On the other hand, in the distributional setting, we show that the inner optimization task of adversarial training objective can be characterized via $\\infty$-curves of maximum slope on an appropriate optimal transport space.         ",
    "url": "https://arxiv.org/abs/2406.05376",
    "authors": [
      "Lukas Weigand",
      "Tim Roith",
      "Martin Burger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)"
    ]
  },
  {
    "id": "arXiv:2406.11130",
    "title": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis",
    "abstract": "           Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time.         ",
    "url": "https://arxiv.org/abs/2406.11130",
    "authors": [
      "Yonghyun Jun",
      "Hwanhee Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2406.17112",
    "title": "Integrating Generative AI with Network Digital Twins for Enhanced Network Operations",
    "abstract": "           As telecommunications networks become increasingly complex, the integration of advanced technologies such as network digital twins and generative artificial intelligence (AI) emerges as a pivotal solution to enhance network operations and resilience. This paper explores the synergy between network digital twins, which provide a dynamic virtual representation of physical networks, and generative AI, particularly focusing on Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). We propose a novel architectural framework that incorporates these technologies to significantly improve predictive maintenance, network scenario simulation, and real-time data-driven decision-making. Through extensive simulations, we demonstrate how generative AI can enhance the accuracy and operational efficiency of network digital twins, effectively handling real-world complexities such as unpredictable traffic loads and network failures. The findings suggest that this integration not only boosts the capability of digital twins in scenario forecasting and anomaly detection but also facilitates a more adaptive and intelligent network management system.         ",
    "url": "https://arxiv.org/abs/2406.17112",
    "authors": [
      "Kassi Muhammad",
      "Teef David",
      "Giulia Nassisid",
      "Tina Farus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Graphics (cs.GR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2407.00513",
    "title": "Dynamic Optimization of Video Streaming Quality Using Network Digital Twin Technology",
    "abstract": "           This paper introduces a novel dynamic optimization framework for video streaming that leverages Network Digital Twin (NDT) technology to address the challenges posed by fluctuating wireless network conditions. Traditional adaptive streaming methods often struggle with rapid changes in network bandwidth, latency, and packet loss, leading to suboptimal user experiences characterized by frequent buffering and reduced video quality. Our proposed framework integrates a sophisticated NDT that models the wireless network in real-time and employs predictive analytics to forecast near-future network states. Utilizing machine learning techniques, specifically Random Forest and Neural Networks, the NDT predicts bandwidth availability, latency trends, and potential packet losses before they impact video transmission. Based on these predictions, our adaptive streaming algorithm dynamically adjusts video bitrates, resolution, and buffering strategies, thus ensuring an uninterrupted and high-quality viewing experience. Experimental validations demonstrate that our approach significantly enhances the Quality of Experience (QoE) by reducing buffering times by up to 50\\% and improving resolution in varied network conditions compared to conventional streaming methods. This paper underscores the potential of integrating digital twin technology into multimedia transmission, paving the way for more resilient and user-centric video streaming solutions.         ",
    "url": "https://arxiv.org/abs/2407.00513",
    "authors": [
      "Zurh Farus",
      "Betty Searcy",
      "Tina Nassisid",
      "Kevin Muhammad"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2407.07580",
    "title": "InstructLayout: Instruction-Driven 2D and 3D Layout Synthesis with Semantic Graph Prior",
    "abstract": "           Comprehending natural language instructions is a charming property for both 2D and 3D layout synthesis systems. Existing methods implicitly model object joint distributions and express object relations, hindering generation's controllability. We introduce InstructLayout, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 2D and 3D layout synthesis. The proposed semantic graph prior learns layout appearances and object distributions simultaneously, demonstrating versatility across various downstream tasks in a zero-shot manner. To facilitate the benchmarking for text-driven 2D and 3D scene synthesis, we respectively curate two high-quality datasets of layout-instruction pairs from public Internet resources with large language and multimodal models. Extensive experimental results reveal that the proposed method outperforms existing state-of-the-art approaches by a large margin in both 2D and 3D layout synthesis tasks. Thorough ablation studies confirm the efficacy of crucial design components.         ",
    "url": "https://arxiv.org/abs/2407.07580",
    "authors": [
      "Chenguo Lin",
      "Yuchen Lin",
      "Panwang Pan",
      "Xuanyang Zhang",
      "Yadong Mu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.11020",
    "title": "Enhancing Vehicular Networks with Generative AI: Opportunities and Challenges",
    "abstract": "           In the burgeoning field of intelligent transportation systems, the integration of Generative Artificial Intelligence (AI) into vehicular networks presents a transformative potential for the automotive industry. This paper explores the innovative applications of generative AI in enhancing communication protocols, optimizing traffic management, and bolstering security frameworks within vehicular networks. By examining current technologies and recent advancements, we identify key challenges such as scalability, real-time data processing, and security vulnerabilities that come with AI integration. Additionally, we propose novel applications and methodologies that leverage generative AI to simulate complex network scenarios, generate adaptive communication schemes, and enhance predictive capabilities for traffic conditions. This study not only reviews the state of the art but also highlights significant opportunities where generative AI can lead to groundbreaking improvements in vehicular network efficiency and safety. Through this comprehensive exploration, our findings aim to guide future research directions and foster a deeper understanding of generative AI's role in the next generation of vehicular technologies.         ",
    "url": "https://arxiv.org/abs/2407.11020",
    "authors": [
      "Teef David",
      "Kassi Muhammad",
      "Kevin Nassisid",
      "Bronny Farus"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2407.13541",
    "title": "On the Discriminability of Self-Supervised Representation Learning",
    "abstract": "           Self-supervised learning (SSL) has recently shown notable success in various visual tasks. However, in terms of discriminability, SSL is still not on par with supervised learning (SL). This paper identifies a key issue, the ``crowding problem,\" where features from different classes are not well-separated, and there is high intra-class variance. In contrast, SL ensures clear class separation. Our analysis reveals that SSL objectives do not adequately constrain the relationships between samples and their augmentations, leading to poorer performance in complex tasks. We further establish a theoretical framework that connects SSL objectives to cross-entropy risk bounds, explaining how reducing intra-class variance and increasing inter-class separation can improve generalization. To address this, we propose the Dynamic Semantic Adjuster (DSA), a learnable regulator that enhances feature aggregation and separation while being robust to outliers. Comprehensive experiments conducted on diverse benchmark datasets validate that DSA leads to substantial gains in SSL performance, narrowing the performance gap with SL.         ",
    "url": "https://arxiv.org/abs/2407.13541",
    "authors": [
      "Zeen Song",
      "Wenwen Qiang",
      "Changwen Zheng",
      "Fuchun Sun",
      "Hui Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.05456",
    "title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation",
    "abstract": "           Unified graph representation learning aims to generate node embeddings, which can be applied to multiple downstream applications of graph analytics. However, existing studies based on graph neural networks and language models either suffer from the limitations of numerous training needs toward specific downstream predictions, poor generalization, or shallow semantic features. In this work, we propose a novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate our proposed path features. Our Path-LLM framework consists of four well-designed techniques. First, we develop a new mechanism of long-to-short shortest path (L2SP) selection, which can cover key connections between different dense groups. An in-depth analysis and comparison of different path selections is conducted to justify the rationale behind our designed L2SP method. Next, we design path textualization to obtain L2SP-based training texts with key phrase selection from node text attributes. We then feed the texts into a self-supervised LLM training process to align next node/edge generation in L2SP with next token generation in causal language modeling for graph representation learning and finally extract the unified graph embeddings. We theoretically analyze the algorithm complexity of our Path-LLM approach. Extensive experiments on large-scale graph benchmarks validate the superiority of Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and GraphTranslator on two classical graph learning tasks (node classification and edge validation) and one NP-hard graph query processing task (keyword search). Compared with WalkLM, our approach saves more than 90% of training paths on millions-scale graphs and runs at most 35x faster.         ",
    "url": "https://arxiv.org/abs/2408.05456",
    "authors": [
      "Wenbo Shang",
      "Xuliang Zhu",
      "Xin Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.07606",
    "title": "Confrontation of capitalism and socialism in Wikipedia networks",
    "abstract": "           We introduce the Ising Network Opinion Formation (INOF) model and apply it for the analysis of networks of 6 Wikipedia language editions. In the model, Ising spins are placed at network nodes/articles and the steady-state opinion polarization of spins is determined from the Monte Carlo iterations in which a given spin orientation is determined by in-going links from other spins. The main consideration is done for opinion confrontation between {\\it capitalism, imperialism} (blue opinion) and {\\it socialism, communism} (red opinion). These nodes have fixed spin/opinion orientation while other nodes achieve their steady-state opinions in the process of Monte Carlo iterations. We find that the global network opinion favors {\\it socialism, communism} for all 6 editions. The model also determines the opinion preferences for world countries and political leaders, showing good agreement with heuristic expectations. We also present results for opinion competition between {\\it Christianity} and {\\it Islam}, and USA Democratic and Republican parties. We argue that the INOF approach can find numerous applications for directed complex networks.         ",
    "url": "https://arxiv.org/abs/2408.07606",
    "authors": [
      "Leonardo Ermann",
      "Dima L. Shepelyansky"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2409.00029",
    "title": "Attack Anything: Blind DNNs via Universal Background Adversarial Attack",
    "abstract": "           It has been widely substantiated that deep neural networks (DNNs) are susceptible and vulnerable to adversarial perturbations. Existing studies mainly focus on performing attacks by corrupting targeted objects (physical attack) or images (digital attack), which is intuitively acceptable and understandable in terms of the attack's effectiveness. In contrast, our focus lies in conducting background adversarial attacks in both digital and physical domains, without causing any disruptions to the targeted objects themselves. Specifically, an effective background adversarial attack framework is proposed to attack anything, by which the attack efficacy generalizes well between diverse objects, models, and tasks. Technically, we approach the background adversarial attack as an iterative optimization problem, analogous to the process of DNN learning. Besides, we offer a theoretical demonstration of its convergence under a set of mild but sufficient conditions. To strengthen the attack efficacy and transferability, we propose a new ensemble strategy tailored for adversarial perturbations and introduce an improved smooth constraint for the seamless connection of integrated perturbations. We conduct comprehensive and rigorous experiments in both digital and physical domains across various objects, models, and tasks, demonstrating the effectiveness of attacking anything of the proposed method. The findings of this research substantiate the significant discrepancy between human and machine vision on the value of background variations, which play a far more critical role than previously recognized, necessitating a reevaluation of the robustness and reliability of DNNs. The code will be publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2409.00029",
    "authors": [
      "Jiawei Lian",
      "Shaohui Mei",
      "Xiaofei Wang",
      "Yi Wang",
      "Lefan Wang",
      "Yingjie Lu",
      "Mingyang Ma",
      "Lap-Pui Chau"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.10878",
    "title": "P2 Explore: Efficient Exploration in Unknown Cluttered Environment with Floor Plan Prediction",
    "abstract": "           Robot exploration aims at the reconstruction of unknown environments, and it is important to achieve it with shorter paths. Traditional methods focus on optimizing the visiting order of frontiers based on current observations, which may lead to local-minimal results. Recently, by predicting the structure of the unseen environment, the exploration efficiency can be further improved. However, in a cluttered environment, due to the randomness of obstacles, the ability to predict is weak. Moreover, this inaccuracy will lead to limited improvement in exploration. Therefore, we propose FPUNet which can be efficient in predicting the layout of noisy indoor environments. Then, we extract the segmentation of rooms and construct their topological connectivity based on the predicted map. The visiting order of these predicted rooms is optimized which can provide high-level guidance for exploration. The FPUNet is compared with other network architectures which demonstrates it is the SOTA method for this task. Extensive experiments in simulations show that our method can shorten the path length by 2.18% to 34.60% compared to the baselines.         ",
    "url": "https://arxiv.org/abs/2409.10878",
    "authors": [
      "Kun Song",
      "Gaoming Chen",
      "Masayoshi Tomizuka",
      "Wei Zhan",
      "Zhenhua Xiong",
      "Mingyu Ding"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2410.17505",
    "title": "PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting",
    "abstract": "           Previous methods utilize the Neural Radiance Field (NeRF) for panoptic lifting, while their training and rendering speed are unsatisfactory. In contrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due to its rapid training and rendering speed. However, unlike NeRF, the conventional 3DGS may not satisfy the basic smoothness assumption as it does not rely on any parameterized structures to render (e.g., MLPs). Consequently, the conventional 3DGS is, in nature, more susceptible to noisy 2D mask supervision. In this paper, we propose a new method called PLGS that enables 3DGS to generate consistent panoptic segmentation masks from noisy 2D segmentation masks while maintaining superior efficiency compared to NeRF-based methods. Specifically, we build a panoptic-aware structured 3D Gaussian model to introduce smoothness and design effective noise reduction strategies. For the semantic field, instead of initialization with structure from motion, we construct reliable semantic anchor points to initialize the 3D Gaussians. We then use these anchor points as smooth regularization during training. Additionally, we present a self-training approach using pseudo labels generated by merging the rendered masks with the noisy masks to enhance the robustness of PLGS. For the instance field, we project the 2D instance masks into 3D space and match them with oriented bounding boxes to generate cross-view consistent instance masks for supervision. Experiments on various benchmarks demonstrate that our method outperforms previous state-of-the-art methods in terms of both segmentation quality and speed.         ",
    "url": "https://arxiv.org/abs/2410.17505",
    "authors": [
      "Yu Wang",
      "Xiaobao Wei",
      "Ming Lu",
      "Guoliang Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.20555",
    "title": "Privacy-Enhanced Adaptive Authentication: User Profiling with Privacy Guarantees",
    "abstract": "           User profiling is a critical component of adaptive risk-based authentication, yet it raises significant privacy concerns, particularly when handling sensitive data. Profiling involves collecting and aggregating various user features, potentially creating quasi-identifiers that can reveal identities and compromise privacy. Even anonymized profiling methods remain vulnerable to re-identification attacks through these quasi-identifiers. This paper introduces a novel privacy-enhanced adaptive authentication protocol that leverages Oblivious Pseudorandom Functions (OPRF), anonymous tokens, and Differential Privacy (DP) to provide robust privacy guarantees. Our proposed approach dynamically adjusts authentication requirements based on real-time risk assessments, enhancing security while safeguarding user privacy. By integrating privacy considerations into the core of adaptive risk-based adaptive authentication, this approach addresses a gap often overlooked in traditional models. Advanced cryptographic techniques ensure confidentiality, integrity, and unlinkability of user data, while differential privacy mechanisms minimize the impact of individual data points on overall analysis. Formal security and privacy proofs demonstrate the protocol's resilience against various threats and its ability to provide strong privacy guarantees. Additionally, a comprehensive performance evaluation reveals that the computational and communication overheads are manageable, making the protocol practical for real-world deployment. By adhering to data protection regulations such as GDPR and CCPA, our protocol not only enhances security but also fosters user trust and compliance with legal standards.         ",
    "url": "https://arxiv.org/abs/2410.20555",
    "authors": [
      "Yaser Baseri",
      "Abdelhakim Senhaji Hafid",
      "Dimitrios Makrakis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.22089",
    "title": "Hierarchical Structure Sharing Empowers Multi-task Heterogeneous GNNs for Customer Expansion",
    "abstract": "           Customer expansion, i.e., growing a business existing customer base by acquiring new customers, is critical for scaling operations and sustaining the long-term profitability of logistics companies. Although state-of-the-art works model this task as a single-node classification problem under a heterogeneous graph learning framework and achieve good performance, they struggle with extremely positive label sparsity issues in our scenario. Multi-task learning (MTL) offers a promising solution by introducing a correlated, label-rich task to enhance the label-sparse task prediction through knowledge sharing. However, existing MTL methods result in performance degradation because they fail to discriminate task-shared and task-specific structural patterns across tasks. This issue arises from their limited consideration of the inherently complex structure learning process of heterogeneous graph neural networks, which involves the multi-layer aggregation of multi-type relations. To address the challenge, we propose a Structure-Aware Hierarchical Information Sharing Framework (SrucHIS), which explicitly regulates structural information sharing across tasks in logistics customer expansion. SrucHIS breaks down the structure learning phase into multiple stages and introduces sharing mechanisms at each stage, effectively mitigating the influence of task-specific structural patterns during each stage. We evaluate StrucHIS on both private and public datasets, achieving a 51.41% average precision improvement on the private dataset and a 10.52% macro F1 gain on the public dataset. StrucHIS is further deployed at one of the largest logistics companies in China and demonstrates a 41.67% improvement in the success contract-signing rate over existing strategies, generating over 453K new orders within just two months.         ",
    "url": "https://arxiv.org/abs/2410.22089",
    "authors": [
      "Xinyue Feng",
      "Shuxin Zhong",
      "Jinquan Hang",
      "Wenjun Lyu",
      "Yuequn Zhang",
      "Guang Yang",
      "Haotian Wang",
      "Desheng Zhang",
      "Guang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.04358",
    "title": "Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation",
    "abstract": "           Large Language Models (LLMs) are highly resource-intensive to fine-tune due to their enormous size. While low-rank adaptation is a prominent parameter-efficient fine-tuning approach, it suffers from sensitivity to hyperparameter choices, leading to instability in model performance on fine-tuning downstream tasks. This paper highlights the importance of effective parameterization in low-rank fine-tuning to reduce estimator variance and enhance the stability of final model outputs. We propose MonteCLoRA, an efficient fine-tuning technique that employs Monte Carlo estimation to learn an unbiased posterior estimation of low-rank parameters with low expected variance, stabilizing fine-tuned LLMs with only O(r) additional parameters, for a given rank r. MonteCLoRA shows 0.5% and 1.6% improvements in accuracy and robustness over unregularized low-rank adaptation method on natural language understanding tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with pre-trained LLaMA-1-7B and LLaMA-3.2-3B-Instruct, MonteCLoRA demonstrates robust performance with 50% and 62% lower spreads respectively than the contemporary efficient fine-tuning methods. The theoretical and empirical results presented in the paper underscore how parameterization and hyperpriors balance exploration-exploitation in the low-rank parametric space, therefore leading to more optimal and robust parameter estimation during efficient fine-tuning.         ",
    "url": "https://arxiv.org/abs/2411.04358",
    "authors": [
      "Ayan Sengupta",
      "Vaibhav Seth",
      "Arinjay Pathak",
      "Aastha Verma",
      "Natraj Raman",
      "Sriram Gopalakrishnan",
      "Niladri Chatterjee",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.05060",
    "title": "A Guide to Misinformation Detection Data and Evaluation",
    "abstract": "           Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.05060",
    "authors": [
      "Camille Thibault",
      "Jacob-Junqi Tian",
      "Gabrielle Peloquin-Skulski",
      "Taylor Lynn Curtis",
      "James Zhou",
      "Florence Laflamme",
      "Yuxiang Guan",
      "Reihaneh Rabbany",
      "Jean-Fran\u00e7ois Godbout",
      "Kellin Pelrine"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2411.13927",
    "title": "Multimodal 3D Reasoning Segmentation with Complex Scenes",
    "abstract": "           The recent development in multimodal learning has greatly advanced the research in 3D scene understanding in various real-world tasks such as embodied AI. However, most existing studies are facing two common challenges: 1) they are short of reasoning ability for interaction and interpretation of human intentions and 2) they focus on scenarios with single-category objects and over-simplified textual descriptions and neglect multi-object scenarios with complicated spatial relations among objects. We address the above challenges by proposing a 3D reasoning segmentation task for reasoning segmentation with multiple objects in scenes. The task allows producing 3D segmentation masks and detailed textual explanations as enriched by 3D spatial relations among objects. To this end, we create ReasonSeg3D, a large-scale and high-quality benchmark that integrates 3D segmentation masks and 3D spatial relations with generated question-answer pairs. In addition, we design MORE3D, a novel 3D reasoning network that works with queries of multiple objects and is tailored for 3D scene understanding. MORE3D learns detailed explanations on 3D relations and employs them to capture spatial information of objects and reason textual outputs. Extensive experiments show that MORE3D excels in reasoning and segmenting complex multi-object 3D scenes. In addition, the created ReasonSeg3D offers a valuable platform for future exploration of 3D reasoning segmentation. The data and code will be released.         ",
    "url": "https://arxiv.org/abs/2411.13927",
    "authors": [
      "Xueying Jiang",
      "Lewei Lu",
      "Ling Shao",
      "Shijian Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.14796",
    "title": "Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action Recognition with Virtual Connections",
    "abstract": "           The shared topology of human skeletons motivated the recent investigation of graph convolutional network (GCN) solutions for action recognition. However, most of the existing GCNs rely on the binary connection of two neighboring vertices (joints) formed by an edge (bone), overlooking the potential of constructing multi-vertex convolution structures. Although some studies have attempted to utilize hyper-graphs to represent the topology, they rely on a fixed construction strategy, which limits their adaptivity in uncovering the intricate latent relationships within the action. In this paper, we address this oversight and explore the merits of an adaptive hyper-graph convolutional network (Hyper-GCN) to achieve the aggregation of rich semantic information conveyed by skeleton vertices. In particular, our Hyper-GCN adaptively optimises the hyper-graphs during training, revealing the action-driven multi-vertex relations. Besides, virtual connections are often designed to support efficient feature aggregation, implicitly extending the spectrum of dependencies within the skeleton. By injecting virtual connections into hyper-graphs, the semantic clues of diverse action categories can be highlighted. The results of experiments conducted on the NTU-60, NTU-120, and NW-UCLA datasets demonstrate the merits of our Hyper-GCN, compared to the state-of-the-art methods. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.14796",
    "authors": [
      "Youwei Zhou",
      "Tianyang Xu",
      "Cong Wu",
      "Xiaojun Wu",
      "Josef Kittler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.14937",
    "title": "Geminio: Language-Guided Gradient Inversion Attacks in Federated Learning",
    "abstract": "           Foundation models that bridge vision and language have made significant progress. While they have inspired many life-enriching applications, their potential for abuse in creating new threats remains largely unexplored. In this paper, we reveal that vision-language models (VLMs) can be weaponized to enhance gradient inversion attacks (GIAs) in federated learning (FL), where an FL server attempts to reconstruct private data samples from gradients shared by victim clients. Despite recent advances, existing GIAs struggle to reconstruct high-resolution images when the victim has a large local data batch. One promising direction is to focus reconstruction on valuable samples rather than the entire batch, but current methods lack the flexibility to target specific data of interest. To address this gap, we propose Geminio, the first approach to transform GIAs into semantically meaningful, targeted attacks. It enables a brand new privacy attack experience: attackers can describe, in natural language, the data they consider valuable, and Geminio will prioritize reconstruction to focus on those high-value samples. This is achieved by leveraging a pretrained VLM to guide the optimization of a malicious global model that, when shared with and optimized by a victim, retains only gradients of samples that match the attacker-specified query. Geminio can be launched at any FL round and has no impact on normal training (i.e., the FL server can steal clients' data while still producing a high-utility ML model as in benign scenarios). Extensive experiments demonstrate its effectiveness in pinpointing and reconstructing targeted samples, with high success rates across complex datasets and large batch sizes with resilience against defenses.         ",
    "url": "https://arxiv.org/abs/2411.14937",
    "authors": [
      "Junjie Shan",
      "Ziqi Zhao",
      "Jialin Lu",
      "Rui Zhang",
      "Siu Ming Yiu",
      "Ka-Ho Chow"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.19440",
    "title": "Gradient Inversion Attack on Graph Neural Networks",
    "abstract": "           Graph federated learning is of essential importance for training over large graph datasets while protecting data privacy, where each client stores a subset of local graph data, while the server collects the local gradients and broadcasts only the aggregated gradients. Recent studies reveal that a malicious attacker can steal private image data from the gradient exchange of neural networks during federated learning. However, the vulnerability of graph data and graph neural networks under such attacks, i.e., reconstructing both node features and graph structure from gradients, remains largely underexplored. To answer this question, this paper studies the problem of whether private data can be reconstructed from leaked gradients in both node classification and graph classification tasks and proposes a novel attack named Graph Leakage from Gradients (GLG). Two widely used GNN frameworks are analyzed, namely GCN and GraphSAGE. The effects of different model settings on reconstruction are extensively discussed. Theoretical analysis and empirical validation demonstrate that, by leveraging the unique properties of graph data and GNNs, GLG achieves more accurate reconstruction of both nodal features and graph structure from gradients.         ",
    "url": "https://arxiv.org/abs/2411.19440",
    "authors": [
      "Divya Anand Sinha",
      "Ruijie Du",
      "Yezi Liu",
      "Athina Markopolou",
      "Yanning Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.00560",
    "title": "Friend or Foe? Harnessing Controllable Overfitting for Anomaly Detection",
    "abstract": "           Overfitting has traditionally been viewed as detrimental to anomaly detection, where excessive generalization often limits models' sensitivity to subtle anomalies. Our work challenges this conventional view by introducing Controllable Overfitting-based Anomaly Detection (COAD), a novel framework that strategically leverages overfitting to enhance anomaly discrimination capabilities. We propose the Aberrance Retention Quotient (ARQ), a novel metric that systematically quantifies the extent of overfitting, enabling the identification of an optimal golden overfitting interval wherein model sensitivity to anomalies is maximized without sacrificing generalization. To comprehensively capture how overfitting affects detection performance, we further propose the Relative Anomaly Distribution Index (RADI), a metric superior to traditional AUROC by explicitly modeling the separation between normal and anomalous score distributions. Theoretically, RADI leverages ARQ to track and evaluate how overfitting impacts anomaly detection, offering an integrated approach to understanding the relationship between overfitting dynamics and model efficacy. We also rigorously validate the statistical efficacy of Gaussian noise as pseudo-anomaly generators, reinforcing the method's broad applicability. Empirical evaluations demonstrate that our controllable overfitting method achieves State-Of-The-Art(SOTA) performance in both one-class and multi-class anomaly detection tasks, thus redefining overfitting as a powerful strategy rather than a limitation.         ",
    "url": "https://arxiv.org/abs/2412.00560",
    "authors": [
      "Long Qian",
      "Bingke Zhu",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.06861",
    "title": "CSI-BERT2: A BERT-inspired Framework for Efficient CSI Prediction and Classification in Wireless Communication and Sensing",
    "abstract": "           Channel state information (CSI) is a fundamental component in both wireless communication and sensing systems, enabling critical functions such as radio resource optimization and environmental perception. In wireless sensing, data scarcity and packet loss hinder efficient model training, while in wireless communication, high-dimensional CSI matrices and short coherent times caused by high mobility present challenges in CSI this http URL address these issues, we propose a unified framework named CSI-BERT2 for CSI prediction and classification tasks. Building on CSI-BERT, we introduce a two-stage training method that first uses a mask language model (MLM) to enable the model to learn general feature extraction from scarce datasets in an unsupervised manner, followed by fine-tuning for specific downstream tasks. Specifically, we extend MLM into a mask prediction model (MPM), which efficiently addresses the CSI prediction task. We also introduce an adaptive re-weighting layer (ARL) to enhance subcarrier representation and a multi-layer perceptron (MLP) based temporal embedding module to mitigate permutation invariance issues in time-series CSI data. This significantly improves the CSI classification performance of the original CSI-BERT model. Extensive experiments on both real-world collected and simulated datasets demonstrate that CSI-BERT2 achieves state-of-the-art performance across all tasks. Our results further show that CSI-BERT2 generalizes effectively across varying sampling rates and robustly handles discontinuous CSI sequences caused by packet loss-challenges that conventional methods fail to address.         ",
    "url": "https://arxiv.org/abs/2412.06861",
    "authors": [
      "Zijian Zhao",
      "Fanyi Meng",
      "Zhonghao Lyu",
      "Hang Li",
      "Xiaoyang Li",
      "Guangxu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2412.15305",
    "title": "Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling",
    "abstract": "           Solving complex reasoning tasks is a key real-world application of agents. Thanks to the pretraining of Large Language Models (LLMs) on code data, recent approaches like CodeAct successfully use code as LLM agents' action, achieving good results. However, CodeAct greedily generates the next action's code block by relying on fragmented thoughts, resulting in inconsistency and instability. Moreover, CodeAct lacks action-related ground-truth (GT), making its supervision signals and termination conditions questionable in multi-turn interactions. To address these issues, we first introduce a simple yet effective end-to-end code generation paradigm, CodeProgram, which leverages code's systematic logic to align with global reasoning and enable cohesive problem-solving. Then, we propose Tree-of-Code (ToC), which self-grows CodeProgram nodes based on the executable nature of the code and enables self-supervision in a GT-free scenario. Experimental results on two datasets using ten popular zero-shot LLMs show ToC remarkably boosts accuracy by nearly 20% over CodeAct with less than 1/4 turns. Several LLMs even perform better on one-turn CodeProgram than on multi-turn CodeAct. To further investigate the trade-off between efficacy and efficiency, we test different ToC tree sizes and exploration mechanisms. We also highlight the potential of ToC's end-to-end data generation for supervised and reinforced fine-tuning.         ",
    "url": "https://arxiv.org/abs/2412.15305",
    "authors": [
      "Ziyi Ni",
      "Yifan Li",
      "Ning Yang",
      "Dou Shen",
      "Pin Lv",
      "Daxiang Dong"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.15681",
    "title": "Asynchronous Vector Consensus over Matrix-Weighted Networks",
    "abstract": "           We study the distributed consensus of state vectors in a discrete-time multi-agent network with matrix edge weights using stochastic matrix convergence theory. We present a distributed asynchronous time update model wherein one randomly selected agent updates its state vector at a time by interacting with its neighbors. We prove that all agents converge to same state vector almost surely when every edge weight matrix is positive definite. We study vector consensus in cooperative-competitive networks with edge weights being either positive or negative definite matrices and present a necessary and sufficient condition to achieve bipartite vector consensus in such networks. We study the network structures on which agents achieve zero consensus. We also present a convergence result on nonhomogenous matrix products which is of independent interest in matrix convergence theory. All the results hold true for the synchronous time update model as well in which all agents update their states simultaneously.         ",
    "url": "https://arxiv.org/abs/2412.15681",
    "authors": [
      "P Raghavendra Rao",
      "Pooja Vyavahare"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2412.16772",
    "title": "Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans?",
    "abstract": "           The ongoing revolution in language modeling has led to various novel applications, some of which rely on the emerging social abilities of large language models (LLMs). Already, many turn to the new cyber friends for advice during the pivotal moments of their lives and trust them with the deepest secrets, implying that accurate shaping of the LLM's personality is paramount. To this end, state-of-the-art approaches exploit a vast variety of training data, and prompt the model to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e., make decisions when presented with a social situation) in line with the ascribed personality (ii) if their behavior can be finely controlled. We use classic psychological experiments, the Milgram experiment and the Ultimatum Game, as social interaction testbeds and apply personality prompting to open- and closed-source LLMs from 4 different vendors. Our experiments reveal failure modes of the prompt-based modulation of the models' behavior that are shared across all models tested and persist under prompt perturbations. These findings challenge the optimistic sentiment toward personality prompting generally held in the community.         ",
    "url": "https://arxiv.org/abs/2412.16772",
    "authors": [
      "Ivan Zakazov",
      "Mikolaj Boronski",
      "Lorenzo Drudi",
      "Robert West"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.10684",
    "title": "Deep Operator Networks for Bayesian Parameter Estimation in PDEs",
    "abstract": "           We present a novel framework combining Deep Operator Networks (DeepONets) with Physics-Informed Neural Networks (PINNs) to solve partial differential equations (PDEs) and estimate their unknown parameters. By integrating data-driven learning with physical constraints, our method achieves robust and accurate solutions across diverse scenarios. Bayesian training is implemented through variational inference, allowing for comprehensive uncertainty quantification for both aleatoric and epistemic uncertainties. This ensures reliable predictions and parameter estimates even in noisy conditions or when some of the physical equations governing the problem are missing. The framework demonstrates its efficacy in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. This approach provides a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.         ",
    "url": "https://arxiv.org/abs/2501.10684",
    "authors": [
      "Amogh Raj",
      "Carol Eunice Gudumotou",
      "Sakol Bun",
      "Keerthana Srinivasa",
      "Arash Sarshar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2501.12723",
    "title": "Anomaly Detection in Double-entry Bookkeeping Data by Federated Learning System with Non-model Sharing Approach",
    "abstract": "           Anomaly detection is crucial in financial auditing, and effective detection requires large volumes of data from multiple organizations. However, journal entry data is highly sensitive, making it infeasible to share them directly across audit firms. To address this challenge, journal entry anomaly detection methods based on model share-type federated learning (FL) have been proposed. These methods require multiple rounds of communication with external servers to exchange model parameters, which necessitates connecting devices storing confidential data to external networks -- a practice not recommended for sensitive data such as journal entries. To overcome these limitations, a novel anomaly detection framework based on data collaboration (DC) analysis, a non-model share-type FL approach, is proposed. The method first transforms raw journal entry data into secure intermediate representations via dimensionality reduction and then constructs a collaboration representation used to train an anomaly detection autoencoder. Notably, the approach does not require raw data to be exposed or devices to be connected to external networks, and the entire process needs only a single round of communication. The proposed method was evaluated on both synthetic and real-world journal entry data collected from eight healthcare organizations. The experimental results demonstrated that the framework not only outperforms the baseline trained on individual data but also achieves higher detection performance than model-sharing FL methods such as FedAvg and FedProx, particularly under non-i.i.d. settings that simulate practical audit environments. This study addresses the critical need to integrate organizational knowledge while preserving data confidentiality, contributing to the development of practical intelligent auditing systems.         ",
    "url": "https://arxiv.org/abs/2501.12723",
    "authors": [
      "Sota Mashiko",
      "Yuji Kawamata",
      "Tomoru Nakayama",
      "Tetsuya Sakurai",
      "Yukihiko Okada"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.12934",
    "title": "Correctness Assessment of Code Generated by Large Language Models Using Internal Representations",
    "abstract": "           Ensuring the correctness of code generated by Large Language Models (LLMs) presents a significant challenge in AI-driven software development. Existing approaches predominantly rely on black-box (closed-box) approaches that evaluate correctness post-generation, failing to utilize the rich insights embedded in the LLMs' internal states during code generation. In this paper, we introduce OPENIA, a novel white-box (open-box) framework that leverages these internal representations to assess the correctness of LLM-generated code. OPENIA systematically analyzes the intermediate states of representative open-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and MagicCoder, across diverse code generation benchmarks. Our empirical analysis reveals that these internal representations encode latent information, which strongly correlates with the correctness of the generated code. Building on these insights, OPENIA uses a white-box/open-box approach to make informed predictions about code correctness, offering significant advantages in adaptability and robustness over traditional classification-based methods and zero-shot approaches. Experimental results demonstrate that OPENIA consistently outperforms baseline models, achieving higher accuracy, precision, recall, and F1-Scores with up to a 2X improvement in standalone code generation and a 46% enhancement in repository-specific scenarios. By unlocking the potential of in-process signals, OPENIA paves the way for more proactive and efficient quality assurance mechanisms in LLM-assisted code generation.         ",
    "url": "https://arxiv.org/abs/2501.12934",
    "authors": [
      "Tuan-Dung Bui",
      "Thanh Trong Vu",
      "Thu-Trang Nguyen",
      "Son Nguyen",
      "Hieu Dinh Vo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.02194",
    "title": "Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study",
    "abstract": "           Integrated Development Environments increasingly implement AI-powered code completion tools (CCTs), which promise to enhance developer efficiency, accuracy, and productivity. However, interaction challenges with CCTs persist, mainly due to mismatches between developers' mental models and the unpredictable behavior of AI-generated suggestions, which is an aspect underexplored in the literature. We conducted an elicitation study with 56 developers using co-design workshops to elicit their mental models when interacting with CCTs. Different important findings that might drive the interaction design with CCTs emerged. For example, developers expressed diverse preferences on when and how code suggestions should be triggered (proactive, manual, hybrid), where and how they are displayed (inline, sidebar, popup, chatbot), as well as the level of detail. It also emerged that developers need to be supported by customization of activation timing, display modality, suggestion granularity, and explanation content, to better fit the CCT to their preferences. To demonstrate the feasibility of these and the other guidelines that emerged during the study, we developed ATHENA, a proof-of-concept CCT that dynamically adapts to developers' coding preferences and environments, ensuring seamless integration into diverse workflows.         ",
    "url": "https://arxiv.org/abs/2502.02194",
    "authors": [
      "Giuseppe Desolda",
      "Andrea Esposito",
      "Francesco Greco",
      "Cesare Tucci",
      "Paolo Buono",
      "Antonio Piccinno"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.02410",
    "title": "Privacy Amplification by Structured Subsampling for Deep Differentially Private Time Series Forecasting",
    "abstract": "           Many forms of sensitive data, such as web traffic, mobility data, or hospital occupancy, are inherently sequential. The standard method for training machine learning models while ensuring privacy for units of sensitive information, such as individual hospital visits, is differentially private stochastic gradient descent (DP-SGD). However, we observe in this work that the formal guarantees of DP-SGD are incompatible with time series specific tasks like forecasting, since they rely on the privacy amplification attained by training on small, unstructured batches sampled from an unstructured dataset. In contrast, batches for forecasting are generated by (1) sampling sequentially structured time series from a dataset, (2) sampling contiguous subsequences from these series, and (3) partitioning them into context and ground-truth forecast windows. We theoretically analyze the privacy amplification attained by this structured subsampling to enable the training of forecasting models with sound and tight event- and user-level privacy guarantees. Towards more private models, we additionally prove how data augmentation amplifies privacy in self-supervised training of sequence models. Our empirical evaluation demonstrates that amplification by structured subsampling enables the training of forecasting models with strong formal privacy guarantees.         ",
    "url": "https://arxiv.org/abs/2502.02410",
    "authors": [
      "Jan Schuchardt",
      "Mina Dalirrooyfard",
      "Jed Guzelkabaagac",
      "Anderson Schneider",
      "Yuriy Nevmyvaka",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.04182",
    "title": "Fast In-Spectrum Graph Watermarks",
    "abstract": "           We address the problem of watermarking graph objects, which consists in hiding information within them, to prove their origin. The two existing methods to watermark graphs use subgraph matching or graph isomorphism techniques, which are known to be intractable for large graphs. To reduce the operational complexity, we propose FFG, a new graph watermarking scheme adapted from an image watermarking scheme, since graphs and images can be represented as matrices. We analyze and compare FFG, whose novelty lies in embedding the watermark in the Fourier transform of the adjacency matrix of a graph. Our technique enjoys a much lower complexity than that of related works (i.e. in $\\mathcal{O}\\left(N^2 \\log N\\right)$), while performing better or at least as well as the two state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2502.04182",
    "authors": [
      "Jade Garcia Bourr\u00e9e",
      "Anne-Marie Kermarrec",
      "Erwan Le Merrer",
      "Othmane Safsafi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2502.04400",
    "title": "Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks",
    "abstract": "           Multimodal Federated Learning (MFL) with mixed modalities enables unimodal and multimodal clients to collaboratively train models while ensuring clients' privacy. As a representative sample of local data, prototypes offer an approach with low resource consumption and no reliance on prior knowledge for MFL with mixed modalities. However, existing prototype-based MFL methods assume unified labels across clients and identical tasks per client, which is impractical in MFL with mixed modalities. In this work, we propose an Adaptive prototype-based Multimodal Federated Learning (AproMFL) framework for mixed modalities to address the aforementioned issues. Our AproMFL transfers knowledge through adaptively-constructed prototypes without unified labels. Clients adaptively select prototype construction methods in line with labels; server converts client prototypes into unified multimodal prototypes and cluster them to form global prototypes. To address model aggregation issues in task heterogeneity, we develop a client relationship graph-based scheme to dynamically adjust aggregation weights. Furthermore, we propose a global prototype knowledge transfer loss and a global model knowledge transfer loss to enable the transfer of global knowledge to local knowledge. Experimental results show that AproMFL outperforms four baselines on three highly heterogeneous datasets ($\\alpha=0.1$) and two heterogeneous tasks, with the optimal results in accuracy and recall being 0.42%~6.09% and 1.6%~3.89% higher than those of FedIoT (FedAvg-based MFL), respectively.         ",
    "url": "https://arxiv.org/abs/2502.04400",
    "authors": [
      "Keke Gai",
      "Mohan Wang",
      "Jing Yu",
      "Dongjue Wang",
      "Qi Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2502.05213",
    "title": "DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models",
    "abstract": "           As large language models (LLMs) grow more powerful, concerns over copyright infringement of LLM-generated texts have intensified. LLM watermarking has been proposed to trace unauthorized redistribution or resale of generated content by embedding identifiers within the text. Existing approaches primarily rely on one-bit watermarking, which only verifies whether a text was generated by a specific LLM. In contrast, multi-bit watermarking encodes richer information, enabling the identification of the specific LLM and user involved in generated or distributed content. However, current multi-bit methods directly embed the watermark into the text without considering its watermark capacity, which can result in failures, especially in low-entropy texts. In this paper, we analyze that the watermark embedding follows a normal distribution. We then derive a formal inequality to optimally segment the text for watermark embedding. Building upon this, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method that divides the text into variable-length segments for each watermark bit during the inference. Moreover, DERMARK incurs negligible overhead since no additional intermediate matrices are generated and achieves robustness against text editing by minimizing watermark extraction loss. Experiments demonstrate that, compared to SOTA, on average, our method reduces the number of tokens required per embedded bit by 25\\%, reduces watermark embedding time by 50\\%, and maintains high robustness against text modifications and watermark erasure attacks.         ",
    "url": "https://arxiv.org/abs/2502.05213",
    "authors": [
      "Qihao Lin",
      "Chen Tang",
      "Lan zhang",
      "Junyang zhang",
      "Xiangyang Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.07239",
    "title": "Contextual Gesture: Co-Speech Gesture Video Generation through Context-aware Gesture Representation",
    "abstract": "           Co-speech gesture generation is crucial for creating lifelike avatars and enhancing human-computer interactions by synchronizing gestures with speech. Despite recent advancements, existing methods struggle with accurately identifying the rhythmic or semantic triggers from audio for generating contextualized gesture patterns and achieving pixel-level realism. To address these challenges, we introduce Contextual Gesture, a framework that improves co-speech gesture video generation through three innovative components: (1) a chronological speech-gesture alignment that temporally connects two modalities, (2) a contextualized gesture tokenization that incorporate speech context into motion pattern representation through distillation, and (3) a structure-aware refinement module that employs edge connection to link gesture keypoints to improve video generation. Our extensive experiments demonstrate that Contextual Gesture not only produces realistic and speech-aligned gesture videos but also supports long-sequence generation and video gesture editing applications, shown in Fig.1.         ",
    "url": "https://arxiv.org/abs/2502.07239",
    "authors": [
      "Pinxin Liu",
      "Pengfei Zhang",
      "Hyeongwoo Kim",
      "Pablo Garrido",
      "Ari Sharpio",
      "Kyle Olszewski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.08611",
    "title": "Robustly Learning Monotone Generalized Linear Models via Data Augmentation",
    "abstract": "           We study the task of learning Generalized Linear models (GLMs) in the agnostic model under the Gaussian distribution. We give the first polynomial-time algorithm that achieves a constant-factor approximation for \\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners succeed for a substantially smaller class of activations. Our work resolves a well-known open problem, by developing a robust counterpart to the classical GLMtron algorithm (Kakade et al., 2011). Our robust learner applies more generally, encompassing all monotone activations with bounded $(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially necessary. To obtain our results, we leverage a novel data augmentation technique with decreasing Gaussian noise injection and prove a number of structural results that may be useful in other settings.         ",
    "url": "https://arxiv.org/abs/2502.08611",
    "authors": [
      "Nikos Zarifis",
      "Puqian Wang",
      "Ilias Diakonikolas",
      "Jelena Diakonikolas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2502.10374",
    "title": "Robustness tests for biomedical foundation models should tailor to specifications",
    "abstract": "           The rise of biomedical foundation models creates new hurdles in model testing and authorization given their broad capabilities and susceptibility to complex distribution shifts. We suggest tailoring robustness tests according to task-dependent priorities and propose to integrate granular notions of robustness in a predefined specification to guide implementation. Our approach facilitates the standardization of robustness assessments in the model lifecycle and connects abstract AI regulatory frameworks with concrete testing procedures.         ",
    "url": "https://arxiv.org/abs/2502.10374",
    "authors": [
      "R. Patrick Xian",
      "Noah R. Baker",
      "Tom David",
      "Qiming Cui",
      "A. Jay Holmgren",
      "Stefan Bauer",
      "Madhumita Sushil",
      "Reza Abbasi-Asl"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2502.10706",
    "title": "Raising the Bar in Graph OOD Generalization: Invariant Learning Beyond Explicit Environment Modeling",
    "abstract": "           Out-of-distribution (OOD) generalization has emerged as a critical challenge in graph learning, as real-world graph data often exhibit diverse and shifting environments that traditional models fail to generalize across. A promising solution to address this issue is graph invariant learning (GIL), which aims to learn invariant representations by disentangling label-correlated invariant subgraphs from environment-specific subgraphs. However, existing GIL methods face two major challenges: (1) the difficulty of capturing and modeling diverse environments in graph data, and (2) the semantic cliff, where invariant subgraphs from different classes are difficult to distinguish, leading to poor class separability and increased misclassifications. To tackle these challenges, we propose a novel method termed Multi-Prototype Hyperspherical Invariant Learning (MPHIL), which introduces two key innovations: (1) hyperspherical invariant representation extraction, enabling robust and highly discriminative hyperspherical invariant feature extraction, and (2) multi-prototype hyperspherical classification, which employs class prototypes as intermediate variables to eliminate the need for explicit environment modeling in GIL and mitigate the semantic cliff issue. Derived from the theoretical framework of GIL, we introduce two novel objective functions: the invariant prototype matching loss to ensure samples are matched to the correct class prototypes, and the prototype separation loss to increase the distinction between prototypes of different classes in the hyperspherical space. Extensive experiments on 11 OOD generalization benchmark datasets demonstrate that MPHIL achieves state-of-the-art performance, significantly outperforming existing methods across graph data from various domains and with different distribution shifts.         ",
    "url": "https://arxiv.org/abs/2502.10706",
    "authors": [
      "Xu Shen",
      "Yixin Liu",
      "Yili Wang",
      "Rui Miao",
      "Yiwei Dai",
      "Shirui Pan",
      "Yi Chang",
      "Xin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.12172",
    "title": "Application-oriented automatic hyperparameter optimization for spiking neural network prototyping",
    "abstract": "           Hyperparameter optimization (HPO) is of paramount importance in the development of high-performance, specialized artificial intelligence (AI) models, ranging from well-established machine learning (ML) solutions to the deep learning (DL) domain and the field of spiking neural networks (SNNs). The latter introduce further complexity due to the neuronal computational units and their additional hyperparameters, whose inadequate setting can dramatically impact the final model performance. At the cost of possible reduced generalization capabilities, the most suitable strategy to fully disclose the power of SNNs is to adopt an application-oriented approach and perform extensive HPO experiments. To facilitate these operations, automatic pipelines are fundamental, and their configuration is crucial. In this document, the Neural Network Intelligence (NNI) toolkit is used as reference framework to present one such solution, with a use case example providing evidence of the corresponding results. In addition, a summary of published works employing the presented pipeline is reported as possible source of insights into application-oriented HPO experiments for SNN prototyping.         ",
    "url": "https://arxiv.org/abs/2502.12172",
    "authors": [
      "Vittorio Fra"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13207",
    "title": "Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation",
    "abstract": "           Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Dealing with this trade-off is still an open challenge in designing AI systems for creativity. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We show that our score can be used as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments considering a variety of creative tasks, such as poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.         ",
    "url": "https://arxiv.org/abs/2502.13207",
    "authors": [
      "Giorgio Franceschelli",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14037",
    "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation",
    "abstract": "           Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose DiffSampling, a new decoding method that leverages a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. In addition, we also propose two variations of the proposed method that aim to correct the subtle inconsistencies of common sampling strategies. Experiments involving four different text-generation tasks demonstrate that our approach consistently performs at least on par with the existing methods it builds upon in terms of quality, while potentially improving output diversity.         ",
    "url": "https://arxiv.org/abs/2502.14037",
    "authors": [
      "Giorgio Franceschelli",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14611",
    "title": "Enumerating minimal dominating sets and variants in chordal bipartite graphs",
    "abstract": "           Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem. To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\u00e9, Kratsch, Saether, and Villanger running in incremental-polynomial time. We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs. Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class. This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant. Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs.         ",
    "url": "https://arxiv.org/abs/2502.14611",
    "authors": [
      "Emanuel Castelo",
      "Oscar Defrain",
      "Guilherme C. M. Gomes"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2502.16580",
    "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
    "abstract": "           Prompt injection attacks manipulate large language models (LLMs) by misleading them to deviate from the original input instructions and execute maliciously injected instructions, because of their instruction-following capabilities and inability to distinguish between the original input instructions and maliciously injected instructions. To defend against such attacks, recent studies have developed various detection mechanisms. If we restrict ourselves specifically to works which perform detection rather than direct defense, most of them focus on direct prompt injection attacks, while there are few works for the indirect scenario, where injected instructions are indirectly from external tools, such as a search engine. Moreover, current works mainly investigate injection detection methods and pay less attention to the post-processing method that aims to mitigate the injection after detection. In this paper, we investigate the feasibility of detecting and removing indirect prompt injection attacks, and we construct a benchmark dataset for evaluation. For detection, we assess the performance of existing LLMs and open-source detection models, and we further train detection models using our crafted training datasets. For removal, we evaluate two intuitive methods: (1) the segmentation removal method, which segments the injected document and removes parts containing injected instructions, and (2) the extraction removal method, which trains an extraction model to identify and remove injected instructions.         ",
    "url": "https://arxiv.org/abs/2502.16580",
    "authors": [
      "Yulin Chen",
      "Haoran Li",
      "Yuan Sui",
      "Yufei He",
      "Yue Liu",
      "Yangqiu Song",
      "Bryan Hooi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.17366",
    "title": "Distributed Coordination for Heterogeneous Non-Terrestrial Networks",
    "abstract": "           To guarantee global coverage and ubiquitous connectivity, the Non-terrestrial Network (NTN) technology has been regarded as a key enabling technology in the Six Generation (6G) network, which consists of the unmanned aerial vehicle (UAV), high-altitude platform (HAP), and satellite. It is noted that the unique characteristics of various NTN platforms directly impact the design and implementation of NTNs, which results in highly dynamic and heterogeneous networks. Even within the same tier, such as the space tier, the NTNs are developed based on different platforms including Low Earth Orbit (LEO), Medium Earth Orbit (MEO), and Geostationary Earth Orbit (GEO). Therefore, distributed coordination among heterogeneous NTNs remains an important challenge. Although distributed learning framework finds a wide range of applications by leveraging rich distributed data and computation resources. The explicit and systematic analysis of the individual layers' challenges, and corresponding distributed coordination solutions in heterogeneous NTNs has not been proposed yet. In this article, we first summarize the unique characteristics of each NTN platform, and analyze the corresponding impact on the design and implementation of the NTN. We then identify the communication challenges of heterogeneous NTNs in individual layers, where the potential coordinated solutions are identified. We further illustrate the multi-agent deep reinforcement learning (MADRL) algorithms tailored for coordinated solutions in heterogeneous NTNs. Last but not least, we present a case study of the user scheduling optimization problem in heterogeneous UAVs-based cellular networks, where the multi-agent deep deterministic policy gradient (MADDPG) technique is developed to validate the effectiveness of distributed coordination in heterogeneous NTNs.         ",
    "url": "https://arxiv.org/abs/2502.17366",
    "authors": [
      "Jikang Deng",
      "Hui Zhou",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.19166",
    "title": "CodeIF: Benchmarking the Instruction-Following Capabilities of Large Language Models for Code Generation",
    "abstract": "           With the rapid advancement of Large Language Models (LLMs), the demand for robust instruction-following capabilities in code generation tasks has grown significantly. Code generation not only facilitates faster prototyping and automated testing, but also augments developer efficiency through improved maintainability and reusability of code. In this paper, we introduce CodeIF, the first benchmark specifically designed to assess the abilities of LLMs to adhere to task-oriented instructions within diverse code generation scenarios. CodeIF encompasses a broad range of tasks, including function synthesis, error debugging, algorithmic refactoring, and code explanation, thereby providing a comprehensive suite to evaluate model performance across varying complexity levels and programming domains. We conduct extensive experiments with LLMs, analyzing their strengths and limitations in meeting the demands of these tasks. The experimental results offer valuable insights into how well current models align with human instructions, as well as the extent to which they can generate consistent, maintainable, and contextually relevant code. Our findings not only underscore the critical role that instruction-following LLMs can play in modern software development, but also illuminate pathways for future research aimed at enhancing their adaptability, reliability, and overall effectiveness in automated code generation. CodeIF data and code are publicly available: this https URL ",
    "url": "https://arxiv.org/abs/2502.19166",
    "authors": [
      "Kaiwen Yan",
      "Hongcheng Guo",
      "Xuanqing Shi",
      "Shaosheng Cao",
      "Donglin Di",
      "Zhoujun Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.19741",
    "title": "Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption",
    "abstract": "           Estimating causal effects under networked interference from observational data is a crucial yet challenging problem. Most existing methods mainly rely on the networked unconfoundedness assumption, which guarantees the identification of networked effects. However, this assumption is often violated due to the latent confounders inherent in observational data, thereby hindering the identification of networked effects. To address this issue, we leverage the rich interaction patterns between units in networks, which provide valuable information for recovering these latent confounders. Building on this insight, we develop a confounder recovery framework that explicitly characterizes three categories of latent confounders in networked settings: those affecting only the unit, those affecting only the unit's neighbors, and those influencing both. Based on this framework, we design a networked effect estimator using identifiable representation learning techniques. From a theoretical standpoint, we prove the identifiability of all three types of latent confounders and, by leveraging the recovered confounders, establish a formal identification result for networked effects. Extensive experiments validate our theoretical findings and demonstrate the effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2502.19741",
    "authors": [
      "Weilin Chen",
      "Ruichu Cai",
      "Jie Qiao",
      "Yuguang Yan",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.20528",
    "title": "ConfuGuard: Using Metadata to Detect Active and Stealthy Package Confusion Attacks Accurately and at Scale",
    "abstract": "           Package confusion attacks such as typosquatting threaten software supply chains. Attackers make packages with names that syntactically or semantically resemble legitimate ones, tricking engineers into installing malware. While prior work has developed defenses against package confusions in some software package registries, notably NPM, PyPI, and RubyGems, gaps remain: high false-positive rates, generalization to more software package ecosystems, and insights from real-world deployment. In this work, we introduce ConfuGuard, a state-of-art detector for package confusion threats. We begin by presenting the first empirical analysis of benign signals derived from prior package confusion data, uncovering their threat patterns, engineering practices, and measurable attributes. Advancing existing detectors, we leverage package metadata to distinguish benign packages, and extend support from three up to seven software package registries. Our approach significantly reduces false positive rates (from 80% to 28%), at the cost of an additional 14s average latency to filter out benign packages by analyzing the package metadata. ConfuGuard is used in production at our industry partner, whose analysts have already confirmed 630 real attacks detected by ConfuGuard.         ",
    "url": "https://arxiv.org/abs/2502.20528",
    "authors": [
      "Wenxin Jiang",
      "Berk \u00c7akar",
      "Mikola Lysenko",
      "James C. Davis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.21077",
    "title": "Enhancing deep neural networks through complex-valued representations and Kuramoto synchronization dynamics",
    "abstract": "           Neural synchrony is hypothesized to play a crucial role in how the brain organizes visual scenes into structured representations, enabling the robust encoding of multiple objects within a scene. However, current deep learning models often struggle with object binding, limiting their ability to represent multiple objects effectively. Inspired by neuroscience, we investigate whether synchrony-based mechanisms can enhance object encoding in artificial models trained for visual categorization. Specifically, we combine complex-valued representations with Kuramoto dynamics to promote phase alignment, facilitating the grouping of features belonging to the same object. We evaluate two architectures employing synchrony: a feedforward model and a recurrent model with feedback connections to refine phase synchronization using top-down information. Both models outperform their real-valued counterparts and complex-valued models without Kuramoto synchronization on tasks involving multi-object images, such as overlapping handwritten digits, noisy inputs, and out-of-distribution transformations. Our findings highlight the potential of synchrony-driven mechanisms to enhance deep learning models, improving their performance, robustness, and generalization in complex visual categorization tasks.         ",
    "url": "https://arxiv.org/abs/2502.21077",
    "authors": [
      "Sabine Muzellec",
      "Andrea Alamia",
      "Thomas Serre",
      "Rufin VanRullen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2503.01347",
    "title": "Spatial Transcriptomics Analysis of Spatially Dense Gene Expression Prediction",
    "abstract": "           Spatial transcriptomics (ST) measures gene expression at fine-grained spatial resolution, offering insights into tissue molecular landscapes. Previous methods for spatial gene expression prediction typically crop spots of interest from histopathology slide images, and train models to map each spot to a corresponding gene expression profile. However, these methods inherently lose the spatial resolution in gene expression: 1) each spot often contains multiple cells with distinct gene expression profiles; 2) spots are typically defined at fixed spatial resolutions, limiting the ability to predict gene expression at varying scales. To address these limitations, this paper presents PixNet, a dense prediction network capable of predicting spatially resolved gene expression across spots of varying sizes and scales directly from histopathology slide images. Different from previous methods that map individual spots to gene expression values, we generate a spatially dense continuous gene expression map from the histopathology slide image, and aggregate values within spots of interest to predict the gene expression. Our PixNet outperforms state-of-the-art methods on four common ST datasets in multiple spatial scales. The source code will be publicly available.         ",
    "url": "https://arxiv.org/abs/2503.01347",
    "authors": [
      "Ruikun Zhang",
      "Yan Yang",
      "Liyuan Pan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.02662",
    "title": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared Small-Target Detection",
    "abstract": "           The widespread deployment of Infrared Small-Target Detection (IRSTD) algorithms on edge devices necessitates the exploration of model compression techniques. Binarized neural networks (BNNs) are distinguished by their exceptional efficiency in model compression. However, the small size of infrared targets introduces stringent precision requirements for the IRSTD task, while the inherent precision loss during binarization presents a significant challenge. To address this, we propose the Binarized Infrared Small-Target Detection Network (BiisNet), which preserves the core operations of binarized convolutions while integrating full-precision features into the network's information flow. Specifically, we propose the Dot Binary Convolution, which retains fine-grained semantic information in feature maps while still leveraging the binarized convolution operations. In addition, we introduce a smooth and adaptive Dynamic Softsign function, which provides more comprehensive and progressively finer gradient during backpropagation, enhancing model stability and promoting an optimal weight distribution. Experimental results demonstrate that BiisNet not only significantly outperforms other binary architectures but also has strong competitiveness among state-of-the-art full-precision models.         ",
    "url": "https://arxiv.org/abs/2503.02662",
    "authors": [
      "Biqiao Xin",
      "Qianchen Mao",
      "Bingshu Wang",
      "Jiangbin Zheng",
      "Yong Zhao",
      "C.L. Philip Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.04154",
    "title": "CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection",
    "abstract": "           Weakly supervised monocular 3D detection, while less annotation-intensive, often struggles to capture the global context required for reliable 3D reasoning. Conventional label-efficient methods focus on object-centric features, neglecting contextual semantic relationships that are critical in complex scenes. In this work, we propose a Context-Aware Weak Supervision for Monocular 3D object detection, namely CA-W3D, to address this limitation in a two-stage training paradigm. Specifically, we first introduce a pre-training stage employing Region-wise Object Contrastive Matching (ROCM), which aligns regional object embeddings derived from a trainable monocular 3D encoder and a frozen open-vocabulary 2D visual grounding model. This alignment encourages the monocular encoder to discriminate scene-specific attributes and acquire richer contextual knowledge. In the second stage, we incorporate a pseudo-label training process with a Dual-to-One Distillation (D2OD) mechanism, which effectively transfers contextual priors into the monocular encoder while preserving spatial fidelity and maintaining computational efficiency during inference. Extensive experiments conducted on the public KITTI benchmark demonstrate the effectiveness of our approach, surpassing the SoTA method over all metrics, highlighting the importance of contextual-aware knowledge in weakly-supervised monocular 3D detection.         ",
    "url": "https://arxiv.org/abs/2503.04154",
    "authors": [
      "Chupeng Liu",
      "Runkai Zhao",
      "Weidong Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.06453",
    "title": "Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via Neuron Activation Variation",
    "abstract": "           In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks.         ",
    "url": "https://arxiv.org/abs/2503.06453",
    "authors": [
      "Shengfang Zhai",
      "Jiajun Li",
      "Yue Liu",
      "Huanran Chen",
      "Zhihua Tian",
      "Wenjie Qu",
      "Qingni Shen",
      "Ruoxi Jia",
      "Yinpeng Dong",
      "Jiaheng Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2503.06461",
    "title": "Long-tailed Adversarial Training with Self-Distillation",
    "abstract": "           Adversarial training significantly enhances adversarial robustness, yet superior performance is predominantly achieved on balanced datasets. Addressing adversarial robustness in the context of unbalanced or long-tailed distributions is considerably more challenging, mainly due to the scarcity of tail data instances. Previous research on adversarial robustness within long-tailed distributions has primarily focused on combining traditional long-tailed natural training with existing adversarial robustness this http URL this study, we provide an in-depth analysis for the challenge that adversarial training struggles to achieve high performance on tail classes in long-tailed distributions. Furthermore, we propose a simple yet effective solution to advance adversarial robustness on long-tailed distributions through a novel self-distillation technique. Specifically, this approach leverages a balanced self-teacher model, which is trained using a balanced dataset sampled from the original long-tailed dataset. Our extensive experiments demonstrate state-of-the-art performance in both clean and robust accuracy for long-tailed adversarial robustness, with significant improvements in tail class performance on various datasets. We improve the accuracy against PGD attacks for tail classes by 20.3, 7.1, and 3.8 percentage points on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, while achieving the highest robust accuracy.         ",
    "url": "https://arxiv.org/abs/2503.06461",
    "authors": [
      "Seungju Cho",
      "Hongsin Lee",
      "Changick Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.12218",
    "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels",
    "abstract": "           Deep learning has shown remarkable success in medical image analysis, but its reliance on large volumes of high-quality labeled data limits its applicability. While noisy labeled data are easier to obtain, directly incorporating them into training can degrade model performance. To address this challenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC) self-ensemble framework for robust medical image segmentation with noisy labels. The framework leverages the Mean Teacher architecture to ensure consistent learning under noise perturbations. It includes an adaptive label refinement mechanism that dynamically captures and weights differences across multiple disturbance versions to enhance the quality of noisy labels. Additionally, a sample-level uncertainty-based label selection algorithm is introduced to prioritize high-confidence samples for network updates, mitigating the impact of noisy annotations. Consistency learning is integrated to align the predictions of the student and teacher networks, further enhancing model robustness. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed framework, showing significant improvements in segmentation performance. By fully exploiting the strengths of the Mean Teacher structure, the ALC framework effectively processes noisy labels, adapts to challenging scenarios, and achieves competitive results compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2503.12218",
    "authors": [
      "Chengxuan Qian",
      "Kai Han",
      "Jianxia Ding",
      "Lei Liu",
      "Chongwen Lyu",
      "Zhenlong Yuan",
      "Jun Chen",
      "Zhe Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13123",
    "title": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
    "abstract": "           Simulating the complex interactions between soft tissues and rigid anatomy is critical for applications in surgical training, planning, and robotic-assisted interventions. Traditional Finite Element Method (FEM)-based simulations, while accurate, are computationally expensive and impractical for real-time scenarios. Learning-based approaches have shown promise in accelerating predictions but have fallen short in modeling soft-rigid interactions effectively. We introduce MIXPINN, a physics-informed Graph Neural Network (GNN) framework for mixed-material simulations, explicitly capturing soft-rigid interactions using graph-based augmentations. Our approach integrates Virtual Nodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint satisfaction while preserving computational efficiency. By leveraging a graph-based representation of biomechanical structures, MIXPINN learns high-fidelity deformations from FEM-generated data and achieves real-time inference with sub-millimeter accuracy. We validate our method in a realistic clinical scenario, demonstrating superior performance compared to baseline GNN models and traditional FEM methods. Our results show that MIXPINN reduces computational cost by an order of magnitude while maintaining high physical accuracy, making it a viable solution for real-time surgical simulation and robotic-assisted procedures.         ",
    "url": "https://arxiv.org/abs/2503.13123",
    "authors": [
      "Xintian Yuan",
      "Yunke Ao",
      "Boqi Chen",
      "Philipp Fuernstahl"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.13652",
    "title": "Web Artifact Attacks Disrupt Vision Language Models",
    "abstract": "           Vision-language models (VLMs) (e.g. CLIP, LLaVA) are trained on large-scale, lightly curated web datasets, leading them to learn unintended correlations between semantic concepts and unrelated visual signals. These associations degrade model accuracy by causing predictions to rely on incidental patterns rather than genuine visual understanding. Prior work has weaponized these correlations as an attack vector to manipulate model predictions, such as inserting a deceiving class text onto the image in a \"typographic\" attack. These attacks succeed due to VLMs' text-heavy bias-a result of captions that echo visible words rather than describing content. However, this attack has focused solely on text that matches the target class exactly, overlooking a broader range of correlations, including non-matching text and graphical symbols, which arise from the abundance of branding content in web-scale data. To address this gap, we introduce \"artifact-based\" attacks: a novel class of manipulations that mislead models using both non-matching text and graphical elements. Unlike typographic attacks, these artifacts are not predefined, making them simultaneously harder to defend against and more challenging to find. We address this by framing artifact attacks as a search problem and demonstrate their effectiveness across five datasets, with some artifacts reinforcing each other to reach 100% attack success rates. These attacks transfer across models with up to 90% effectiveness, making it possible to attack unseen models. To defend against these attacks, we extend prior work's artifact aware prompting to the graphical setting. We see a moderate reduction of success rates of up to 15% relative to standard prompts, suggesting a promising direction for enhancing model robustness. Code: this https URL ",
    "url": "https://arxiv.org/abs/2503.13652",
    "authors": [
      "Maan Qraitem",
      "Piotr Teterwak",
      "Kate Saenko",
      "Bryan A. Plummer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.22452",
    "title": "On the Solvability of Byzantine-tolerant Reliable Communication in Dynamic Networks",
    "abstract": "           A reliable communication primitive guarantees the delivery, integrity, and authorship of messages exchanged between processes of a distributed system. We investigate the necessary and sufficient conditions for reliable communication in dynamic networks, where the network topology evolves over time despite the presence of a limited number of Byzantine faulty processes that may behave arbitrarily (i.e., in the globally bounded Byzantine failure model). We identify classes of dynamic networks where such conditions are satisfied, and extend our analysis to message losses, local computation with unbounded finite delay, and authenticated messages. Our investigation builds on the seminal characterization by Maurer, Tixeuil, and D{\u00e9}fago (2015)         ",
    "url": "https://arxiv.org/abs/2503.22452",
    "authors": [
      "Silvia Bonomi",
      "Giovanni Farina",
      "S\u00e9bastien Tixeuil"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2504.01308",
    "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
    "abstract": "           Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.01308",
    "authors": [
      "Jiawei Wang",
      "Yushen Zuo",
      "Yuanjun Chai",
      "Zhendong Liu",
      "Yicheng Fu",
      "Yichun Feng",
      "Kin-Man Lam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05108",
    "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
    "abstract": "           Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on combinatorial optimization tasks demonstrate that integrating RL with evolutionary search accelerates the discovery of superior algorithms, showcasing the potential of RL-enhanced evolutionary strategies for algorithm design.         ",
    "url": "https://arxiv.org/abs/2504.05108",
    "authors": [
      "Anja Surina",
      "Amin Mansouri",
      "Lars Quaedvlieg",
      "Amal Seddas",
      "Maryna Viazovska",
      "Emmanuel Abbe",
      "Caglar Gulcehre"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.08129",
    "title": "Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning",
    "abstract": "           Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal dependencies between edge events. Prior work justified sinusoidal encodings because their inner products depend on the time spans between events, which are crucial features for modeling inter-event relations. However, sinusoidal encodings inherently lose temporal information due to their many-to-one nature and therefore require high dimensions. In this paper, we rigorously study a simpler alternative: the linear time encoder, which avoids temporal information loss caused by sinusoidal functions and reduces the need for high-dimensional time encoders. We show that the self-attention mechanism can effectively learn to compute time spans between events from linear time encodings and extract relevant temporal patterns. Through extensive experiments on six dynamic graph datasets, we demonstrate that the linear time encoder improves the performance of TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead to significant savings in model parameters with minimal performance loss. For example, compared to a 100-dimensional sinusoidal time encoder, TGAT with a 2-dimensional linear time encoder saves 43% of parameters and achieves higher average precision on five datasets. While both encoders can be used simultaneously, our study highlights the often-overlooked advantages of linear time features in modern dynamic graph models. These findings can positively impact the design choices of various dynamic graph learning architectures and eventually benefit temporal network applications such as recommender systems, communication networks, and traffic forecasting.         ",
    "url": "https://arxiv.org/abs/2504.08129",
    "authors": [
      "Hsing-Huan Chung",
      "Shravan Chaudhari",
      "Xing Han",
      "Yoav Wald",
      "Suchi Saria",
      "Joydeep Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.12970",
    "title": "MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven Bi-Level Optimization for Anomaly Detection",
    "abstract": "           Currently, industrial anomaly detection suffers from two bottlenecks: (i) the rarity of real-world defect images and (ii) the opacity of sample quality when synthetic data are used. Existing synthetic strategies (e.g., cut-and-paste) overlook the underlying physical causes of defects, leading to inconsistent, low-fidelity anomalies that hamper model generalization to real-world complexities. In this paper, we introduce a novel and lightweight pipeline that generates synthetic anomalies through Math-Phys model guidance, refines them via a Coarse-to-Fine approach and employs a bi-level optimization strategy with a Synthesis Quality Estimator (SQE). By combining physical modeling of the three most typical physics-driven defect mechanisms: Fracture Line (FL), Pitting Loss (PL), and Plastic Warpage (PW), our method produces realistic defect masks, which are subsequently enhanced in two phases. The first stage (npcF) enforces a PDE-based consistency to achieve a globally coherent anomaly structure, while the second stage (npcF++) further improves local fidelity. Additionally, we leverage SQE-driven weighting, ensuring that high-quality synthetic samples receive greater emphasis during training. To validate our method, we conduct experiments on three anomaly detection benchmarks: MVTec AD, VisA, and BTAD. Across these datasets, our method achieves state-of-the-art results in both image- and pixel-AUROC, confirming the effectiveness of our MaPhC2F dataset and BiSQAD method. All code will be released.         ",
    "url": "https://arxiv.org/abs/2504.12970",
    "authors": [
      "Long Qian",
      "Bingke Zhu",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.17349",
    "title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition",
    "abstract": "           Personalized image generation has emerged as a promising direction in multimodal content creation. It aims to synthesize images tailored to individual style preferences (e.g., color schemes, character appearances, layout) and semantic intentions (e.g., emotion, action, scene contexts) by leveraging user-interacted history images and multimodal instructions. Despite notable progress, existing methods -- whether based on diffusion models, large language models, or Large Multimodal Models (LMMs) -- struggle to accurately capture and fuse user style preferences and semantic intentions. In particular, the state-of-the-art LMM-based method suffers from the entanglement of visual features, leading to Guidance Collapse, where the generated images fail to preserve user-preferred styles or reflect the specified semantics. To address these limitations, we introduce DRC, a novel personalized image generation framework that enhances LMMs through Disentangled Representation Composition. DRC explicitly extracts user style preferences and semantic intentions from history images and the reference image, respectively, to form user-specific latent instructions that guide image generation within LMMs. Specifically, it involves two critical learning stages: 1) Disentanglement learning, which employs a dual-tower disentangler to explicitly separate style and semantic features, optimized via a reconstruction-driven paradigm with difficulty-aware importance sampling; and 2) Personalized modeling, which applies semantic-preserving augmentations to effectively adapt the disentangled representations for robust personalized generation. Extensive experiments on two benchmarks demonstrate that DRC shows competitive performance while effectively mitigating the guidance collapse issue, underscoring the importance of disentangled representation learning for controllable and effective personalized image generation.         ",
    "url": "https://arxiv.org/abs/2504.17349",
    "authors": [
      "Yiyan Xu",
      "Wuqiang Zheng",
      "Wenjie Wang",
      "Fengbin Zhu",
      "Xinting Hu",
      "Yang Zhang",
      "Fuli Feng",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2505.00404",
    "title": "iMacHSR: Intermediate Multi-Access Heterogeneous Supervision and Regularization Scheme Toward Architecture-Agnostic Training",
    "abstract": "           While deep supervision is a powerful training strategy by supervising intermediate layers with auxiliary losses, it faces three underexplored problems: (I) Existing deep supervision techniques are generally bond with specific model architectures strictly, lacking generality. (II) The identical loss function for intermediate and output layers causes intermediate layers to prioritize output-specific features prematurely, limiting generalizable representations. (III) Lacking regularization on hidden activations risks overconfident predictions, reducing generalization to unseen scenarios. To tackle these challenges, we propose an architecture-agnostic, intermediate Multi-access Heterogeneous Supervision and Regularization (iMacHSR) scheme. Specifically, the proposed iMacHSR introduces below integral strategies: (I) we select multiple intermediate layers based on predefined architecture-agnostic standards; (II) loss functions (different from output-layer loss) are applied to those selected intermediate layers, which can guide intermediate layers to learn diverse and hierarchical representations; and (III) negative entropy regularization on selected layers' hidden features discourages overconfident predictions and mitigates overfitting. These intermediate terms are combined into the output-layer training loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. We then take the semantic understanding task as an example to assess iMacHSR and apply iMacHSR to several model architectures. Extensive experiments on multiple datasets demonstrate that iMacHSR outperforms conventional output-layer single-point supervision method up to 9.19% in mIoU.         ",
    "url": "https://arxiv.org/abs/2505.00404",
    "authors": [
      "Wei-Bin Kou",
      "Guangxu Zhu",
      "Yichen Jin",
      "Bingyang Cheng",
      "Shuai Wang",
      "Ming Tang",
      "Yik-Chung Wu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.02331",
    "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection",
    "abstract": "           Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage~1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage~2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations.         ",
    "url": "https://arxiv.org/abs/2505.02331",
    "authors": [
      "Hao Cheng",
      "Zhiwei Zhao",
      "Yichao He",
      "Zhenzhen Hu",
      "Jia Li",
      "Meng Wang",
      "Richang Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2505.07167",
    "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models",
    "abstract": "           Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \\texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \\ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods.         ",
    "url": "https://arxiv.org/abs/2505.07167",
    "authors": [
      "Haoran Gu",
      "Handing Wang",
      "Yi Mei",
      "Mengjie Zhang",
      "Yaochu Jin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.12332",
    "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
    "abstract": "           Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.12332",
    "authors": [
      "Qianyue Hu",
      "Junyan Wu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.12842",
    "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents",
    "abstract": "           Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\\% over the best-performing baseline while only increasing training time by 4.9\\% and testing time by 6.5\\%. We also experimentally demonstrate that GEM can improve the step-wise success rate by 9.40\\% by requesting assistance from the cloud model when encountering OOD samples. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.12842",
    "authors": [
      "Zheng Wu",
      "Pengzhou Cheng",
      "Zongru Wu",
      "Lingzhong Dong",
      "Zhuosheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.12994",
    "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
    "abstract": "           Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.         ",
    "url": "https://arxiv.org/abs/2505.12994",
    "authors": [
      "Xuanjun Chen",
      "I-Ming Lin",
      "Lin Zhang",
      "Jiawei Du",
      "Haibin Wu",
      "Hung-yi Lee",
      "Jyh-Shing Roger Jang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.13023",
    "title": "Anti-Inpainting: A Proactive Defense Approach against Malicious Diffusion-based Inpainters under Unknown Conditions",
    "abstract": "           With the increasing prevalence of diffusion-based malicious image manipulation, existing proactive defense methods struggle to safeguard images against tampering under unknown conditions. To address this, we propose Anti-Inpainting, a proactive defense approach that achieves protection comprising three novel modules. First, we introduce a multi-level deep feature extractor to obtain intricate features from the diffusion denoising process, enhancing protective effectiveness. Second, we design a multi-scale, semantic-preserving data augmentation technique to enhance the transferability of adversarial perturbations across unknown conditions. Finally, we propose a selection-based distribution deviation optimization strategy to bolster protection against manipulations guided by diverse random seeds. Extensive experiments on InpaintGuardBench and CelebA-HQ demonstrate that Anti-Inpainting effectively defends against diffusion-based inpainters under unknown conditions. Additionally, our approach demonstrates robustness against various image purification methods and transferability across different diffusion model versions.         ",
    "url": "https://arxiv.org/abs/2505.13023",
    "authors": [
      "Yimao Guo",
      "Zuomin Qu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2505.16350",
    "title": "Sensing-Enhanced Handover Criterion for Low-Altitude Wireless Network (LAWNs)",
    "abstract": "           With the rapid growth of the low-altitude economy, the demand for cellular-enabled low-altitude wireless networks (LAWN) is rising significantly. The three-dimensional mobility of drones will lead to frequent handovers (HOs) in cellular networks, while traditional reference signal received power (RSRP)-based criteria may fail to capture the dynamic environment, causing redundant HOs or HO failures. To address this issue and motivated by the underutilization of sensing information in conventional HO mechanisms, we propose a novel HO activation criterion for drone systems that integrates both sensing parameters provided by integrated sensing and communication (ISAC) signals and RSRP. First, we construct an ISAC signal model tailored for low-altitude scenarios and derive the Cram\u00e9r--Rao lower bound for sensing distance estimation. Subsequently, we propose a novel joint HO criterion that extends the conventional RSRP-based method by integrating sensing information from ISAC signals, enabling more reliable HOs in dynamic drone environments. Simulation results show that the joint HO criterion outperforms the baseline RSRP-based criterion under different signal-to-noise ratio (SNR) and sensing pilot ratio conditions. Particularly, when the SNR exceeds 0dB and the sensing pilot ratio is 20%, the proposed joint HO criterion reduces the average HO region length by 75.20% and improves the activation probability by 76.31%.         ",
    "url": "https://arxiv.org/abs/2505.16350",
    "authors": [
      "Jingli Li",
      "Yiyan Ma",
      "Bo Ai",
      "Weijie Yuan",
      "Qingqing Cheng",
      "Guoyu Ma",
      "Mi Yang",
      "Yunlong Lu",
      "Wenwei Yue",
      "Zhangdui Zhong"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.19785",
    "title": "medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support",
    "abstract": "           Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses can vary significantly and evolve over time. Clinical data used to support these treatment decisions are often irregularly sampled, where missing data frequencies may implicitly convey information about the patient's condition. Existing Reinforcement Learning (RL) based clinical decision support systems often ignore the missing patterns and distort them with coarse discretization and simple imputation. They are also predominantly model-free and largely depend on retrospective data, which could lead to insufficient exploration and bias by historical behaviors. To address these limitations, we propose medDreamer, a novel model-based reinforcement learning framework for personalized treatment recommendation. medDreamer contains a world model with an Adaptive Feature Integration module that simulates latent patient states from irregular data and a two-phase policy trained on a hybrid of real and imagined trajectories. This enables learning optimal policies that go beyond the sub-optimality of historical clinical decisions, while remaining close to real clinical data. We evaluate medDreamer on both sepsis and mechanical ventilation treatment tasks using two large-scale Electronic Health Records (EHRs) datasets. Comprehensive evaluations show that medDreamer significantly outperforms model-free and model-based baselines in both clinical outcomes and off-policy metrics.         ",
    "url": "https://arxiv.org/abs/2505.19785",
    "authors": [
      "Qianyi Xu",
      "Gousia Habib",
      "Dilruk Perera",
      "Mengling Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.22011",
    "title": "Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming",
    "abstract": "           Livestreaming often involves interactions between streamers and objects, which is critical for understanding and regulating web content. While human-object interaction (HOI) detection has made some progress in general-purpose video downstream tasks, when applied to recognize the interaction behaviors between a streamer and different objects in livestreaming, it tends to focuses too much on the objects and neglects their interactions with the streamer, which leads to object bias. To solve this issue, we propose a prototype embedding optimization for human-object interaction detection (PeO-HOI). First, the livestreaming is preprocessed using object detection and tracking techniques to extract features of the human-object (HO) pairs. Then, prototype embedding optimization is adopted to mitigate the effect of object bias on HOI. Finally, after modelling the spatio-temporal context between HO pairs, the HOI detection results are obtained by the prediction head. The experimental results show that the detection accuracy of the proposed PeO-HOI method has detection accuracies of 37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset BJUT-HOI, which effectively improves the HOI detection performance in livestreaming.         ",
    "url": "https://arxiv.org/abs/2505.22011",
    "authors": [
      "Menghui Zhang",
      "Jing Zhang",
      "Lin Chen",
      "Li Zhuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.23124",
    "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals",
    "abstract": "           We initiate the study of a repeated principal-agent problem over a finite horizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of agents arriving in an adversarial order. At each round, the principal strategically chooses one of the $N$ arms to incentivize for an arriving agent of unknown type. The agent then chooses an arm based on its own utility and the provided incentive, and the principal receives a corresponding reward. The objective is to minimize regret against the best incentive in hindsight. Without prior knowledge of agent behavior, we show that the problem becomes intractable, leading to linear regret. We analyze two key settings where sublinear regret is achievable. In the first setting, the principal knows the arm each agent type would select greedily for any given incentive. Under this setting, we propose an algorithm that achieves a regret bound of $O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to a $\\log K$ factor. In the second setting, an agent's response varies smoothly with the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under this setting, we show that there is an algorithm with a regret bound of $\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to logarithmic factors. Finally, we extend our algorithmic results for both settings by allowing the principal to incentivize multiple arms simultaneously in each round.         ",
    "url": "https://arxiv.org/abs/2505.23124",
    "authors": [
      "Junyan Liu",
      "Arnab Maiti",
      "Artin Tajdini",
      "Kevin Jamieson",
      "Lillian J. Ratliff"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.24110",
    "title": "Neural Networks as Universal Finite-State Machines: A Constructive Feedforward Simulation Framework for NFAs",
    "abstract": "           We present a formal and constructive simulation framework for nondeterministic finite automata (NFAs) using standard feedforward neural networks. Unlike prior approaches that rely on recurrent architectures or post hoc extraction methods, our formulation symbolically encodes automaton states as binary vectors, transitions as sparse matrix transformations, and nondeterministic branching-including $\\varepsilon$-closures-as compositions of shared thresholded updates. We prove that every regular language can be recognized exactly by a depth-unrolled feedforward network with shared parameters, independent of input length. Our construction yields not only formal equivalence between NFAs and neural networks, but also practical trainability: we demonstrate that these networks can learn NFA acceptance behavior through gradient descent using standard supervised data. Extensive experiments validate all theoretical results, achieving perfect or near-perfect agreement on acceptance, state propagation, and closure dynamics. This work establishes a new bridge between symbolic automata theory and modern neural architectures, showing that feedforward networks can perform precise, interpretable, and trainable symbolic computation.         ",
    "url": "https://arxiv.org/abs/2505.24110",
    "authors": [
      "Sahil Rajesh Dhayalkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2506.01212",
    "title": "Dynamic Modes as Time Representation for Spatiotemporal Forecasting",
    "abstract": "           This paper introduces a data-driven time embedding method for modeling long-range seasonal dependencies in spatiotemporal forecasting tasks. The proposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal modes directly from observed data, eliminating the need for explicit timestamps or hand-crafted time features. These temporal modes serve as time representations that can be seamlessly integrated into deep spatiotemporal forecasting models. Unlike conventional embeddings such as time-of-day indicators or sinusoidal functions, our method captures complex multi-scale periodicity through spectral analysis of spatiotemporal data. Extensive experiments on urban mobility, highway traffic, and climate datasets demonstrate that the DMD-based embedding consistently improves long-horizon forecasting accuracy, reduces residual correlation, and enhances temporal generalization. The method is lightweight, model-agnostic, and compatible with any architecture that incorporates time covariates.         ",
    "url": "https://arxiv.org/abs/2506.01212",
    "authors": [
      "Menglin Kong",
      "Vincent Zhihao Zheng",
      "Xudong Wang",
      "Lijun Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.01752",
    "title": "A High-Performance Evolutionary Multiobjective Community Detection Algorithm",
    "abstract": "           Community detection in complex networks is fundamental across social, biological, and technological domains. While traditional single-objective methods like Louvain and Leiden are computationally efficient, they suffer from resolution bias and structural degeneracy. Multi-objective evolutionary algorithms (MOEAs) address these limitations by simultaneously optimizing conflicting structural criteria, however, their high computational costs have historically limited their application to small networks. We present HP-MOCD, a High-Performance Evolutionary Multiobjective Community Detection Algorithm built on Non-dominated Sorting Genetic Algorithm II (NSGA-II), which overcomes these barriers through topology-aware genetic operators, full parallelization, and bit-level optimizations, achieving theoretical O(GN_p|V|) complexity. We conduct experiments on both synthetic and real-world networks. Results demonstrate strong scalability, with HP-MOCD processing networks of over 1,000,000 nodes while maintaining high quality across varying noise levels. It outperforms other MOEAs by more than 531 times in runtime on synthetic datasets, achieving runtimes as low as 57 seconds for graphs with 40,000 nodes on moderately powered hardware. Across 14 real-world networks, HP-MOCD was the only MOEA capable of processing the six largest datasets within a reasonable time, with results competitive with single-objective approaches. Unlike single-solution methods, HP-MOCD produces a Pareto Front, enabling individual-specific trade-offs and providing decision-makers with a spectrum of high-quality community structures. It introduces the first open-source Python MOEA library compatible with networkx and igraph for large-scale community detection.         ",
    "url": "https://arxiv.org/abs/2506.01752",
    "authors": [
      "Guilherme O. Santos",
      "Lucas S. Vieira",
      "Giulio Rossetti",
      "Carlos H. G. Ferreira",
      "Gladston Moreira"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2506.05982",
    "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks",
    "abstract": "           As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.         ",
    "url": "https://arxiv.org/abs/2506.05982",
    "authors": [
      "Zonglin Wu",
      "Yule Xue",
      "Xin Wei",
      "Yiren Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.08163",
    "title": "SpINRv2: Implicit Neural Representation for Passband FMCW Radars",
    "abstract": "           We present SpINRv2, a neural framework for high-fidelity volumetric reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar. Extending our prior work (SpINR), this version introduces enhancements that allow accurate learning under high start frequencies-where phase aliasing and sub-bin ambiguity become prominent. Our core contribution is a fully differentiable frequency-domain forward model that captures the complex radar response using closed-form synthesis, paired with an implicit neural representation (INR) for continuous volumetric scene modeling. Unlike time-domain baselines, SpINRv2 directly supervises the complex frequency spectrum, preserving spectral fidelity while drastically reducing computational overhead. Additionally, we introduce sparsity and smoothness regularization to disambiguate sub-bin ambiguities that arise at fine range resolutions. Experimental results show that SpINRv2 significantly outperforms both classical and learning-based baselines, especially under high-frequency regimes, establishing a new benchmark for neural radar-based 3D imaging.         ",
    "url": "https://arxiv.org/abs/2506.08163",
    "authors": [
      "Harshvardhan Takawale",
      "Nirupam Roy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.08255",
    "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense",
    "abstract": "           Continual learning under adversarial conditions remains an open problem, as existing methods often compromise either robustness, scalability, or both. We propose a novel framework that integrates Interval Bound Propagation (IBP) with a hypernetwork-based architecture to enable certifiably robust continual learning across sequential tasks. Our method, SHIELD, generates task-specific model parameters via a shared hypernetwork conditioned solely on compact task embeddings, eliminating the need for replay buffers or full model copies and enabling efficient over time. To further enhance robustness, we introduce Interval MixUp, a novel training strategy that blends virtual examples represented as $\\ell_{\\infty}$ balls centered around MixUp points. Leveraging interval arithmetic, this technique guarantees certified robustness while mitigating the wrapping effect, resulting in smoother decision boundaries. We evaluate SHIELD under strong white-box adversarial attacks, including PGD and AutoAttack, across multiple benchmarks. It consistently outperforms existing robust continual learning methods, achieving state-of-the-art average accuracy while maintaining both scalability and certification. These results represent a significant step toward practical and theoretically grounded continual learning in adversarial settings.         ",
    "url": "https://arxiv.org/abs/2506.08255",
    "authors": [
      "Patryk Krukowski",
      "\u0141ukasz Gorczyca",
      "Piotr Helm",
      "Kamil Ksi\u0105\u017cek",
      "Przemys\u0142aw Spurek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.08363",
    "title": "FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs",
    "abstract": "           In the architectural design process, floorplan design is often a dynamic and iterative process. Architects progressively draw various parts of the floorplan according to their ideas and requirements, continuously adjusting and refining throughout the design process. Therefore, the ability to predict a complete floorplan from a partial one holds significant value in the design process. Such prediction can help architects quickly generate preliminary designs, improve design efficiency, and reduce the workload associated with repeated modifications. To address this need, we propose FloorplanMAE, a self-supervised learning framework for restoring incomplete floor plans into complete ones. First, we developed a floor plan reconstruction dataset, FloorplanNet, specifically trained on architectural floor plans. Secondly, we propose a floor plan reconstruction method based on Masked Autoencoders (MAE), which reconstructs missing parts by masking sections of the floor plan and training a lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally, we validated the model using real sketches from the early stages of architectural design. Experimental results show that the FloorplanMAE model can generate high-quality complete floor plans from incomplete partial plans. This framework provides a scalable solution for floor plan generation, with broad application prospects.         ",
    "url": "https://arxiv.org/abs/2506.08363",
    "authors": [
      "Jun Yin",
      "Jing Zhong",
      "Pengyu Zeng",
      "Peilin Li",
      "Miao Zhang",
      "Ran Luo",
      "Shuai Lu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.10207",
    "title": "FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification",
    "abstract": "           Federated Learning (FL) offers a privacy-preserving framework for training audio classification (AC) models across decentralized clients without sharing raw data. However, Federated Audio Classification (FedAC) faces three major challenges: data heterogeneity, model heterogeneity, and data poisoning, which degrade performance in real-world settings. While existing methods often address these issues separately, a unified and robust solution remains underexplored. We propose FedMLAC, a mutual learning-based FL framework that tackles all three challenges simultaneously. Each client maintains a personalized local AC model and a lightweight, globally shared Plug-in model. These models interact via bidirectional knowledge distillation, enabling global knowledge sharing while adapting to local data distributions, thus addressing both data and model heterogeneity. To counter data poisoning, we introduce a Layer-wise Pruning Aggregation (LPA) strategy that filters anomalous Plug-in updates based on parameter deviations during aggregation. Extensive experiments on four diverse audio classification benchmarks, including both speech and non-speech tasks, show that FedMLAC consistently outperforms state-of-the-art baselines in classification accuracy and robustness to noisy data.         ",
    "url": "https://arxiv.org/abs/2506.10207",
    "authors": [
      "Jun Bai",
      "Rajib Rana",
      "Di Wu",
      "Youyang Qu",
      "Xiaohui Tao",
      "Ji Zhang",
      "Carlos Busso",
      "Shivakumara Palaiahnakote"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.12509",
    "title": "Graph of Verification: Structured Verification of LLM Reasoning with Directed Acyclic Graphs",
    "abstract": "           Verifying the complex and multi-step reasoning of Large Language Models (LLMs) is a critical challenge, as holistic methods often overlook localized flaws. Step-by-step validation is a promising alternative, yet existing methods are often rigid. They struggle to adapt to diverse reasoning structures, from formal proofs to informal natural language narratives. To address this adaptability gap, we propose the Graph of Verification (GoV), a novel framework for adaptable and multi-granular verification. GoV's core innovation is its flexible \"node block\" architecture. This mechanism allows GoV to adaptively adjust its verification granularity--from atomic steps for formal tasks to entire paragraphs for natural language--to match the native structure of the reasoning process. This flexibility allows GoV to resolve the fundamental trade-off between verification precision and robustness. Experiments on both well-structured and loosely-structured benchmarks demonstrate GoV's versatility. The results show that GoV's adaptive approach significantly outperforms both holistic baselines and other state-of-the-art decomposition-based methods, establishing a new standard for training-free reasoning verification.         ",
    "url": "https://arxiv.org/abs/2506.12509",
    "authors": [
      "Jiwei Fang",
      "Bin Zhang",
      "Changwei Wang",
      "Jin Wan",
      "Zhiwei Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17679",
    "title": "CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection",
    "abstract": "           Convolutional neural networks (CNNs) have long been the cornerstone of target detection, but they are often limited by limited receptive fields, which hinders their ability to capture global contextual information. We re-examined the DETR-inspired detection head and found substantial redundancy in its self-attention module. To solve these problems, we introduced the Context-Gated Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header inspired by human visual perception: when observing an object, we always concentrate on one site, perceive the surrounding environment, and glance around the object. This mechanism enables each region of interest (ROI) to adaptively select and combine feature dimensions and scale information from different patterns. CSDN provides more powerful global context modeling capabilities and can better adapt to objects of different sizes and structures. Our proposed detection head can directly replace the native heads of various CNN-based detectors, and only a few rounds of fine-tuning on the pre-trained weights can significantly improve the detection accuracy.         ",
    "url": "https://arxiv.org/abs/2506.17679",
    "authors": [
      "Haolin Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.21260",
    "title": "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic",
    "abstract": "           Real-world object detection systems, such as those in autonomous driving and surveillance, must continuously learn new object categories and simultaneously adapt to changing environmental conditions. Existing approaches, Class Incremental Object Detection (CIOD) and Domain Incremental Object Detection (DIOD) only address one aspect of this challenge. CIOD struggles in unseen domains, while DIOD suffers from catastrophic forgetting when learning new classes, limiting their real-world applicability. To overcome these limitations, we introduce Dual Incremental Object Detection (DuIOD), a more practical setting that simultaneously handles class and domain shifts in an exemplar-free manner. We propose DuET, a Task Arithmetic-based model merging framework that enables stable incremental learning while mitigating sign conflicts through a novel Directional Consistency Loss. Unlike prior methods, DuET is detector-agnostic, allowing models like YOLO11 and RT-DETR to function as real-time incremental object detectors. To comprehensively evaluate both retention and adaptation, we introduce the Retention-Adaptability Index (RAI), which combines the Average Retention Index (Avg RI) for catastrophic forgetting and the Average Generalization Index for domain adaptability into a common ground. Extensive experiments on the Pascal Series and Diverse Weather Series demonstrate DuET's effectiveness, achieving a +13.12% RAI improvement while preserving 89.3% Avg RI on the Pascal Series (4 tasks), as well as a +11.39% RAI improvement with 88.57% Avg RI on the Diverse Weather Series (3 tasks), outperforming existing methods.         ",
    "url": "https://arxiv.org/abs/2506.21260",
    "authors": [
      "Munish Monga",
      "Vishal Chudasama",
      "Pankaj Wasnik",
      "Biplab Banerjee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.21549",
    "title": "SiM3D: Single-instance Multiview Multimodal and Multisetup 3D Anomaly Detection Benchmark",
    "abstract": "           We propose SiM3D, the first benchmark considering the integration of multiview and multimodal information for comprehensive 3D anomaly detection and segmentation (ADS), where the task is to produce a voxel-based Anomaly Volume. Moreover, SiM3D focuses on a scenario of high interest in manufacturing: single-instance anomaly detection, where only one object, either real or synthetic, is available for training. In this respect, SiM3D stands out as the first ADS benchmark that addresses the challenge of generalising from synthetic training data to real test data. SiM3D includes a novel multimodal multiview dataset acquired using top-tier industrial sensors and robots. The dataset features multiview high-resolution images (12 Mpx) and point clouds (7M points) for 333 instances of eight types of objects, alongside a CAD model for each type. We also provide manually annotated 3D segmentation GTs for anomalous test samples. To establish reference baselines for the proposed multiview 3D ADS task, we adapt prominent singleview methods and assess their performance using novel metrics that operate on Anomaly Volumes.         ",
    "url": "https://arxiv.org/abs/2506.21549",
    "authors": [
      "Alex Costanzino",
      "Pierluigi Zama Ramirez",
      "Luigi Lella",
      "Matteo Ragaglia",
      "Alessandro Oliva",
      "Giuseppe Lisanti",
      "Luigi Di Stefano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.21562",
    "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction",
    "abstract": "           In the architectural design process, floor plan generation is inherently progressive and iterative. However, existing generative models for floor plans are predominantly end-to-end generation that produce an entire pixel-based layout in a single pass. This paradigm is often incompatible with the incremental workflows observed in real-world architectural practice. To address this issue, we draw inspiration from the autoregressive 'next token prediction' mechanism commonly used in large language models, and propose a novel 'next room prediction' paradigm tailored to architectural floor plan modeling. Experimental evaluation indicates that FPDS demonstrates competitive performance in comparison to diffusion models and Tell2Design in the text-to-floorplan task, indicating its potential applicability in supporting future intelligent architectural design.         ",
    "url": "https://arxiv.org/abs/2506.21562",
    "authors": [
      "Jun Yin",
      "Pengyu Zeng",
      "Jing Zhong",
      "Peilin Li",
      "Miao Zhang",
      "Ran Luo",
      "Shuai Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2506.21835",
    "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts",
    "abstract": "           The recent advancements in large foundation models have driven the success of open-set image segmentation, a task focused on segmenting objects beyond predefined categories. Among various prompt types (such as points, boxes, texts, and visual references), visual reference segmentation stands out for its unique flexibility and strong zero-shot capabilities. Recently, several SAM-based methods have made notable progress in this task by automatically generating prompts to guide SAM. However, these methods often generate prompts at boundaries of target regions due to suboptimal prompt encoder, which results in instability and reduced robustness. In this work, we introduce ProSAM, a simple but effective method to address the stability challenges we identified in existing SAM-based visual reference segmentation approaches. By learning a variational prompt encoder to predict multivariate prompt distributions, ProSAM avoids generating prompts that lie in unstable regions, overcoming the instability caused by less robust prompts. Our approach consistently surpasses state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets, providing a more robust solution for visual reference segmentation.         ",
    "url": "https://arxiv.org/abs/2506.21835",
    "authors": [
      "Xiaoqi Wang",
      "Clint Sebastian",
      "Wenbin He",
      "Liu Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.21848",
    "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification",
    "abstract": "           Deep learning has significantly advanced NLP, but its reliance on large black-box models introduces critical interpretability and computational efficiency concerns. This paper proposes LinguaSynth, a novel text classification framework that strategically integrates five complementary linguistic feature types: lexical, syntactic, entity-level, word-level semantics, and document-level semantics within a transparent logistic regression model. Unlike transformer-based architectures, LinguaSynth maintains interpretability and computational efficiency, achieving an accuracy of 84.89 percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by 3.32 percent. Through rigorous feature interaction analysis, we show that syntactic and entity-level signals provide essential disambiguation and effectively complement distributional semantics. LinguaSynth sets a new benchmark for interpretable, resource-efficient NLP models and challenges the prevailing assumption that deep neural networks are necessary for high-performing text classification.         ",
    "url": "https://arxiv.org/abs/2506.21848",
    "authors": [
      "Duo Zhang",
      "Junyi Mo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2507.01607",
    "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems",
    "abstract": "           The widespread use of deep learning face recognition raises several security concerns. Although prior works point at existing vulnerabilities, DNN backdoor attacks against real-life, unconstrained systems dealing with images captured in the wild remain a blind spot of the literature. This paper conducts the first system-level study of backdoors in deep learning-based face recognition systems. This paper yields four contributions by exploring the feasibility of DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the first time two backdoor attacks on the face detection task: face generation and face landmark shift attacks. We then show that face feature extractors trained with large margin losses also fall victim to backdoor attacks. Combining our models, we then show using 20 possible pipeline configurations and 15 attack cases that a single backdoor enables an attacker to bypass the entire function of a system. Finally, we provide stakeholders with several best practices and countermeasures.         ",
    "url": "https://arxiv.org/abs/2507.01607",
    "authors": [
      "Quentin Le Roux",
      "Yannick Teglia",
      "Teddy Furon",
      "Philippe Loubet-Moundi",
      "Eric Bourbao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.02724",
    "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms",
    "abstract": "           Recent advances in AI for science have highlighted the power of contrastive learning in bridging heterogeneous biological data modalities. Building on this paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction across Organisms), a hierarchical contrastive framework for protein-protein interaction(PPI) prediction, where protein sequences and their hierarchical attributes are aligned through multi-tiered biological representation matching. The proposed approach incorporates hierarchical contrastive loss functions that emulate the structured relationship among functional classes of proteins. The framework adaptively incorporates domain and family knowledge through a data-driven penalty mechanism, enforcing consistency between the learned embedding space and the intrinsic hierarchy of protein functions. Experiments on benchmark datasets demonstrate that HIPPO achieves state-of-the-art performance, outperforming existing methods and showing robustness in low-data regimes. Notably, the model demonstrates strong zero-shot transferability to other species without retraining, enabling reliable PPI prediction and functional inference even in less characterized or rare organisms where experimental data are limited. Further analysis reveals that hierarchical feature fusion is critical for capturing conserved interaction determinants, such as binding motifs and functional annotations. This work advances cross-species PPI prediction and provides a unified framework for interaction prediction in scenarios with sparse or imbalanced multi-species data.         ",
    "url": "https://arxiv.org/abs/2507.02724",
    "authors": [
      "Shiyi Liu",
      "Buwen Liang",
      "Yuetong Fang",
      "Zixuan Jiang",
      "Renjing Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2507.08445",
    "title": "Clue-RAG: Towards Accurate and Cost-Efficient Graph-based RAG via Multi-Partite Graph and Query-Driven Iterative Retrieval",
    "abstract": "           Despite the remarkable progress of Large Language Models (LLMs), their performance in question answering (QA) remains limited by the lack of domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external information, often from graph-structured data. However, existing graph-based RAG methods suffer from poor graph quality due to incomplete extraction and insufficient utilization of query information during retrieval. To overcome these limitations, we propose Clue-RAG, a novel approach that introduces (1) a multi-partite graph index incorporates Chunk, knowledge unit, and entity to capture semantic content at multiple levels of granularity, coupled with a hybrid extraction strategy that reduces LLM token usage while still producing accurate and disambiguated knowledge units, and (2) Q-Iter, a query-driven iterative retrieval strategy that enhances relevance through semantic search and constrained graph traversal. Experiments on three QA benchmarks show that Clue-RAG significantly outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy and 113.51% higher F1 score while reducing indexing costs by 72.58%. Remarkably, Clue-RAG matches or outperforms baselines even without using an LLM for indexing. These results demonstrate the effectiveness and cost-efficiency of Clue-RAG in advancing graph-based RAG systems.         ",
    "url": "https://arxiv.org/abs/2507.08445",
    "authors": [
      "Yaodong Su",
      "Yixiang Fang",
      "Yingli Zhou",
      "Quanqing Xu",
      "Chuanhui Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.08597",
    "title": "ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection",
    "abstract": "           Machine learning models are commonly used for malware classification; however, they suffer from performance degradation over time due to concept drift. Adapting these models to changing data distributions requires frequent updates, which rely on costly ground truth annotations. While active learning can reduce the annotation burden, leveraging unlabeled data through semi-supervised learning remains a relatively underexplored approach in the context of malware detection. In this research, we introduce \\texttt{ADAPT}, a novel pseudo-labeling semi-supervised algorithm for addressing concept drift. Our model-agnostic method can be applied to various machine learning models, including neural networks and tree-based algorithms. We conduct extensive experiments on five diverse malware detection datasets spanning Android, Windows, and PDF domains. The results demonstrate that our method consistently outperforms baseline models and competitive benchmarks. This work paves the way for more effective adaptation of machine learning models to concept drift in malware detection.         ",
    "url": "https://arxiv.org/abs/2507.08597",
    "authors": [
      "Md Tanvirul Alam",
      "Aritran Piplai",
      "Nidhi Rastogi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2507.10340",
    "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models",
    "abstract": "           Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets.         ",
    "url": "https://arxiv.org/abs/2507.10340",
    "authors": [
      "Hongjae Lee",
      "Myungjun Son",
      "Dongjea Kang",
      "Seung-Won Jung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.14230",
    "title": "Intent-Based Network for RAN Management with Large Language Models",
    "abstract": "           Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems.         ",
    "url": "https://arxiv.org/abs/2507.14230",
    "authors": [
      "Fransiscus Asisi Bimo",
      "Maria Amparo Canaveras Galdon",
      "Chun-Kai Lai",
      "Ray-Guang Cheng",
      "Edwin K. P. Chong"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.14668",
    "title": "Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model",
    "abstract": "           Deep learning models have been widely adopted for False Data Injection Attack (FDIA) detection in smart grids due to their ability to capture unstructured and sparse features. However, the increasing system scale and data dimensionality introduce significant computational and memory burdens, particularly in large-scale industrial datasets, limiting detection efficiency. To address these issues, this paper proposes Rec-AD, a computationally efficient framework that integrates Tensor Train decomposition with the Deep Learning Recommendation Model (DLRM). Rec-AD enhances training and inference efficiency through embedding compression, optimized data access via index reordering, and a pipeline training mechanism that reduces memory communication overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing FDIA detection systems without code modifications. Experimental results show that Rec-AD significantly improves computational throughput and real-time detection performance, narrowing the attack window and increasing attacker cost. These advancements strengthen edge computing capabilities and scalability, providing robust technical support for smart grid security.         ",
    "url": "https://arxiv.org/abs/2507.14668",
    "authors": [
      "Yunfeng Li",
      "Junhong Liu",
      "Zhaohui Yang",
      "Guofu Liao",
      "Chuyun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.14999",
    "title": "Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data",
    "abstract": "           False Data Injection Attacks (FDIAs) pose severe security risks to smart grids by manipulating measurement data collected from spatially distributed devices such as SCADA systems and PMUs. These measurements typically exhibit Non-Independent and Identically Distributed (Non-IID) characteristics across different regions, which significantly challenges the generalization ability of detection models. Traditional centralized training approaches not only face privacy risks and data sharing constraints but also incur high transmission costs, limiting their scalability and deployment feasibility. To address these issues, this paper proposes a privacy-preserving federated learning framework, termed Federated Cluster Average (FedClusAvg), designed to improve FDIA detection in Non-IID and resource-constrained environments. FedClusAvg incorporates cluster-based stratified sampling and hierarchical communication (client-subserver-server) to enhance model generalization and reduce communication overhead. By enabling localized training and weighted parameter aggregation, the algorithm achieves accurate model convergence without centralizing sensitive data. Experimental results on benchmark smart grid datasets demonstrate that FedClusAvg not only improves detection accuracy under heterogeneous data distributions but also significantly reduces communication rounds and bandwidth consumption. This work provides an effective solution for secure and efficient FDIA detection in large-scale distributed power systems.         ",
    "url": "https://arxiv.org/abs/2507.14999",
    "authors": [
      "Yunfeng Li",
      "Junhong Liu",
      "Zhaohui Yang",
      "Guofu Liao",
      "Chuyun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2507.15542",
    "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation",
    "abstract": "           Zero-shot human-object interaction (HOI) detection remains a challenging task, particularly in generalizing to unseen actions. Existing methods address this challenge by tapping Vision-Language Models (VLMs) to access knowledge beyond the training data. However, they either struggle to distinguish actions involving the same object or demonstrate limited generalization to unseen classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both enhances generalization to unseen classes and improves action distinction. In training, HOLa decomposes VLM text features for given HOI classes via low-rank factorization, producing class-shared basis features and adaptable weights. These features and weights form a compact HOI representation that preserves shared information across classes, enhancing generalization to unseen classes. Subsequently, we refine action distinction by adapting weights for each HOI class and introducing human-object tokens to enrich visual interaction representations. To further distinguish unseen actions, we guide the weight adaptation with LLM-derived action regularization. Experimental results show that our method sets a new state-of-the-art across zero-shot HOI settings on HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.15542",
    "authors": [
      "Qinqian Lei",
      "Bo Wang",
      "Robby T. Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.18661",
    "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back",
    "abstract": "           Next Location Prediction is a fundamental task in the study of human mobility, with wide-ranging applications in transportation planning, urban governance, and epidemic forecasting. In practice, when humans attempt to predict the next location in a trajectory, they often visualize the trajectory on a map and reason based on road connectivity and movement trends. However, the vast majority of existing next-location prediction models do not reason over maps \\textbf{in the way that humans do}. Fortunately, the recent development of Vision-Language Models (VLMs) has demonstrated strong capabilities in visual perception and even visual reasoning. This opens up a new possibility: by rendering both the road network and trajectory onto an image and leveraging the reasoning abilities of VLMs, we can enable models to perform trajectory inference in a human-like manner. To explore this idea, we first propose a method called Vision-Guided Location Search (VGLS), which evaluates whether a general-purpose VLM is capable of trajectory-based reasoning without modifying any of its internal parameters. Based on insights from the VGLS results, we further propose our main approach: VLMLocPredictor, which is composed of two stages: In the first stage, we design two Supervised Fine-Tuning (SFT) tasks that help the VLM understand road network and trajectory structures and acquire basic reasoning ability on such visual inputs. In the second stage, we introduce Reinforcement Learning from Visual Map Feedback, enabling the model to self-improve its next-location prediction ability through interaction with the environment. Experiments conducted on datasets from four different cities show that our method achieves state-of-the-art (SOTA) performance and exhibits superior cross-city generalization compared to other LLM-based approaches.         ",
    "url": "https://arxiv.org/abs/2507.18661",
    "authors": [
      "Ruixing Zhang",
      "Yang Zhang",
      "Tongyu Zhu",
      "Leilei Sun",
      "Weifeng Lv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2507.20162",
    "title": "IFD: A Large-Scale Benchmark for Insider Filing Violation Detection",
    "abstract": "           Insider trading violations, particularly delayed disclosures of Form 4 filings, remain a persistent challenge for financial market surveillance. Despite regulatory requirements such as the two-business-day rule of the Securities and Exchange Commission (SEC), enforcement is limited by the lack of large-scale, labeled datasets and task-specific benchmarks. In this paper, we introduce the Insider Filing Delay (IFD) dataset, the first and largest publicly available resource for insider disclosure behavior, comprising over one million Form 4 transactions spanning two decades (2002 to 2025), with structured annotations on delay status, insider roles, governance factors, and firm-level financial indicators. IFD enables the first large-scale formulation of strategic disclosure violation detection as a binary classification task grounded in regulatory compliance. To demonstrate the utility of IFD, we propose MaBoost, a hybrid framework combining a Mamba-based state space encoder with XGBoost, achieving high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments across statistical baselines, deep learning models, and large language models confirm that MaBoost outperforms prior approaches, achieving an F1 score of up to 99.47 percent under constrained regulatory settings. IFD provides a realistic, reproducible, and behavior-rich dataset for developing AI models in financial compliance, regulatory forensics, and interpretable time series classification. All data and codes are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2507.20162",
    "authors": [
      "Cheng Huang",
      "Fan Gao",
      "Yutong Liu",
      "Yadi Liu",
      "Xiaoli Ma",
      "Ye Aung Moe",
      "Yuhan Zhang",
      "Yao Ma",
      "Hao Wang",
      "Xiangxiang Wang",
      "Yongbin Yu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2507.20356",
    "title": "Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach",
    "abstract": "           The virtual content in augmented reality (AR) can introduce misleading or harmful information, leading to semantic misunderstandings or user errors. In this work, we focus on visual information manipulation (VIM) attacks in AR, where virtual content changes the meaning of real-world scenes in subtle but impactful ways. We introduce a taxonomy that categorizes these attacks into three formats: character, phrase, and pattern manipulation, and three purposes: information replacement, information obfuscation, and extra wrong information. Based on the taxonomy, we construct a dataset, AR-VIM, which consists of 452 raw-AR video pairs spanning 202 different scenes, each simulating a real-world AR scenario. To detect the attacks in the dataset, we propose a multimodal semantic reasoning framework, VIM-Sense. It combines the language and visual understanding capabilities of vision-language models (VLMs) with optical character recognition (OCR)-based textual analysis. VIM-Sense achieves an attack detection accuracy of 88.94% on AR-VIM, consistently outperforming vision-only and text-only baselines. The system achieves an average attack detection latency of 7.07 seconds in a simulated video processing framework and 7.17 seconds in a real-world evaluation conducted on a mobile Android AR application.         ",
    "url": "https://arxiv.org/abs/2507.20356",
    "authors": [
      "Yanming Xiu",
      "Maria Gorlatova"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.20453",
    "title": "Your Attention Matters: to Improve Model Robustness to Noise and Spurious Correlations",
    "abstract": "           Self-attention mechanisms are foundational to Transformer architectures, supporting their impressive success in a wide range of tasks. While there are many self-attention variants, their robustness to noise and spurious correlations has not been well studied. This study evaluates Softmax, Sigmoid, Linear, Doubly Stochastic, and Cosine attention within Vision Transformers under different data corruption scenarios. Through testing across the CIFAR-10, CIFAR-100, and Imagenette datasets, we show that Doubly Stochastic attention is the most robust. It consistently outperformed the next best mechanism by $0.1\\%-5.1\\%$ when training data, or both training and testing data, were corrupted. Our findings inform self-attention selection in contexts with imperfect data. The code used is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.20453",
    "authors": [
      "Camilo Tamayo-Rousseau",
      "Yunjia Zhao",
      "Yiqun Zhang",
      "Randall Balestriero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.20966",
    "title": "Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL",
    "abstract": "           In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user mobility necessitates updating the set of serving access points to maintain the user-centric clustering. Such updates are typically performed through handoff (HO) operations; however, frequent HOs lead to overheads associated with the allocation and release of resources. This paper presents a deep reinforcement learning (DRL)-based solution to predict and manage these connections for mobile users. Our solution employs the Soft Actor-Critic algorithm, with continuous action space representation, to train a deep neural network to serve as the HO policy. We present a novel proposition for a reward function that integrates a HO penalty in order to balance the attainable rate and the associated overhead related to HOs. We develop two variants of our system; the first one uses mobility direction-assisted (DA) observations that are based on the user movement pattern, while the second one uses history-assisted (HA) observations that are based on the history of the large-scale fading (LSF). Simulation results show that our DRL-based continuous action space approach is more scalable than discrete space counterpart, and that our derived HO policy automatically learns to gather HOs in specific time slots to minimize the overhead of initiating HOs. Our solution can also operate in real time with a response time less than 0.4 ms.         ",
    "url": "https://arxiv.org/abs/2507.20966",
    "authors": [
      "Hussein A. Ammar",
      "Raviraj Adve",
      "Shahram Shahbazpanahi",
      "Gary Boudreau",
      "Israfil Bahceci"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2507.21358",
    "title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy",
    "abstract": "           Vision-based bird's-eye-view (BEV) 3D object detection has advanced significantly in autonomous driving by offering cost-effectiveness and rich contextual information. However, existing methods often construct BEV representations by collapsing extracted object features, neglecting intrinsic environmental contexts, such as roads and pavements. This hinders detectors from comprehensively perceiving the characteristics of the physical world. To alleviate this, we introduce a multi-task learning framework, Collaborative Perceiver (CoP), that leverages spatial occupancy as auxiliary information to mine consistent structural and conceptual similarities shared between 3D object detection and occupancy prediction tasks, bridging gaps in spatial representations and feature refinement. To this end, we first propose a pipeline to generate dense occupancy ground truths incorporating local density information (LDO) for reconstructing detailed environmental information. Next, we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained local features according to distinct object properties. Furthermore, we develop a global-local collaborative feature fusion (CFF) module that seamlessly integrates complementary knowledge between both tasks, thus composing more robust BEV representations. Extensive experiments on the nuScenes benchmark demonstrate that CoP outperforms existing vision-based frameworks, achieving 49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are available at this link this https URL.         ",
    "url": "https://arxiv.org/abs/2507.21358",
    "authors": [
      "Jicheng Yuan",
      "Manh Nguyen Duc",
      "Qian Liu",
      "Manfred Hauswirth",
      "Danh Le Phuoc"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.21462",
    "title": "Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals",
    "abstract": "           We investigate whether tactile charts support comprehension and learning of complex visualizations for blind and low-vision (BLV) individuals and contribute four tactile chart designs and an interview study. Visualizations are powerful tools for conveying data, yet BLV individuals typically can rely only on assistive technologies -- primarily alternative texts -- to access this information. Prior research shows the importance of mental models of chart types for interpreting these descriptions, yet BLV individuals have no means to build such a mental model based on images of visualizations. Tactile charts show promise to fill this gap in supporting the process of building mental models. Yet studies on tactile data representations mostly focus on simple chart types, and it is unclear whether they are also appropriate for more complex charts as would be found in scientific publications. Working with two BLV researchers, we designed 3D-printed tactile template charts with exploration instructions for four advanced chart types: UpSet plots, violin plots, clustered heatmaps, and faceted line charts. We then conducted an interview study with 12 BLV participants comparing whether using our tactile templates improves mental models and understanding of charts and whether this understanding translates to novel datasets experienced through alt texts. Thematic analysis shows that tactile models support chart type understanding and are the preferred learning method by BLV individuals. We also report participants' opinions on tactile chart design and their role in BLV education.         ",
    "url": "https://arxiv.org/abs/2507.21462",
    "authors": [
      "Tingying He",
      "Maggie McCracken",
      "Daniel Hajas",
      "Sarah Creem-Regehr",
      "Alexander Lex"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2507.22254",
    "title": "Opinion formation in Wikipedia Ising networks",
    "abstract": "           We study properties of opinion formation on Wikipedia Ising Networks. Each Wikipedia article is represented as a node and links are formed by citations of one article to another generating a directed network of a given language edition with millions of nodes. Ising spins are placed at each node and their orientation up or down is determined by a majority vote of connected neighbors. At the initial stage there are only a few nodes from two groups with fixed competing opinions up and down while other nodes are assumed to have no initial opinion with no effect on the vote. The competition of two opinions is modeled by an asynchronous Monte Carlo process converging to a spin polarized steady-state phase. This phase remains stable with respect to small fluctuations induced by an effective temperature of the Monte Carlo process. The opinion polarization at the steady-state provides opinion (spin) preferences for each node. In the framework of this Ising Network Opinion Formation model we analyze the influence and competition between political leaders, world countries and social concepts. This approach is also generalized to the competition between three groups of different opinions described by three colors, for example Donald Trump, Vladimir Putin, Xi Jinping or USA, Russia, China within English, Russian and Chinese editions of Wikipedia of March 2025. We argue that this approach provides a generic description of opinion formation in various complex networks.         ",
    "url": "https://arxiv.org/abs/2507.22254",
    "authors": [
      "Leonardo Ermann",
      "Klaus M. Frahm",
      "Dima L. Shepelyansky"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2507.22828",
    "title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models",
    "abstract": "           As Vision-Language Models (VLMs) are increasingly deployed in split-DNN configurations--with visual encoders (e.g., ResNet, ViT) operating on user devices and sending intermediate features to the cloud--there is a growing privacy risk from semantic information leakage. Existing approaches to reconstructing images from these intermediate features often result in blurry, semantically ambiguous images. To directly address semantic leakage, we propose CapRecover, a cross-modality inversion framework that recovers high-level semantic content, such as labels or captions, directly from intermediate features without image reconstruction. We evaluate CapRecover on multiple datasets and victim models, demonstrating strong performance in semantic recovery. Specifically, CapRecover achieves up to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis further reveals that deeper convolutional layers encode significantly more semantic information compared to shallow layers. To mitigate semantic leakage, we introduce a simple yet effective protection method: adding random noise to intermediate features at each layer and removing the noise in the next layer. Experimental results show that this approach prevents semantic leakage without additional training costs.         ",
    "url": "https://arxiv.org/abs/2507.22828",
    "authors": [
      "Kedong Xiu",
      "Sai Qian Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.23268",
    "title": "PixNerd: Pixel Neural Field Diffusion",
    "abstract": "           The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID on ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.         ",
    "url": "https://arxiv.org/abs/2507.23268",
    "authors": [
      "Shuai Wang",
      "Ziteng Gao",
      "Chenhui Zhu",
      "Weilin Huang",
      "Limin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.23291",
    "title": "Evaluating the Dynamics of Membership Privacy in Deep Learning",
    "abstract": "           Membership inference attacks (MIAs) pose a critical threat to the privacy of training data in deep learning. Despite significant progress in attack methodologies, our understanding of when and how models encode membership information during training remains limited. This paper presents a dynamic analytical framework for dissecting and quantifying privacy leakage dynamics at the individual sample level. By tracking per-sample vulnerabilities on an FPR-TPR plane throughout training, our framework systematically measures how factors such as dataset complexity, model architecture, and optimizer choice influence the rate and severity at which samples become vulnerable. Crucially, we discover a robust correlation between a sample's intrinsic learning difficulty, and find that the privacy risk of samples highly vulnerable in the final trained model is largely determined early during training. Our results thus provide a deeper understanding of how privacy risks dynamically emerge during training, laying the groundwork for proactive, privacy-aware model training strategies.         ",
    "url": "https://arxiv.org/abs/2507.23291",
    "authors": [
      "Yuetian Chen",
      "Zhiqi Wang",
      "Nathalie Baracaldo",
      "Swanand Ravindra Kadhe",
      "Lei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2507.23309",
    "title": "PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving",
    "abstract": "           With the growing interest in autonomous driving, there is an increasing demand for accurate and reliable road perception technologies. In complex environments without high-definition map support, autonomous vehicles must independently interpret their surroundings to ensure safe and robust decision-making. However, these scenarios pose significant challenges due to the large number, complex geometries, and frequent occlusions of road elements. A key limitation of existing approaches lies in their insufficient exploitation of the structured priors inherently present in road elements, resulting in irregular, inaccurate predictions. To address this, we propose PriorFusion, a unified framework that effectively integrates semantic, geometric, and generative priors to enhance road element perception. We introduce an instance-aware attention mechanism guided by shape-prior features, then construct a data-driven shape template space that encodes low-dimensional representations of road elements, enabling clustering to generate anchor points as reference priors. We design a diffusion-based framework that leverages these prior anchors to generate accurate and complete predictions. Experiments on large-scale autonomous driving datasets demonstrate that our method significantly improves perception accuracy, particularly under challenging conditions. Visualization results further confirm that our approach produces more accurate, regular, and coherent predictions of road elements.         ",
    "url": "https://arxiv.org/abs/2507.23309",
    "authors": [
      "Xuewei Tang",
      "Mengmeng Yang",
      "Tuopu Wen",
      "Peijin Jia",
      "Le Cui",
      "Mingshang Luo",
      "Kehua Sheng",
      "Bo Zhang",
      "Diange Yang",
      "Kun Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.23449",
    "title": "Manifold-regularised Large-Margin $\\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection",
    "abstract": "           We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to exploit the geometry of data distribution via manifold regularising for time series anomaly detection. Specifically, we formulate a manifold-regularised variant of the $\\ell_p$-SVDD method to encourage label smoothness on the underlying manifold to capture structural information for improved detection performance. Drawing on an existing Representer theorem, we then provide an effective optimisation technique for the proposed method. We theoretically study the proposed approach using Rademacher complexities to analyse its generalisation performance and also provide an experimental assessment of the proposed method across various data sets to compare its performance against other methods.         ",
    "url": "https://arxiv.org/abs/2507.23449",
    "authors": [
      "Shervin Rahimzadeh Arashloo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.00117",
    "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection",
    "abstract": "           Liver diseases are a serious health concern in the world, which requires precise and timely diagnosis to enhance the survival chances of patients. The current literature implemented numerous machine learning and deep learning models to classify liver diseases, but most of them had some issues like high misclassification error, poor interpretability, prohibitive computational expense, and lack of good preprocessing strategies. In order to address these drawbacks, we introduced StackLiverNet in this study; an interpretable stacked ensemble model tailored to the liver disease detection task. The framework uses advanced data preprocessing and feature selection technique to increase model robustness and predictive ability. Random undersampling is performed to deal with class imbalance and make the training balanced. StackLiverNet is an ensemble of several hyperparameter-optimized base classifiers, whose complementary advantages are used through a LightGBM meta-model. The provided model demonstrates excellent performance, with the testing accuracy of 99.89%, Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and efficient training and inference speeds that are amenable to clinical practice (training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local Interpretable Model-Agnostic Explanations (LIME) are applied to generate transparent explanations of individual predictions, revealing high concentrations of Alkaline Phosphatase and moderate SGOT as important observations of liver disease. Also, SHAP was used to rank features by their global contribution to predictions, while the Morris method confirmed the most influential features through sensitivity analysis.         ",
    "url": "https://arxiv.org/abs/2508.00117",
    "authors": [
      "Md. Ehsanul Haque",
      "S. M. Jahidul Islam",
      "Shakil Mia",
      "Rumana Sharmin",
      "Ashikuzzaman",
      "Md Samir Morshed",
      "Md. Tahmidul Huque"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.00293",
    "title": "ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks",
    "abstract": "           Ransomware (RW) presents a significant and widespread threat in the digital landscape, necessitating effective countermeasures. Active cyber deception is a promising strategy to thwart RW and limiting its propagation by misleading it with false information and revealing its true behaviors. Furthermore, RW often acts as a communication conduit between attackers and defenders, allowing deception to return false data to attackers and deplete their resources. This paper introduces ranDecepter, a novel approach that combines active cyber deception with real-time analysis to enhance defenses against RW attacks. The ranDecepter identifies RW in real-time and isolates it within a deceptive environment, autonomously identifying critical elements in the RW code to create a loop mechanism. By repeatedly restarting the malware and transmitting counterfeit encryption information and secret keys to the attacker, it forces the attacker to store these fabricated details for each victim, thereby depleting their resources. Our comprehensive evaluation of ranDecepter, conducted using 1,134 real-world malware samples and twelve benign applications, demonstrates a remarkable 100% accuracy in RW identification, with no false positives and minimal impact on response times. Furthermore, within 24-hours, ranDecepter generates up to 9,223K entries in the attacker's database using 50 agents, showcasing its potential to undermine attacker resources.         ",
    "url": "https://arxiv.org/abs/2508.00293",
    "authors": [
      "Md Sajidul Islam Sajid",
      "Jinpeng Wei",
      "Ehab Al-Shaer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.00599",
    "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
    "abstract": "           We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.         ",
    "url": "https://arxiv.org/abs/2508.00599",
    "authors": [
      "Junzhe Lu",
      "Jing Lin",
      "Hongkun Dou",
      "Ailing Zeng",
      "Yue Deng",
      "Xian Liu",
      "Zhongang Cai",
      "Lei Yang",
      "Yulun Zhang",
      "Haoqian Wang",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1802.06015",
    "title": "Interdisciplinarity Revealed by Transitive Reduction of Citation Networks",
    "abstract": "           We investigate the impact of transitive reduction on citation networks. Our hypothesis is that documents which lose fewer citations under transitive reduction are likely to be interdisciplinary, while a large loss of citations suggests a document is primarily cited within a single discipline. We test this hypothesis by using an artificial model of a citation network and by using data on citations from three sources: academic papers, court decisions and patents. Where needed, we applied modularity-based clustering techniques on a network defined using bibliographic coupling to classify documents by topic. A cluster-dependent measure was then used to classify the nodes as interdisciplinary or intradisciplinary. Our results provide strong support for our hypothesis in three of the four cases, with somewhat weaker but still positive support in the case of patents.         ",
    "url": "https://arxiv.org/abs/1802.06015",
    "authors": [
      "H. AlMuhanna",
      "V. Vasiliauskaite",
      "T.S. Evans"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Digital Libraries (cs.DL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2302.12024",
    "title": "Comparison of Affine and Rational Quadratic Spline Coupling and Autoregressive Flows through Robust Statistical Tests",
    "abstract": "           Normalizing flows have emerged as a powerful brand of generative models, as they not only allow for efficient sampling of complicated target distributions but also deliver density estimation by construction. We propose here an in-depth comparison of coupling and autoregressive flows, both based on symmetric (affine) and non-symmetric (rational quadratic spline) bijectors, considering four different architectures: real-valued non-Volume preserving (RealNVP), masked autoregressive flow (MAF), coupling rational quadratic spline (C-RQS), and autoregressive rational quadratic spline (A-RQS). We focus on a set of multimodal target distributions of increasing dimensionality ranging from 4 to 400. The performances were compared by means of different test statistics for two-sample tests, built from known distance measures: the sliced Wasserstein distance, the dimension-averaged one-dimensional Kolmogorov--Smirnov test, and the Frobenius norm of the difference between correlation matrices. Furthermore, we included estimations of the variance of both the metrics and the trained models. Our results indicate that the A-RQS algorithm stands out both in terms of accuracy and training speed. Nonetheless, all the algorithms are generally able, without too much fine-tuning, to learn complicated distributions with limited training data and in a reasonable time of the order of hours on a Tesla A40 GPU. The only exception is the C-RQS, which takes significantly longer to train, does not always provide good accuracy, and becomes unstable for large dimensionalities. All algorithms were implemented using \\textsc{TensorFlow2} and \\textsc{TensorFlow Probability} and have been made available on \\href{this https URL}{GitHub}.         ",
    "url": "https://arxiv.org/abs/2302.12024",
    "authors": [
      "Andrea Coccaro",
      "Marco Letizia",
      "Humberto Reyes-Gonzalez",
      "Riccardo Torre"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ]
  },
  {
    "id": "arXiv:2405.00636",
    "title": "Robustness of graph embedding methods for community detection",
    "abstract": "           This study investigates the robustness of graph embedding methods for community detection in the face of network perturbations, specifically edge deletions. Graph embedding techniques, which represent nodes as low-dimensional vectors, are widely used for various graph machine learning tasks due to their ability to capture structural properties of networks effectively. However, the impact of perturbations on the performance of these methods remains relatively understudied. The research considers state-of-the-art graph embedding methods from two families: matrix factorization (e.g., LE, LLE, HOPE, M-NMF) and random walk-based (e.g., DeepWalk, LINE, node2vec). Through experiments conducted on both synthetic and real-world networks, the study reveals varying degrees of robustness within each family of graph embedding methods. The robustness is found to be influenced by factors such as network size, initial community partition strength, and the type of perturbation. Notably, node2vec and LLE consistently demonstrate higher robustness for community detection across different scenarios, including networks with degree and community size heterogeneity. These findings highlight the importance of selecting an appropriate graph embedding method based on the specific characteristics of the network and the task at hand, particularly in scenarios where robustness to perturbations is crucial.         ",
    "url": "https://arxiv.org/abs/2405.00636",
    "authors": [
      "Zhi-Feng Wei",
      "Pablo Moriano",
      "Ramakrishnan Kannan"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2406.16525",
    "title": "Enhancing OOD Detection Using Latent Diffusion",
    "abstract": "           Out-of-distribution (OOD) detection is crucial for the reliable deployment of machine learning models in real-world scenarios, enabling the identification of unknown samples or objects. A prominent approach to enhance OOD detection performance involves leveraging auxiliary datasets for training. Recent efforts have explored using generative models, such as Stable Diffusion (SD), to synthesize outlier data in the pixel space. However, synthesizing OOD data in the pixel space can lead to reduced robustness due to over-generation. To address this challenge, we propose Outlier-Aware Learning (OAL), a novel framework that generates synthetic OOD training data within the latent space, taking a further step to study how to utilize Stable Diffusion for developing a latent-based outlier synthesis approach. This improvement facilitates network training with fewer outliers and less computational cost. Besides, to regularize the model's decision boundary, we develop a mutual information-based contrastive learning module (MICL) that amplifies the distinction between In-Distribution (ID) and collected OOD data. Moreover, we develop a knowledge distillation module to prevent the degradation of ID classification accuracy when training with OOD data. The superior performance of our method on several benchmark datasets demonstrates its efficiency and effectiveness. Source code is available in this https URL.         ",
    "url": "https://arxiv.org/abs/2406.16525",
    "authors": [
      "Heng Gao",
      "Jun Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2409.00032",
    "title": "ADformer: A Multi-Granularity Spatial-Temporal Transformer for EEG-Based Alzheimer Detection",
    "abstract": "           Electroencephalography (EEG) has emerged as a cost-effective and efficient tool to support neurologists in the detection of Alzheimer's Disease (AD). However, most existing approaches rely heavily on manual feature engineering or data transformation. While such techniques may provide benefits when working with small-scale datasets, they often lead to information loss and distortion when applied to large-scale data, ultimately limiting model performance. Moreover, the limited subject scale and demographic diversity of datasets used in prior studies hinder comprehensive evaluation of model robustness and generalizability, thus restricting their applicability in real-world clinical settings. To address these challenges, we propose ADformer, a novel multi-granularity spatial-temporal transformer designed to capture both temporal and spatial features from raw EEG signals, enabling effective end-to-end representation learning. Our model introduces multi-granularity embedding strategies across both spatial and temporal dimensions, leveraging a two-stage intra-inter granularity self-attention mechanism to learn both local patterns within each granularity and global dependencies across granularities. We evaluate ADformer on 4 large-scale datasets comprising a total of 1,713 subjects, representing one of the largest corpora for EEG-based AD detection to date, under a cross-validated, subject-independent setting. Experimental results demonstrate that ADformer consistently outperforms existing methods, achieving subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, respectively, in distinguishing AD from healthy control (HC) subjects.         ",
    "url": "https://arxiv.org/abs/2409.00032",
    "authors": [
      "Yihe Wang",
      "Nadia Mammone",
      "Darina Petrovsky",
      "Alexandros T. Tzallas",
      "Francesco C. Morabito",
      "Xiang Zhang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.13620",
    "title": "Align-ULCNet: Towards Low-Complexity and Robust Acoustic Echo and Noise Reduction",
    "abstract": "           The successful deployment of deep learning-based acoustic echo and noise reduction (AENR) methods in consumer devices has spurred interest in developing low-complexity solutions, while emphasizing the need for robust performance in real-life applications. In this work, we propose a hybrid approach to enhance the state-of-the-art (SOTA) ULCNet model by integrating time alignment and parallel encoder blocks for the model inputs, resulting in better echo reduction and comparable noise reduction performance to existing SOTA methods. We also propose a channel-wise sampling-based feature reorientation method, ensuring robust performance across many challenging scenarios, while maintaining overall low computational and memory requirements.         ",
    "url": "https://arxiv.org/abs/2410.13620",
    "authors": [
      "Shrishti Saha Shetu",
      "Naveen Kumar Desiraju",
      "Wolfgang Mack",
      "Emanu\u00ebl A. P. Habets"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.02605",
    "title": "Tunable Terahertz Detection and Generation using FETs operating in the saturation regime",
    "abstract": "           I report on the experimental observation of DC instability and self-amplification through stimulated emission of 0.2 and 1.63 THz radiation using InGaAs/GaAs HEMT operating in the deep saturation regime at room temperature. I demonstrate both theoretically and experimentally, that the Sub-THz and THz response of FETs are attributable to the rectification of the nonlinear dependence of the device's current-voltage characteristics. FETs function as nonlinear THz mixers and rectifiers, with their open-drain responsivity described by an expression analogous to that of a zero-bias Schottky diode detector. However, operating FETs in the deep saturation regime permits precise tuning of the device to the quantum localized resonance condition and the negative resistance mode at room temperature. Consequently, FETs can be adjusted in the deep saturation regime to facilitate tunable sub-THz and THz detection and generation as well as tunable sub-THz and THz lasing effect. These results are anticipated to significantly impact technological advancements across various fields in the near future.         ",
    "url": "https://arxiv.org/abs/2411.02605",
    "authors": [
      "Tamer Elkhatib"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.00302",
    "title": "Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions",
    "abstract": "           How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency for consecutive time steps. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts.         ",
    "url": "https://arxiv.org/abs/2502.00302",
    "authors": [
      "Yixuan He",
      "Aaron Sandel",
      "David Wipf",
      "Mihai Cucuringu",
      "John Mitani",
      "Gesine Reinert"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2502.12327",
    "title": "Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV",
    "abstract": "           The rampdown phase of a tokamak pulse is difficult to simulate and often exacerbates multiple plasma instabilities. To reduce the risk of disrupting operations, we leverage advances in Scientific Machine Learning (SciML) to combine physics with data-driven models, developing a neural state-space model (NSSM) that predicts plasma dynamics during Tokamak \u00e0 Configuration Variable (TCV) rampdowns. The NSSM efficiently learns dynamics from a modest dataset of 311 pulses with only five pulses in a reactor-relevant high-performance regime. The NSSM is parallelized across uncertainties, and reinforcement learning (RL) is applied to design trajectories that avoid instability limits. High-performance experiments at TCV show statistically significant improvements in relevant metrics. A predict-first experiment, increasing plasma current by 20% from baseline, demonstrates the NSSM's ability to make small extrapolations. The developed approach paves the way for designing tokamak controls with robustness to considerable uncertainty and demonstrates the relevance of SciML for fusion experiments.         ",
    "url": "https://arxiv.org/abs/2502.12327",
    "authors": [
      "Allen M. Wang",
      "Alessandro Pau",
      "Cristina Rea",
      "Oswin So",
      "Charles Dawson",
      "Olivier Sauter",
      "Mark D. Boyer",
      "Anna Vu",
      "Cristian Galperti",
      "Chuchu Fan",
      "Antoine Merle",
      "Yoeri Poels",
      "Cristina Venturini",
      "Stefano Marchioni",
      "TCV Team"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.20619",
    "title": "Style Content Decomposition-based Data Augmentation for Domain Generalizable Medical Image Segmentation",
    "abstract": "           Due to domain shifts across diverse medical imaging modalities, learned segmentation models often suffer significant performance degradation during deployment. These domain shifts, typically caused by variations in imaging systems, generally comprise two principal components: 1) \\textbf{\"style\" shifts}, referring to global disparities in image properties such as illumination, contrast, and color; and 2) \\textbf{\"content\" shifts}, which involve local discrepancies in anatomical structures. To address domain shifts in medical image segmentation, a core challenge arises: how can we decouple the factors within images that determine their \"style\" and \"content\" components? To this end, we first propose a linear style-content decomposition method that factorizes an image into style codes and content maps, explicitly modeling the \"style\" and \"content\" components. Building on this, we introduce a \\textbf{Sty}le-\\textbf{Con}tent decomposition-based data \\textbf{a}ugmentation algorithm (StyCona), which leverages this decomposition strategy to guide augmentation of both the global style and local content of source-domain images, enabling the training of a well-generalized model for domain-generalizable medical image segmentation. StyCona is a simple yet effective plug-and-play module that substantially improves model generalization without requiring additional training parameters or modifications to segmentation model architectures. Experiments on cardiac magnetic resonance imaging and fundus photography segmentation tasks, with single and multiple target domains respectively, demonstrate the effectiveness of StyCona and its superiority over state-of-the-art domain generalization methods. The code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.20619",
    "authors": [
      "Zhiqiang Shen",
      "Peng Cao",
      "Jinzhu Yang",
      "Osmar R. Zaiane",
      "Zhaolin Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00942",
    "title": "On the use of the principle of maximum entropy to improve the robustness of bivariate spline least-squares approximation",
    "abstract": "           We consider fitting a bivariate spline regression model to data using a weighted least-squares cost function, with weights that sum to one to form a discrete probability distribution. By applying the principle of maximum entropy, the weight distribution is determined by maximizing the associated entropy function. This approach, previously applied successfully to polynomials and spline curves, enhances the robustness of the regression model by automatically detecting and down-weighting anomalous data during the fitting process. To demonstrate the effectiveness of the method, we present applications to two image processing problems and further illustrate its potential through two synthetic examples. Unlike the standard ordinary least-squares method, the maximum entropy formulation leads to a nonlinear algebraic system whose solvability requires careful theoretical analysis. We provide preliminary results in this direction and discuss the computational implications of solving the associated constrained optimization problem, which calls for dedicated iterative algorithms. These aspects suggest natural directions for further research on both the theoretical and algorithmic fronts.         ",
    "url": "https://arxiv.org/abs/2503.00942",
    "authors": [
      "Pierluigi Amodio",
      "Luigi Brugnano",
      "Felice Iavernaro"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.02134",
    "title": "Robust Channel Estimation for Optical Wireless Communications Using Neural Network",
    "abstract": "           Optical Wireless Communication (OWC) has gained significant attention due to its high-speed data transmission and throughput. Optical wireless channels are often assumed to be flat, but we evaluate frequency selective channels to consider high data rate optical wireless or very dispersive environments. To address this for optical scenarios, this paper presents a robust channel estimation framework with low-complexity to mitigate frequency-selective effects, then to improve system reliability and performance. This channel estimation framework contains a neural network that can estimate general optical wireless channels without prior channel information about the environment. Based on this estimate and the corresponding delay spread, one of several candidate offline-trained neural networks will be activated to predict this channel. Simulation results demonstrate that the proposed method has improved and robust normalized mean square error (NMSE) and bit error rate (BER) performance compared to conventional estimation methods while maintaining computational efficiency. These findings highlight the potential of neural network solutions in enhancing the performance of OWC systems under indoor channel conditions.         ",
    "url": "https://arxiv.org/abs/2504.02134",
    "authors": [
      "Dianxin Luan",
      "John Thompson"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.24739",
    "title": "Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI",
    "abstract": "           Accurate placental segmentation is essential for quantitative analysis of the placenta. However, this task is particularly challenging in T2*-weighted placental imaging due to: (1) weak and inconsistent boundary contrast across individual echoes; (2) the absence of manual ground truth annotations for all echo times; and (3) motion artifacts across echoes caused by fetal and maternal movement. In this work, we propose a contrast-augmented segmentation framework that leverages complementary information across multi-echo T2*-weighted MRI to learn robust, contrast-invariant representations. Our method integrates: (i) masked autoencoding (MAE) for self-supervised pretraining on unlabeled multi-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain adaptation across echo times; and (iii) global-local collaboration to align fine-grained features with global anatomical context. We further introduce a semantic matching loss to encourage representation consistency across echoes of the same subject. Experiments on a clinical multi-echo placental MRI dataset demonstrate that our approach generalizes effectively across echo times and outperforms both single-echo and naive fusion baselines. To our knowledge, this is the first work to systematically exploit multi-echo T2*-weighted MRI for placental segmentation.         ",
    "url": "https://arxiv.org/abs/2505.24739",
    "authors": [
      "Xinliu Zhong",
      "Ruiying Liu",
      "Emily S. Nichols",
      "Xuzhe Zhang",
      "Andrew F. Laine",
      "Emma G. Duerden",
      "Yun Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.02044",
    "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder",
    "abstract": "           As large language models (LLMs) continue to revolutionize AI research, there is a growing interest in building large-scale brain foundation models to advance neuroscience. While most existing brain foundation models are pre-trained on time-series signals or connectome features, we propose a novel graph-based pre-training paradigm for constructing a brain graph foundation model. In this paper, we introduce the Brain Graph Foundation Model, termed BrainGFM, a unified framework that leverages graph contrastive learning and graph masked autoencoders for large-scale fMRI-based pre-training. BrainGFM is pre-trained on a diverse mixture of brain atlases with varying parcellations, significantly expanding the pre-training corpus and enhancing the model's ability to generalize across heterogeneous fMRI-derived brain representations. To support efficient and versatile downstream transfer, we integrate both graph prompts and language prompts into the model design, enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological and psychiatric disorders, and task settings. Furthermore, we employ meta-learning to optimize the graph prompts, facilitating strong generalization to previously unseen disorders under both few-shot and zero-shot learning conditions via language-guided prompting. BrainGFM is pre-trained on 27 neuroimaging datasets spanning 25 common neurological and psychiatric disorders, encompassing 2 types of brain atlases (functional and anatomical) across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000 fMRI scans, and a total of 400,000 graph samples aggregated across all atlases and parcellations. The code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2506.02044",
    "authors": [
      "Xinxu Wei",
      "Kanhao Zhao",
      "Yong Jiao",
      "Lifang He",
      "Yu Zhang"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.02668",
    "title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection",
    "abstract": "           Colorectal polyp segmentation is critical for early detection of colorectal cancer, yet weak and low contrast boundaries significantly limit automated accuracy. Existing deep models either blur fine edge details or rely on handcrafted filters that perform poorly under variable imaging conditions. We propose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects directional, parameter free Haar wavelet edge maps into each decoder stage to recalibrate semantic features. The key novelties of MEGANet-W include a two-level Haar wavelet head for multi-orientation edge extraction; and Wavelet Edge Guided Attention (W-EGA) modules that fuse wavelet cues with boundary and input branches. On five public polyp datasets, MEGANet-W consistently outperforms existing methods, improving mIoU by up to 2.3% and mDice by 1.2%, while introducing no additional learnable parameters. This approach improves reliability in difficult cases and offers a robust solution for medical image segmentation tasks requiring precise boundary detection.         ",
    "url": "https://arxiv.org/abs/2507.02668",
    "authors": [
      "Zhe Yee Tan",
      "Ashwaq Qasem"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.19093",
    "title": "Graph Neural Network-Based Predictor for Optimal Quantum Hardware Selection",
    "abstract": "           The growing variety of quantum hardware technologies, each with unique peculiarities such as connectivity and native gate sets, creates challenges when selecting the best platform for executing a specific quantum circuit. This selection process usually involves a brute-force approach: compiling the circuit on various devices and evaluating performance based on factors such as circuit depth and gate fidelity. However, this method is computationally expensive and does not scale well as the number of available quantum processors increases. In this work, we propose a Graph Neural Network (GNN)-based predictor that automates hardware selection by analyzing the Directed Acyclic Graph (DAG) representation of a quantum circuit. Our study evaluates 498 quantum circuits (up to 27 qubits) from the MQT Bench dataset, compiled using Qiskit on four devices: three superconducting quantum processors (IBM-Kyiv, IBM-Brisbane, IBM-Sherbrooke) and one trapped-ion processor (IONQ-Forte). Performance is estimated using a metric that integrates circuit depth and gate fidelity, resulting in a dataset where 93 circuits are optimally compiled on the trapped-ion device, while the remaining circuits prefer superconducting platforms. By exploiting graph-based machine learning, our approach avoids extracting the circuit features for the model evaluation but directly embeds it as a graph, significantly accelerating the optimal target decision-making process and maintaining all the information. Experimental results prove 94.4% accuracy and an 85.5% F1 score for the minority class, effectively predicting the best compilation target. The developed code is publicly available on GitHub (this https URL).         ",
    "url": "https://arxiv.org/abs/2507.19093",
    "authors": [
      "Antonio Tudisco",
      "Deborah Volpe",
      "Giacomo Orlandi",
      "Giovanna Turvani"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  }
]