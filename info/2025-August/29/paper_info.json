[
  {
    "id": "arXiv:2508.20117",
    "title": "Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?",
    "abstract": "           Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.         ",
    "url": "https://arxiv.org/abs/2508.20117",
    "authors": [
      "Liang Li",
      "Yuntian Li",
      "Wenxin Zhao",
      "Shan Ye",
      "Yun Lu"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.20119",
    "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?",
    "abstract": "           In this paper we evaluate how far LLMs have advanced in generating code for real-world problems. Specifically, we explore code synthesis for microservice-based applications, a widely used architecture pattern. We define a standard template for specifying these applications, and we propose a metric for judging the difficulty level of a specification. The higher the score, the more difficult it is to generate code for the specification. We develop a framework to automate the process of testing LLM-synthesized code for a microservice using unit tests. Our experimental results show that strong LLMs (like GPT-3o-mini) do fairly well on medium difficulty specifications but do very poorly on those of higher difficulty levels. This is due to more intricate business logic, a greater use of external services, database integration and inclusion of non-functional capabilities such as authentication. We analyzed the errors in LLM-synthesized code and report on the key challenges LLMs face in generating code for these specifications thereby suggesting future research directions to improve code synthesis for real-world problems.         ",
    "url": "https://arxiv.org/abs/2508.20119",
    "authors": [
      "Daniel M. Yellin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20121",
    "title": "Task-Aware Tuning of Time Constants in Spiking Neural Networks for Multimodal Classification",
    "abstract": "           Spiking Neural Networks (SNNs) are promising candidates for low-power edge computing in domains such as wearable sensing and time-series analysis. A key neuronal parameter, the leaky time constant (LTC), governs temporal integration of information in Leaky Integrateand-Fire (LIF) neurons, yet its impact on feedforward SNN performance across different data modalities remains underexplored. This study investigates the role of LTC in a temporally adaptive feedforward SNN applied to static image, dynamic image, and biosignal time-series classification. Presented experiments demonstrate that LTCs critically affect inference accuracy, synaptic weight distributions, and firing dynamics. For static and dynamic images, intermediate LTCs yield higher accuracy and compact, centered weight histograms, reflecting stable feature encoding. In time-series tasks, optimal LTCs enhance temporal feature retention and result in broader weight sparsity, allowing for tolerance of LTC variations. The provided results show that inference accuracy peaks at specific LTC ranges, with significant degradation beyond this optimal band due to over-integration or excessive forgetting. Firing rate analysis reveals a strong interplay between LTC, network depth, and energy efficiency, underscoring the importance of balanced spiking activity. These findings reveal that task-specific LTC tuning is essential for efficient spike coding and robust learning. The results provide practical guidelines for hardware-aware SNN optimization and highlight how neuronal time constants can be designed to match task dynamics. This work contributes toward scalable, ultra-lowpower SNN deployment for real-time classification tasks in neuromorphic computing.         ",
    "url": "https://arxiv.org/abs/2508.20121",
    "authors": [
      "Chiu-Chang Cheng",
      "Kapil Bhardwaj",
      "Ya-Ning Chang",
      "Sayani Majumdar",
      "Chao-Hung Wang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.20124",
    "title": "Towards Better Correctness and Efficiency in Code Generation",
    "abstract": "           While code large language models have demonstrated remarkable progress in code generation, the generated code often exhibits poor runtime efficiency, limiting its practical application in performance-sensitive scenarios. To address this limitation, we propose an efficiency-oriented reinforcement learning framework guided by a novel performance reward. Based on this framework, we take a deeper dive into the code efficiency problem, identifying then proposing methods to overcome key bottlenecks: (1) Dynamic exploration overcomes the static data constraints of offline fine-tuning, enabling the discovery of more efficient code implementations. (2) The error-insensitive reinforcement learning method and high-contrast efficiency signals are crucial for mitigating systematic errors and achieving effective optimization. (3) Online exploration is most effective when starting from a high-correctness baseline, as this allows for efficiency improvements without sacrificing accuracy. With these discoveries, we finally propose a two-stage tuning method, which achieves high and balanced performance across correctness and efficiency. The results of experiments show the effectiveness of the method, which improves code correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model, achieving performance comparable to much larger model.         ",
    "url": "https://arxiv.org/abs/2508.20124",
    "authors": [
      "Yunlong Feng",
      "Yang Xu",
      "Xiao Xu",
      "Binyuan Hui",
      "Junyang Lin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20125",
    "title": "Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms",
    "abstract": "           Purpose: Spiking neural networks (SNNs) have recently gained attention as energy-efficient, biologically plausible alternatives to conventional deep learning models. Their application in high-stakes biomedical imaging remains almost entirely unexplored. Methods: This study introduces SNNDeep, the first tailored SNN specifically optimized for binary classification of liver health status from computed tomography (CT) features. To ensure clinical relevance and broad generalizability, the model was developed and evaluated using the Task03\\Liver dataset from the Medical Segmentation Decathlon (MSD), a standardized benchmark widely used for assessing performance across diverse medical imaging tasks. We benchmark three fundamentally different learning algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and Bio-Inspired Active Learning across three architectural variants: a fully customized low-level model built from scratch, and two implementations using leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter optimization was performed using Optuna. Results: Our results demonstrate that the custom-built SNNDeep consistently outperforms framework-based implementations, achieving a maximum validation accuracy of 98.35%, superior adaptability across learning rules, and significantly reduced training overhead. Conclusion:This study provides the first empirical evidence that low-level, highly tunable SNNs can surpass standard frameworks in medical imaging, especially in data-limited, temporally constrained diagnostic settings, thereby opening a new pathway for neuro-inspired AI in precision medicine.         ",
    "url": "https://arxiv.org/abs/2508.20125",
    "authors": [
      "Zofia Rudnicka",
      "Janusz Szczepanski",
      "Agnieszka Pregowska"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2508.20133",
    "title": "Proactive HIV Care: AI-Based Comorbidity Prediction from Routine EHR Data",
    "abstract": "           People living with HIV face a high burden of comorbidities, yet early detection is often limited by symptom-driven screening. We evaluate the potential of AI to predict multiple comorbidities from routinely collected Electronic Health Records. Using data from 2,200 HIV-positive patients in South East London, comprising 30 laboratory markers and 7 demographic/social attributes, we compare demographic-aware models (which use both laboratory/social variables and demographic information as input) against demographic-unaware models (which exclude all demographic information). Across all methods, demographic-aware models consistently outperformed unaware counterparts. Demographic recoverability experiments revealed that gender and age can be accurately inferred from laboratory data, underscoring both the predictive value and fairness considerations of demographic features. These findings show that combining demographic and laboratory data can improve automated, multi-label comorbidity prediction in HIV care, while raising important questions about bias and interpretability in clinical AI.         ",
    "url": "https://arxiv.org/abs/2508.20133",
    "authors": [
      "Solomon Russom",
      "Dimitrios Kollias",
      "Qianni Zhang"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.20142",
    "title": "Evaluation of A National Digitally-Enabled Health Promotion Campaign for Mental Health Awareness using Social Media Platforms Tik Tok, Facebook, Instagram, and YouTube",
    "abstract": "           Mental health disorders rank among the 10 leading contributors to the global burden of diseases, yet persistent stigma and care barriers delay early intervention. This has inspired efforts to leverage digital platforms for scalable health promotion to engage at-risk populations. To evaluate the effectiveness of a digitally-enabled mental health promotion (DEHP) campaign, we conducted an observational cross-sectional study of a 3-month (February-April 2025) nation-wide campaign in Singapore. Campaign materials were developed using a marketing funnel framework and disseminated across YouTube, Facebook, Instagram, and TikTok. This included narrative videos and infographics to promote symptom awareness, coping strategies, and/or patient navigation to this http URL, as the intended endpoint for user engagement and support. Primary outcomes include anonymised performance analytics (impressions, unique reach, video content view, engagements) stratified by demographics, device types, and sector. Secondary outcomes measured cost-efficiency metrics and traffic to this http URL respectively. This campaign generated 3.49 million total impressions and reached 1.39 million unique residents, with a Cost per Mille at \\$26.90, Cost per Click at \\$29.33, and Cost per Action at \\$6.06. Narrative videos accumulated over 630,000 views and 18,768 engagements. Overall, we demonstrate that DEHP campaigns can achieve national engagement for mental health awareness through multi-channel distribution and creative, narrative-driven designs.         ",
    "url": "https://arxiv.org/abs/2508.20142",
    "authors": [
      "Samantha Bei Yi Yan",
      "Dinesh Visva Gunasekeran",
      "Caitlyn Tan",
      "Kai En Chan",
      "Caleb Tan",
      "Charmaine Shi Min Lim",
      "Audrey Chia",
      "Hsien-Hsien Lei",
      "Robert Morris",
      "Janice Huiqin Weng",
      "Onno P Kampman",
      "Creighton Heaukulani",
      "Yan Yan Hu",
      "Julian Kui Yu Chang",
      "Akash Perera",
      "Ye Sheng Pang",
      "Alton Ming Kai Chew",
      "Krishna Vikneson",
      "Kishanti Ponampalam",
      "Kang-An Wong",
      "Kavita Govintharasah",
      "Kayshandra Tangasamy",
      "Grace Enyan Aik",
      "Pavanni Ponampalam",
      "Hazirah Hoosainsah",
      "Charmaine Ruling Lim",
      "Thisum Kankanamge Thisum",
      "XinYi Hong",
      "Mary Grace Yeo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.20201",
    "title": "Social Bias in Multilingual Language Models: A Survey",
    "abstract": "           Pretrained multilingual models exhibit the same social bias as models processing English texts. This systematic review analyzes emerging research that extends bias evaluation and mitigation approaches into multilingual and non-English contexts. We examine these studies with respect to linguistic diversity, cultural awareness, and their choice of evaluation metrics and mitigation techniques. Our survey illuminates gaps in the field's dominant methodological design choices (e.g., preference for certain languages, scarcity of multilingual mitigation experiments) while cataloging common issues encountered and solutions implemented in adapting bias benchmarks across languages and cultures. Drawing from the implications of our findings, we chart directions for future research that can reinforce the multilingual bias literature's inclusivity, cross-cultural appropriateness, and alignment with state-of-the-art NLP advancements.         ",
    "url": "https://arxiv.org/abs/2508.20201",
    "authors": [
      "Lance Calvin Lim Gamboa",
      "Yue Feng",
      "Mark Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20212",
    "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture",
    "abstract": "           Applying deep learning to malware detection has drawn great attention due to its notable performance. With the increasing prevalence of cyberattacks targeting IoT devices, there is a parallel rise in the development of malware across various Instruction Set Architectures (ISAs). It is thus important to extend malware detection capacity to multiple ISAs. However, training a deep learning-based malware detection model usually requires a large number of labeled malware samples. The process of collecting and labeling sufficient malware samples to build datasets for each ISA is labor-intensive and time-consuming. To reduce the burden of data collection, we propose to leverage the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for malware detection. Specifically, when dealing with malware in a certain ISA, we translate it to an ISA with sufficient malware samples (like X86-64). This allows us to apply a model trained on one ISA to analyze malware from another ISA. Our approach reduces the data collection effort by enabling malware detection across multiple ISAs using a model trained on a single ISA.         ",
    "url": "https://arxiv.org/abs/2508.20212",
    "authors": [
      "Minghao Hu",
      "Junzhe Wang",
      "Weisen Zhao",
      "Qiang Zeng",
      "Lannan Luo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.20221",
    "title": "Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos",
    "abstract": "           Omnidirectional videos (ODVs) are redefining viewer experiences in virtual reality (VR) by offering an unprecedented full field-of-view (FOV). This study extends the domain of saliency prediction to 360-degree environments, addressing the complexities of spherical distortion and the integration of spatial audio. Contextually, ODVs have transformed user experience by adding a spatial audio dimension that aligns sound direction with the viewer's perspective in spherical scenes. Motivated by the lack of comprehensive datasets for 360-degree audio-visual saliency prediction, our study curates YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying audio-visual conditions. Our goal is to explore how to utilize audio-visual cues to effectively predict visual saliency in 360-degree videos. Towards this aim, we propose two novel saliency prediction models: SalViT360, a vision-transformer-based framework for ODVs equipped with spherical geometry-aware spatio-temporal attention layers, and SalViT360-AV, which further incorporates transformer adapters conditioned on audio input. Our results on a number of benchmark datasets, including our YT360-EyeTracking, demonstrate that SalViT360 and SalViT360-AV significantly outperform existing methods in predicting viewer attention in 360-degree scenes. Interpreting these results, we suggest that integrating spatial audio cues in the model architecture is crucial for accurate saliency prediction in omnidirectional videos. Code and dataset will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20221",
    "authors": [
      "Mert Cokelek",
      "Halit Ozsoy",
      "Nevrez Imamoglu",
      "Cagri Ozcinar",
      "Inci Ayhan",
      "Erkut Erdem",
      "Aykut Erdem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20228",
    "title": "Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID",
    "abstract": "           Recent advances in LLM watermarking methods such as SynthID-Text by Google DeepMind offer promising solutions for tracing the provenance of AI-generated text. However, our robustness assessment reveals that SynthID-Text is vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste modifications, and back-translation, which can significantly degrade watermark detectability. To address these limitations, we propose SynGuard, a hybrid framework that combines the semantic alignment strength of Semantic Information Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text. Our approach jointly embeds watermarks at both lexical and semantic levels, enabling robust provenance tracking while preserving the original meaning. Experimental results across multiple attack scenarios show that SynGuard improves watermark recovery by an average of 11.1\\% in F1 score compared to SynthID-Text. These findings demonstrate the effectiveness of semantic-aware watermarking in resisting real-world tampering. All code, datasets, and evaluation scripts are publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20228",
    "authors": [
      "Xia Han",
      "Qi Li",
      "Jianbing Ni",
      "Mohammad Zulkernine"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20231",
    "title": "Bounds on Perfect Node Classification: A Convex Graph Clustering Perspective",
    "abstract": "           We present an analysis of the transductive node classification problem, where the underlying graph consists of communities that agree with the node labels and node features. For node classification, we propose a novel optimization problem that incorporates the node-specific information (labels and features) in a spectral graph clustering framework. Studying this problem, we demonstrate a synergy between the graph structure and node-specific information. In particular, we show that suitable node-specific information guarantees the solution of our optimization problem perfectly recovering the communities, under milder conditions than the bounds on graph clustering alone. We present algorithmic solutions to our optimization problem and numerical experiments that confirm such a synergy.         ",
    "url": "https://arxiv.org/abs/2508.20231",
    "authors": [
      "Firooz Shahriari-Mehr",
      "Javad Aliakbari",
      "Alexandre Graell i Amat",
      "Ashkan Panahi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2508.20243",
    "title": "Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification",
    "abstract": "           Rapid and reliable qualification of advanced materials remains a bottleneck in industrial manufacturing, particularly for heterogeneous structures produced via non-conventional additive manufacturing processes. This study introduces a novel framework that links microstructure informatics with a range of expert characterization knowledge using customized and hybrid vision-language representations (VLRs). By integrating deep semantic segmentation with pre-trained multi-modal models (CLIP and FLAVA), we encode both visual microstructural data and textual expert assessments into shared representations. To overcome limitations in general-purpose embeddings, we develop a customized similarity-based representation that incorporates both positive and negative references from expert-annotated images and their associated textual descriptions. This allows zero-shot classification of previously unseen microstructures through a net similarity scoring approach. Validation on an additively manufactured metal matrix composite dataset demonstrates the framework's ability to distinguish between acceptable and defective samples across a range of characterization criteria. Comparative analysis reveals that FLAVA model offers higher visual sensitivity, while the CLIP model provides consistent alignment with the textual criteria. Z-score normalization adjusts raw unimodal and cross-modal similarity scores based on their local dataset-driven distributions, enabling more effective alignment and classification in the hybrid vision-language framework. The proposed method enhances traceability and interpretability in qualification pipelines by enabling human-in-the-loop decision-making without task-specific model retraining. By advancing semantic interoperability between raw data and expert knowledge, this work contributes toward scalable and domain-adaptable qualification strategies in engineering informatics.         ",
    "url": "https://arxiv.org/abs/2508.20243",
    "authors": [
      "Mutahar Safdar",
      "Gentry Wood",
      "Max Zimmermann",
      "Guy Lamouche",
      "Priti Wanjara",
      "Yaoyao Fiona Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20259",
    "title": "Latent Variable Modeling for Robust Causal Effect Estimation",
    "abstract": "           Latent variable models provide a powerful framework for incorporating and inferring unobserved factors in observational data. In causal inference, they help account for hidden factors influencing treatment or outcome, thereby addressing challenges posed by missing or unmeasured covariates. This paper proposes a new framework that integrates latent variable modeling into the double machine learning (DML) paradigm to enable robust causal effect estimation in the presence of such hidden factors. We consider two scenarios: one where a latent variable affects only the outcome, and another where it may influence both treatment and outcome. To ensure tractability, we incorporate latent variables only in the second stage of DML, separating representation learning from latent inference. We demonstrate the robustness and effectiveness of our method through extensive experiments on both synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2508.20259",
    "authors": [
      "Tetsuro Morimura",
      "Tatsushi Oka",
      "Yugo Suzuki",
      "Daisuke Moriwaki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20288",
    "title": "Neural Spline Operators for Risk Quantification in Stochastic Systems",
    "abstract": "           Accurately quantifying long-term risk probabilities in diverse stochastic systems is essential for safety-critical control. However, existing sampling-based and partial differential equation (PDE)-based methods often struggle to handle complex varying dynamics. Physics-informed neural networks learn surrogate mappings for risk probabilities from varying system parameters of fixed and finite dimensions, yet can not account for functional variations in system dynamics. To address these challenges, we introduce physics-informed neural operator (PINO) methods to risk quantification problems, to learn mappings from varying \\textit{functional} system dynamics to corresponding risk probabilities. Specifically, we propose Neural Spline Operators (NeSO), a PINO framework that leverages B-spline representations to improve training efficiency and achieve better initial and boundary condition enforcements, which are crucial for accurate risk quantification. We provide theoretical analysis demonstrating the universal approximation capability of NeSO. We also present two case studies, one with varying functional dynamics and another with high-dimensional multi-agent dynamics, to demonstrate the efficacy of NeSO and its significant online speed-up over existing methods. The proposed framework and the accompanying universal approximation theorem are expected to be beneficial for other control or PDE-related problems beyond risk quantification.         ",
    "url": "https://arxiv.org/abs/2508.20288",
    "authors": [
      "Zhuoyuan Wang",
      "Raffaele Romagnoli",
      "Kamyar Azizzadenesheli",
      "Yorie Nakahira"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20290",
    "title": "Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation",
    "abstract": "           This paper introduce a novel metric of an objective function f, we say VC (value change) to measure the difficulty and approximation affection when conducting an neural network approximation task, and it numerically supports characterizing the local performance and behavior of neural network approximation. Neural networks often suffer from unpredictable local performance, which can hinder their reliability in critical applications. VC addresses this issue by providing a quantifiable measure of local value changes in network behavior, offering insights into the stability and performance for achieving the neural-network approximation. We investigate some fundamental theoretical properties of VC and identified two intriguing phenomena in neural network approximation: the VC-tendency and the minority-tendency. These trends respectively characterize how pointwise errors evolve in relation to the distribution of VC during the approximation this http URL addition, we propose a novel metric based on VC, which measures the distance between two functions from the perspective of variation. Building upon this metric, we further propose a new preprocessing framework for neural network approximation. Numerical results including the real-world experiment and the PDE-related scientific problem support our discovery and pre-processing acceleration method.         ",
    "url": "https://arxiv.org/abs/2508.20290",
    "authors": [
      "Pengcheng Xie",
      "Zihao Zhou",
      "Zijian Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2508.20295",
    "title": "FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation",
    "abstract": "           Parameter-efficient fine-tuning (PEFT) has attracted significant attention for adapting large pre-trained models by modifying a small subset of parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an effective alternative. ReFT shifts the fine-tuning paradigm from updating model weights to directly manipulating hidden representations that capture rich semantic information, and performs better than state-of-the-art PEFTs in standalone settings. However, its application in Federated Learning (FL) remains challenging due to heterogeneity in clients' data distributions, model capacities, and computational resources. To address these challenges, we introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to fine-tune the client's hidden representation. FedReFT applies sparse intervention layers to steer hidden representations directly, offering a lightweight and semantically rich fine-tuning alternative ideal for edge devices. However, representation-level updates are especially vulnerable to aggregation mismatch under different task heterogeneity, where naive averaging can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me (ABM) aggregation, where each client receives the aggregated updates of others and partially incorporates them, enabling stable and personalized learning by balancing local focus with global knowledge. We evaluate FedReFT on commonsense reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x higher parameter efficiency compared to leading LoRA-based approaches.         ",
    "url": "https://arxiv.org/abs/2508.20295",
    "authors": [
      "Fatema Siddika",
      "Md Anwar Hossen",
      "J. Pablo Mu\u00f1oz",
      "Tanya Roosta",
      "Anuj Sharma",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20302",
    "title": "Reducing Shortcut and Hopset Constructions to Shallow Graphs",
    "abstract": "           We introduce a blackbox framework that simplifies all known parallel algorithms with near-linear work for single-source reachability and shortest paths in directed graphs. Specifically, existing reachability algorithms rely on constructing shortcuts; our blackbox allows these algorithms that construct shortcuts with hopbound $h$ to assume the input graph $G$ is ``shallow'', meaning if vertex $s$ can reach vertex $t$, it can do so in approximately $h$ hops. This assumption significantly simplifies shortcut construction [Fin18, JLS19], resulting in simpler parallel reachability algorithms. Furthermore, our blackbox extends naturally to simplify parallel algorithms for constructing hopsets and, consequently, for computing shortest paths [CFR20 , CF23 , RHM+23 ].         ",
    "url": "https://arxiv.org/abs/2508.20302",
    "authors": [
      "Bernhard Haeupler",
      "Yonggang Jiang",
      "Thatchaphol Saranurak"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2508.20305",
    "title": "Directed and Undirected Vertex Connectivity Problems are Equivalent for Dense Graphs",
    "abstract": "           Vertex connectivity and its variants are among the most fundamental problems in graph theory, with decades of extensive study and numerous algorithmic advances. The directed variants of vertex connectivity are usually solved by manually extending fast algorithms for undirected graphs, which has required considerable effort. In this paper, we present a simple, black-box randomized reduction from directed to undirected vertex connectivity for dense graphs. As immediate corollaries, we largely simplify the proof for directed vertex connectivity in $n^{2+o(1)}$ time [LNP+25], and obtain a parallel vertex connectivity algorithm for directed graphs with $n^{\\omega+o(1)}$ work and $n^{o(1)}$ depth, via the undirected vertex connectivity algorithm of [BJMY25]. Our reduction further extends to the weighted version of the problem. By combining our reduction with the recent subcubic-time algorithm for undirected weighted vertex cuts [CT25], we obtain the first subcubic-time algorithm for weighted directed vertex connectivity, improving upon a three-decade-old bound [HRG00] for dense graphs.         ",
    "url": "https://arxiv.org/abs/2508.20305",
    "authors": [
      "Yonggang Jiang",
      "Sagnik Mukhopadhyay",
      "Sorrachai Yingchareonthawornchai"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2508.20328",
    "title": "Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails",
    "abstract": "           Internal talent recommendation is a critical strategy for organizational continuity, yet conventional approaches suffer from structural limitations, often overlooking qualified candidates by relying on the narrow perspective of a few managers. To address this challenge, we propose a novel framework that models two distinct dimensions of an employee's position fit from email data: WHAT they do (semantic similarity of tasks) and HOW they work (structural characteristics of their interactions and collaborations). These dimensions are represented as independent graphs and adaptively fused using a Dual Graph Convolutional Network (GCN) with a gating mechanism. Experiments show that our proposed gating-based fusion model significantly outperforms other fusion strategies and a heuristic baseline, achieving a top performance of 40.9% on Hit@100. Importantly, it is worth noting that the model demonstrates high interpretability by learning distinct, context-aware fusion strategies for different job families. For example, it learned to prioritize relational (HOW) data for 'sales and marketing' job families while applying a balanced approach for 'research' job families. This research offers a quantitative and comprehensive framework for internal talent discovery, minimizing the risk of candidate omission inherent in traditional methods. Its primary contribution lies in its ability to empirically determine the optimal fusion ratio between task alignment (WHAT) and collaborative patterns (HOW), which is required for employees to succeed in the new positions, thereby offering important practical implications.         ",
    "url": "https://arxiv.org/abs/2508.20328",
    "authors": [
      "Soo Hyun Kim",
      "Jang-Hyun Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20330",
    "title": "FORGE: Foundational Optimization Representations from Graph Embeddings",
    "abstract": "           Combinatorial optimization problems are ubiquitous in science and engineering, yet learning-based approaches to accelerate their solution often require solving a large number of hard-to-solve optimization instances to collect training data, incurring significant computational overhead. Existing methods require training dedicated models for each problem distribution for each downstream task, severely limiting their scalability and generalization. In this work, we introduce Forge, a method of pre-training a vector-quantized graph autoencoder on a large and diverse collection of mixed-integer programming (MIP) instances in an unsupervised fashion without dependency on their solution. The vector quantization process creates discrete code assignments that act as a vocabulary to represent optimization instances. We evaluate our approach under both supervised and unsupervised settings. For the unsupervised setting, we demonstrate that Forge embeddings effectively differentiate and cluster unseen instances. For the supervised setting, we fine-tune Forge embeddings and show that a single model predicts both the variables for warm-starts and integrality gaps for cut-generation across multiple problem type distributions. Both predictions help improve performance of a state-of-the-art, commercial optimization solver. Finally, we release our code and pre-trained Forge weights to encourage further research and practical use of instance-level MIP embeddings at this https URL ",
    "url": "https://arxiv.org/abs/2508.20330",
    "authors": [
      "Zohair Shafi",
      "Serdar Kadioglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20339",
    "title": "Artificial neural network solver for Fokker-Planck and Koopman eigenfunctions",
    "abstract": "           For a stochastic differential equation (SDE) that is an It\u00f4 diffusion or Langevin equation, the Fokker-Planck operator governs the evolution of the probability density, while its adjoint, the infinitesimal generator of the stochastic Koopman operator, governs the evolution of system observables, in the mean. The eigenfunctions of these operators provide a powerful framework to analyze SDEs, and have shown to be particularly useful for systems of stochastic oscillators. However, computing these eigenfunctions typically requires solving high-dimensional PDEs on unbounded domains, which is numerically challenging. Building on previous work, we propose a data-driven artificial neural network solver for Koopman and Fokker-Planck eigenfunctions. Our approach incorporates the differential operator into the loss function, improving accuracy and reducing dependence on large amounts of accurate training data. We demonstrate our approach on several numerical examples in two, three, and four dimensions.         ",
    "url": "https://arxiv.org/abs/2508.20339",
    "authors": [
      "Max Kreider",
      "Peter J. Thomas",
      "Yao Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.20367",
    "title": "Delay-adaptive Control of Nonlinear Systems with Approximate Neural Operator Predictors",
    "abstract": "           In this work, we propose a rigorous method for implementing predictor feedback controllers in nonlinear systems with unknown and arbitrarily long actuator delays. To address the analytically intractable nature of the predictor, we approximate it using a learned neural operator mapping. This mapping is trained once, offline, and then deployed online, leveraging the fast inference capabilities of neural networks. We provide a theoretical stability analysis based on the universal approximation theorem of neural operators and the transport partial differential equation (PDE) representation of the delay. We then prove, via a Lyapunov-Krasovskii functional, semi-global practical convergence of the dynamical system dependent on the approximation error of the predictor and delay bounds. Finally, we validate our theoretical results using a biological activator/repressor system, demonstrating speedups of 15 times compared to traditional numerical methods.         ",
    "url": "https://arxiv.org/abs/2508.20367",
    "authors": [
      "Luke Bhan",
      "Miroslav Krstic",
      "Yuanyuan Shi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2508.20373",
    "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems",
    "abstract": "           Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at this https URL, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.         ",
    "url": "https://arxiv.org/abs/2508.20373",
    "authors": [
      "Yuyao Wang",
      "Bowen Liu",
      "Jianheng Tang",
      "Nuo Chen",
      "Yuhan Li",
      "Qifan Zhang",
      "Jia Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20374",
    "title": "TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning",
    "abstract": "           Diverse instruction data is vital for effective instruction tuning of large language models, as it enables the model to generalize across different types of inputs . Building such diversified instruction dataset is an essential step in this process. Existing approaches often leverage large language models to automatically explore and generate diverse instructions, ensuring both data diversity and quality. However, they tend to overlook an important factor in real-world applications: on-task relevance. In practice, only a few real-world applications require a truly general-purpose model; most benefit from task-specific knowledge tailored to their particular use case. Therefore, it is vital to develop instruction augmentation methods that not only maintain diversity but are also optimized for specific, real-world scenarios. We thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment. By representing instructions in a discrete query-constraints space, TCIA creates a rich set of task-relevant instructions and enables models to generalize to these task-specific instructions without sacrificing overall performance. Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications, and in some cases outperforming leading closed-source models. These improvements do not compromise general instruction-following ability, making TCIA a scalable and efficient solution for adapting LLMs to real-world, task-focused applications.         ",
    "url": "https://arxiv.org/abs/2508.20374",
    "authors": [
      "Simin Ma",
      "Shujian Liu",
      "Jun Tan",
      "Yebowen Hu",
      "Song Wang",
      "Sathish Reddy Indurthi",
      "Sanqiang Zhao",
      "Liwei Wu",
      "Jianbing Han",
      "Kaiqiang Song"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20375",
    "title": "CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference",
    "abstract": "           The impressive performance of transformer models has sparked the deployment of intelligent applications on resource-constrained edge devices. However, ensuring high-quality service for real-time edge systems is a significant challenge due to the considerable computational demands and resource requirements of these models. Existing strategies typically either offload transformer computations to other devices or directly deploy compressed models on individual edge devices. These strategies, however, result in either considerable communication overhead or suboptimal trade-offs between accuracy and efficiency. To tackle these challenges, we propose a collaborative inference system for general transformer models, termed CoFormer. The central idea behind CoFormer is to exploit the divisibility and integrability of transformer. An off-the-shelf large transformer can be decomposed into multiple smaller models for distributed inference, and their intermediate results are aggregated to generate the final output. We formulate an optimization problem to minimize both inference latency and accuracy degradation under heterogeneous hardware constraints. DeBo algorithm is proposed to first solve the optimization problem to derive the decomposition policy, and then progressively calibrate decomposed models to restore performance. We demonstrate the capability to support a wide range of transformer models on heterogeneous edge devices, achieving up to 3.1$\\times$ inference speedup with large transformer models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6 billion parameters on edge devices, reducing memory requirements by 76.3\\%. CoFormer can also reduce energy consumption by approximately 40\\% while maintaining satisfactory inference performance.         ",
    "url": "https://arxiv.org/abs/2508.20375",
    "authors": [
      "Guanyu Xu",
      "Zhiwei Hao",
      "Li Shen",
      "Yong Luo",
      "Fuhui Sun",
      "Xiaoyan Wang",
      "Han Hu",
      "Yonggang Wen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2508.20376",
    "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction",
    "abstract": "           Sufficient cross-task interaction is crucial for success in multi-task dense prediction. However, sufficient interaction often results in high computational complexity, forcing existing methods to face the trade-off between interaction completeness and computational efficiency. To address this limitation, this work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel scanning mechanisms to adapt the Mamba modeling approach for multi-task dense prediction. On the one hand, we introduce a novel Bidirectional Interaction Scan (BI-Scan) mechanism, which constructs task-specific representations as bidirectional sequences during interaction. By integrating task-first and position-first scanning modes within a unified linear complexity architecture, BI-Scan efficiently preserves critical cross-task information. On the other hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve multi-granularity scene modeling. This design not only meets the diverse granularity requirements of various tasks but also enhances nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, \\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its state-of-the-art competitors.         ",
    "url": "https://arxiv.org/abs/2508.20376",
    "authors": [
      "Mang Cao",
      "Sanping Zhou",
      "Yizhe Li",
      "Ye Deng",
      "Wenli Huang",
      "Le Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20379",
    "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts",
    "abstract": "           Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.         ",
    "url": "https://arxiv.org/abs/2508.20379",
    "authors": [
      "Hyeonyu Kim",
      "Seokhoon Jeong",
      "Seonghee Han",
      "Chanhyuk Choi",
      "Taehwan Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20392",
    "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection",
    "abstract": "           Spiking Neural Networks (SNNs), inspired by the brain, are characterized by minimal power consumption and swift inference capabilities on neuromorphic hardware, and have been widely applied to various visual perception tasks. Current ANN-SNN conversion methods have achieved excellent results in classification tasks with ultra-low time-steps, but their performance in visual detection tasks remains suboptimal. In this paper, we propose a delay-spike approach to mitigate the issue of residual membrane potential caused by heterogeneous spiking patterns. Furthermore, we propose a novel temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This enables Integrate-and-fire (IF) neurons to dynamically adjust their accumulation and firing behaviors based on the temporal order of time-steps. Our method enables spikes to exhibit distinct temporal properties, rather than relying solely on frequency-based representations. Moreover, the tdIF neuron maintains energy consumption on par with traditional IF neuron. We demonstrate that our method achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. In this study, we conduct extensive evaluation of the tdIF method across two critical vision tasks: object detection and lane line detection. The results demonstrate that the proposed method surpasses current ANN-SNN conversion approaches, achieving state-of-the-art performance with ultra-low latency (within 5 time-steps).         ",
    "url": "https://arxiv.org/abs/2508.20392",
    "authors": [
      "Chengjun Zhang",
      "Yuhao Zhang",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20398",
    "title": "TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin",
    "abstract": "           Electrocardiogram (ECG) signals serve as a foundational data source for cardiac digital twins, yet their diagnostic utility is frequently compromised by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a novel one-dimensional deep neural network that integrates a U-Net-based encoder-decoder architecture with a Transformer encoder, guided by a hybrid time-frequency domain loss. The model is designed to simultaneously capture local morphological features and long-range temporal dependencies, which are critical for preserving the diagnostic integrity of ECG signals. To enhance denoising robustness, we introduce a dual-domain loss function that jointly optimizes waveform reconstruction in the time domain and spectral fidelity in the frequency domain. In particular, the frequency-domain component effectively suppresses high-frequency noise while maintaining the spectral structure of the signal, enabling recovery of subtle but clinically significant waveform components. We evaluate TF-TransUNet1D using synthetically corrupted signals from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database (NSTDB). Comparative experiments against state-of-the-art baselines demonstrate consistent superiority of our model in terms of SNR improvement and error metrics, achieving a mean absolute error of 0.1285 and Pearson correlation coefficient of 0.9540. By delivering high-precision denoising, this work bridges a critical gap in pre-processing pipelines for cardiac digital twins, enabling more reliable real-time monitoring and personalized modeling.         ",
    "url": "https://arxiv.org/abs/2508.20398",
    "authors": [
      "Shijie Wang",
      "Lei Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20403",
    "title": "pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification",
    "abstract": "           Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or sparsifier, whose Laplacian matrix closely approximates the spectral properties of the original graph, enabling substantial reductions in computational complexity for computationally intensive problems in scientific computing. The state-of-the-art method for efficient GSS is feGRASS, consisting of two steps: 1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS suffers from two main issues: 1) difficulties in parallelizing the recovery step for strict data dependencies, and 2) performance degradation on skewed inputs, often requiring multiple passes to recover sufficient edges. To address these challenges, we propose parallel density-aware Graph Spectral Sparsification (pdGRASS), a parallel algorithm that organizes edges into disjoint subtasks without data dependencies between them, enabling efficient parallelization and sufficient edge recovery in a single pass. We empirically evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2) sparsifier quality, measured by the iteration count required for convergence in a preconditioned conjugate gradient (PCG) application. The evaluation demonstrates that, depending on the number of edges recovered, pdGRASS achieves average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show between 1.2x higher and 1.8x lower PCG iteration counts, with further improvements as more edges are recovered. Additionally, pdGRASS mitigates the worst-case runtimes of feGRASS with over 1000x speedup. These results highlight pdGRASS's significant improvements in scalability and performance for the graph spectral sparsification problem.         ",
    "url": "https://arxiv.org/abs/2508.20403",
    "authors": [
      "Tiancheng Zhao",
      "Zekun Yin",
      "Huihai An",
      "Xiaoyu Yang",
      "Zhou Jin",
      "Jiasi Shen",
      "Helen Xu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.20408",
    "title": "Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers",
    "abstract": "           Dense retrievers and rerankers are central to retrieval-augmented generation (RAG) pipelines, where accurately retrieving factual information is crucial for maintaining system trustworthiness and defending against RAG poisoning. However, little is known about how much factual competence these components inherit or lose from the large language models (LLMs) they are based on. We pair 12 publicly released embedding checkpoints with their original base LLMs and evaluate both sets on a factuality benchmark. Across every model evaluated, the embedding variants achieve markedly lower accuracy than their bases, with absolute drops ranging from 12 to 43 percentage points (median 28 pts) and typical retriever accuracies collapsing into the 25-35 % band versus the 60-70 % attained by the generative models. This degradation intensifies under a more demanding condition: when the candidate pool per question is expanded from four options to one thousand, the strongest retriever's top-1 accuracy falls from 33 % to 26 %, revealing acute sensitivity to distractor volume. Statistical tests further show that, for every embedding model, cosine-similarity scores between queries and correct completions are significantly higher than those for incorrect ones (p < 0.01), indicating decisions driven largely by surface-level semantic proximity rather than factual reasoning. To probe this weakness, we employed GPT-4.1 to paraphrase each correct completion, creating a rewritten test set that preserved factual truth while masking lexical cues, and observed that over two-thirds of previously correct predictions flipped to wrong, reducing overall accuracy to roughly one-third of its original level. Taken together, these findings reveal a systematic trade-off introduced by contrastive learning for retrievers: gains in semantic retrieval are paid for with losses in parametric factual knowledge......         ",
    "url": "https://arxiv.org/abs/2508.20408",
    "authors": [
      "Haoyu Wu",
      "Qingcheng Zeng",
      "Kaize Ding"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.20412",
    "title": "MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph",
    "abstract": "           The Model Context Protocol (MCP) is increasingly adopted to standardize the interaction between LLM agents and external tools. However, this trend introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is poisoned to induce the agent to perform unauthorized operations. Existing defenses that primarily focus on behavior-level analysis are fundamentally ineffective against TPA, as poisoned tools need not be executed, leaving no behavioral trace to monitor. Thus, we propose MindGuard, a decision-level guardrail for LLM agents, providing provenance tracking of call decisions, policy-agnostic detection, and poisoning source attribution against TPA. While fully explaining LLM decision remains challenging, our empirical findings uncover a strong correlation between LLM attention mechanisms and tool invocation decisions. Therefore, we choose attention as an empirical signal for decision tracking and formalize this as the Decision Dependence Graph (DDG), which models the LLM's reasoning process as a weighted, directed graph where vertices represent logical concepts and edges quantify the attention-based dependencies. We further design robust DDG construction and graph-based anomaly analysis mechanisms that efficiently detect and attribute TPA attacks. Extensive experiments on real-world datasets demonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting poisoned invocations, 95\\%-100\\% attribution accuracy, with processing times under one second and no additional token cost. Moreover, DDG can be viewed as an adaptation of the classical Program Dependence Graph (PDG), providing a solid foundation for applying traditional security policies at the decision level.         ",
    "url": "https://arxiv.org/abs/2508.20412",
    "authors": [
      "Zhiqiang Wang",
      "Junyang Zhang",
      "Guanquan Shi",
      "HaoRan Cheng",
      "Yunhao Yao",
      "Kaiwen Guo",
      "Haohua Du",
      "Xiang-Yang Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.20415",
    "title": "Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection",
    "abstract": "           In view of the problems that existing salient object detection (SOD) methods are prone to losing details, blurring edges, and insufficient fusion of single-modal information in complex scenes, this paper proposes a dynamic uncertainty propagation and multimodal collaborative reasoning network (DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is designed to propagate uncertainty between layers through a sparse graph constructed based on spatial semantic distance, and combined with channel adaptive interaction, it effectively improves the detection accuracy of small structures and edge regions. Secondly, a multimodal collaborative fusion strategy (MCF) is proposed, which uses learnable modality gating weights to weightedly fuse the attention maps of RGB, depth, and edge features. It can dynamically adjust the importance of each modality according to different scenes, effectively suppress redundant or interfering information, and strengthen the semantic complementarity and consistency between cross-modalities, thereby improving the ability to identify salient regions under occlusion, weak texture or background interference. Finally, the detection performance at the pixel level and region level is optimized through multi-scale BCE and IoU loss, cross-scale consistency constraints, and uncertainty-guided supervision mechanisms. Extensive experiments show that DUP-MCRNet outperforms various SOD methods on most common benchmark datasets, especially in terms of edge clarity and robustness to complex backgrounds. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20415",
    "authors": [
      "Yuqi Xiong",
      "Wuzhen Shi",
      "Yang Wen",
      "Ruhan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20417",
    "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval",
    "abstract": "           The integration of knowledge graphs (KGs) with large language models (LLMs) offers significant potential to improve the retrieval phase of retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR, a novel framework for Contextual Query Retrieval (CQR) that enhances the retrieval phase by enriching the contextual representation of complex input queries using a corpus-centric KG. Unlike existing methods that primarily address corpus-level context loss, KG-CQR focuses on query enrichment through structured relation representations, extracting and completing relevant KG subgraphs to generate semantically rich query contexts. Comprising subgraph extraction, completion, and contextual generation modules, KG-CQR operates as a model-agnostic pipeline, ensuring scalability across LLMs of varying sizes without additional training. Experimental results on RAGBench and MultiHop-RAG datasets demonstrate KG-CQR's superior performance, achieving a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. Furthermore, evaluations on challenging RAG tasks such as multi-hop question answering show that, by incorporating KG-CQR, the performance consistently outperforms the existing baseline in terms of retrieval effectiveness         ",
    "url": "https://arxiv.org/abs/2508.20417",
    "authors": [
      "Chi Minh Bui",
      "Ngoc Mai Thieu",
      "Van Vinh Nguyen",
      "Json J.Jung",
      "Khac-Hoai Nam Bui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2508.20440",
    "title": "D3PINNs: A Novel Physics-Informed Neural Network Framework for Staged Solving of Time-Dependent Partial Differential Equations",
    "abstract": "           In this paper, we propose a novel framework, Dynamic Domain Decomposition Physics-Informed Neural Networks (D3PINNs), for solving time-dependent partial differential equations (PDEs). In this framework, solutions of time-dependent PDEs are dynamically captured. First, an approximate solution is obtained by the Physics-Informed Neural Networks (PINNs) containing the domain decomposition, then the time derivative terms in the PDE will be retained and the other terms associated with the solution will be replaced with the approximate solution. As a result, the PDE reduces to an ordinary differential equations (ODEs). Finally, the time-varying solution will be solved by the classical numerical methods for ODEs. D3PINNs retain the computational efffciency and ffexibility inherent to PINNs and enhance the ability for capturing solutions of time-dependent PDEs. Numerical experiments validate the effectiveness of the proposed methods.         ",
    "url": "https://arxiv.org/abs/2508.20440",
    "authors": [
      "Xun Yang",
      "Guanqiu Ma"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.20447",
    "title": "MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection",
    "abstract": "           Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end trainable deep learning methods have progressed greatly. However, they often struggle to detect pedestrians with consistently small or large scales in views or with vastly different scales between views. This is because they do not exploit multi-scale image features to generate the BEV feature and detect pedestrians. To overcome this problem, we propose a novel MVPD method, called Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV features by projecting multi-scale image features extracted from individual views into the BEV space, scale-by-scale. Each of these BEV features inherits the properties of its corresponding scale image features from multiple views. Therefore, these BEV features help the precise detection of pedestrians with consistently small or large scales in views. Then, MSMVD combines information at different scales of multiple views by processing the multi-scale BEV features using a feature pyramid network. This improves the detection of pedestrians with vastly different scales between views. Extensive experiments demonstrate that exploiting multi-scale image features via multi-scale BEV features greatly improves the detection performance, and MSMVD outperforms the previous highest MODA by $4.5$ points on the GMVD dataset.         ",
    "url": "https://arxiv.org/abs/2508.20447",
    "authors": [
      "Taiga Yamane",
      "Satoshi Suzuki",
      "Ryo Masumura",
      "Shota Orihashi",
      "Tomohiro Tanaka",
      "Mana Ihori",
      "Naoki Makishima",
      "Naotaka Kawata"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20449",
    "title": "A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection",
    "abstract": "           With the rapid advancement of real-time deepfake generation techniques, forged content is becoming increasingly realistic and widespread across applications like video conferencing and social media. Although state-of-the-art detectors achieve high accuracy on standard benchmarks, their heavy computational cost hinders real-time deployment in practical applications. To address this, we propose the Spatial-Frequency Aware Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture for real-time deepfake detection. We design a spatial-frequency hybrid aware module that jointly leverages spatial textures and frequency artifacts through a gated mechanism, enhancing sensitivity to subtle manipulations. A token-selective cross attention mechanism enables efficient multi-level feature interaction, while a residual-enhanced blur pooling structure helps retain key semantic cues during downsampling. Experiments on several benchmark datasets show that SFMFNet achieves a favorable balance between accuracy and efficiency, with strong generalization and practical value for real-time applications.         ",
    "url": "https://arxiv.org/abs/2508.20449",
    "authors": [
      "Libo Lv",
      "Tianyi Wang",
      "Mengxiao Huang",
      "Ruixia Liu",
      "Yinglong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20453",
    "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers",
    "abstract": "           We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20453",
    "authors": [
      "Zhenting Wang",
      "Qi Chang",
      "Hemani Patel",
      "Shashank Biju",
      "Cheng-En Wu",
      "Quan Liu",
      "Aolin Ding",
      "Alireza Rezazadeh",
      "Ankit Shah",
      "Yujia Bao",
      "Eugene Siow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20460",
    "title": "Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques",
    "abstract": "           Background Predicting mortality and resource utilization from electronic health records (EHRs) is challenging yet crucial for optimizing patient outcomes and managing costs in intensive care unit (ICU). Existing approaches predominantly focus on structured EHRs, often ignoring the valuable clinical insights in free-text notes. Additionally, the potential of textual information within structured data is not fully leveraged. This study aimed to introduce and assess a deep learning framework using natural language processing techniques that integrates multimodal EHRs to predict mortality and resource utilization in critical care settings. Methods Utilizing two real-world EHR datasets, we developed and evaluated our model on three clinical tasks with leading existing methods. We also performed an ablation study on three key components in our framework: medical prompts, free-texts, and pre-trained sentence encoder. Furthermore, we assessed the model's robustness against the corruption in structured EHRs. Results Our experiments on two real-world datasets across three clinical tasks showed that our proposed model improved performance metrics by 1.6\\%/0.8\\% on BACC/AUROC for mortality prediction, 0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical duration estimation compared to the best existing methods. It consistently demonstrated superior performance compared to other baselines across three tasks at different corruption rates. Conclusions The proposed framework is an effective and accurate deep learning approach for predicting mortality and resource utilization in critical care. The study also highlights the success of using prompt learning with a transformer encoder in analyzing multimodal EHRs. Importantly, the model showed strong resilience to data corruption within structured data, especially at high corruption levels.         ",
    "url": "https://arxiv.org/abs/2508.20460",
    "authors": [
      "Yucheng Ruan",
      "Xiang Lan",
      "Daniel J. Tan",
      "Hairil Rizal Abdullah",
      "Mengling Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20462",
    "title": "Automated Quality Assessment for LLM-Based Complex Qualitative Coding: A Confidence-Diversity Framework",
    "abstract": "           While previous research demonstrated effective automated quality assessment for accessible LLM coding tasks, a fundamental question remains: can confidence-diversity frameworks maintain reliability for complex analytical tasks requiring specialized domain expertise and extensive text comprehension? Traditional inter-coder reliability measures become prohibitively expensive at scale, yet the lack of reliable automated quality assessment methods creates methodological barriers to AI adoption in sophisticated qualitative research. This study extends dual-signal quality assessment combining model confidence and inter-model consensus from accessible to complex analytical domains. We systematically validate this approach across three domains: legal reasoning (390 Supreme Court cases), political analysis (645 hyperpartisan articles), and medical classification (1,000 clinical transcripts). Results demonstrate that uncertainty-based indicators maintain predictive validity in complex tasks, with external entropy showing consistent negative correlations with accuracy (r = -0.179 to -0.273, p < 0.001) and confidence exhibiting positive correlations in two domains (r = 0.104 to 0.429). Systematic weight optimization achieves 6.6 to 113.7 percent improvements over single-signal approaches, with optimized weights transferring effectively across domains (100 percent success rate). An intelligent triage system reduces manual verification effort by 44.6 percent while maintaining quality standards. These findings establish that automated quality assessment can scale from accessible to complex analytical tasks, providing practical tools for expanding AI-assisted qualitative research. Future work will focus on addressing long-tail challenges in high-disagreement, low-confidence cases to further enhance screening efficiency.         ",
    "url": "https://arxiv.org/abs/2508.20462",
    "authors": [
      "Zhilong Zhao",
      "Yindi Liu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.20488",
    "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts",
    "abstract": "           Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical applications like autonomous driving, yet its reliability deteriorates significantly under real-world domain shifts caused by environmental or sensor variations. To address these shifts, Test-Time Adaptation (TTA) methods have emerged, enabling models to adapt to target distributions during inference. While prior TTA approaches recognize the positive correlation between low uncertainty and high generalization ability, they fail to address the dual uncertainty inherent to M3OD: semantic uncertainty (ambiguous class predictions) and geometric uncertainty (unstable spatial localization). To bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA framework designed to jointly minimize both uncertainties for robust M3OD. Through a convex optimization lens, we introduce an innovative convex structure of the focal loss and further derive a novel unsupervised version, enabling label-agnostic uncertainty weighting and balanced learning for high-uncertainty objects. In parallel, we design a semantic-aware normal field constraint that preserves geometric coherence in regions with clear semantic cues, reducing uncertainty from the unstable 3D representation. This dual-branch mechanism forms a complementary loop: enhanced spatial perception improves semantic classification, and robust semantic predictions further refine spatial understanding. Extensive experiments demonstrate the superiority of DUO over existing methods across various datasets and domain shift types.         ",
    "url": "https://arxiv.org/abs/2508.20488",
    "authors": [
      "Zixuan Hu",
      "Dongxiao Li",
      "Xinzhu Ma",
      "Shixiang Tang",
      "Xiaotong Li",
      "Wenhan Yang",
      "Ling-Yu Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20492",
    "title": "IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection",
    "abstract": "           Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce an novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet achieves a new state-of-the-art with a markedly lower false positive rate, underscoring its practical value for industrial deployment.         ",
    "url": "https://arxiv.org/abs/2508.20492",
    "authors": [
      "Xuanming Cao",
      "Chengyu Tao",
      "Yifeng Cheng",
      "Juan Du"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20493",
    "title": "Adaptive Control of Heterogeneous Platoons with Guaranteed Collision Avoidance",
    "abstract": "           This work proposes a framework for Cooperative Adaptive Cruise Control of a vehicular platoon characterized by unidirectional communication and heterogeneous parameters. In the proposed framework, the actual (heterogeneous) platoon is made to converge to a reference (homogeneous) platoon via adaptive laws designed using of set-theoretic model reference adaptive control. Yet, in contrast to the state-of-art that is based on ensuring collision avoidance on the reference platoon dynamics only, the approach we propose can ensure collision avoidance on the actual platoon dynamics. This result is possible thanks to the introduction of a novel concept of virtual platoon, only used for analysis, but that does not interact with the actual platoon. The stability and convergence properties of the proposed framework are established using Lyapunov-based analysis in conjunction with the aforementioned virtual platoon concept.         ",
    "url": "https://arxiv.org/abs/2508.20493",
    "authors": [
      "Ashutosh Chandra Pandey",
      "Sayan Basu Roy",
      "Simone Baldi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.20500",
    "title": "Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records",
    "abstract": "           Electronic Health Records (EHR) systematically organize patient health data through standardized medical codes, serving as a comprehensive and invaluable source for predictive modeling. Graph neural networks (GNNs) have demonstrated effectiveness in modeling interactions between medical codes within EHR. However, existing GNN-based methods are inadequate due to: a) their reliance on pairwise relations fails to capture the inherent higher-order dependencies in clinical data, and b) the localized message-passing scheme limits representation power. To address these issues, this paper proposes a novel Structure-aware HyperGraph Transformer (SHGT) framework following three-fold ideas: a) employing a hypergraph structural encoder to capture higher-order interactions among medical codes, b) integrating the Transformer architecture to reason over the entire hypergraph, and c) designing a tailored loss function incorporating hypergraph reconstruction to preserve the hypergraph's original structure. Experiments on real-world EHR datasets demonstrate that the proposed SHGT outperforms existing state-of-the-art models on diagnosis prediction.         ",
    "url": "https://arxiv.org/abs/2508.20500",
    "authors": [
      "Haiyan Wang",
      "Ye Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20517",
    "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining",
    "abstract": "           Cross-chain bridges play a vital role in enabling blockchain interoperability. However, due to the inherent design flaws and the enormous value they hold, they have become prime targets for hacker attacks. Existing detection methods show progress yet remain limited, as they mainly address single-chain behaviors and fail to capture cross-chain semantics. To address this gap, we leverage heterogeneous graph attention networks, which are well-suited for modeling multi-typed entities and relations, to capture the complex execution semantics of cross-chain behaviors. We propose BridgeShield, a detection framework that jointly models the source chain, off-chain coordination, and destination chain within a unified heterogeneous graph representation. BridgeShield incorporates intra-meta-path attention to learn fine-grained dependencies within cross-chain paths and inter-meta-path attention to highlight discriminative cross-chain patterns, thereby enabling precise identification of attack behaviors. Extensive experiments on 51 real-world cross-chain attack events demonstrate that BridgeShield achieves an average F1-score of 92.58%, representing a 24.39% improvement over state-of-the-art baselines. These results validate the effectiveness of BridgeShield as a practical solution for securing cross-chain bridges and enhancing the resilience of multi-chain ecosystems.         ",
    "url": "https://arxiv.org/abs/2508.20517",
    "authors": [
      "Dan Lin",
      "Shunfeng Lu",
      "Ziyan Liu",
      "Jiajing Wu",
      "Junyuan Fang",
      "Kaixin Lin",
      "Bowen Song",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20530",
    "title": "Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection",
    "abstract": "           Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$\\%$ mAP on the nuScenes validation benchmark.         ",
    "url": "https://arxiv.org/abs/2508.20530",
    "authors": [
      "Mingqian Ji",
      "Jian Yang",
      "Shanshan Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20544",
    "title": "Local Observability of a Class of Feedforward Neural Networks",
    "abstract": "           Beyond the traditional neural network training methods based on gradient descent and its variants, state estimation techniques have been proposed to determine a set of ideal weights from a control-theoretic perspective. Hence, the concept of observability becomes relevant in neural network training. In this paper, we investigate local observability of a class of two-layer feedforward neural networks~(FNNs) with rectified linear unit~(ReLU) activation functions. We analyze local observability of FNNs by evaluating an observability rank condition with respect to the weight matrix and the input sequence. First, we show that, in general, the weights of FNNs are not locally observable. Then, we provide sufficient conditions on the network structures and the weights that lead to local observability. Moreover, we propose an input design approach to render the weights distinguishable and show that this input also excites other weights inside a neighborhood. Finally, we validate our results through a numerical example.         ",
    "url": "https://arxiv.org/abs/2508.20544",
    "authors": [
      "Yi Yang",
      "Victor G. Lopez",
      "Matthias A. M\u00fcller"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.20546",
    "title": "MM-HSD: Multi-Modal Hate Speech Detection in Videos",
    "abstract": "           While hate speech detection (HSD) has been extensively studied in text, existing multi-modal approaches remain limited, particularly in videos. As modalities are not always individually informative, simple fusion methods fail to fully capture inter-modal dependencies. Moreover, previous work often omits relevant modalities such as on-screen text and audio, which may contain subtle hateful content and thus provide essential cues, both individually and in combination with others. In this paper, we present MM-HSD, a multi-modal model for HSD in videos that integrates video frames, audio, and text derived from speech transcripts and from frames (i.e.~on-screen text) together with features extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an early feature extractor for HSD in videos, to systematically compare query/key configurations, and to evaluate the interactions between different modalities in the CMA block. Our approach leads to improved performance when on-screen text is used as a query and the rest of the modalities serve as a key. Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art methods on M-F1 score (0.874), using concatenation of transcript, audio, video, on-screen text, and CMA for feature extraction on raw embeddings of the modalities. The code is available at this https URL ",
    "url": "https://arxiv.org/abs/2508.20546",
    "authors": [
      "Berta C\u00e9spedes-Sarrias",
      "Carlos Collado-Capell",
      "Pablo Rodenas-Ruiz",
      "Olena Hrynenko",
      "Andrea Cavallaro"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20551",
    "title": "Contrastive Learning through Auxiliary Branch for Video Object Detection",
    "abstract": "           Video object detection is a challenging task because videos often suffer from image deterioration such as motion blur, occlusion, and deformable shapes, making it significantly more difficult than detecting objects in still images. Prior approaches have improved video object detection performance by employing feature aggregation and complex post-processing techniques, though at the cost of increased computational demands. To improve robustness to image degradation without additional computational load during inference, we introduce a straightforward yet effective Contrastive Learning through Auxiliary Branch (CLAB) method. First, we implement a constrastive auxiliary branch using a contrastive loss to enhance the feature representation capability of the video object detector's backbone. Next, we propose a dynamic loss weighting strategy that emphasizes auxiliary feature learning early in training while gradually prioritizing the detection task as training converges. We validate our approach through comprehensive experiments and ablation studies, demonstrating consistent performance gains. Without bells and whistles, CLAB reaches a performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101, respectively, on the ImageNet VID dataset, thus achieving state-of-the-art performance for CNN-based models without requiring additional post-processing methods.         ",
    "url": "https://arxiv.org/abs/2508.20551",
    "authors": [
      "Lucas Rakotoarivony"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20570",
    "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
    "abstract": "           Typographic attacks exploit multi-modal systems by injecting text into images, leading to targeted misclassifications, malicious content generation and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP vision encoders behave under typographic attacks, locating specialized attention heads in the latter half of the model's layers that causally extract and transmit typographic information to the cls token. Building on these insights, we introduce a method to defend CLIP models against typographic attacks by selectively ablating a typographic circuit, consisting of attention heads. Without requiring finetuning, our method improves performance by up to 19.6% on a typographic variant of ImageNet-100, while reducing standard ImageNet-100 accuracy by less than 1%. Notably, our training-free approach remains competitive with current state-of-the-art typographic defenses that rely on finetuning. To this end, we release a family of dyslexic CLIP models which are significantly more robust against typographic attacks. These models serve as suitable drop-in replacements for a broad range of safety-critical applications, where the risks of text-based manipulation outweigh the utility of text recognition.         ",
    "url": "https://arxiv.org/abs/2508.20570",
    "authors": [
      "Lorenz Hufe",
      "Constantin Venhoff",
      "Maximilian Dreyer",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20578",
    "title": "Human-AI Collaborative Bot Detection in MMORPGs",
    "abstract": "           In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling bots exploit automated programs to level up characters at scale, undermining gameplay balance and fairness. Detecting such bots is challenging, not only because they mimic human behavior, but also because punitive actions require explainable justification to avoid legal and user experience issues. In this paper, we present a novel framework for detecting auto-leveling bots by leveraging contrastive representation learning and clustering techniques in a fully unsupervised manner to identify groups of characters with similar level-up patterns. To ensure reliable decisions, we incorporate a Large Language Model (LLM) as an auxiliary reviewer to validate the clustered groups, effectively mimicking a secondary human judgment. We also introduce a growth curve-based visualization to assist both the LLM and human moderators in assessing leveling behavior. This collaborative approach improves the efficiency of bot detection workflows while maintaining explainability, thereby supporting scalable and accountable bot regulation in MMORPGs.         ",
    "url": "https://arxiv.org/abs/2508.20578",
    "authors": [
      "Jaeman Son",
      "Hyunsoo Kim"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.20579",
    "title": "GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition",
    "abstract": "           Facial expression recognition (FER) is a crucial task in computer vision with wide range of applications including human computer interaction, surveillance, and assistive technologies. However, challenges such as occlusion, expression variability, and lack of interpretability hinder the performance of traditional FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by modeling relational dependencies between facial landmarks, enabling structured and interpretable learning. In this paper, we propose GLaRE, a novel Graph-based Landmark Region Embedding network for emotion recognition. Facial landmarks are extracted using 3D facial alignment, and a quotient graph is constructed via hierarchical coarsening to preserve spatial structure while reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet and 94.24 percentage on FERG, outperforming several existing baselines. Additionally, ablation studies have demonstrated that region-level embeddings from quotient graphs have contributed to improved prediction performance.         ",
    "url": "https://arxiv.org/abs/2508.20579",
    "authors": [
      "Debasis Maji",
      "Debaditya Barman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20583",
    "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models",
    "abstract": "           Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language.         ",
    "url": "https://arxiv.org/abs/2508.20583",
    "authors": [
      "Soham Petkar",
      "Hari Aakash K",
      "Anirudh Vempati",
      "Akshit Sinha",
      "Ponnurangam Kumarauguru",
      "Chirag Agarwal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20587",
    "title": "SemSR: Semantics aware robust Session-based Recommendations",
    "abstract": "           Session-based recommendation (SR) models aim to recommend items to anonymous users based on their behavior during the current session. While various SR models in the literature utilize item sequences to predict the next item, they often fail to leverage semantic information from item titles or descriptions impeding session intent identification and interpretability. Recent research has explored Large Language Models (LLMs) as promising approaches to enhance session-based recommendations, with both prompt-based and fine-tuning based methods being widely investigated. However, prompt-based methods struggle to identify optimal prompts that elicit correct reasoning and lack task-specific feedback at test time, resulting in sub-optimal recommendations. Fine-tuning methods incorporate domain-specific knowledge but incur significant computational costs for implementation and maintenance. In this paper, we present multiple approaches to utilize LLMs for session-based recommendation: (i) in-context LLMs as recommendation agents, (ii) LLM-generated representations for semantic initialization of deep learning SR models, and (iii) integration of LLMs with data-driven SR models. Through comprehensive experiments on two real-world publicly available datasets, we demonstrate that LLM-based methods excel at coarse-level retrieval (high recall values), while traditional data-driven techniques perform well at fine-grained ranking (high Mean Reciprocal Rank values). Furthermore, the integration of LLMs with data-driven SR models significantly out performs both standalone LLM approaches and data-driven deep learning models, as well as baseline SR models, in terms of both Recall and MRR metrics.         ",
    "url": "https://arxiv.org/abs/2508.20587",
    "authors": [
      "Jyoti Narwariya",
      "Priyanka Gupta",
      "Muskan Gupta",
      "Jyotsana Khatri",
      "Lovekesh Vig"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20594",
    "title": "UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching",
    "abstract": "           The thermal camera excels at perceiving outdoor environments under low-light conditions, making it ideal for applications such as nighttime autonomous driving and unmanned navigation. However, thermal cameras encounter challenges when capturing signage from objects made of similar materials, which can pose safety risks for accurately understanding semantics in autonomous driving systems. In contrast, the neuromorphic vision camera, also known as an event camera, detects changes in light intensity asynchronously and has proven effective in high-speed, low-light traffic environments. Recognizing the complementary characteristics of these two modalities, this paper proposes UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage in low-illumination environments, targeting elements such as license plates and roadblock indicators. To address the signage blind spots of thermal imaging and the non-uniform sampling of event cameras, we developed a dual-boosting mechanism that fuses thermal frames and event signals for consistent signage representation over time. The proposed method utilizes thermal frames to provide accurate motion cues as temporal references for aligning the uneven event signals. At the same time, event signals contribute subtle signage content to the raw thermal frames, enhancing the overall understanding of the environment. The proposed method is validated on datasets collected from real-world scenarios, demonstrating superior quality in traffic signage sketching and improved detection accuracy at the perceptual level.         ",
    "url": "https://arxiv.org/abs/2508.20594",
    "authors": [
      "Yuqi Han",
      "Songqian Zhang",
      "Weijian Su",
      "Ke Li",
      "Jiayu Yang",
      "Jinli Suo",
      "Qiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20595",
    "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations",
    "abstract": "           Deepfake technology, driven by Generative Adversarial Networks (GANs), poses significant risks to privacy and societal security. Existing detection methods are predominantly passive, focusing on post-event analysis without preventing attacks. To address this, we propose an active defense method based on low-frequency perceptual perturbations to disrupt face swapping manipulation, reducing the performance and naturalness of generated content. Unlike prior approaches that used low-frequency perturbations to impact classification accuracy,our method directly targets the generative process of deepfake techniques. We combine frequency and spatial domain features to strengthen defenses. By introducing artifacts through low-frequency perturbations while preserving high-frequency details, we ensure the output remains visually plausible. Additionally, we design a complete architecture featuring an encoder, a perturbation generator, and a decoder, leveraging discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW demonstrate significant reductions in face-swapping effectiveness, improved defense success rates, and preservation of visual quality.         ",
    "url": "https://arxiv.org/abs/2508.20595",
    "authors": [
      "Mengxiao Huang",
      "Minglei Shu",
      "Shuwang Zhou",
      "Zhaoyang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20597",
    "title": "Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks",
    "abstract": "           Over-squashing is a challenge in training graph neural networks for tasks involving long-range dependencies. In such tasks, a GNN's receptive field should be large enough to enable communication between distant nodes. However, gathering information from a wide range of neighborhoods and squashing its content into fixed-size node representations makes message-passing vulnerable to bottlenecks. Graph rewiring and adding virtual nodes are commonly studied remedies that create additional pathways around bottlenecks to mitigate over-squashing. However, these techniques alter the input graph's global topology and disrupt the domain knowledge encoded in the original graph structure, both of which could be essential to specific tasks and domains. This study presents Local Virtual Nodes (LVN) with trainable embeddings to alleviate the effects of over-squashing without significantly corrupting the global structure of the input graph. The position of the LVNs is determined by the node centrality, which indicates the existence of potential bottlenecks. Thus, the proposed approach aims to improve the connectivity in the regions with likely bottlenecks. Furthermore, trainable LVN embeddings shared across selected central regions facilitate communication between distant nodes without adding more layers. Extensive experiments on benchmark datasets demonstrate that LVNs can enhance structural connectivity and significantly improve performance on graph and node classification tasks. The code can be found at this https URL}{this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20597",
    "authors": [
      "Tu\u011frul Hasan Karabulut",
      "\u0130nci M. Bayta\u015f"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20613",
    "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization",
    "abstract": "           The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.         ",
    "url": "https://arxiv.org/abs/2508.20613",
    "authors": [
      "Yixiang Qiu",
      "Yanhan Liu",
      "Hongyao Yu",
      "Hao Fang",
      "Bin Chen",
      "Shu-Tao Xia",
      "Ke Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.20621",
    "title": "Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification",
    "abstract": "           Breast cancer is one of the leading causes of cancer-related mortality in women, and early detection is essential for improving outcomes. Magnetic resonance imaging (MRI) is a highly sensitive tool for breast cancer detection, particularly in women at high risk or with dense breast tissue, where mammography is less effective. The ODELIA consortium organized a multi-center challenge to foster AI-based solutions for breast cancer diagnosis and classification. The dataset included 511 studies from six European centers, acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study was labeled for the left and right breast as no lesion, benign lesion, or malignant lesion. We developed a SwinUNETR-based deep learning framework that incorporates breast region masking, extensive data augmentation, and ensemble learning to improve robustness and generalizability. Our method achieved second place on the challenge leaderboard, highlighting its potential to support clinical breast MRI interpretation. We publicly share our codebase at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20621",
    "authors": [
      "Smriti Joshi",
      "Lidia Garrucho",
      "Richard Osuala",
      "Oliver Diaz",
      "Karim Lekadir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20622",
    "title": "Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications",
    "abstract": "           We investigated the adaptation and performance of Masked Autoencoders (MAEs) with Vision Transformer (ViT) architectures for self-supervised representation learning on one-dimensional (1D) ultrasound signals. Although MAEs have demonstrated significant success in computer vision and other domains, their use for 1D signal analysis, especially for raw ultrasound data, remains largely unexplored. Ultrasound signals are vital in industrial applications such as non-destructive testing (NDT) and structural health monitoring (SHM), where labeled data are often scarce and signal processing is highly task-specific. We propose an approach that leverages MAE to pre-train on unlabeled synthetic ultrasound signals, enabling the model to learn robust representations that enhance performance in downstream tasks, such as time-of-flight (ToF) classification. This study systematically investigated the impact of model size, patch size, and masking ratio on pre-training efficiency and downstream accuracy. Our results show that pre-trained models significantly outperform models trained from scratch and strong convolutional neural network (CNN) baselines optimized for the downstream task. Additionally, pre-training on synthetic data demonstrates superior transferability to real-world measured signals compared with training solely on limited real datasets. This study underscores the potential of MAEs for advancing ultrasound signal analysis through scalable, self-supervised learning.         ",
    "url": "https://arxiv.org/abs/2508.20622",
    "authors": [
      "Immanuel Ro\u00dfteutscher",
      "Klaus S. Drese",
      "Thorsten Uphues"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20625",
    "title": "Relay Selection in Wireless Networks as Restless Bandits",
    "abstract": "           We consider a wireless network in which a source node needs to transmit a large file to a destination node. The direct wireless link between the source and the destination is assumed to be blocked. Multiple candidate relays are available to forward packets from the source to the destination. A holding cost is incurred for each packet stored at every relay in each time slot. The objective is to design a policy for selecting a relay in each time slot to which the source attempts to send a packet, so as to minimize the expected long-run time-averaged total packet holding cost at the relays. This problem is an instance of the restless multi-armed bandit (RMAB) problem, which is provably hard to solve. We prove that this relay selection problem is Whittle-indexable, and propose a method to compute the Whittle index of each relay in every time slot. In each time slot, our relay selection policy transmits a packet to the relay with the smallest Whittle index. Using simulations, we show that the proposed policy outperforms the relay selection policies proposed in prior work in terms of average cost, delay, as well as throughput.         ",
    "url": "https://arxiv.org/abs/2508.20625",
    "authors": [
      "Mandar R. Nalavade",
      "Ravindra S. Tomar",
      "Gaurav S. Kasbekar"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.20637",
    "title": "GDS Agent: A Graph Algorithmic Reasoning Agent",
    "abstract": "           Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We also introduce a new benchmark that evaluates intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.         ",
    "url": "https://arxiv.org/abs/2508.20637",
    "authors": [
      "Borun Shi",
      "Ioannis Panagiotas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20643",
    "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics",
    "abstract": "           Large Language Model (LLM) agents are powerful tools for automating complex tasks. In cybersecurity, researchers have primarily explored their use in red-team operations such as vulnerability discovery and penetration tests. Defensive uses for incident response and forensics have received comparatively less attention and remain at an early stage. This work presents a systematic study of LLM-agent design for the forensic investigation of realistic web application attacks. We propose CyberSleuth, an autonomous agent that processes packet-level traces and application logs to identify the targeted service, the exploited vulnerability (CVE), and attack success. We evaluate the consequences of core design decisions - spanning tool integration and agent architecture - and provide interpretable guidance for practitioners. We benchmark four agent architectures and six LLM backends on 20 incident scenarios of increasing complexity, identifying CyberSleuth as the best-performing design. In a separate set of 10 incidents from 2025, CyberSleuth correctly identifies the exact CVE in 80% of cases. At last, we conduct a human study with 22 experts, which rated the reports of CyberSleuth as complete, useful, and coherent. They also expressed a slight preference for DeepSeek R1, a good news for open source LLM. To foster progress in defensive LLM research, we release both our benchmark and the CyberSleuth platform as a foundation for fair, reproducible evaluation of forensic agents.         ",
    "url": "https://arxiv.org/abs/2508.20643",
    "authors": [
      "Stefano Fumero",
      "Kai Huang",
      "Matteo Boffa",
      "Danilo Giordano",
      "Marco Mellia",
      "Zied Ben Houidi",
      "Dario Rossi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.20645",
    "title": "A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks",
    "abstract": "           With the increasing scale and dynamics of data, distributed online optimization has become essential for real-time decision-making in various applications. However, existing algorithms often rely on bounded gradient assumptions and overlook the impact of stochastic gradients, especially in time-varying directed networks. This study proposes a novel Time-Varying Hybrid Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid stochastic gradient tracking and variance reduction mechanisms. Specifically, TV-HSGT integrates row-stochastic and column-stochastic communication schemes over time-varying digraphs, eliminating the need for Perron vector estimation or out-degree information. By combining current and recursive stochastic gradients, it effectively reduces gradient variance while accurately tracking global descent directions. Theoretical analysis demonstrates that TV-HSGT can achieve improved bounds on dynamic regret without assuming gradient boundedness. Experimental results on logistic regression tasks confirm the effectiveness of TV-HSGT in dynamic and resource-constrained environments.         ",
    "url": "https://arxiv.org/abs/2508.20645",
    "authors": [
      "Xinli Shi",
      "Xingxing Yuan",
      "Longkang Zhu",
      "Guanghui Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2508.20650",
    "title": "Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach",
    "abstract": "           In this work, we propose a novel framework to enhance the efficiency and accuracy of neural operators through self-composition, offering both theoretical guarantees and practical benefits. Inspired by iterative methods in solving numerical partial differential equations (PDEs), we design a specific neural operator by repeatedly applying a single neural operator block, we progressively deepen the model without explicitly adding new blocks, improving the model's capacity. To train these models efficiently, we introduce an adaptive train-and-unroll approach, where the depth of the neural operator is gradually increased during training. This approach reveals an accuracy scaling law with model depth and offers significant computational savings through our adaptive training strategy. Our architecture achieves state-of-the-art (SOTA) performance on standard benchmarks. We further demonstrate its efficacy on a challenging high-frequency ultrasound computed tomography (USCT) problem, where a multigrid-inspired backbone enables superior performance in resolving complex wave phenomena. The proposed framework provides a computationally tractable, accurate, and scalable solution for large-scale data-driven scientific machine learning applications.         ",
    "url": "https://arxiv.org/abs/2508.20650",
    "authors": [
      "Juncai He",
      "Xinliang Liu",
      "Jinchao Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20656",
    "title": "Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation",
    "abstract": "           This work investigates whether time series of natural phenomena can be understood as being generated by sequences of latent states which are ordered in systematic and regular ways. We focus on clinical time series and ask whether clinical measurements can be interpreted as being generated by meaningful physiological states whose succession follows systematic principles. Uncovering the underlying compositional structure will allow us to create synthetic data to alleviate the notorious problem of sparse and low-resource data settings in clinical time series forecasting, and deepen our understanding of clinical data. We start by conceptualizing compositionality for time series as a property of the data generation process, and then study data-driven procedures that can reconstruct the elementary states and composition rules of this process. We evaluate the success of this methods using two empirical tests originating from a domain adaptation perspective. Both tests infer the similarity of the original time series distribution and the synthetic time series distribution from the similarity of expected risk of time series forecasting models trained and tested on original and synthesized data in specific ways. Our experimental results show that the test set performance achieved by training on compositionally synthesized data is comparable to training on original clinical time series data, and that evaluation of models on compositionally synthesized test data shows similar results to evaluating on original test data, outperforming randomization-based data augmentation. An additional downstream evaluation of the prediction task of sequential organ failure assessment (SOFA) scores shows significant performance gains when model training is entirely based on compositionally synthesized data compared to training on original data.         ",
    "url": "https://arxiv.org/abs/2508.20656",
    "authors": [
      "Michael Hagmann",
      "Michael Staniek",
      "Stefan Riezler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20670",
    "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware Synthetic Image Detection",
    "abstract": "           Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 \"in the wild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to \"in the wild\" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.         ",
    "url": "https://arxiv.org/abs/2508.20670",
    "authors": [
      "Anastasios Skoularikis",
      "Stefanos-Iordanis Papadopoulos",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2508.20705",
    "title": "EEGDM: Learning EEG Representation with Latent Diffusion Model",
    "abstract": "           While electroencephalography (EEG) signal analysis using deep learning has shown great promise, existing approaches still face significant challenges in learning generalizable representations that perform well across diverse tasks, particularly when training data is limited. Current EEG representation learning methods including EEGPT and LaBraM typically rely on simple masked reconstruction objective, which may not fully capture the rich semantic information and complex patterns inherent in EEG signals. In this paper, we propose EEGDM, a novel self-supervised EEG representation learning method based on the latent diffusion model, which leverages EEG signal generation as a self-supervised objective, turning the diffusion model into a strong representation learner capable of capturing EEG semantics. EEGDM incorporates an EEG encoder that distills EEG signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively learns robust representations, and (3) achieves competitive performance with modest pre-training data size across diverse downstream tasks, underscoring its generalizability and practical utility.         ",
    "url": "https://arxiv.org/abs/2508.20705",
    "authors": [
      "Shaocong Wang",
      "Tong Liu",
      "Ming Li",
      "Minjing Yu",
      "Yong-Jin Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20709",
    "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network",
    "abstract": "           Neural Video Compression (NVC) has achieved remarkable performance in recent years. However, precise rate control remains a challenge due to the inherent limitations of learning-based codecs. To solve this issue, we propose a dynamic video compression framework designed for variable bitrate scenarios. First, to achieve variable bitrate implementation, we propose the Dynamic-Route Autoencoder with variable coding routes, each occupying partial computational complexity of the whole network and navigating to a distinct RD trade-off. Second, to approach the target bitrate, the Rate Control Agent estimates the bitrate of each route and adjusts the coding route of DRA at run time. To encompass a broad spectrum of variable bitrates while preserving overall RD performance, we employ the Joint-Routes Optimization strategy, achieving collaborative training of various routes. Extensive experiments on the HEVC and UVG datasets show that the proposed method achieves an average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods while maintaining an average bitrate error of 1.66%, achieving Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and bitrate-constrained applications. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20709",
    "authors": [
      "Chenhao Zhang",
      "Wei Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20715",
    "title": "Multi-cluster distributed optimization in open multi-agent systems over directed graphs with acknowledgement messages",
    "abstract": "           In this paper, we tackle the problem of distributed optimization over directed networks in open multi-agent systems (OMAS), where agents may dynamically join or leave, causing persistent changes in network topology and problem dimension. These disruptions not only pose significant challenges to maintaining convergence and stability in distributed optimization algorithms, but could also break the network topology into multiple clusters, each one associated with its own set of objective functions. To address this, we propose a novel Open Distributed Optimization Algorithm with Gradient Tracking (OPEN-GT), which employs: (a) a dynamic mechanism for detecting active out-neighbors through acknowledgement messages, and (b) a fully distributed max-consensus procedure to spread information regarding agent departures, in possibly unbalanced directed networks. We show that when all active agents execute OPEN-GT, the optimization process in each formed cluster remains consistent, while the agents converge to their cluster-wide optimal solution if there exists a time after which the network remains unchanged. Finally, we validate our approach in a simulated environment with dynamically changing agent populations, demonstrating its resilience to network variations and its ability to support distributed optimization under OMAS dynamics.         ",
    "url": "https://arxiv.org/abs/2508.20715",
    "authors": [
      "Evagoras Makridis",
      "Gabriele Oliva",
      "Themistoklis Charalambous"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.20717",
    "title": "Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions",
    "abstract": "           Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.         ",
    "url": "https://arxiv.org/abs/2508.20717",
    "authors": [
      "Ran Piao",
      "Yuan Lu",
      "Hareld Kemps",
      "Tong Xia",
      "Aaqib Saeed"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20734",
    "title": "CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network",
    "abstract": "           Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.         ",
    "url": "https://arxiv.org/abs/2508.20734",
    "authors": [
      "Reza Akbari Movahed",
      "Abuzar Rezaee",
      "Arezoo Zakeri",
      "Colin Berry",
      "Edmond S. L. Ho",
      "Ali Gooya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20736",
    "title": "Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees",
    "abstract": "           Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under local DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter $\\varepsilon$. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high $\\varepsilon$ values. Addressing this challenge, we introduce DP-ST, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the divide-and-conquer paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a privatization neighborhood. When combined with LLM post-processing, our method allows for coherent text generation even at lower $\\varepsilon$ values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable $\\varepsilon$ levels.         ",
    "url": "https://arxiv.org/abs/2508.20736",
    "authors": [
      "Stephen Meisenbacher",
      "Maulik Chevli",
      "Florian Matthes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20740",
    "title": "Non-expert to Expert Motion Translation Using Generative Adversarial Networks",
    "abstract": "           Decreasing skilled workers is a very serious problem in the world. To deal with this problem, the skill transfer from experts to robots has been researched. These methods which teach robots by human motion are called imitation learning. Experts' skills generally appear in not only position data, but also force data. Thus, position and force data need to be saved and reproduced. To realize this, a lot of research has been conducted in the framework of a motion-copying system. Recent research uses machine learning methods to generate motion commands. However, most of them could not change tasks by following human intention. Some of them can change tasks by conditional training, but the labels are limited. Thus, we propose the flexible motion translation method by using Generative Adversarial Networks. The proposed method enables users to teach robots tasks by inputting data, and skills by a trained model. We evaluated the proposed system with a 3-DOF calligraphy robot.         ",
    "url": "https://arxiv.org/abs/2508.20740",
    "authors": [
      "Yuki Tanaka",
      "Seiichiro Katsura"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.20750",
    "title": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets",
    "abstract": "           Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score.         ",
    "url": "https://arxiv.org/abs/2508.20750",
    "authors": [
      "Vassiliy Cheremetiev",
      "Quang Long Ho Ngo",
      "Chau Ying Kot",
      "Alina Elena Baia",
      "Andrea Cavallaro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20757",
    "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation",
    "abstract": "           Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel \"Glocal\" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20757",
    "authors": [
      "Yuanhao Ding",
      "Esteban Garces Arias",
      "Meimingwei Li",
      "Julian Rodemann",
      "Matthias A\u00dfenmacher",
      "Danlu Chen",
      "Gaojuan Fan",
      "Christian Heumann",
      "Chongsheng Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20760",
    "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
    "abstract": "           Vision-language models (VLMs) like CLIP enable zero-shot classification by aligning images and text in a shared embedding space, offering advantages for defense applications with scarce labeled data. However, CLIP's robustness in challenging military environments, with partial occlusion and degraded signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP variants' robustness to occlusion using a custom dataset of 18 military vehicle classes and evaluate using Normalized Area Under the Curve (NAUC) across occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP models consistently outperform CNNs, (2) fine-grained, dispersed occlusions degrade performance more than larger contiguous occlusions, (3) despite improved accuracy, performance of linear-probed models sharply drops at around 35% occlusion, (4) by finetuning the model's backbone, this performance drop occurs at more than 60% occlusion. These results underscore the importance of occlusion-specific augmentations during training and the need for further exploration into patch-level sensitivity and architectural resilience for real-world deployment of CLIP.         ",
    "url": "https://arxiv.org/abs/2508.20760",
    "authors": [
      "Jan Erik van Woerden",
      "Gertjan Burghouts",
      "Lotte Nijskens",
      "Alma M. Liezenga",
      "Sabina van Rooij",
      "Frank Ruis",
      "Hugo J. Kuijf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20762",
    "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer",
    "abstract": "           Focusing on the development of an end-to-end autonomous vehicle model with pixel-to-pixel context awareness, this research proposes the SKGE-Swin architecture. This architecture utilizes the Swin Transformer with a skip-stage mechanism to broaden feature representation globally and at various network levels. This approach enables the model to extract information from distant pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head Self-Attention (SW-MSA) mechanism and to retain critical information from the initial to the final stages of feature extraction, thereby enhancing its capability to comprehend complex patterns in the vehicle's surroundings. The model is evaluated on the CARLA platform using adversarial scenarios to simulate real-world conditions. Experimental results demonstrate that the SKGE-Swin architecture achieves a superior Driving Score compared to previous methods. Furthermore, an ablation study will be conducted to evaluate the contribution of each architectural component, including the influence of skip connections and the use of the Swin Transformer, in improving model performance.         ",
    "url": "https://arxiv.org/abs/2508.20762",
    "authors": [
      "Fachri Najm Noer Kartiman",
      "Rasim",
      "Yaya Wihardi",
      "Nurul Hasanah",
      "Oskar Natan",
      "Bambang Wahono",
      "Taufik Ibnu Salim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.20784",
    "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
    "abstract": "           Bus bunching remains a challenge for urban transit due to stochastic traffic and passenger demand. Traditional solutions rely on multi-agent reinforcement learning (MARL) in loop-line settings, which overlook realistic operations characterized by heterogeneous routes, timetables, fluctuating demand, and varying fleet sizes. We propose a novel single-agent reinforcement learning (RL) framework for bus holding control that avoids the data imbalance and convergence issues of MARL under near-realistic simulation. A bidirectional timetabled network with dynamic passenger demand is constructed. The key innovation is reformulating the multi-agent problem into a single-agent one by augmenting the state space with categorical identifiers (vehicle ID, station ID, time period) in addition to numerical features (headway, occupancy, velocity). This high-dimensional encoding enables single-agent policies to capture inter-agent dependencies, analogous to projecting non-separable inputs into a higher-dimensional space. We further design a structured reward function aligned with operational goals: instead of exponential penalties on headway deviations, a ridge-shaped reward balances uniform headways and schedule adherence. Experiments show that our modified soft actor-critic (SAC) achieves more stable and superior performance than benchmarks, including MADDPG (e.g., -430k vs. -530k under stochastic conditions). These results demonstrate that single-agent deep RL, when enhanced with categorical structuring and schedule-aware rewards, can effectively manage bus holding in non-loop, real-world contexts. This paradigm offers a robust, scalable alternative to MARL frameworks, particularly where agent-specific experiences are imbalanced.         ",
    "url": "https://arxiv.org/abs/2508.20784",
    "authors": [
      "Yifan Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20805",
    "title": "Exploring Machine Learning and Language Models for Multimodal Depression Detection",
    "abstract": "           This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction.         ",
    "url": "https://arxiv.org/abs/2508.20805",
    "authors": [
      "Javier Si Zhao Hong",
      "Timothy Zoe Delaya",
      "Sherwyn Chan Yin Kit",
      "Pai Chet Ng",
      "Xiaoxiao Miao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2508.20813",
    "title": "Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training",
    "abstract": "           Accurate dental caries detection from panoramic X-rays plays a pivotal role in preventing lesion progression. However, current detection methods often yield suboptimal accuracy due to subtle contrast variations and diverse lesion morphology of dental caries. In this work, inspired by the clinical workflow where dentists systematically combine whole-image screening with detailed tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training network for accurate dental caries detection. Our DVCTNet starts with employing automated tooth detection to establish two complementary views: a global view from panoramic X-ray images and a local view from cropped tooth images. We then pretrain two vision foundation models separately on the two views. The global-view foundation model serves as the detection backbone, generating region proposals and global features, while the local-view model extracts detailed features from corresponding cropped tooth patches matched by the region proposals. To effectively integrate information from both views, we introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically fuses dual-view features, enhancing the detection pipeline by integrating the fused features back into the detection model for final caries detection. To rigorously evaluate our DVCTNet, we test it on a public dataset and further validate its performance on a newly curated, high-precision dental caries detection dataset, annotated using both intra-oral images and panoramic X-rays for double verification. Experimental results demonstrate DVCTNet's superior performance against existing state-of-the-art (SOTA) methods on both datasets, indicating the clinical applicability of our method. Our code and labeled dataset are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.20813",
    "authors": [
      "Tao Luo",
      "Han Wu",
      "Tong Yang",
      "Dinggang Shen",
      "Zhiming Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20817",
    "title": "FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning",
    "abstract": "           Most visible and infrared image fusion (VIF) methods focus primarily on optimizing fused image quality. Recent studies have begun incorporating downstream tasks, such as semantic segmentation and object detection, to provide semantic guidance for VIF. However, semantic segmentation requires extensive annotations, while object detection, despite reducing annotation efforts compared with segmentation, faces challenges in highly crowded scenes due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd counting has gained increasing attention in recent years, no studies have integrated VIF and crowd counting into a unified framework. To address these challenges, we propose FusionCounting, a novel multi-task learning framework that integrates crowd counting into the VIF process. Crowd counting provides a direct quantitative measure of population density with minimal annotation, making it particularly suitable for dense scenes. Our framework leverages both input images and population density information in a mutually beneficial multi-task design. To accelerate convergence and balance tasks contributions, we introduce a dynamic loss function weighting strategy. Furthermore, we incorporate adversarial training to enhance the robustness of both VIF and crowd counting, improving the model's stability and resilience to adversarial attacks. Experimental results on public datasets demonstrate that FusionCounting not only enhances image fusion quality but also achieves superior crowd counting performance.         ",
    "url": "https://arxiv.org/abs/2508.20817",
    "authors": [
      "He Li",
      "Xinyu Liu",
      "Weihang Kong",
      "Xingchen Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20829",
    "title": "ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks",
    "abstract": "           Financial fraud detection is essential to safeguard billions of dollars, yet the intertwined entities and fast-changing transaction behaviors in modern financial systems routinely defeat conventional machine learning models. Recent graph-based detectors make headway by representing transactions as networks, but they still overlook two fraud hallmarks rooted in time: (1) temporal motifs--recurring, telltale subgraphs that reveal suspicious money flows as they unfold--and (2) account-specific intervals of anomalous activity, when fraud surfaces only in short bursts unique to each entity. To exploit both signals, we introduce ATM-GAD, an adaptive graph neural network that leverages temporal motifs for financial anomaly detection. A Temporal Motif Extractor condenses each account's transaction history into the most informative motifs, preserving both topology and temporal patterns. These motifs are then analyzed by dual-attention blocks: IntraA reasons over interactions within a single motif, while InterA aggregates evidence across motifs to expose multi-step fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner tailors the observation window for every node, allowing the model to focus precisely on the most revealing time slices. Experiments on four real-world datasets show that ATM-GAD consistently outperforms seven strong anomaly-detection baselines, uncovering fraud patterns missed by earlier methods.         ",
    "url": "https://arxiv.org/abs/2508.20829",
    "authors": [
      "Zeyue Zhang",
      "Lin Song",
      "Erkang Bao",
      "Xiaoling Lv",
      "Xinyue Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20863",
    "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
    "abstract": "           Large Language Models (LLMs) are increasingly being integrated into the scientific peer-review process, raising new questions about their reliability and resilience to manipulation. In this work, we investigate the potential for hidden prompt injection attacks, where authors embed adversarial text within a paper's PDF to influence the LLM-generated review. We begin by formalising three distinct threat models that envision attackers with different motivations -- not all of which implying malicious intent. For each threat model, we design adversarial prompts that remain invisible to human readers yet can steer an LLM's output toward the author's desired outcome. Using a user study with domain scholars, we derive four representative reviewing prompts used to elicit peer reviews from LLMs. We then evaluate the robustness of our adversarial prompts across (i) different reviewing prompts, (ii) different commercial LLM-based systems, and (iii) different peer-reviewed papers. Our results show that adversarial prompts can reliably mislead the LLM, sometimes in ways that adversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and empirically assess methods to reduce detectability of adversarial prompts under automated content checks.         ",
    "url": "https://arxiv.org/abs/2508.20863",
    "authors": [
      "Matteo Gioele Collu",
      "Umberto Salviati",
      "Roberto Confalonieri",
      "Mauro Conti",
      "Giovanni Apruzzese"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.20865",
    "title": "Deep Multiple Quantization Network on Long Behavior Sequence for Click-Through Rate Prediction",
    "abstract": "           In Click-Through Rate (CTR) prediction, the long behavior sequence, comprising the user's long period of historical interactions with items has a vital influence on assessing the user's interest in the candidate item. Existing approaches strike efficiency and effectiveness through a two-stage paradigm: first retrieving hundreds of candidate-related items and then extracting interest intensity vector through target attention. However, we argue that the discrepancy in target attention's relevance distribution between the retrieved items and the full long behavior sequence inevitably leads to a performance decline. To alleviate the discrepancy, we propose the Deep Multiple Quantization Network (DMQN) to process long behavior sequence end-to-end through compressing the long behavior sequence. Firstly, the entire spectrum of long behavior sequence will be quantized into multiple codeword sequences based on multiple independent codebooks. Hierarchical Sequential Transduction Unit is incorporated to facilitate the interaction of reduced codeword sequences. Then, attention between the candidate and multiple codeword sequences will output the interest vector. To enable online serving, intermediate representations of the codeword sequences are cached, significantly reducing latency. Our extensive experiments on both industrial and public datasets confirm the effectiveness and efficiency of DMQN. The A/B test in our advertising system shows that DMQN improves CTR by 3.5% and RPM by 2.0%.         ",
    "url": "https://arxiv.org/abs/2508.20865",
    "authors": [
      "Zhuoxing Wei",
      "Qi Liu",
      "Qingchen Xie"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.20866",
    "title": "AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning",
    "abstract": "           The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the critical need for effective automated vulnerability detection and repair systems. Traditional methods, such as static program analysis, face significant challenges related to scalability, adaptability, and high false-positive and false-negative rates. AI-driven approaches, particularly those using machine learning and deep learning models, show promise but are heavily reliant on the quality and quantity of training data. This paper introduces a novel framework designed to automatically introduce realistic, category-specific vulnerabilities into secure C/C++ codebases to generate datasets. The proposed approach coordinates multiple AI agents that simulate expert reasoning, along with function agents and traditional code analysis tools. It leverages Retrieval-Augmented Generation for contextual grounding and employs Low-Rank approximation of weights for efficient model fine-tuning. Our experimental study on 116 code samples from three different benchmarks suggests that our approach outperforms other techniques with regard to dataset accuracy, achieving between 89\\% and 95\\% success rates in injecting vulnerabilities at function level.         ",
    "url": "https://arxiv.org/abs/2508.20866",
    "authors": [
      "Amine Lbath",
      "Massih-Reza Amini",
      "Aurelien Delaitre",
      "Vadim Okun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20869",
    "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models",
    "abstract": "           Improvements in training data scale and quality have led to significant advances, yet its influence in speech recognition remains underexplored. In this paper, we present a large-scale dataset, OLMoASR-Pool, and series of models, OLMoASR, to study and develop robust zero-shot speech recognition models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio and 17M transcripts, we design text heuristic filters to remove low-quality or mistranscribed data. Our curation pipeline produces a new dataset containing 1M hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M (this http URL) to 1.5B (this http URL) parameters. Across all model scales, OLMoASR achieves comparable average performance to OpenAI's Whisper on short and long-form speech recognition benchmarks. Notably, this http URL attains a 12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest English-only model this http URL's 12.4\\% and 10.5\\% WER for short and long-form recognition respectively (at equivalent parameter count). OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will be made publicly available to further research on robust speech processing.         ",
    "url": "https://arxiv.org/abs/2508.20869",
    "authors": [
      "Huong Ngo",
      "Matt Deitke",
      "Martijn Bartelds",
      "Sarah Pratt",
      "Josh Gardner",
      "Matt Jordan",
      "Ludwig Schmidt"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.20877",
    "title": "Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis",
    "abstract": "           Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms of cancer, with a five-year survival rate below 10% primarily due to late detection. This research develops and validates a deep learning framework for early PDAC detection through analysis of dual-modality imaging: autofluorescence and second harmonic generation (SHG). We analyzed 40 unique patient samples to create a specialized neural network capable of distinguishing between normal, fibrotic, and cancerous tissue. Our methodology evaluated six distinct deep learning architectures, comparing traditional Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs). Through systematic experimentation, we identified and overcome significant challenges in medical image analysis, including limited dataset size and class imbalance. The final optimized framework, based on a modified ResNet architecture with frozen pre-trained layers and class-weighted training, achieved over 90% accuracy in cancer detection. This represents a significant improvement over current manual analysis methods an demonstrates potential for clinical deployment. This work establishes a robust pipeline for automated PDAC detection that can augment pathologists' capabilities while providing a foundation for future expansion to other cancer types. The developed methodology also offers valuable insights for applying deep learning to limited-size medical imaging datasets, a common challenge in clinical applications.         ",
    "url": "https://arxiv.org/abs/2508.20877",
    "authors": [
      "Dennis Slobodzian",
      "Karissa Tilbury",
      "Amir Kordijazi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20885",
    "title": "SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization",
    "abstract": "           Voice activity detection (VAD) is essential for speech-driven applications, but remains far from perfect in noisy and resource-limited environments. Existing methods often lack robustness to noise, and their frame-wise classification losses are only loosely coupled with the evaluation metric of VAD. To address these challenges, we propose SincQDR-VAD, a compact and robust framework that combines a Sinc-extractor front-end with a novel quadratic disparity ranking loss. The Sinc-extractor uses learnable bandpass filters to capture noise-resistant spectral features, while the ranking loss optimizes the pairwise score order between speech and non-speech frames to improve the area under the receiver operating characteristic curve (AUROC). A series of experiments conducted on representative benchmark datasets show that our framework considerably improves both AUROC and F2-Score, while using only 69% of the parameters compared to prior arts, confirming its efficiency and practical viability.         ",
    "url": "https://arxiv.org/abs/2508.20885",
    "authors": [
      "Chien-Chun Wang",
      "En-Lun Yu",
      "Jeih-Weih Hung",
      "Shih-Chieh Huang",
      "Berlin Chen"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2508.20888",
    "title": "AERO-LQG: Aerial-Enabled Robust Optimization for LQG-Based Quadrotor Flight Controller",
    "abstract": "           Quadrotors are indispensable in civilian, industrial, and military domains, undertaking complex, high-precision tasks once reserved for specialized systems. Across all contexts, energy efficiency remains a critical constraint: quadrotors must reconcile the high power demands of agility with the minimal consumption required for extended endurance. Meeting this trade-off calls for mode-specific optimization frameworks that adapt to diverse mission profiles. At their core lie optimal control policies defining error functions whose minimization yields robust, mission-tailored performance. While solutions are straightforward for fixed weight matrices, selecting those weights is a far greater challenge-lacking analytical guidance and thus relying on exhaustive or stochastic search. This interdependence can be framed as a bi-level optimization problem, with the outer loop determining weights a priori. This work introduces an aerial-enabled robust optimization for LQG tuning (AERO-LQG), a framework employing evolutionary strategy to fine-tune LQG weighting parameters. Applied to the linearized hovering mode of quadrotor flight, AERO-LQG achieves performance gains of several tens of percent, underscoring its potential for enabling high-performance, energy-efficient quadrotor control. The project is available at GitHub.         ",
    "url": "https://arxiv.org/abs/2508.20888",
    "authors": [
      "Daniel Engelsman",
      "Itzik Klein"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.20906",
    "title": "Turning Tabular Foundation Models into Graph Foundation Models",
    "abstract": "           While foundation models have revolutionized such fields as natural language processing and computer vision, their application and potential within graph machine learning remain largely unexplored. One of the key challenges in designing graph foundation models (GFMs) is handling diverse node features that can vary across different graph datasets. Although many works on GFMs have been focused exclusively on text-attributed graphs, the problem of handling arbitrary features of other types in GFMs has not been fully addressed. However, this problem is not unique to the graph domain, as it also arises in the field of machine learning for tabular data. In this work, motivated by the recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a simple graph foundation model that employs TabPFNv2 as a backbone. Specifically, G2T-FM augments the original node features with neighborhood feature aggregation, adds structural embeddings, and then applies TabPFNv2 to the constructed node representations. Even in a fully in-context regime, our model achieves strong results, significantly outperforming publicly available GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the potential of the proposed approach. More broadly, our paper reveals a previously overlooked direction of utilizing tabular foundation models for graph machine learning tasks.         ",
    "url": "https://arxiv.org/abs/2508.20906",
    "authors": [
      "Dmitry Eremeev",
      "Gleb Bazhenov",
      "Oleg Platonov",
      "Artem Babenko",
      "Liudmila Prokhorenkova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20911",
    "title": "Deep Learning Based Concurrency Bug Detection and Localization",
    "abstract": "           Concurrency bugs, caused by improper synchronization of shared resources in multi-threaded or distributed systems, are notoriously hard to detect and thus compromise software reliability and security. The existing deep learning methods face three main limitations. First, there is an absence of large and dedicated datasets of diverse concurrency bugs for them. Second, they lack sufficient representation of concurrency semantics. Third, binary classification results fail to provide finer-grained debug information such as precise bug lines. To address these problems, we propose a novel method for effective concurrency bug detection as well as localization. We construct a dedicated concurrency bug dataset to facilitate model training and evaluation. We then integrate a pre-trained model with a heterogeneous graph neural network (GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that concisely and effectively characterizes concurrency semantics. To further facilitate debugging, we employ SubgraphX, a GNN-based interpretability method, which explores the graphs to precisely localize concurrency bugs, mapping them to specific lines of source code. On average, our method demonstrates an improvement of 10\\% in accuracy and precision and 26\\% in recall compared to state-of-the-art methods across diverse evaluation settings.         ",
    "url": "https://arxiv.org/abs/2508.20911",
    "authors": [
      "Zuocheng Feng",
      "Kaiwen Zhang",
      "Miaomiao Wang",
      "Yiming Cheng",
      "Yuandao Cai",
      "Xiaofeng Li",
      "Guanjun Liu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.20914",
    "title": "Learning Robust Spatial Representations from Binaural Audio through Feature Distillation",
    "abstract": "           Recently, deep representation learning has shown strong performance in multiple audio tasks. However, its use for learning spatial representations from multichannel audio is underexplored. We investigate the use of a pretraining stage based on feature distillation to learn a robust spatial representation of binaural speech without the need for data labels. In this framework, spatial features are computed from clean binaural speech samples to form prediction labels. These clean features are then predicted from corresponding augmented speech using a neural network. After pretraining, we throw away the spatial feature predictor and use the learned encoder weights to initialize a DoA estimation model which we fine-tune for DoA estimation. Our experiments demonstrate that the pretrained models show improved performance in noisy and reverberant environments after fine-tuning for direction-of-arrival estimation, when compared to fully supervised models and classic signal processing methods.         ",
    "url": "https://arxiv.org/abs/2508.20914",
    "authors": [
      "Holger Severin Bovbjerg",
      "Jan \u00d8stergaard",
      "Jesper Jensen",
      "Shinji Watanabe",
      "Zheng-Hua Tan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.20931",
    "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $\u03c4$-bench",
    "abstract": "           Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.         ",
    "url": "https://arxiv.org/abs/2508.20931",
    "authors": [
      "Venkatesh Mishra",
      "Amir Saeidi",
      "Satyam Raj",
      "Mutsumi Nakamura",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Ali Payani",
      "Chitta Baral"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20934",
    "title": "Enhancing Soft Happiness via Evolutionary Algorithms",
    "abstract": "           For $0\\leq \\rho\\leq 1$, a $\\rho$-happy vertex $v$ in a coloured graph shares colour with at least $\\rho\\mathrm{deg}(v)$ of its neighbours. Soft happy colouring of a graph $G$ with $k$ colours extends a partial $k$-colouring to a complete vertex $k$-colouring such that the number of $\\rho$-happy vertices is maximum among all such colouring extensions. The problem is known to be NP-hard, and an optimal solution has a direct relation with the community structure of the graph. In addition, some heuristics and local search algorithms, such as {\\sf Local Maximal Colouring} ({\\sf LMC}) and {\\sf Local Search} ({\\sf LS}), have already been introduced in the literature. In this paper, we design Genetic and Memetic Algorithms for soft happy colouring and test them for a large set of randomly generated partially coloured graphs. Memetic Algorithms yield a higher number of $\\rho$-happy vertices, but Genetic Algorithms can perform well only when their initial populations are locally improved by {\\sf LMC} or {\\sf LS}. Statistically significant results indicate that both Genetic and Memetic Algorithms achieve high average accuracy in community detection when their initial populations are enhanced using {\\sf LMC}. Moreover, among the competing methods, the evolutionary algorithms identified the greatest number of complete solutions.         ",
    "url": "https://arxiv.org/abs/2508.20934",
    "authors": [
      "Mohammad Hadi Shekarriza",
      "Dhananjay Thiruvadya",
      "Asef Nazari"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2508.20957",
    "title": "Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks",
    "abstract": "           The growing demand for services and the rapid deployment of virtualized network functions (VNFs) pose significant challenges for achieving low-latency and energy-efficient orchestration in modern edge-core network infrastructures. To address these challenges, this study proposes a Digital Twin (DT)-empowered Deep Reinforcement Learning framework for intelligent VNF migration that jointly minimizes average end-to-end (E2E) delay and energy consumption. By formulating the VNF migration problem as a Markov Decision Process and utilizing the Advantage Actor-Critic model, the proposed framework enables adaptive and real-time migration decisions. A key innovation of the proposed framework is the integration of a DT module composed of a multi-task Variational Autoencoder and a multi-task Long Short-Term Memory network. This combination collectively simulates environment dynamics and generates high-quality synthetic experiences, significantly enhancing training efficiency and accelerating policy convergence. Simulation results demonstrate substantial performance gains, such as significant reductions in both average E2E delay and energy consumption, thereby establishing new benchmarks for intelligent VNF migration in edge-core networks.         ",
    "url": "https://arxiv.org/abs/2508.20957",
    "authors": [
      "Faisal Ahmed",
      "Suresh Subramaniam",
      "Motoharu Matsuura",
      "Hiroshi Hasegawa",
      "Shih-Chun Lin"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.20985",
    "title": "RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN",
    "abstract": "           Radio Access Network (RAN) systems are inherently complex, requiring continuous monitoring to prevent performance degradation and ensure optimal user experience. The RAN leverages numerous key performance indicators (KPIs) to evaluate system performance, generating vast amounts of data each second. This immense data volume can make troubleshooting and accurate diagnosis of performance anomalies more difficult. Furthermore, the highly dynamic nature of RAN performance demands adaptive methodologies capable of capturing temporal dependencies to detect anomalies reliably. In response to these challenges, we introduce \\textbf{RANGAN}, an anomaly detection framework that integrates a Generative Adversarial Network (GAN) with a transformer architecture. To enhance the capability of capturing temporal dependencies within the data, RANGAN employs a sliding window approach during data preprocessing. We rigorously evaluated RANGAN using the publicly available RAN performance dataset from the Spotlight project \\cite{sun-2024}. Experimental results demonstrate that RANGAN achieves promising detection accuracy, notably attaining an F1-score of up to $83\\%$ in identifying network contention issues.         ",
    "url": "https://arxiv.org/abs/2508.20985",
    "authors": [
      "Douglas Liao",
      "Jiping Luo",
      "Jens Vevstad",
      "Nikolaos Pappas"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.20986",
    "title": "Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets",
    "abstract": "           Data has become a foundational asset driving innovation across domains such as finance, healthcare, and e-commerce. In these areas, predictive modeling over relational tables is commonly employed, with increasing emphasis on reducing manual effort through automated machine learning (AutoML) techniques. This raises an interesting question: can feature augmentation itself be automated and identify and utilize task-related relational signals? To address this challenge, we propose an end-to-end automated feature augmentation framework, ReCoGNN, which enhances initial datasets using features extracted from multiple relational tables to support predictive tasks. ReCoGNN first captures semantic dependencies within each table by modeling intra-table attribute relationships, enabling it to partition tables into structured, semantically coherent segments. It then constructs a heterogeneous weighted graph that represents inter-row relationships across all segments. Finally, ReCoGNN leverages message-passing graph neural networks to propagate information through the graph, guiding feature selection and augmenting the original dataset. Extensive experiments conducted on ten real-life and synthetic datasets demonstrate that ReCoGNN consistently outperforms existing methods on both classification and regression tasks.         ",
    "url": "https://arxiv.org/abs/2508.20986",
    "authors": [
      "Lianpeng Qiao",
      "Ziqi Cao",
      "Kaiyu Feng",
      "Ye Yuan",
      "Guoren Wang"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.21007",
    "title": "Rapid Mismatch Estimation via Neural Network Informed Variational Inference",
    "abstract": "           With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming-based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system.         ",
    "url": "https://arxiv.org/abs/2508.21007",
    "authors": [
      "Mateusz Jaszczuk",
      "Nadia Figueroa"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.21010",
    "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering",
    "abstract": "           Existing Causal-Why Video Question Answering (VideoQA) models often struggle with higher-order reasoning, relying on opaque, monolithic pipelines that entangle video understanding, causal inference, and answer generation. These black-box approaches offer limited interpretability and tend to depend on shallow heuristics. We propose a novel, modular framework that explicitly decouples causal reasoning from answer generation, introducing natural language causal chains as interpretable intermediate representations. Inspired by human cognitive models, these structured cause-effect sequences bridge low-level video content with high-level causal reasoning, enabling transparent and logically coherent inference. Our two-stage architecture comprises a Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. To address the lack of annotated reasoning traces, we introduce a scalable method for generating high-quality causal chains from existing datasets using large language models. We also propose CauCo, a new evaluation metric for causality-oriented captioning. Experiments on three large-scale benchmarks demonstrate that our approach not only outperforms state-of-the-art models, but also yields substantial gains in explainability, user trust, and generalization -- positioning the CCE as a reusable causal reasoning engine across diverse domains. Project page: this https URL ",
    "url": "https://arxiv.org/abs/2508.21010",
    "authors": [
      "Paritosh Parmar",
      "Eric Peh",
      "Basura Fernando"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.21019",
    "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
    "abstract": "           The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.         ",
    "url": "https://arxiv.org/abs/2508.21019",
    "authors": [
      "Jiaxiang Cheng",
      "Bing Ma",
      "Xuhua Ren",
      "Hongyi Jin",
      "Kai Yu",
      "Peng Zhang",
      "Wenyue Li",
      "Yuan Zhou",
      "Tianxiang Zheng",
      "Qinglin Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.21033",
    "title": "Mitosis detection in domain shift scenarios: a Mamba-based approach",
    "abstract": "           Mitosis detection in histopathology images plays a key role in tumor assessment. Although machine learning algorithms could be exploited for aiding physicians in accurately performing such a task, these algorithms suffer from significative performance drop when evaluated on images coming from domains that are different from the training ones. In this work, we propose a Mamba-based approach for mitosis detection under domain shift, inspired by the promising performance demonstrated by Mamba in medical imaging segmentation tasks. Specifically, our approach exploits a VM-UNet architecture for carrying out the addressed task, as well as stain augmentation operations for further improving model robustness against domain shift. Our approach has been submitted to the track 1 of the MItosis DOmain Generalization (MIDOG) challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show large room for improvement for the proposed method.         ",
    "url": "https://arxiv.org/abs/2508.21033",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.21035",
    "title": "A multi-task neural network for atypical mitosis recognition under domain shift",
    "abstract": "           Recognizing atypical mitotic figures in histopathology images allows physicians to correctly assess tumor aggressiveness. Although machine learning models could be exploited for automatically performing such a task, under domain shift these models suffer from significative performance drops. In this work, an approach based on multi-task learning is proposed for addressing this problem. By exploiting auxiliary tasks, correlated to the main classification task, the proposed approach, submitted to the track 2 of the MItosis DOmain Generalization (MIDOG) challenge, aims to aid the model to focus only on the object to classify, ignoring the domain varying background of the image. The proposed approach shows promising performance in a preliminary evaluation conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25 challenge.         ",
    "url": "https://arxiv.org/abs/2508.21035",
    "authors": [
      "Gennaro Percannella",
      "Mattia Sarno",
      "Francesco Tortorella",
      "Mario Vento"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.21047",
    "title": "DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks",
    "abstract": "           The modern Internet supports diverse applications with heterogeneous quality of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations offer a promising solution to meet these needs, enhancing coverage in rural areas and complementing terrestrial networks in urban regions. Ensuring QoS in such networks requires joint optimization of routing, bandwidth allocation, and dynamic queue scheduling, as traffic handling is critical for maintaining service performance. This paper formulates a joint routing and bandwidth allocation problem where QoS requirements are treated as soft constraints, aiming to maximize user experience. An adaptive scheduling approach is introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree search (MCTS)-inspired method to solve the NP-hard route and bandwidth allocation problem, with Lyapunov optimization-based scheduling applied during reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we compare end-user experience and fairness between our proposed DSROQ algorithm and a benchmark scheme. Results show that DSROQ improves both performance metrics and demonstrates the advantage of joint routing and bandwidth decisions. Furthermore, we observe that the dominant performance factor shifts from scheduling to routing and bandwidth allocation as traffic sensitivity changes from latency-driven to bandwidth-driven.         ",
    "url": "https://arxiv.org/abs/2508.21047",
    "authors": [
      "Dhiraj Bhattacharjee",
      "Pablo G. Madoery",
      "Abhishek Naik",
      "Halim Yanikomeroglu",
      "Gunes Karabulut Kurt",
      "Stephane Martel",
      "Khaled Ahmed"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.21048",
    "title": "Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning",
    "abstract": "           Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as \"planning\" and \"self-reflection\" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.         ",
    "url": "https://arxiv.org/abs/2508.21048",
    "authors": [
      "Hao Tan",
      "Jun Lan",
      "Zichang Tan",
      "Ajian Liu",
      "Chuanbiao Song",
      "Senyuan Shi",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Jun Wan",
      "Zhen Lei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.14712",
    "title": "Is Audio Spoof Detection Robust to Laundering Attacks?",
    "abstract": "           Voice-cloning (VC) systems have seen an exceptional increase in the realism of synthesized speech in recent years. The high quality of synthesized speech and the availability of low-cost VC services have given rise to many potential abuses of this technology. Several detection methodologies have been proposed over the years that can detect voice spoofs with reasonably good accuracy. However, these methodologies are mostly evaluated on clean audio databases, such as ASVSpoof 2019. This paper evaluates SOTA Audio Spoof Detection approaches in the presence of laundering attacks. In that regard, a new laundering attack database, called the ASVSpoof Laundering Database, is created. This database is based on the ASVSpoof 2019 (LA) eval database comprising a total of 1388.22 hours of audio recordings. Seven SOTA audio spoof detection approaches are evaluated on this laundered database. The results indicate that SOTA systems perform poorly in the presence of aggressive laundering attacks, especially reverberation and additive noise attacks. This suggests the need for robust audio spoof detection.         ",
    "url": "https://arxiv.org/abs/2408.14712",
    "authors": [
      "Hashim Ali",
      "Surya Subramani",
      "Shefali Sudhir",
      "Raksha Varahamurthy",
      "Hafiz Malik"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.20108",
    "title": "Mitigating Distribution Shift in Stock Price Data via Return-Volatility Normalization for Accurate Prediction",
    "abstract": "           How can we address distribution shifts in stock price data to improve stock price prediction accuracy? Stock price prediction has attracted attention from both academia and industry, driven by its potential to uncover complex market patterns and enhance decisionmaking. However, existing methods often fail to handle distribution shifts effectively, focusing on scaling or representation adaptation without fully addressing distributional discrepancies and shape misalignments between training and test data. We propose ReVol (Return-Volatility Normalization for Mitigating Distribution Shift in Stock Price Data), a robust method for stock price prediction that explicitly addresses the distribution shift problem. ReVol leverages three key strategies to mitigate these shifts: (1) normalizing price features to remove sample-specific characteristics, including return, volatility, and price scale, (2) employing an attention-based module to estimate these characteristics accurately, thereby reducing the influence of market anomalies, and (3) reintegrating the sample characteristics into the predictive process, restoring the traits lost during normalization. Additionally, ReVol combines geometric Brownian motion for long-term trend modeling with neural networks for short-term pattern recognition, unifying their complementary strengths. Extensive experiments on real-world datasets demonstrate that ReVol enhances the performance of the state-of-the-art backbone models in most cases, achieving an average improvement of more than 0.03 in IC and over 0.7 in SR across various settings.         ",
    "url": "https://arxiv.org/abs/2508.20108",
    "authors": [
      "Hyunwoo Lee",
      "Jihyeong Jeon",
      "Jaemin Hong",
      "U Kang"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20317",
    "title": "Universal vulnerability in strong modular networks with various degree distributions between inequality and equality",
    "abstract": "           Generally, networks are classified into two sides of inequality and equality with respect to the number of links at nodes by the types of degree distributions. One side includes many social, technological, and biological networks which consist of a few nodes with many links, and many nodes with a few links, whereas the other side consists of all nodes with an equal number of links. In comprehensive investigations between them, we have found that, as a more equal network, the tolerance of whole connectivity is stronger without fragmentation against the malfunction of nodes in a wide class of randomized networks. However, we newly find that all networks which include typical well-known network structures between them become extremely vulnerable, if a strong modular (or community) structure is added with commonalities of areas, interests, religions, purpose, and so on. These results will encourage avoiding too dense unions by connecting nodes and taking into account the balanced resource allocation between intra- and inter-links of weak communities. We must reconsider not only efficiency but also tolerance against attacks or disasters, unless no community that is really impossible.         ",
    "url": "https://arxiv.org/abs/2508.20317",
    "authors": [
      "Yukio Hayashi",
      "Taishi Ogawa"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Discrete Mathematics (cs.DM)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.20446",
    "title": "Self-consistent clustering analysis for homogenisation of heterogeneous plates",
    "abstract": "           This work introduces a reduced-order model for plate structures with periodic micro-structures by coupling self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of heterogeneous plates. A plate-specific SCA scheme is derived for the first time and features two key elements: (i) an offline-online strategy that combines Green's functions with k-means data compression, and (ii) an online self-consistent update that exploits the weak sensitivity of the reference medium. The framework handles both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, and its performance is verified on linear isotropic perforated plates and woven composites, as well as on non-linear elasto-plastic perforated plates and woven composites with damage. Across all cases the proposed model matches the accuracy of FFT-based direct numerical simulation while reducing computational cost by over an order of magnitude.         ",
    "url": "https://arxiv.org/abs/2508.20446",
    "authors": [
      "Menglei Li",
      "Haolin Li",
      "Bing Wang",
      "Bing Wang"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Applied Physics (physics.app-ph)"
    ]
  },
  {
    "id": "arXiv:2508.20469",
    "title": "Prediction of Distant Metastasis for Head and Neck Cancer Patients Using Multi-Modal Tumor and Peritumoral Feature Fusion Network",
    "abstract": "           Metastasis remains the major challenge in the clinical management of head and neck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction of metastatic risk is crucial for optimizing treatment strategies and prognosis. This study develops a deep learning-based multimodal framework to predict metastasis risk in HNSCC patients by integrating computed tomography (CT) images, radiomics, and clinical data. 1497 HNSCC patients were included. Tumor and organ masks were derived from pretreatment CT images. A 3D Swin Transformer extracted deep features from tumor regions. Meanwhile, 1562 radiomics features were obtained using PyRadiomics, followed by correlation filtering and random forest selection, leaving 36 features. Clinical variables including age, sex, smoking, and alcohol status were encoded and fused with imaging-derived features. Multimodal features were fed into a fully connected network to predict metastasis risk. Performance was evaluated using five-fold cross-validation with area under the curve (AUC), accuracy (ACC), sensitivity (SEN), and specificity (SPE). The proposed fusion model outperformed single-modality models. The 3D deep learning module alone achieved an AUC of 0.715, and when combined with radiomics and clinical features, predictive performance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analysis showed generalizability across tumor subtypes. Ablation studies indicated complementary information from different modalities. Evaluation showed the 3D Swin Transformer provided more robust representation learning than conventional networks. This multimodal fusion model demonstrated high accuracy and robustness in predicting metastasis risk in HNSCC, offering a comprehensive representation of tumor biology. The interpretable model has potential as a clinical decision-support tool for personalized treatment planning.         ",
    "url": "https://arxiv.org/abs/2508.20469",
    "authors": [
      "Zizhao Tang",
      "Changhao Liu",
      "Nuo Tong",
      "Shuiping Gou",
      "Mei Shi"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20825",
    "title": "Computer-assisted graph theory: a survey",
    "abstract": "           Computers and algorithms play an ever-increasing role in obtaining new results in graph theory. In this survey, we present a broad range of techniques used in computer-assisted graph theory, including the exhaustive generation of all pairwise non-isomorphic graphs within a given class, the use of searchable databases containing graphs and invariants as well as other established and emerging algorithmic paradigms. We cover approaches based on mixed integer linear programming, semidefinite programming, dynamic programming, SAT solving, metaheuristics and machine learning. The techniques are illustrated with numerous detailed results covering several important subareas of graph theory such as extremal graph theory, graph coloring, structural graph theory, spectral graph theory, regular graphs, topological graph theory, special sets in graphs, algebraic graph theory and chemical graph theory. We also present some smaller new results that demonstrate how readily a computer-assisted graph theory approach can be applied once the appropriate tools have been developed.         ",
    "url": "https://arxiv.org/abs/2508.20825",
    "authors": [
      "Jorik Jooken"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2508.20907",
    "title": "Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant",
    "abstract": "           Qiskit is an open-source quantum computing framework that allows users to design, simulate, and run quantum circuits on real quantum hardware. We explore post-training techniques for LLMs to assist in writing Qiskit code. We introduce quantum verification as an effective method for ensuring code quality and executability on quantum hardware. To support this, we developed a synthetic data pipeline that generates quantum problem-unit test pairs and used it to create preference data for aligning LLMs with DPO. Additionally, we trained models using GRPO, leveraging quantum-verifiable rewards provided by the quantum hardware. Our best-performing model, combining DPO and GRPO, surpasses the strongest open-source baselines on the challenging Qiskit-HumanEval-hard benchmark.         ",
    "url": "https://arxiv.org/abs/2508.20907",
    "authors": [
      "Nicolas Dupuis",
      "Adarsh Tiwari",
      "Youssef Mroueh",
      "David Kremer",
      "Ismael Faro",
      "Juan Cruz-Benito"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20946",
    "title": "Localized Clique Bounds in Bounded-Degree and Bounded-Path Graphs",
    "abstract": "           Let $\\mathcal{F}$ be a family of graphs. A graph is said to be $\\mathcal{F}$-free if it contains no member of $\\mathcal{F}$. The generalized Tur\u00e1n number $ex(n,H,\\mathcal{F})$ denotes the maximum number of copies of a graph $H$ in an $n$-vertex $\\mathcal{F}$-free graph, while the generalized edge Tur\u00e1n number $mex(m,H,\\mathcal{F})$ denotes the maximum number of copies of $H$ in an $m$-edge $\\mathcal{F}$-free graph. It is well known that if a graph has maximum degree $d$, then it is $K_{1,d+1}$-free. Wood \\cite{wood} proved that $ex(n,K_t,K_{1,d+1}) \\leq \\frac{n}{d+1}\\binom{d+1}{t}$. More recently, Chakraborty and Chen \\cite{CHAKRABORTI2024103955} established analogous bounds for graphs with bounded maximum path length: $mex(m,K_t,P_{r+1}) \\leq \\frac{m}{\\binom{r}{2}}\\binom{r}{t}$. In this paper, we improve these bounds using the localization technique, based on suitably defined local parameters. Furthermore, we characterize the extremal graphs attaining these improved bounds.         ",
    "url": "https://arxiv.org/abs/2508.20946",
    "authors": [
      "Rajat Adak",
      "L. Sunil Chandran"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2508.20983",
    "title": "Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System",
    "abstract": "           The SAFE Challenge evaluates synthetic speech detection across three tasks: unmodified audio, processed audio with compression artifacts, and laundered audio designed to evade detection. We systematically explore self-supervised learning (SSL) front-ends, training data compositions, and audio length configurations for robust deepfake detection. Our AASIST-based approach incorporates WavLM large frontend with RawBoost augmentation, trained on a multilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS systems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS. Through extensive experimentation with different SSL front-ends, three training data versions, and two audio lengths, we achieved second place in both Task 1 (unmodified audio detection) and Task 3 (laundered audio detection), demonstrating strong generalization and robustness.         ",
    "url": "https://arxiv.org/abs/2508.20983",
    "authors": [
      "Hashim Ali",
      "Surya Subramani",
      "Lekha Bollinani",
      "Nithin Sai Adupa",
      "Sali El-Loh",
      "Hafiz Malik"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.05385",
    "title": "PyGenStability: Multiscale community detection with generalized Markov Stability",
    "abstract": "           We present PyGenStability, a general-use Python software package that provides a suite of analysis and visualisation tools for unsupervised multiscale community detection in graphs. PyGenStability finds optimized partitions of a graph at different levels of resolution by maximizing the generalized Markov Stability quality function with the Louvain or Leiden algorithms. The package includes automatic detection of robust graph partitions and allows the flexibility to choose quality functions for weighted undirected, directed and signed graphs, and to include other user-defined quality functions.         ",
    "url": "https://arxiv.org/abs/2303.05385",
    "authors": [
      "Alexis Arnaudon",
      "Juni Schindler",
      "Robert L. Peach",
      "Adam Gosztolai",
      "Maxwell Hodges",
      "Michael T. Schaub",
      "Mauricio Barahona"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Mathematical Software (cs.MS)"
    ]
  },
  {
    "id": "arXiv:2304.09513",
    "title": "NetGPT: Generative Pretrained Transformer for Network Traffic",
    "abstract": "           All data on the Internet are transferred by network traffic, thus accurately modeling network traffic can help improve network services quality and protect data privacy. Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as application classification, attack detection and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks. To tackle these challenges, in this paper, we make the first attempt to provide a generative pretrained model NetGPT for both traffic understanding and generation tasks. We propose the multi-pattern network traffic modeling to construct unified text inputs and support both traffic understanding and generation tasks. We further optimize the adaptation effect of the pretrained model to diversified tasks by shuffling header fields, segmenting packets in flows, and incorporating diverse task labels with prompts. With diverse traffic datasets from encrypted software, DNS, private industrial protocols and cryptocurrency mining, expensive experiments demonstrate the effectiveness of our NetGPT in a range of traffic understanding and generation tasks on traffic datasets, and outperform state-of-the-art baselines by a wide margin.         ",
    "url": "https://arxiv.org/abs/2304.09513",
    "authors": [
      "Xuying Meng",
      "Chungang Lin",
      "Yequan Wang",
      "Yujun Zhang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2308.00186",
    "title": "Learning Complex Motion Plans using Neural ODEs with Safety and Stability Guarantees",
    "abstract": "           We propose a Dynamical System (DS) approach to learn complex, possibly periodic motion plans from kinesthetic demonstrations using Neural Ordinary Differential Equations (NODE). To ensure reactivity and robustness to disturbances, we propose a novel approach that selects a target point at each time step for the robot to follow, by combining tools from control theory and the target trajectory generated by the learned NODE. A correction term to the NODE model is computed online by solving a quadratic program that guarantees stability and safety using control Lyapunov functions and control barrier functions, respectively. Our approach outperforms baseline DS learning techniques on the LASA handwriting dataset and complex periodic trajectories. It is also validated on the Franka Emika robot arm to produce stable motions for wiping and stirring tasks that do not have a single attractor, while being robust to perturbations and safe around humans and obstacles.         ",
    "url": "https://arxiv.org/abs/2308.00186",
    "authors": [
      "Farhad Nawaz",
      "Tianyu Li",
      "Nikolai Matni",
      "Nadia Figueroa"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2402.10659",
    "title": "Network Formation and Dynamics Among Multi-LLMs",
    "abstract": "           Social networks profoundly influence how humans form opinions, exchange information, and organize collectively. As large language models (LLMs) are increasingly embedded into social and professional environments, it is critical to understand whether their interactions approximate human-like network dynamics. We develop a framework to study the network formation behaviors of multiple LLM agents and benchmark them against human decisions. Across synthetic and real-world settings, including friendship, telecommunication, and employment networks, we find that LLMs consistently reproduce fundamental micro-level principles such as preferential attachment, triadic closure, and homophily, as well as macro-level properties including community structure and small-world effects. Importantly, the relative emphasis of these principles adapts to context: for example, LLMs favor homophily in friendship networks but heterophily in organizational settings, mirroring patterns of social mobility. A controlled human-subject survey confirms strong alignment between LLMs and human participants in link-formation decisions. These results establish that LLMs can serve as powerful tools for social simulation and synthetic data generation, while also raising critical questions about bias, fairness, and the design of AI systems that participate in human networks.         ",
    "url": "https://arxiv.org/abs/2402.10659",
    "authors": [
      "Marios Papachristou",
      "Yuan Yuan"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2402.14648",
    "title": "Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off",
    "abstract": "           Adversarial training often suffers from a robustness-accuracy trade-off, where achieving high robustness comes at the cost of accuracy. One approach to mitigate this trade-off is leveraging invariance regularization, which encourages model invariance under adversarial perturbations; however, it still leads to accuracy loss. In this work, we closely analyze the challenges of using invariance regularization in adversarial training and understand how to address them. Our analysis identifies two key issues: (1) a ``gradient conflict\" between invariance and classification objectives, leading to suboptimal convergence, and (2) the mixture distribution problem arising from diverged distributions between clean and adversarial inputs. To address these issues, we propose Asymmetric Representation-regularized Adversarial Training (ARAT), which incorporates asymmetric invariance loss with stop-gradient operation and a predictor to avoid gradient conflict, and a split-BatchNorm (BN) structure to resolve the mixture distribution problem. Our detailed analysis demonstrates that each component effectively addresses the identified issues, offering novel insights into adversarial defense. ARAT shows superiority over existing methods across various settings. Finally, we discuss the implications of our findings to knowledge distillation-based defenses, providing a new perspective on their relative successes.         ",
    "url": "https://arxiv.org/abs/2402.14648",
    "authors": [
      "Futa Waseda",
      "Ching-Chun Chang",
      "Isao Echizen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.03707",
    "title": "Investigating the Robustness of Counterfactual Learning to Rank Models: A Reproducibility Study",
    "abstract": "           Counterfactual learning to rank (CLTR) has attracted extensive attention in the IR community for its ability to leverage massive logged user interaction data to train ranking models. While the CLTR models can be theoretically unbiased when the user behavior assumption is correct and the propensity estimation is accurate, their effectiveness is usually empirically evaluated via simulation-based experiments due to a lack of widely available, large-scale, real click logs. However, many previous simulation-based experiments are somewhat limited because they may have one or more of the following deficiencies: 1) using a weak production ranker to generate initial ranked lists, 2) relying on a simplified user simulation model to simulate user clicks, and 3) generating a fixed number of synthetic click logs. As a result, the robustness of CLTR models in complex and diverse situations is largely unknown and needs further investigation. To address this problem, in this paper, we aim to investigate the robustness of existing CLTR models in a reproducibility study with extensive simulation-based experiments that (1) use production rankers with different ranking performance, (2) leverage multiple user simulation models with different user behavior assumptions, and (3) generate different numbers of synthetic sessions for the training queries. We find that the IPS-DCM, DLA-PBM, and UPE models show better robustness under various simulation settings than other CLTR models. Moreover, existing CLTR models often fail to outperform naive click baselines when the production ranker is strong and the number of training sessions is limited, indicating a pressing need for new CLTR algorithms tailored to these conditions.         ",
    "url": "https://arxiv.org/abs/2404.03707",
    "authors": [
      "Zechun Niu",
      "Zhilin Zhang",
      "Jiaxin Mao",
      "Qingyao Ai",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2405.08979",
    "title": "drGT: Attention-Guided Gene Assessment of Drug Response Utilizing a Drug-Cell-Gene Heterogeneous Network",
    "abstract": "           A challenge in drug response prediction is result interpretation compared to established knowledge. drGT is a graph deep learning model that predicts sensitivity and aids in biomarker identification using attention coefficients (ACs). drGT leverages a heterogeneous graph composed of relationships drawn from drugs, genes, and cell line responses. The model is trained and evaluated using major benchmark datasets: Sanger GDSC, NCI60, and Broad CTRP, which cover a wide range of drugs and cancer cell lines. drGT demonstrates AUROC of up to 94.5% under random splitting, 84.4% for unseen drugs, and 70.6% for unseen cell lines, comparable to existing benchmark methods while also providing interpretability. Regarding interpretability, we review drug-gene co-occurrences by text-mining PubMed abstracts for high-coefficient genes mentioning particular drugs. Across 976 drugs from NCI60 with known drug-target interactions (DTIs), model predictions utilized both known DTIs (36.9%) as well as additional predictive associations, many supported by literature. In addition, we compare the drug-gene associations identified by drGT with those from an established DTI prediction model and find that 63.67% are supported by either PubMed literature or predictions from the DTI model. Further, we describe the utilization of ACs to identify affected biological processes by each drug via enrichment analyses, thereby enhancing biological interpretability. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.08979",
    "authors": [
      "Yoshitaka Inoue",
      "Hunmin Lee",
      "Tianfan Fu",
      "Augustin Luna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Molecular Networks (q-bio.MN)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2407.18760",
    "title": "Maven-Hijack: Software Supply Chain Attack Exploiting Packaging Order",
    "abstract": "           Java projects frequently rely on package managers such as Maven to manage complex webs of external dependencies. While these tools streamline development, they also introduce subtle risks to the software supply chain. In this paper, we present Maven-Hijack, a novel attack that exploits the order in which Maven packages dependencies and the way the Java Virtual Machine resolves classes at runtime. By injecting a malicious class with the same fully qualified name as a legitimate one into a dependency that is packaged earlier, an attacker can silently override core application behavior without modifying the main codebase or library names. We demonstrate the real-world feasibility of this attack by compromising the Corona-Warn-App, a widely used open-source COVID-19 contact tracing system, and gaining control over its database connection logic. We evaluate three mitigation strategies, such as sealed JARs, Java Modules, and the Maven Enforcer plugin. Our results show that, while Java Modules offer strong protection, the Maven Enforcer plugin with duplicate class detection provides the most practical and effective defense for current Java projects. These findings highlight the urgent need for improved safeguards in Java's build and dependency management processes to prevent stealthy supply chain attacks.         ",
    "url": "https://arxiv.org/abs/2407.18760",
    "authors": [
      "Frank Reyes",
      "Federico Bono",
      "Aman Sharma",
      "Benoit Baudry",
      "Martin Monperrus"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2409.00061",
    "title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language",
    "abstract": "           Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving the best accuracy of 0.8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.         ",
    "url": "https://arxiv.org/abs/2409.00061",
    "authors": [
      "Arief Purnama Muharram",
      "Ayu Purwarianti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.15912",
    "title": "Explaining word embeddings with perfect fidelity: Case study in research impact prediction",
    "abstract": "           The best-performing approaches for scholarly document quality prediction are based on embedding models. In addition to their performance when used in classifiers, embedding models can also provide predictions even for words that were not contained in the labelled training data for the classification model, which is important in the context of the ever-evolving research terminology. Although model-agnostic explanation methods, such as Local interpretable model-agnostic explanations, can be applied to explain machine learning classifiers trained on embedding models, these produce results with questionable correspondence to the model. We introduce a new feature importance method, Self-model Rated Entities (SMER), for logistic regression-based classification models trained on word embeddings. We show that SMER has theoretically perfect fidelity with the explained model, as the average of logits of SMER scores for individual words (SMER explanation) exactly corresponds to the logit of the prediction of the explained model. Quantitative and qualitative evaluation is performed through five diverse experiments conducted on 50,000 research articles (papers) from the CORD-19 corpus. Through an AOPC curve analysis, we experimentally demonstrate that SMER produces better explanations than LIME, SHAP and global tree surrogates.         ",
    "url": "https://arxiv.org/abs/2409.15912",
    "authors": [
      "Lucie Dvorackova",
      "Marcin P. Joachimiak",
      "Michal Cerny",
      "Adriana Kubecova",
      "Vilem Sklenak",
      "Tomas Kliegr"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2411.03845",
    "title": "Reconsidering the Performance of GAE in Link Prediction",
    "abstract": "           Recent advancements in graph neural networks (GNNs) for link prediction have introduced sophisticated training techniques and model architectures. However, reliance on outdated baselines may exaggerate the benefits of these new approaches. To tackle this issue, we systematically explore Graph Autoencoders (GAEs) by applying model-agnostic tricks in recent methods and tuning hyperparameters. We find that a well-tuned GAE can match the performance of recent sophisticated models while offering superior computational efficiency on widely-used link prediction benchmarks. Our approach delivers substantial performance gains on datasets where structural information dominates and feature data is limited. Specifically, our GAE achieves a state-of-the-art Hits@100 score of 78.41\\% on the ogbl-ppa dataset. Furthermore, we examine the impact of various tricks to uncover the reasons behind our success and to guide the design of future methods. Our study emphasizes the critical need to update baselines for a more accurate assessment of progress in GNNs for link prediction. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.03845",
    "authors": [
      "Weishuo Ma",
      "Yanbo Wang",
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.18948",
    "title": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis",
    "abstract": "           Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving information from the relevant knowledge database, enabling them to produce responses that are more accurate and contextually appropriate. It is worth noting that the knowledge database, being sourced from publicly available channels such as Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves injecting malicious texts into the knowledge database, ultimately leading to the generation of the attacker's target response (also called poisoned response). However, there are currently limited methods available for detecting such poisoning attacks. We aim to bridge the gap in this work. Particularly, we introduce RevPRAG, a flexible and automated detection pipeline that leverages the activations of LLMs for poisoned response detection. Our investigation uncovers distinct patterns in LLMs' activations when generating correct responses versus poisoned responses. Our results on multiple benchmark datasets and RAG architectures show our approach could achieve 98% true positive rate, while maintaining false positive rates close to 1%.         ",
    "url": "https://arxiv.org/abs/2411.18948",
    "authors": [
      "Xue Tan",
      "Hao Luan",
      "Mingyu Luo",
      "Xiaoyan Sun",
      "Ping Chen",
      "Jun Dai"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.19770",
    "title": "Noro: Noise-Robust One-shot Voice Conversion with Hidden Speaker Representation Learning",
    "abstract": "           The effectiveness of one-shot voice conversion (VC) decreases in real-world scenarios where reference speeches, which are often sourced from the internet, contain various disturbances like background noise. To address this issue, we introduce Noro, a noise-robust one-shot VC system. Noro features innovative components tailored for VC using noisy reference speeches, including a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss. Experimental results demonstrate that Noro outperforms our baseline system in both clean and noisy scenarios, highlighting its efficacy for real-world applications. Additionally, we investigate the hidden speaker representation capabilities of our baseline system by repurposing its reference encoder as a speaker encoder. The results show that it is competitive with several advanced self-supervised learning models for speaker representation under the SUPERB settings, highlighting the potential for advancing speaker representation learning through one-shot VC tasks.         ",
    "url": "https://arxiv.org/abs/2411.19770",
    "authors": [
      "Haorui He",
      "Yuchen Song",
      "Yuancheng Wang",
      "Haoyang Li",
      "Xueyao Zhang",
      "Li Wang",
      "Gongping Huang",
      "Eng Siong Chng",
      "Zhizheng Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2412.10483",
    "title": "Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models",
    "abstract": "           Automated program verification has always been an important component of building trustworthy software. While the analysis of real-world programs remains a theoretical challenge, the automation of loop invariant analysis has effectively resolved the problem. However, real-world programs that often mix complex data structures and control flows pose challenges to traditional loop invariant generation tools. To enhance the applicability of invariant generation techniques, we proposed ACInv, an Automated Complex program loop Invariant generation tool, which combines static analysis with Large Language Models (LLMs) to generate the proper loop invariants. We utilize static analysis to extract the necessary information for each loop and embed it into prompts for the LLM to generate invariants for each loop. Subsequently, we employ an LLM-based evaluator to assess the generated invariants, refining them by either strengthening, weakening, or rejecting them based on their correctness, ultimately obtaining enhanced invariants. We conducted experiments on ACInv, which showed that ACInv outperformed previous tools on data sets with data structures, and maintained similar performance to the state-of-the-art tool AutoSpec on numerical programs without data structures. For the total data set, ACInv can solve 21% more examples than AutoSpec and can generate reference data structure templates.         ",
    "url": "https://arxiv.org/abs/2412.10483",
    "authors": [
      "Ruibang Liu",
      "Guoqiang Li",
      "Minyu Chen",
      "Ling-I Wu",
      "Jingyu Ke"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2501.01239",
    "title": "High-Order Tensor Regression in Sparse Convolutional Neural Networks",
    "abstract": "           This article presents a generic approach to convolution that significantly differs from conventional methodologies in the current Machine Learning literature. The approach, in its mathematical aspects, proved to be clear and concise, particularly when high-order tensors are involved. In this context, a rational theory of regression in neural networks is developed, as a framework for a generic view of sparse convolutional neural networks, the primary focus of this study. As a direct outcome, the classic Backpropagation Algorithm is redefined to align with this rational tensor-based approach and presented in its simplest, most generic form.         ",
    "url": "https://arxiv.org/abs/2501.01239",
    "authors": [
      "Roberto Dias Algarte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.17443",
    "title": "Gradual Domain Adaptation for Graph Learning",
    "abstract": "           Existing machine learning literature lacks graph-based domain adaptation techniques capable of handling large distribution shifts, primarily due to the difficulty in simulating a coherent evolutionary path from source to target graph. To meet this challenge, we present a graph gradual domain adaptation (GGDA) framework, which constructs a compact domain sequence that minimizes information loss during adaptation. Our approach starts with an efficient generation of knowledge-preserving intermediate graphs over the Fused Gromov-Wasserstein (FGW) metric. A GGDA domain sequence is then constructed upon this bridging data pool through a novel vertex-based progression, which involves selecting \"close\" vertices and performing adaptive domain advancement to enhance inter-domain transferability. Theoretically, our framework provides implementable upper and lower bounds for the intractable inter-domain Wasserstein distance, $W_p(\\mu_t,\\mu_{t+1})$, enabling its flexible adjustment for optimal domain formation. Extensive experiments across diverse transfer scenarios demonstrate the superior performance of our GGDA framework.         ",
    "url": "https://arxiv.org/abs/2501.17443",
    "authors": [
      "Pui Ieng Lei",
      "Ximing Chen",
      "Yijun Sheng",
      "Yanyan Liu",
      "Zhiguo Gong",
      "Qiang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.05318",
    "title": "Diagonal Symmetrization of Neural Network Solvers for the Many-Electron Schr\u00f6dinger Equation",
    "abstract": "           Incorporating group symmetries into neural networks has been a cornerstone of success in many AI-for-science applications. Diagonal groups of isometries, which describe the invariance under a simultaneous movement of multiple objects, arise naturally in many-body quantum problems. Despite their importance, diagonal groups have received relatively little attention, as they lack a natural choice of invariant maps except in special cases. We study different ways of incorporating diagonal invariance in neural network ans\u00e4tze trained via variational Monte Carlo methods, and consider specifically data augmentation, group averaging and canonicalization. We show that, contrary to standard ML setups, in-training symmetrization destabilizes training and can lead to worse performance. Our theoretical and numerical results indicate that this unexpected behavior may arise from a unique computational-statistical tradeoff not found in standard ML analyses of symmetrization. Meanwhile, we demonstrate that post hoc averaging is less sensitive to such tradeoffs and emerges as a simple, flexible and effective method for improving neural network solvers.         ",
    "url": "https://arxiv.org/abs/2502.05318",
    "authors": [
      "Kevin Han Huang",
      "Ni Zhan",
      "Elif Ertekin",
      "Peter Orbanz",
      "Ryan P. Adams"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2502.14536",
    "title": "Algorithms for the preordering problem and their application to the task of jointly clustering and ordering the accounts of a social network",
    "abstract": "           The NP-hard maximum value preordering problem is both a joint relaxation and a hybrid of the clique partition problem (a clustering problem) and the partial ordering problem. Toward approximate solutions and lower bounds, we introduce a linear-time 4-approximation algorithm that constructs a maximum dicut of a subgraph and define local search heuristics. Toward upper bounds, we tighten a linear program relaxation by the class of odd closed walk inequalities that define facets, as we show, of the preorder polytope. We contribute implementations of the algorithms, apply these to the task of jointly clustering and partially ordering the accounts of published social networks, and compare the output and efficiency qualitatively and quantitatively.         ",
    "url": "https://arxiv.org/abs/2502.14536",
    "authors": [
      "Jannik Irmai",
      "Maximilian Moeller",
      "Bjoern Andres"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.16429",
    "title": "A Joint Learning Framework for Bridging Defect Prediction and Interpretation",
    "abstract": "           Over the past fifty years, numerous software defect prediction (SDP) approaches have been proposed. However, the ability to explain why predictors make certain predictions remains limited. Explainable SDP has emerged as a promising solution by using explainable artificial intelligence (XAI) methods to clarify the decision-making processes of predictors. Despite this progress, there is still significant potential to enhance the reliability of existing approaches. To address this limitation, we treat defect prediction and the corresponding interpretation as two distinct but closely related tasks and propose a joint learning framework that allows for the simultaneous training of the predictor and its interpreter. The novelty of our approach lies in two main aspects: 1. We design feedback loops that convey the decision-making logic from the predictor to the interpreter. This ensures a high level of conciseness in decision logic and feature engineering for both the predictor and the interpreter, enabling the interpreter to achieve reliable local and global interpretability. 2. We incorporate the interpretation results as a penalty term in the loss function of the joint-learning framework. This not only improves the accuracy of the predictor but also imposes a stronger constraint on the reliability of the interpreter. We validated our proposed method against several existing explainable SDPs across multiple datasets. The results demonstrate its effectiveness in both interpretation and defect prediction. The source code for the proposed method is available at: this https URL ",
    "url": "https://arxiv.org/abs/2502.16429",
    "authors": [
      "Guifang Xu",
      "Zhiling Zhu",
      "Xingcheng Guo",
      "Wei Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.18026",
    "title": "ExPath: Targeted Pathway Inference for Biological Knowledge Bases via Graph Learning and Explanation",
    "abstract": "           Retrieving targeted pathways in biological knowledge bases, particularly when incorporating wet-lab experimental data, remains a challenging task and often requires downstream analyses and specialized expertise. In this paper, we frame this challenge as a solvable graph learning and explaining task and propose a novel subgraph inference framework, ExPAth, that explicitly integrates experimental data to classify various graphs (bio-networks) in biological databases. The links (representing pathways) that contribute more to classification can be considered as targeted pathways. Our framework can seamlessly integrate biological foundation models to encode the experimental molecular data. We propose ML-oriented biological evaluations and a new metric. The experiments involving 301 bio-networks evaluations demonstrate that pathways inferred by ExPath are biologically meaningful, achieving up to 4.5x higher Fidelity+ (necessity) and 14x lower Fidelity- (sufficiency) than explainer baselines, while preserving signaling chains up to 4x longer.         ",
    "url": "https://arxiv.org/abs/2502.18026",
    "authors": [
      "Rikuto Kotoge",
      "Ziwei Yang",
      "Zheng Chen",
      "Yushun Dong",
      "Yasuko Matsubara",
      "Jimeng Sun",
      "Yasushi Sakurai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.00065",
    "title": "ADAGE: Active Defenses Against GNN Extraction",
    "abstract": "           Graph Neural Networks (GNNs) achieve high performance in various real-world applications, such as drug discovery, traffic states prediction, and recommendation systems. The fact that building powerful GNNs requires a large amount of training data, powerful computing resources, and human expertise turns the models into lucrative targets for model stealing attacks. Prior work has revealed that the threat vector of stealing attacks against GNNs is large and diverse, as an attacker can leverage various heterogeneous signals ranging from node labels to high-dimensional node embeddings to create a local copy of the target GNN at a fraction of the original training costs. This diversity in the threat vector renders the design of effective and general defenses challenging and existing defenses usually focus on one particular stealing setup. Additionally, they solely provide means to identify stolen model copies rather than preventing the attack. To close this gap, we propose the first and general Active Defense Against GNN Extraction (ADAGE). ADAGE builds on the observation that stealing a model's full functionality requires highly diverse queries to leak its behavior across the input space. Our defense monitors this query diversity and progressively perturbs outputs as the accumulated leakage grows. In contrast to prior work, ADAGE can prevent stealing across all common attack setups. Our extensive experimental evaluation using six benchmark datasets, four GNN models, and three types of adaptive attackers shows that ADAGE penalizes attackers to the degree of rendering stealing impossible, whilst preserving predictive performance on downstream tasks. ADAGE, thereby, contributes towards securely sharing valuable GNNs in the future.         ",
    "url": "https://arxiv.org/abs/2503.00065",
    "authors": [
      "Jing Xu",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.08189",
    "title": "STCKGE:Continual Knowledge Graph Embedding Based on Spatial Transformation",
    "abstract": "           Current Continual Knowledge Graph Embedding (CKGE) methods primarily rely on translation-based embedding approaches, leveraging previously acquired knowledge to initialize new facts. While these methods often integrate fine-tuning or continual learning strategies to enhance efficiency, they compromise prediction accuracy and lack support for complex relational structures (e.g., multi-hop relations). To address these limitations, we propose STCKGE, a novel CKGE framework based on spatial transformation. In this framework, entity positions are jointly determined by base position vectors and offset vectors, enabling the model to represent complex relations more effectively while supporting efficient embedding updates for both new and existing knowledge through simple spatial operations, without relying on traditional continual learning techniques. Furthermore, we introduce a bidirectional collaborative update strategy and a balanced embedding method to guide parameter updates, effectively minimizing training costs while improving model accuracy. We comprehensively evaluate our model on seven public datasets and a newly constructed dataset (MULTI) focusing on multi-hop relationships. Experimental results confirm STCKGE's strong performance in multi-hop relationship learning and prediction accuracy, with an average MRR improvement of 5.4\\%.         ",
    "url": "https://arxiv.org/abs/2503.08189",
    "authors": [
      "Xinyan Wang",
      "Jinshuo Liu",
      "Kaijian Xie",
      "Meng Wang",
      "Cheng Bi",
      "Juan Deng",
      "Jeff Pan"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2503.16441",
    "title": "Safe and Efficient Social Navigation through Explainable Safety Regions Based on Topological Features",
    "abstract": "           The recent adoption of artificial intelligence in robotics has driven the development of algorithms that enable autonomous systems to adapt to complex social environments. In particular, safe and efficient social navigation is a key challenge, requiring AI not only to avoid collisions and deadlocks but also to interact intuitively and predictably with its surroundings. Methods based on probabilistic models and the generation of conformal safety regions have shown promising results in defining safety regions with a controlled margin of error, primarily relying on classification approaches and explicit rules to describe collision-free navigation conditions. This work extends the existing perspective by investigating how topological features can contribute to the creation of explainable safety regions in social navigation scenarios, enabling the classification and characterization of different simulation behaviors. Rather than relying on behaviors parameters to generate safety regions, we leverage topological features through topological data analysis. We first utilize global rule-based classification to provide interpretable characterizations of different simulation behaviors, distinguishing between safe and unsafe scenarios based on topological properties. Next, we define safety regions, $S_\\varepsilon$, representing zones in the topological feature space where collisions are avoided with a maximum classification error of $\\varepsilon$. These regions are constructed using adjustable SVM classifiers and order statistics, ensuring a robust and scalable decision boundary. Our approach initially separates simulations with and without collisions, outperforming methods that not incorporate topological features. We further refine safety regions to ensure deadlock-free simulations and integrate both aspects to define a compliant simulation space that guarantees safe and efficient navigation.         ",
    "url": "https://arxiv.org/abs/2503.16441",
    "authors": [
      "Victor Toscano-Duran",
      "Sara Narteni",
      "Alberto Carlevaro",
      "J\u00e9r\u00f4me Guzzi Rocio Gonzalez-Diaz",
      "Maurizio Mongelli"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "General Topology (math.GN)"
    ]
  },
  {
    "id": "arXiv:2503.18340",
    "title": "Optimized Contact Plan Design for Reflector and Phased Array Terminals in Cislunar Space Networks",
    "abstract": "           Cislunar space is emerging as a critical domain for human exploration, requiring robust infrastructure to support spatial users-spacecraft with navigation and communication demands. Deploying satellites at Earth-Moon three-body orbits offers an effective solution to construct cislunar space infrastructure (CLSI). However, scheduling satellite links to serve users necessitates an appropriate contact plan design (CPD) scheme. Existing CPD schemes focus solely on inter-satellite link scheduling, overlooking their role in providing services to users. This paper introduces a CPD scheme that considers two classes of satellite transponders: Reflector Links (RL) for high-volume data transfers and Phased Array Links (PL) for fast switching and navigation services. Our approach supports both satellites and spatial users in cislunar space. Simulations validate the scheme, demonstrating effective support for user while meeting satellite ranging and communication requirements. These findings provide essential insights for developing future Cislunar Space Infrastructures.         ",
    "url": "https://arxiv.org/abs/2503.18340",
    "authors": [
      "Huan Yan",
      "Juan A. Fraire",
      "Ziqi Yang",
      "Kanglian Zhao",
      "Wenfeng Li",
      "Jinjun Zheng",
      "Chengbin Kang",
      "Huichao Zhou",
      "Xinuo Chang",
      "Lu Wang",
      "Linshan Xue"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.13111",
    "title": "Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification",
    "abstract": "           Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2504.13111",
    "authors": [
      "Kumar Manas",
      "Christian Schlauch",
      "Adrian Paschke",
      "Christian Wirth",
      "Nadja Klein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.20376",
    "title": "When Memory Becomes a Vulnerability: Towards Multi-turn Jailbreak Attacks against Text-to-Image Generation Systems",
    "abstract": "           Modern text-to-image (T2I) generation systems (e.g., DALL$\\cdot$E 3) exploit the memory mechanism, which captures key information in multi-turn interactions for faithful generation. Despite its practicality, the security analyses of this mechanism have fallen far behind. In this paper, we reveal that it can exacerbate the risk of jailbreak attacks. Previous attacks fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or lead to the generation of non-unsafe images due to under- or over-detoxification. In contrast, we propose embedding the malice at the inception of the chat session in memory, addressing the above limitations. Specifically, we propose Inception, the first multi-turn jailbreak attack against real-world text-to-image generation systems that explicitly exploits their memory mechanisms. Inception is composed of two key modules: segmentation and recursion. We introduce Segmentation, a semantic-preserving method that generates multi-round prompts. By leveraging NLP analysis techniques, we design policies to decompose a prompt, together with its malicious intent, according to sentence structure, thereby evading safety filters. Recursion further addresses the challenge posed by unsafe sub-prompts that cannot be separated through simple segmentation. It firstly expands the sub-prompt, then invokes segmentation recursively. To facilitate multi-turn adversarial prompts crafting, we build VisionFlow, an emulation T2I system that integrates two-stage safety filters and industrial-grade memory mechanisms. The experiment results show that Inception successfully allures unsafe image generation, surpassing the SOTA by a 20.0\\% margin in attack success rate. We also conduct experiments on the real-world commercial T2I generation platforms, further validating the threats of Inception in practice.         ",
    "url": "https://arxiv.org/abs/2504.20376",
    "authors": [
      "Shiqian Zhao",
      "Jiayang Liu",
      "Yiming Li",
      "Runyi Hu",
      "Xiaojun Jia",
      "Wenshu Fan",
      "Xinfeng Li",
      "Jie Zhang",
      "Wei Dong",
      "Tianwei Zhang",
      "Luu Anh Tuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.06597",
    "title": "Phase Transitions between Accuracy Regimes in L2 regularized Deep Neural Networks",
    "abstract": "           Increasing the L2 regularization of Deep Neural Networks (DNNs) causes a first-order phase transition into the under-parametrized phase -- the so-called onset-of learning. We explain this transition via the scalar (Ricci) curvature of the error landscape. We predict new transition points as the data complexity is increased and, in accordance with the theory of phase transitions, the existence of hysteresis effects. We confirm both predictions numerically. Our results provide a natural explanation of the recently discovered phenomenon of '\\emph{grokking}' as DNN models getting stuck in a local minimum of the error surface, corresponding to a lower accuracy phase. Our work paves the way for new probing methods of the intrinsic structure of DNNs in and beyond the L2 context.         ",
    "url": "https://arxiv.org/abs/2505.06597",
    "authors": [
      "Ibrahim Talha Ersoy",
      "Karoline Wiesner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2505.07817",
    "title": "Pixel Motion as Universal Representation for Robot Control",
    "abstract": "           We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a weakly-supervised manner, enabling diffusion model training on any video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout this https URL ",
    "url": "https://arxiv.org/abs/2505.07817",
    "authors": [
      "Kanchana Ranasinghe",
      "Xiang Li",
      "E-Ro Nguyen",
      "Cristina Mata",
      "Jongwoo Park",
      "Michael S Ryoo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.17553",
    "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning",
    "abstract": "           In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves specializing functionalities into different experts and sparsely activating them appropriately, has been widely adopted as a promising approach to trade-off between model capacity and computation overhead. However, current MoE variants fall short on heterogeneous datasets, ignoring the fact that experts may learn similar knowledge, resulting in the underutilization of MoE's capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE), a novel method to promote modularization and specialization in MoE, where the experts are trained along with a contrastive objective by sampling from activated and inactivated experts in top-k routing. We demonstrate that such a contrastive objective recovers the mutual-information gap between inputs and the two types of experts. Experiments on several benchmarks and in multi-task settings demonstrate that CoMoE can consistently enhance MoE's capacity and promote modularization among the experts.         ",
    "url": "https://arxiv.org/abs/2505.17553",
    "authors": [
      "Jinyuan Feng",
      "Chaopeng Wei",
      "Tenghai Qiu",
      "Tianyi Hu",
      "Zhiqiang Pu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20130",
    "title": "Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach",
    "abstract": "           This paper focuses on the design of spatial experiments to optimize the amount of information derived from the experimental data and enhance the accuracy of the resulting causal effect estimator. We propose a surrogate function for the mean squared error (MSE) of the estimator, which facilitates the use of classical graph cut algorithms to learn the optimal design. Our proposal offers three key advances: (1) it accommodates moderate to large spatial interference effects; (2) it adapts to different spatial covariance functions; (3) it is computationally efficient. Theoretical results and numerical experiments based on synthetic environments and a dispatch simulator that models a city-scale ridesharing market, further validate the effectiveness of our design. A python implementation of our method is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20130",
    "authors": [
      "Jin Zhu",
      "Jingyi Li",
      "Hongyi Zhou",
      "Yinan Lin",
      "Zhenhua Lin",
      "Chengchun Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.20378",
    "title": "Hyperbolic embedding of multilayer networks",
    "abstract": "           Multilayer networks offer a powerful framework for modeling complex systems across diverse domains, effectively capturing multiple types of connections and interdependent subsystems commonly found in real world scenarios. To analyze these networks, embedding techniques that project nodes into a lower-dimensional geometric space are essential. This paper introduces a novel hyperbolic embedding framework that advances the state of the art in multilayer network analysis. Our method, which supports heterogeneous node sets across networks and inter-layer connections, generates layer-specific hyperbolic embeddings, enabling detailed intra-layer analysis and inter-layer comparisons, while simultaneously preserving the global multilayer structure within hyperbolic space, a capability that sets it apart from existing approaches, which typically rely on independent embedding of layers. Through experiments on synthetic multilayer stochastic block models, we demonstrate that our approach effectively preserves community structure, even when layers consist of different node sets. When applied to real brain networks, the method successfully clusters disease-related brain regions from different patients, outperforming layer-independent approaches and highlighting its relevance for comparative analysis. Overall, this work provides a robust tool for multilayer network analysis, enhancing interpretability and offering new insights into the structure and function of complex systems.         ",
    "url": "https://arxiv.org/abs/2505.20378",
    "authors": [
      "Martin Guillemaud",
      "Vera Dinkelacker",
      "Mario Chavez"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2506.06294",
    "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning",
    "abstract": "           Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \\textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.         ",
    "url": "https://arxiv.org/abs/2506.06294",
    "authors": [
      "Yunqing Liu",
      "Wenqi Fan",
      "Xiaoyong Wei",
      "Qing Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2507.03167",
    "title": "Adversarial Manipulation of Reasoning Models using Internal Representations",
    "abstract": "           Reasoning models generate chain-of-thought (CoT) tokens before their final output, but how this affects their vulnerability to jailbreak attacks remains unclear. While traditional language models make refusal decisions at the prompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B makes these decisions within its CoT generation. We identify a linear direction in activation space during CoT token generation that predicts whether the model will refuse or comply -- termed the \"caution\" direction because it corresponds to cautious reasoning patterns in the generated text. Ablating this direction from model activations increases harmful compliance, effectively jailbreaking the model. We additionally show that intervening only on CoT token activations suffices to control final outputs, and that incorporating this direction into prompt-based attacks improves success rates. Our findings suggest that the chain-of-thought itself is a promising new target for adversarial manipulation in reasoning models. Code available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.03167",
    "authors": [
      "Kureha Yamaguchi",
      "Benjamin Etheridge",
      "Andy Arditi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.04671",
    "title": "DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and Continuous Adaptation",
    "abstract": "           Neural Architecture Search (NAS) has emerged as a powerful approach for automating neural network design. However, existing NAS methods face critical limitations in real-world deployments: architectures lack adaptability across scenarios, each deployment context requires costly separate searches, and performance consistency across diverse platforms remains challenging. We propose DANCE (Dynamic Architectures with Neural Continuous Evolution), which reformulates architecture search as a continuous evolution problem through learning distributions over architectural components. DANCE introduces three key innovations: a continuous architecture distribution enabling smooth adaptation, a unified architecture space with learned selection gates for efficient sampling, and a multi-stage training strategy for effective deployment optimization. Extensive experiments across five datasets demonstrate DANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS approaches in terms of accuracy while significantly reducing search costs. Under varying computational constraints, DANCE maintains robust performance while smoothly adapting architectures to different hardware requirements. The code and appendix can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.04671",
    "authors": [
      "Maolin Wang",
      "Tianshuo Wei",
      "Sheng Zhang",
      "Ruocheng Guo",
      "Wanyu Wang",
      "Shanshan Ye",
      "Lixin Zou",
      "Xuetao Wei",
      "Xiangyu Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.10943",
    "title": "Robust ID-Specific Face Restoration via Alignment Learning",
    "abstract": "           The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.         ",
    "url": "https://arxiv.org/abs/2507.10943",
    "authors": [
      "Yushun Fang",
      "Lu Liu",
      "Xiang Gao",
      "Qiang Hu",
      "Ning Cao",
      "Jianghe Cui",
      "Gang Chen",
      "Xiaoyun Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.00654",
    "title": "LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources",
    "abstract": "           In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.         ",
    "url": "https://arxiv.org/abs/2508.00654",
    "authors": [
      "Rodrigo Escobar D\u00edaz Guerrero",
      "Jamile Mohammad Jafari",
      "Tobias Meyer-Zedler",
      "Michael Schmitt",
      "Juergen Popp",
      "Thomas Bocklitz"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.01970",
    "title": "Improving Hospital Risk Prediction with Knowledge-Augmented Multimodal EHR Modeling",
    "abstract": "           Accurate prediction of clinical outcomes using Electronic Health Records (EHRs) is critical for early intervention, efficient resource allocation, and improved patient care. EHRs contain multimodal data, including both structured data and unstructured clinical notes that provide rich, context-specific information. In this work, we introduce a unified framework that seamlessly integrates these diverse modalities, leveraging all relevant available information through a two-stage architecture for clinical risk prediction. In the first stage, a fine-tuned Large Language Model (LLM) extracts crucial, task-relevant information from clinical notes, which is enhanced by graph-based retrieval of external domain knowledge from sources such as a medical corpus like PubMed, grounding the LLM's understanding. The second stage combines both unstructured representations and features derived from the structured data to generate the final predictions. This approach supports a wide range of clinical tasks. Here, we demonstrate its effectiveness on 30-day readmission and in-hospital mortality prediction. Experimental results show that our framework achieves strong performance, with AUC scores of $0.84$ and $0.92$, respectively, despite these tasks involving severely imbalanced datasets, with positive rates ranging from approximately $4\\%$ to $13\\%$. Moreover, it outperforms all existing baselines and clinical practices, including established risk scoring systems. To the best of our knowledge, this is one of the first frameworks for healthcare prediction which enhances the power of an LLM-based graph-guided knowledge retrieval method by combining it with structured data for improved clinical outcome prediction.         ",
    "url": "https://arxiv.org/abs/2508.01970",
    "authors": [
      "Rituparna Datta",
      "Jiaming Cui",
      "Zihan Guan",
      "Vishal G. Reddy",
      "Joshua C. Eby",
      "Gregory Madden",
      "Rupesh Silwal",
      "Anil Vullikanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.02997",
    "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors",
    "abstract": "           The widespread use of Large Language Models (LLMs) in many applications marks a significant advance in research and practice. However, their complexity and hard-to-understand nature make them vulnerable to attacks, especially jailbreaks designed to produce harmful responses. To counter these threats, developing strong detection methods is essential for the safe and reliable use of LLMs. This paper studies this detection problem using the Contextual Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce environments. We propose a novel method leveraging the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for the effective identification of adversarial and jailbreak prompts. Our evaluations show that this approach achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. This result highlights the strength of our learned patterns, especially when labeled data is scarce. Our method is also significantly faster, speedup ranging from 2.3 to 128.4 times compared to the baseline models.         ",
    "url": "https://arxiv.org/abs/2508.02997",
    "authors": [
      "Sri Durga Sai Sowmya Kadali",
      "Evangelos E. Papalexakis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.03428",
    "title": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments",
    "abstract": "           In this paper, we propose a hybrid MPC local planner that uses a learning-based approximation of a time-varying safe set, derived from local observations and applied as the MPC terminal constraint. This set can be represented as a zero-superlevel set of the value function computed via Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time. We exploit the property that the HJ value function can be expressed as a difference of the corresponding signed distance function (SDF) and a non-negative residual function. The residual component is modeled as a neural network with non-negative output and subtracted from the computed SDF, resulting in a real-time value function estimate that is at least as safe as the SDF by design. Additionally, we parametrize the neural residual by a hypernetwork to improve real-time performance and generalization properties. The proposed method is compared with three state-of-the-art methods in simulations and hardware experiments, achieving up to 30\\% higher success rates compared to the best baseline while requiring a similar computational effort and producing high-quality (low travel-time) solutions.         ",
    "url": "https://arxiv.org/abs/2508.03428",
    "authors": [
      "Bojan Deraji\u0107",
      "Mohamed-Khalil Bouzidi",
      "Sebastian Bernhard",
      "Wolfgang H\u00f6nig"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.03661",
    "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search",
    "abstract": "           Gravitational-wave signal detection with unknown source parameters buried in dynamic detector noise remains a formidable computational challenge. Existing approaches face core limitations from restrictive assumptions: traditional methods rely on predefined theoretical priors, while neural networks introduce hidden biases and lack interpretability. We propose Evolutionary Monte Carlo Tree Search (Evo-MCTS), the first integration of large language model (LLM) guidance with domain-aware physical constraints for automated gravitational wave detection. This framework systematically explores algorithmic solution spaces through tree-structured search enhanced by evolutionary optimization, combining MCTS for strategic exploration with evolutionary algorithms for solution refinement. The LLM component provides domain-aware heuristics while maintaining interpretability through explicit algorithmic pathway generation. Experimental validation demonstrates substantial performance improvements, achieving a 20.2% improvement over state-of-the-art gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset and a remarkable 59.1% improvement over other LLM-based algorithm optimization frameworks. Beyond performance improvements, our framework establishes a transferable methodology for automated algorithmic discovery across computational science domains.         ",
    "url": "https://arxiv.org/abs/2508.03661",
    "authors": [
      "He Wang",
      "Liang Zeng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "High Energy Astrophysical Phenomena (astro-ph.HE)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "General Relativity and Quantum Cosmology (gr-qc)"
    ]
  },
  {
    "id": "arXiv:2508.06312",
    "title": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading",
    "abstract": "           Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.         ",
    "url": "https://arxiv.org/abs/2508.06312",
    "authors": [
      "Lang Cao",
      "Zekun Xi",
      "Long Liao",
      "Ziwei Yang",
      "Zheng Cao"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2508.11716",
    "title": "Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Algorithms (FakeIDet2)",
    "abstract": "           Remote user verification in Internet-based applications is becoming increasingly important nowadays. A popular scenario for it consists of submitting a picture of the user's Identity Document (ID) to a service platform, authenticating its veracity, and then granting access to the requested digital service. An ID is well-suited to verify the identity of an individual, since it is government issued, unique, and nontransferable. However, with recent advances in Artificial Intelligence (AI), attackers can surpass security measures in IDs and create very realistic physical and synthetic fake IDs. Researchers are now trying to develop methods to detect an ever-growing number of these AI-based fakes that are almost indistinguishable from authentic (bona fide) IDs. In this counterattack effort, researchers are faced with an important challenge: the difficulty in using real data to train fake ID detectors. This real data scarcity for research and development is originated by the sensitive nature of these documents, which are usually kept private by the ID owners (the users) and the ID Holders (e.g., government, police, bank, etc.). The main contributions of our study are: 1) We propose and discuss a patch-based methodology to preserve privacy in fake ID detection research. 2) We provide a new public database, FakeIDet2-db, comprising over 900K real/fake ID patches extracted from 2,000 ID images, acquired using different smartphone sensors, illumination and height conditions, etc. In addition, three physical attacks are considered: print, screen, and composite. 3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We release a standard reproducible benchmark that considers physical and synthetic attacks from popular databases in the literature.         ",
    "url": "https://arxiv.org/abs/2508.11716",
    "authors": [
      "Javier Mu\u00f1oz-Haro",
      "Ruben Tolosana",
      "Julian Fierrez",
      "Ruben Vera-Rodriguez",
      "Aythami Morales"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2508.14926",
    "title": "Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving",
    "abstract": "           Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding robust ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that explicitly integrates moral considerations with standard driving objectives. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on rich, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing ethical risk and maintaining driving performance. To our knowledge, this is the first study of ethical decision-making for autonomous vehicles via Safe RL evaluated on real-world, human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments.         ",
    "url": "https://arxiv.org/abs/2508.14926",
    "authors": [
      "Dianzhao Li",
      "Ostap Okhrin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.17387",
    "title": "Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning",
    "abstract": "           Generalizing to unseen graph tasks without task-pecific supervision remains challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces, while Large Language Models (LLMs) lack structural inductive biases. Recent advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via explicit, long chain-of-thought reasoning. Inspired by this, we propose a GNN-free approach that reformulates graph tasks--node classification, link prediction, and graph classification--as textual reasoning problems solved by LRMs. We introduce the first datasets with detailed reasoning traces for these tasks and develop Graph-R1, a reinforcement learning framework that leverages task-specific rethink templates to guide reasoning over linearized graphs. Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in zero-shot settings, producing interpretable and effective predictions. Our work highlights the promise of explicit reasoning for graph learning and provides new resources for future research.         ",
    "url": "https://arxiv.org/abs/2508.17387",
    "authors": [
      "Yicong Wu",
      "Guangyue Lu",
      "Yuan Zuo",
      "Huarong Zhang",
      "Junjie Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.17630",
    "title": "Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning",
    "abstract": "           We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neural network that integrates variational quantum circuits into the attention mechanism. At its core, QGAT employs strongly entangling quantum circuits with amplitude-encoded node features to enable expressive nonlinear interactions. Distinct from classical multi-head attention that separately computes each head, QGAT leverages a single quantum circuit to simultaneously generate multiple attention coefficients. This quantum parallelism facilitates parameter sharing across heads, substantially reducing computational overhead and model complexity. Classical projection weights and quantum circuit parameters are optimized jointly in an end-to-end manner, ensuring flexible adaptation to learning tasks. Empirical results demonstrate QGAT's effectiveness in capturing complex structural dependencies and improved generalization in inductive scenarios, highlighting its potential for scalable quantum-enhanced learning across domains such as chemistry, biology, and network analysis. Furthermore, experiments confirm that quantum embedding enhances robustness against feature and structural noise, suggesting advantages in handling real-world noisy data. The modularity of QGAT also ensures straightforward integration into existing architectures, allowing it to easily augment classical attention-based models.         ",
    "url": "https://arxiv.org/abs/2508.17630",
    "authors": [
      "An Ning",
      "Tai Yue Li",
      "Nan Yow Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.17645",
    "title": "Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph",
    "abstract": "           The emergence of 3D artificial intelligence-generated content (3D-AIGC) has enabled rapid synthesis of intricate geometries. However, a fundamental disconnect persists between AI-generated content and human-centric design paradigms, rooted in representational incompatibilities: conventional AI frameworks predominantly manipulate meshes or neural representations (\\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within parametric modeling tools. This disconnection diminishes the practical value of AI for 3D industry, undermining the efficiency of human-AI collaboration. To resolve this disparity, we focus on generating design operation sequences, which are structured modeling histories that comprehensively capture the step-by-step construction process of 3D assets and align with designers' typical workflows in modern 3D software. We first reformulate fundamental modeling operations (\\emph{e.g.}, \\emph{Extrude}, \\emph{Boolean}) into differentiable units, enabling joint optimization of continuous (\\emph{e.g.}, \\emph{Extrude} height) and discrete (\\emph{e.g.}, \\emph{Boolean} type) parameters via gradient-based learning. Based on these differentiable operations, a hierarchical graph with gating mechanism is constructed and optimized end-to-end by minimizing Chamfer Distance to target geometries. Multi-stage sequence length constraint and domain rule penalties enable unsupervised learning of compact design sequences without ground-truth sequence supervision. Extensive validation demonstrates that the generated operation sequences achieve high geometric fidelity, smooth mesh wiring, rational step composition and flexible editing capacity, with full compatibility within design industry.         ",
    "url": "https://arxiv.org/abs/2508.17645",
    "authors": [
      "Xiaoyang Huang",
      "Bingbing Ni",
      "Wenjun Zhang"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2508.18321",
    "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions",
    "abstract": "           Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.18321",
    "authors": [
      "Maojia Song",
      "Tej Deep Pala",
      "Weisheng Jin",
      "Amir Zadeh",
      "Chuan Li",
      "Dorien Herremans",
      "Soujanya Poria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19071",
    "title": "Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have emerged as the leading paradigm for learning over graph-structured data. However, their performance is limited by issues inherent to graph topology, most notably oversquashing and oversmoothing. Recent advances in graph rewiring aim to mitigate these limitations by modifying the graph topology to promote more effective information propagation. In this work, we introduce TRIGON, a novel framework that constructs enriched, non-planar triangulations by learning to select relevant triangles from multiple graph views. By jointly optimizing triangle selection and downstream classification performance, our method produces a rewired graph with markedly improved structural properties such as reduced diameter, increased spectral gap, and lower effective resistance compared to existing rewiring methods. Empirical results demonstrate that TRIGON outperforms state-of-the-art approaches on node classification tasks across a range of homophilic and heterophilic benchmarks.         ",
    "url": "https://arxiv.org/abs/2508.19071",
    "authors": [
      "Hugo Attali",
      "Thomas Papastergiou",
      "Nathalie Pernelle",
      "Fragkiskos D. Malliaros"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19356",
    "title": "Graph Data Modeling: Molecules, Proteins, & Chemical Processes",
    "abstract": "           Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine. This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes, introduces graphs as mathematical objects in chemistry and shows how learning algorithms (particularly graph neural networks) can operate on them. We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling. Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery.         ",
    "url": "https://arxiv.org/abs/2508.19356",
    "authors": [
      "Jos\u00e9 Manuel Barraza-Chavez",
      "Rana A. Barghout",
      "Ricardo Almada-Monter",
      "Adrian Jinich",
      "Radhakrishnan Mahadevan",
      "Benjamin Sanchez-Lengeling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2508.19430",
    "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks (extended version)",
    "abstract": "           Formal verification is crucial for ensuring the robustness of security protocols against adversarial attacks. The Needham-Schroeder protocol, a foundational authentication mechanism, has been extensively studied, including its integration with Physical Layer Security (PLS) techniques such as watermarking and jamming. Recent research has used ProVerif to verify these mechanisms in terms of secrecy. However, the ProVerif-based approach limits the ability to improve understanding of security beyond verification results. To overcome these limitations, we re-model the same protocol using an Isabelle formalism that generates sound animation, enabling interactive and automated formal verification of security protocols. Our modelling and verification framework is generic and highly configurable, supporting both cryptography and PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy and authenticity in four different eavesdropper locations under both passive and active attacks) using our new web interface. Our findings not only successfully reproduce and reinforce previous results on secrecy but also reveal an uncommon but expected outcome: authenticity is preserved across all examined scenarios, even in cases where secrecy is compromised. We have proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and jamming, and our analysis shows that it is secure for deriving a session key with required authentication. These highlight the advantages of our novel approach, demonstrating its robustness in formally verifying security properties beyond conventional methods.         ",
    "url": "https://arxiv.org/abs/2508.19430",
    "authors": [
      "Kangfeng Ye",
      "Roberto Metere",
      "Jim Woodcock",
      "Poonam Yadav"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2508.19507",
    "title": "A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation",
    "abstract": "           In e-commerce, where users face a vast array of possible item choices, recommender systems are vital for helping them discover suitable items they might otherwise overlook. While many recommender systems primarily rely on a user's purchase history, recent multi-behavior recommender systems incorporate various auxiliary user behaviors, such as item clicks and cart additions, to enhance recommendations. Despite their overall performance gains, their effectiveness varies considerably between visited items (i.e., those a user has interacted with through auxiliary behaviors) and unvisited items (i.e., those with which the user has had no such interactions). Specifically, our analysis reveals that (1) existing multi-behavior recommender systems exhibit a significant gap in recommendation quality between the two item types (visited and unvisited items) and (2) achieving strong performance on both types with a single model architecture remains challenging. To tackle these issues, we propose a novel multi-behavior recommender system, MEMBER. It employs a mixture-of-experts framework, with experts designed to recommend the two item types, respectively. Each expert is trained using a self-supervised method specialized for its design goal. In our comprehensive experiments, we show the effectiveness of MEMBER across both item types, achieving up to 65.46% performance gain over the best competitor in terms of Hit Ratio@20.         ",
    "url": "https://arxiv.org/abs/2508.19507",
    "authors": [
      "Kyungho Kim",
      "Sunwoo Kim",
      "Geon Lee",
      "Kijung Shin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19542",
    "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning",
    "abstract": "           While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs. The data and evaluation code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19542",
    "authors": [
      "Nannan Zhu",
      "Yonghao Dong",
      "Teng Wang",
      "Xueqian Li",
      "Shengjun Deng",
      "Yijia Wang",
      "Zheng Hong",
      "Tiantian Geng",
      "Guo Niu",
      "Hanyan Huang",
      "Xiongfei Yao",
      "Shuaiwei Jiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19884",
    "title": "Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have shown remarkable performance in structured data modeling tasks such as node classification. However, mainstream approaches generally rely on a large number of trainable parameters and fixed aggregation rules, making it difficult to adapt to graph data with strong structural heterogeneity and complex feature distributions. This often leads to over-smoothing of node representations and semantic degradation. To address these issues, this paper proposes a parameter-free graph neural network framework based on structural diversity, namely SDGNN (Structural-Diversity Graph Neural Network). The framework is inspired by structural diversity theory and designs a unified structural-diversity message passing mechanism that simultaneously captures the heterogeneity of neighborhood structures and the stability of feature semantics, without introducing additional trainable parameters. Unlike traditional parameterized methods, SDGNN does not rely on complex model training, but instead leverages complementary modeling from both structure-driven and feature-driven perspectives, thereby effectively improving adaptability across datasets and scenarios. Experimental results show that on eight public benchmark datasets and an interdisciplinary PubMed citation network, SDGNN consistently outperforms mainstream GNNs under challenging conditions such as low supervision, class imbalance, and cross-domain transfer. This work provides a new theoretical perspective and general approach for the design of parameter-free graph neural networks, and further validates the importance of structural diversity as a core signal in graph representation learning. To facilitate reproducibility and further research, the full implementation of SDGNN has been released at: this https URL ",
    "url": "https://arxiv.org/abs/2508.19884",
    "authors": [
      "Mingyue Kong",
      "Yinglong Zhang",
      "Chengda Xu",
      "Xuewen Xia",
      "Xing Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20038",
    "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks",
    "abstract": "           Despite advances in improving large language model (LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.         ",
    "url": "https://arxiv.org/abs/2508.20038",
    "authors": [
      "Sheng Liu",
      "Qiang Sheng",
      "Danding Wang",
      "Yang Li",
      "Guang Yang",
      "Juan Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2405.20825",
    "title": "Prediction of Local Failure after Stereotactic Radiotherapy in Melanoma Brain Metastases Using Ensemble Learning on Clinical, Dosimetric, and Radiomic Data",
    "abstract": "           Background: This study aimed to predict lesion-specific outcomes after stereotactic radiotherapy (SRT) in patients with brain metastases from malignant melanoma (MBM), using clinical, dosimetric, and pretherapeutic MRI data. Methods: In this multicenter retrospective study, 517 MBM from 130 patients treated with single-fraction or hypofractionated SRT at three centers were analyzed. From contrast-enhanced T1-weighted MRI, 1576 radiomic features (RF) were extracted per lesion - 788 from the gross tumor volume (GTV) and 788 from a 3 mm peritumoral margin. Clinical, dosimetric and RF data from one center were used for feature selection and model development via nested cross-validation employing an ensemble learning approach; external validation used data from the other two centers. Results: Local failure occurred in 72/517 lesions (13.9%). Predictive models based on clinical data, RF, or a combination of both achieved c-indices of 0.60 +/- 0.15, 0.65 +/- 0.11, and 0.65 +/- 0.12, respectively. RF-based models outperformed the clinical models; dosimetric data alone were not predictive. Most predictive RF originated from the peritumoral margin (92%) versus GTV (76%). On the first external dataset, all models performed similarly (c-index: 0.60-0.63), but generalization was poor on the second (c-index < 0.50), likely due to differences in patient characteristics and imaging protocols. Conclusions: Pretherapeutic MRI features, particularly from the peritumoral region, show promise for predicting lesion-specific outcomes in MBM after SRT. Their consistent contribution suggests biologically relevant information that may support individualized treatment planning. Combined with clinical data, these markers offer prognostic insight, though generalizability remains limited by data heterogeneity.         ",
    "url": "https://arxiv.org/abs/2405.20825",
    "authors": [
      "Nanna E. Hartong",
      "Ilias Sachpazidis",
      "Oliver Blanck",
      "Lucas Etzel",
      "Jan C. Peeken",
      "Stephanie E. Combs",
      "Horst Urbach",
      "Maxim Zaitsev",
      "Dimos Baltas",
      "Ilinca Popp",
      "Anca-Ligia Grosu",
      "Tobias Fechter"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.04844",
    "title": "Cutting is All You Need: Execution of Large-Scale Quantum Neural Networks on Limited-Qubit Devices",
    "abstract": "           The rapid advancement in Quantum Computing, particularly through Noisy-Intermediate Scale Quantum (NISQ) devices, has spurred significant interest in Quantum Machine Learning (QML) applications. Despite their potential, fully-quantum algorithms remain impractical due to the limitations of current NISQ devices. Hybrid quantum-classical neural networks (HQNNs) have emerged as a viable alternative, leveraging both quantum and classical computations to enhance machine learning capabilities. However, the constrained resources of NISQ devices, particularly the limited number of qubits, pose significant challenges for executing large-scale quantum circuits. This work addresses these current challenges by proposing a novel and practical methodology for quantum circuit cutting of HQNNs, allowing large quantum circuits to be executed on limited-qubit NISQ devices. Our approach not only preserves the accuracy of the original circuits but also supports the training of quantum parameters across all subcircuits, which is crucial for the learning process in HQNNs. We propose a cutting methodology for HQNNs that employs a greedy algorithm for identifying efficient cutting points, and the implementation of trainable subcircuits, all designed to maximize the utility of NISQ devices in HQNNs. The findings suggest that quantum circuit cutting is a promising technique for advancing QML on current quantum hardware, since the cut circuit achieves comparable accuracy and much lower qubit requirements than the original circuit. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.04844",
    "authors": [
      "Alberto Marchisio",
      "Emman Sychiuco",
      "Muhammad Kashif",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2501.18283",
    "title": "Random Feature Representation Boosting",
    "abstract": "           We introduce Random Feature Representation Boosting (RFRBoost), a novel method for constructing deep residual random feature neural networks (RFNNs) using boosting theory. RFRBoost uses random features at each layer to learn the functional gradient of the network representation, enhancing performance while preserving the convex optimization benefits of RFNNs. In the case of MSE loss, we obtain closed-form solutions to greedy layer-wise boosting with random features. For general loss functions, we show that fitting random feature residual blocks reduces to solving a quadratically constrained least squares problem. Through extensive numerical experiments on tabular datasets for both regression and classification, we show that RFRBoost significantly outperforms RFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime where RFNNs are typically applied. Moreover, RFRBoost offers substantial computational benefits, and theoretical guarantees stemming from boosting theory.         ",
    "url": "https://arxiv.org/abs/2501.18283",
    "authors": [
      "Nikita Zozoulenko",
      "Thomas Cass",
      "Lukas Gonon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.10882",
    "title": "A Quantum Speedup in Localizing Transmission Loss Change in Optical Networks",
    "abstract": "           The ability to localize transmission loss change to a subset of links in optical networks is crucial for maintaining network reliability, performance and security. \\emph{Quantum probes}, implemented by sending blocks of $n$ coherent-state pulses augmented with continuous-variable (CV) squeezing ($n=1$) or weak temporal-mode entanglement ($n>1$) over a lossy channel to a receiver with homodyne detection capabilities, are known to be more sensitive than their quasi-classical counterparts in detecting a sudden increase in channel loss. The enhanced sensitivity can be characterized by the increased Kullback-Leibler (KL) divergence of the homodyne output, before and after the loss change occurs. When combined with the theory of quickest change detection (QCD), the increase in KL divergence translates into a decrease in detection latency. In this work, we first revisit quantum probes over a channel, generalizing previous results on $n=1$ (CV squeezed states) to arbitrary values of $n$. Assuming a subset of nodes in an optical network is capable of sending and receiving such probes through intermediate nodes with all-optical switching capabilities, we present a scheme for quickly detecting the links that have suffered a sudden drop in transmissivity. Since quantum probes lose their sensitivity with increasing loss in the channel, we first propose a probe construction algorithm that makes the set of links suffering transmission loss change identifiable, while minimizing the longest distance a probe traverses. We then introduce new cumulative sum (CUSUM) statistics with a stopping rule, which allows us to run the CUSUM algorithm to quickly localize the lossy links using our constructed probes. Finally, we show that the proposed scheme achieves a quantum speedup in decreasing the detection delay.         ",
    "url": "https://arxiv.org/abs/2504.10882",
    "authors": [
      "Yufei Zheng",
      "Yu-Zhen Janice Chen",
      "Prithwish Basu",
      "Don Towsley"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.02644",
    "title": "Non-exchangeable evolutionary and mean field games and their applications",
    "abstract": "           A replicator dynamic for non-exchangeable agents in a continuous action space is formulated and its well-posedness is proven in a space of probability measures. The non-exchangeability allows for the analysis of evolutionary games involving agents with distinct (and possibly infinitely many) types. We also explicitly connect this replicator dynamic to a stationary mean field game, which determines the pairwise actions of the heterogeneous agents. Moreover, as a byproduct of our theoretical results, we show that a class of nonlinear voter models, recently the subject of increasing interest, called q-voter models, can be viewed as a replicator dynamic driven by a utility that is a power of the probability density. This implies that non-exchangeable and/or mean-field game formulations of these models can also be constructed. We also present computational examples of evolutionary and mean field game models using a finite difference method, focusing on tragedy of the commons and the q-voter model with non-exchangeable agents, of which are interesting cases from theoretical and computational perspectives.         ",
    "url": "https://arxiv.org/abs/2506.02644",
    "authors": [
      "H. Yoshioka",
      "M. Tsujimura",
      "T. Tanaka"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2507.04441",
    "title": "The Joys of Categorical Conformal Prediction",
    "abstract": "           Conformal prediction (CP) is an Uncertainty Representation technique that delivers finite-sample calibrated prediction regions for any underlying Machine Learning model. Its status as an Uncertainty Quantification (UQ) tool, though, has remained conceptually opaque: While Conformal Prediction Regions (CPRs) give an ordinal representation of uncertainty (larger regions typically indicate higher uncertainty), they lack the capability to cardinally quantify it (twice as large regions do not imply twice the uncertainty). We adopt a category-theoretic approach to CP -- framing it as a morphism, embedded in a commuting diagram, of two newly-defined categories -- that brings us three joys. First, we show that -- under minimal assumptions -- CP is intrinsically a UQ mechanism, that is, its cardinal UQ capabilities are a structural feature of the method. Second, we demonstrate that CP bridges the Bayesian, frequentist, and imprecise probabilistic approaches to predictive statistical reasoning. Finally, we show that a CPR is the image of a covariant functor. This observation is relevant to AI privacy: It implies that privacy noise added locally does not break the global coverage guarantee.         ",
    "url": "https://arxiv.org/abs/2507.04441",
    "authors": [
      "Michele Caprio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Category Theory (math.CT)"
    ]
  },
  {
    "id": "arXiv:2507.19861",
    "title": "Quantum-informed machine learning for the prediction of chaotic dynamical systems",
    "abstract": "           We introduce a quantum-informed machine learning (QIML) framework for the long-term dynamical behavior of high-dimensional chaotic systems. The method combines a one-time, offline-trained quantum generative model with a classical autoregressive predictor for spatiotemporal field generation. The quantum model learns a quantum prior (Q-Prior) that guides the representation of small-scale interactions and improves the modeling of fine-scale dynamics. We evaluate QIML on three representative systems: the Kuramoto-Sivashinsky equation, the two-dimensional Kolmogorov flow, and a cross-section of fully developed three-dimensional turbulent channel flow used as a realistic inflow condition. Compared to the classical baseline, QIML yields up to 17.25% improvement in predictive distribution accuracy and a 29.36% improvement in the fidelity of the predicted full energy spectrum. For turbulent channel inflow, the Q-Prior is essential: without it, the model fails to evolve in time, while QIML produces stable, physically consistent forecasts that surpass leading machine learning models for PDEs, including the Fourier Neural Operator and Markov Neural Operator, whose errors diverge. Beyond accuracy, QIML also achieves a memory advantage, compressing multi-megabyte datasets into a kilobyte-scale Q-Prior that captures only the invariant measure needed to guide the classical model, thus circumventing Holevo's bound by avoiding full data reconstruction. Our findings provide a practical and scalable pathway for integrating the advantages brought by quantum devices into large-scale scientific, engineering modeling and simulation.         ",
    "url": "https://arxiv.org/abs/2507.19861",
    "authors": [
      "Maida Wang",
      "Xiao Xue",
      "Mingyang Gao",
      "Peter V. Coveney"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  }
]