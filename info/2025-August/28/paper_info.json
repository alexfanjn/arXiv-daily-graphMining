[
  {
    "id": "arXiv:2508.19251",
    "title": "MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks",
    "abstract": "           Symbolic music generation has seen rapid progress with artificial neural networks, yet remains underexplored in the biologically plausible domain of spiking neural networks (SNNs), where both standardized benchmarks and comprehensive evaluation methods are lacking. To address this gap, we introduce MuSpike, a unified benchmark and evaluation framework that systematically assesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM, SNN-GAN and SNN-Transformer) across five typical datasets, covering tonal, structural, emotional, and stylistic variations. MuSpike emphasizes comprehensive evaluation, combining established objective metrics with a large-scale listening study. We propose new subjective metrics, targeting musical impression, autobiographical association, and personal preference, that capture perceptual dimensions often overlooked in prior work. Results reveal that (1) different SNN models exhibit distinct strengths across evaluation dimensions; (2) participants with different musical backgrounds exhibit diverse perceptual patterns, with experts showing greater tolerance toward AI-composed music; and (3) a noticeable misalignment exists between objective and subjective evaluations, highlighting the limitations of purely statistical metrics and underscoring the value of human perceptual judgment in assessing musical quality. MuSpike provides the first systematic benchmark and systemic evaluation framework for SNN models in symbolic music generation, establishing a solid foundation for future research into biologically plausible and cognitively grounded music generation.         ",
    "url": "https://arxiv.org/abs/2508.19251",
    "authors": [
      "Qian Liang",
      "Menghaoran Tang",
      "Yi Zeng"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2508.19256",
    "title": "WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design",
    "abstract": "           Community consultations are integral to urban planning processes intended to incorporate diverse stakeholder perspectives. However, limited resources, visual and spoken language barriers, and uneven power dynamics frequently constrain inclusive decision-making. This paper examines how generative text-to-image methods, specifically Stable Diffusion XL integrated into a custom platform (WeDesign), may support equitable consultations. A half-day workshop in Montreal involved five focus groups, each consisting of architects, urban designers, AI specialists, and residents from varied demographic groups. Additional data was gathered through semi-structured interviews with six urban planning professionals. Participants indicated that immediate visual outputs facilitated creativity and dialogue, yet noted issues in visualizing specific needs of marginalized groups, such as participants with reduced mobility, accurately depicting local architectural elements, and accommodating bilingual prompts. Participants recommended the development of an open-source platform incorporating in-painting tools, multilingual support, image voting functionalities, and preference indicators. The results indicate that generative AI can broaden participation and enable iterative interactions but requires structured facilitation approaches. The findings contribute to discussions on generative AI's role and limitations in participatory urban design.         ",
    "url": "https://arxiv.org/abs/2508.19256",
    "authors": [
      "Rashid Mushkani",
      "Hugo Berard",
      "Shin Koseki"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.19263",
    "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats",
    "abstract": "           As deep learning models grow and deployment becomes more widespread, reducing the storage and transmission costs of neural network weights has become increasingly important. While prior work such as ZipNN has shown that lossless compression methods - particularly those based on Huffman encoding floating-point exponents can significantly reduce model sizes, these techniques have primarily been applied to higher-precision formats such as FP32 and BF16. In this work, we extend the ZipNN approach to lower-precision floating-point formats, specifically FP8 and FP4, which are gaining popularity for efficient inference. We design a compression method that separates and compresses the exponent and mantissa components independently using entropy coding. Our evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also investigate the compressibility of key-value (K/V) cache tensors used in large language models (LLMs), finding that they, too, exhibit compressible patterns, enabling memory savings during deployment.         ",
    "url": "https://arxiv.org/abs/2508.19263",
    "authors": [
      "Anat Heilper",
      "Doron Singer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.19273",
    "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks",
    "abstract": "           The proliferation of cloud-integrated IoT systems has intensified exposure to Distributed Denial of Service (DDoS) attacks due to the expanded attack surface, heterogeneous device behaviors, and limited edge protection. However, DDoS detection in this context remains challenging because of complex traffic dynamics, severe class imbalance, and scarce labeled data. While recent methods have explored solutions to address class imbalance, many still struggle to generalize under limited supervision and dynamic traffic conditions. To overcome these challenges, we propose MixGAN, a hybrid detection method that integrates conditional generation, semi-supervised learning, and robust feature extraction. Specifically, to handle complex temporal traffic patterns, we design a 1-D WideResNet backbone composed of temporal convolutional layers with residual connections, which effectively capture local burst patterns in traffic sequences. To alleviate class imbalance and label scarcity, we use a pretrained CTGAN to generate synthetic minority-class (DDoS attack) samples that complement unlabeled data. Furthermore, to mitigate the effect of noisy pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that constructs smoothed and sharpened targets by averaging predictions over augmented views and reweighting them towards high-confidence classes. Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR compared to state-of-the-art methods, confirming its robustness in large-scale IoT-cloud environments. The source code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19273",
    "authors": [
      "Tongxi Wu",
      "Chenwei Xu",
      "Jin Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19284",
    "title": "A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures",
    "abstract": "           The Denial of Wallet (DoW) attack poses a unique and growing threat to serverless architectures that rely on Function-as-a-Service (FaaS) models, exploiting the cost structure of pay-as-you-go billing to financially burden application owners. Unlike traditional Denial of Service (DoS) attacks, which aim to exhaust resources and disrupt service availability, DoW attacks focus on escalating costs without impacting service operation. This review traces the evolution of DoW research, from initial awareness and attack classification to advancements in detection and mitigation strategies. Key developments include the categorisation of attack types-such as Blast DDoW, Continual Inconspicuous DDoW, and Background Chained DDoW-and the creation of simulation tools like DoWTS, which enable safe experimentation and data generation. Recent advancements highlight machine learning approaches, including systems like Gringotts and DoWNet, which leverage deep learning and anomaly detection to identify malicious traffic patterns. Although substantial progress has been made, challenges persist, notably the lack of real-world data and the need for adaptive billing models. This is the first comprehensive literature review dedicated strictly to Denial of Wallet attacks, providing an in-depth analysis of their financial impacts, attack techniques, mitigation strategies, and detection mechanisms within serverless computing. The paper also presents the first detailed examination of simulation and data generation tools used for DoW research, addressing a critical gap in existing cybersecurity literature. By synthesising these key areas, this study serves as a foundational resource for future research and industry efforts in securing pay-as-you-go cloud environments.         ",
    "url": "https://arxiv.org/abs/2508.19284",
    "authors": [
      "Mark Dorsett",
      "Scott Mann",
      "Jabed Chowdhury",
      "Abdun Mahmood"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19289",
    "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation",
    "abstract": "           We present an unsupervised slide-quality assessment pipeline that combines seven expert-inspired visual-design metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate presentation slides. Trained on 12k professional lecture slides and evaluated on six academic talks (115 slides), our method achieved Pearson correlations up to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual ratings, discriminant validity against speaker-delivery scores, and exploratory alignment with overall impressions. Our results show that augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling scalable, objective feedback in real time.         ",
    "url": "https://arxiv.org/abs/2508.19289",
    "authors": [
      "Tai Inui",
      "Steven Oh",
      "Magdeline Kuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19290",
    "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation",
    "abstract": "           LiDAR-based segmentation is essential for reliable perception in autonomous vehicles, yet modern segmentation networks are highly susceptible to adversarial attacks that can compromise safety. Most existing defenses are designed for networks operating directly on raw 3D point clouds and rely on large, computationally intensive generative models. However, many state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D range view representations. Despite their widespread adoption, dedicated lightweight adversarial defenses for this domain remain largely unexplored. We introduce an efficient model-based purification framework tailored for adversarial defense in 2D range-view LiDAR segmentation. We propose a direct attack formulation in the range-view domain and develop an explainable purification network based on a mathematical justified optimization problem, achieving strong adversarial resilience with minimal computational overhead. Our method achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. More importantly, real-world deployment on a demo vehicle demonstrates the framework's ability to deliver accurate operation in practical autonomous driving scenarios.         ",
    "url": "https://arxiv.org/abs/2508.19290",
    "authors": [
      "Alexandros Gkillas",
      "Ioulia Kapsali",
      "Nikos Piperigkos",
      "Aris S. Lalos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19292",
    "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience",
    "abstract": "           Large language models (LLMs) generate human-aligned content under certain safety constraints. However, the current known technique ``jailbreak prompt'' can circumvent safety-aligned measures and induce LLMs to output malicious content. Research on Jailbreaking can help identify vulnerabilities in LLMs and guide the development of robust security frameworks. To circumvent the issue of attack templates becoming obsolete as models evolve, existing methods adopt iterative mutation and dynamic optimization to facilitate more automated jailbreak attacks. However, these methods face two challenges: inefficiency and repetitive optimization, as they overlook the value of past attack experiences. To better integrate past attack experiences to assist current jailbreak attempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework, which is the first to achieve a formal representation of experience structure, group experiences based on semantic drift, and support the dynamic updating of the experience pool. Extensive experiments demonstrate that JailExpert significantly improves both attack effectiveness and efficiency. Compared to the current state-of-the-art black-box jailbreak methods, JailExpert achieves an average increase of 17\\% in attack success rate and 2.7 times improvement in attack efficiency. Our implementation is available at \\href{this https URL}{XiZaiZai/JailExpert}         ",
    "url": "https://arxiv.org/abs/2508.19292",
    "authors": [
      "Xi Wang",
      "Songlei Jian",
      "Shasha Li",
      "Xiaopeng Li",
      "Bin Ji",
      "Jun Ma",
      "Xiaodong Liu",
      "Jing Wang",
      "Feilong Bao",
      "Jianfeng Zhang",
      "Baosheng Wang",
      "Jie Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19294",
    "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review",
    "abstract": "           The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolutionize object detection and localization. We then explain the architectural innovations, training paradigms, and output flexibility of recent LVLMs for object detection, highlighting how they achieve advanced contextual understanding for object detection. The review thoroughly examines the approaches used in integration of visual and textual information, demonstrating the progress made in object detection using VLMs that facilitate more sophisticated object detection and localization strategies. This review presents comprehensive visualizations demonstrating LVLMs' effectiveness in diverse scenarios including localization and segmentation, and then compares their real-time performance, adaptability, and complexity to traditional deep learning systems. Based on the review, its is expected that LVLMs will soon meet or surpass the performance of conventional methods in object detection. The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field. We conclude, based on this study, that the recent advancement in LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future.         ",
    "url": "https://arxiv.org/abs/2508.19294",
    "authors": [
      "Ranjan Sapkota",
      "Manoj Karkee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.19305",
    "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities",
    "abstract": "           Spatial representation learning is essential for GeoAI applications such as urban analytics, enabling the encoding of shapes, locations, and spatial relationships (topological and distance-based) of geo-entities like points, polylines, and polygons. Existing methods either target a single geo-entity type or, like Poly2Vec, decompose entities into simpler components to enable Fourier transformation, introducing high computational cost. Moreover, since the transformed space lacks geometric alignment, these methods rely on uniform, non-adaptive sampling, which blurs fine-grained features like edges and boundaries. To address these limitations, we introduce Geo2Vec, a novel method inspired by signed distance fields (SDF) that operates directly in the original space. Geo2Vec adaptively samples points and encodes their signed distances (positive outside, negative inside), capturing geometry without decomposition. A neural network trained to approximate the SDF produces compact, geometry-aware, and unified representations for all geo-entity types. Additionally, we propose a rotation-invariant positional encoding to model high-frequency spatial variations and construct a structured and robust embedding space for downstream GeoAI models. Empirical results show that Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications. Code and Data can be found at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19305",
    "authors": [
      "Chen Chu",
      "Cyrus Shahabi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19308",
    "title": "Infant Cry Detection In Noisy Environment Using Blueprint Separable Convolutions and Time-Frequency Recurrent Neural Network",
    "abstract": "           Infant cry detection is a crucial component of baby care system. In this paper, we propose a lightweight and robust method for infant cry detection. The method leverages blueprint separable convolutions to reduce computational complexity, and a time-frequency recurrent neural network for adaptive denoising. The overall framework of the method is structured as a multi-scale convolutional recurrent neural network, which is enhanced by efficient spatial attention mechanism and contrast-aware channel attention module, and acquire local and global information from the input feature of log Mel-spectrogram. Multiple public datasets are adopted to create a diverse and representative dataset, and environmental corruption techniques are used to generate the noisy samples encountered in real-world scenarios. Results show that our method exceeds many state-of-the-art methods in accuracy, F1-score, and complexity under various signal-to-noise ratio conditions. The code is at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19308",
    "authors": [
      "Haolin Yu",
      "Yanxiong Li"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2508.19321",
    "title": "An Investigation on Group Query Hallucination Attacks",
    "abstract": "           With the widespread use of large language models (LLMs), understanding their potential failure modes during user interactions is essential. In practice, users often pose multiple questions in a single conversation with LLMs. Therefore, in this study, we propose Group Query Attack, a technique that simulates this scenario by presenting groups of queries to LLMs simultaneously. We investigate how the accumulated context from consecutive prompts influences the outputs of LLMs. Specifically, we observe that Group Query Attack significantly degrades the performance of models fine-tuned on specific tasks. Moreover, we demonstrate that Group Query Attack induces a risk of triggering potential backdoors of LLMs. Besides, Group Query Attack is also effective in tasks involving reasoning, such as mathematical reasoning and code generation for pre-trained and aligned models.         ",
    "url": "https://arxiv.org/abs/2508.19321",
    "authors": [
      "Kehao Miao",
      "Xiaolong Jin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.19325",
    "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI",
    "abstract": "           Accurate prediction of major adverse cardiac events (MACE) remains a central challenge in cardiovascular prognosis. We present PRISM (Prompt-guided Representation Integration for Survival Modeling), a self-supervised framework that integrates visual representations from non-contrast cardiac cine magnetic resonance imaging with structured electronic health records (EHRs) for survival analysis. PRISM extracts temporally synchronized imaging features through motion-aware multi-view distillation and modulates them using medically informed textual prompts to enable fine-grained risk prediction. Across four independent clinical cohorts, PRISM consistently surpasses classical survival prediction models and state-of-the-art (SOTA) deep learning baselines under internal and external validation. Further clinical findings demonstrate that the combined imaging and EHR representations derived from PRISM provide valuable insights into cardiac risk across diverse cohorts. Three distinct imaging signatures associated with elevated MACE risk are uncovered, including lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior elevated focus during diastole. Prompt-guided attribution further identifies hypertension, diabetes, and smoking as dominant contributors among clinical and physiological EHR factors.         ",
    "url": "https://arxiv.org/abs/2508.19325",
    "authors": [
      "Haoyang Su",
      "Jin-Yi Xiang",
      "Shaohao Rui",
      "Yifan Gao",
      "Xingyu Chen",
      "Tingxuan Yin",
      "Xiaosong Wang",
      "Lian-Ming Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19347",
    "title": "Neural operators for solving nonlinear inverse problems",
    "abstract": "           We consider solving a probably infinite dimensional operator equation, where the operator is not modeled by physical laws but is specified indirectly via training pairs of the input-output relation of the operator. Neural operators have proven to be efficient to approximate operators with such information. In this paper, we analyze Tikhonov regularization with neural operators as surrogates for solving ill-posed operator equations. The analysis is based on balancing approximation errors of neural operators, regularization parameters, and noise. Moreover, we extend the approximation properties of neural operators from sets of continuous functions to Sobolev and Lebesgue spaces, which is crucial for solving inverse problems. Finally, we address the problem of finding an appropriate network structure of neural operators.         ",
    "url": "https://arxiv.org/abs/2508.19347",
    "authors": [
      "Otmar Scherzer",
      "Thi Lan Nhi Vu",
      "Jikai Yan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Functional Analysis (math.FA)"
    ]
  },
  {
    "id": "arXiv:2508.19350",
    "title": "Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC",
    "abstract": "           Wireless underground sensor networks (WUSNs) offer significant social and economic benefits by enabling the monitoring of subterranean entities. However, the communication reliability of WUSNs diminishes in harsh environments where terrestrial network infrastructure is either unavailable or unreliable. To address this challenge, we explore the feasibility of integrating buried massive machine-type communication (mMTC) sensors with non-terrestrial networks (NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms (HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN connectivity for various large-scale underground monitoring applications. To assess the effectiveness of underground-to-NTN connectivity, we develop a Monte Carlo simulator that incorporates a multi-layer underground attenuation model, the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum (LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for short-range UAV communication in rural environments, while LR-FHSS modulation proves to be a promising option for HAP and LEO satellite platforms in massive WUSNs scenarios thanks to its adequate link budget and robustness to the interference. Finally, we demonstrate that the success probability of underground-to-NTN connectivity using LoRa and LR-FHSS is significantly affected by factors such as the monitoring environment, the number of devices, burial depth, and the soil's volumetric water content.         ",
    "url": "https://arxiv.org/abs/2508.19350",
    "authors": [
      "Kaiqiang Lin",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.19352",
    "title": "Memorization in Graph Neural Networks",
    "abstract": "           Deep neural networks (DNNs) have been shown to memorize their training data, yet similar analyses for graph neural networks (GNNs) remain largely under-explored. We introduce NCMemo (Node Classification Memorization), the first framework to quantify label memorization in semi-supervised node classification. We first establish an inverse relationship between memorization and graph homophily, i.e., the property that connected nodes share similar labels/features. We find that lower homophily significantly increases memorization, indicating that GNNs rely on memorization to learn less homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the increased memorization in low homophily graphs is tightly coupled to the GNNs' implicit bias on using graph structure during learning. In low homophily regimes, this structure is less informative, hence inducing memorization of the node labels to minimize training loss. Finally, we show that nodes with higher label inconsistency in their feature-space neighborhood are significantly more prone to memorization. Building on our insights into the link between graph homophily and memorization, we investigate graph rewiring as a means to mitigate memorization. Our results demonstrate that this approach effectively reduces memorization without compromising model performance. Moreover, we show that it lowers the privacy risk for previously memorized data points in practice. Thus, our work not only advances understanding of GNN learning but also supports more privacy-preserving GNN deployment.         ",
    "url": "https://arxiv.org/abs/2508.19352",
    "authors": [
      "Adarsh Jamadandi",
      "Jing Xu",
      "Adam Dziedzic",
      "Franziska Boenisch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19356",
    "title": "Graph Data Modeling: Molecules, Proteins, & Chemical Processes",
    "abstract": "           Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine. This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes, introduces graphs as mathematical objects in chemistry and shows how learning algorithms (particularly graph neural networks) can operate on them. We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling. Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery.         ",
    "url": "https://arxiv.org/abs/2508.19356",
    "authors": [
      "Jos\u00e9 Manuel Barraza-Chavez",
      "Rana A. Barghout",
      "Ricardo Almada-Monter",
      "Benjamin Sanchez-Lengeling",
      "Adrian Jinich",
      "Radhakrishnan Mahadevan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2508.19357",
    "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains",
    "abstract": "           Large Language Models (LLMs) excel in language tasks but are prone to hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG) mitigates these by grounding LLMs in external knowledge. However, in complex domains involving multiple, lengthy, or conflicting documents, traditional RAG suffers from information overload and inefficient synthesis, leading to inaccurate and untrustworthy answers. To address this, we propose CASC (Context-Adaptive Synthesis and Compression), a novel framework that intelligently processes retrieved contexts. CASC introduces a Context Analyzer & Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs key information extraction, cross-document consistency checking and conflict resolution, and question-oriented structured synthesis. This process transforms raw, scattered information into a highly condensed, structured, and semantically rich context, significantly reducing the token count and cognitive load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new challenging multi-document question answering dataset designed for complex scientific domains with inherent redundancies and conflicts. Our extensive experiments demonstrate that CASC consistently outperforms strong baselines.         ",
    "url": "https://arxiv.org/abs/2508.19357",
    "authors": [
      "Peiran Zhou",
      "Junnan Zhu",
      "Yichen Shen",
      "Ruoxi Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.19359",
    "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction",
    "abstract": "           Event Extraction (EE) involves automatically identifying and extracting structured information about events from unstructured text, including triggers, event types, and arguments. Traditional discriminative models demonstrate high precision but often exhibit limited recall, particularly for nuanced or infrequent events. Conversely, generative approaches leveraging Large Language Models (LLMs) provide higher semantic flexibility and recall but suffer from hallucinations and inconsistent predictions. To address these challenges, we propose Agreement-based Reflective Inference System (ARIS), a hybrid approach combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS explicitly leverages structured model consensus, confidence-based filtering, and an LLM reflective inference module to reliably resolve ambiguities and enhance overall event prediction quality. We further investigate decomposed instruction fine-tuning for enhanced LLM event extraction understanding. Experiments demonstrate our approach outperforms existing state-of-the-art event extraction methods across three benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2508.19359",
    "authors": [
      "Fatemeh Haji",
      "Mazal Bethany",
      "Cho-Yu Jason Chiang",
      "Anthony Rios",
      "Peyman Najafirad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19361",
    "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture",
    "abstract": "           Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk of stroke, heart failure, and other cardiovascular complications. While AF detection algorithms perform well in identifying persistent AF, early-stage progression, such as paroxysmal AF (PAF), often goes undetected due to its sudden onset and short duration. However, undetected PAF can progress into sustained AF, increasing the risk of mortality and severe complications. Early prediction of AF offers an opportunity to reduce disease progression through preventive therapies, such as catecholamine-sparing agents or beta-blockers. In this study, we propose a lightweight deep learning model using only RR Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for positional encoding with Mamba, a selective state space model, to enable early prediction of AF through efficient parallel sequence modeling. In subject-wise testing results, our model achieved a sensitivity of 0.908, specificity of 0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our method demonstrates high computational efficiency, with only 73.5 thousand parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and model compactness. Notably, the model can predict AF up to two hours in advance using just 30 minutes of input data, providing enough lead time for preventive interventions.         ",
    "url": "https://arxiv.org/abs/2508.19361",
    "authors": [
      "Yongbin Lee",
      "Ki H. Chon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19364",
    "title": "Towards Reliable Neural Optimizers: Permutation-Equivariant Neural Approximation in Dynamic Data Driven Applications Systems",
    "abstract": "           Dynamic Data Driven Applications Systems (DDDAS) motivate the development of optimization approaches capable of adapting to streaming, heterogeneous, and asynchronous data from sensor networks. Many established optimization solvers, such as branch-and-bound, gradient descent, and Newton-Raphson methods, rely on iterative algorithms whose step-by-step convergence makes them too slow for real-time, multi-sensor environments. In our recent work, we introduced LOOP-PE (Learning to Optimize the Optimization Process, Permutation Equivariance version), a feed-forward neural approximation model with an integrated feasibility recovery function. LOOP-PE processes inputs from a variable number of sensors in arbitrary order, making it robust to sensor dropout, communication delays, and system scaling. Its permutation-equivariant architecture ensures that reordering the input data reorders the corresponding dispatch decisions consistently, without retraining or pre-alignment. Feasibility is enforced via a generalized gauge map, guaranteeing that outputs satisfy physical and operational constraints. We illustrate the approach in a DDDAS-inspired case study of a Virtual Power Plant (VPP) managing multiple distributed generation agents (DERs) to maximize renewable utilization while respecting system limits. Results show that LOOP-PE produces near-optimal, feasible, and highly adaptable decisions under dynamic, unordered, and distributed sensing conditions, significantly outperforming iterative algorithm based solvers in both speed and flexibility. Here, we extend our earlier work by providing additional analysis and explanation of LOOP-PE design and operation, with particular emphasis on its feasibility guarantee and permutation equivariance feature.         ",
    "url": "https://arxiv.org/abs/2508.19364",
    "authors": [
      "Meiyi Li",
      "Javad Mohammadi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.19372",
    "title": "Database Entity Recognition with Data Augmentation and Deep Learning",
    "abstract": "           This paper addresses the challenge of Database Entity Recognition (DB-ER) in Natural Language Queries (NLQ). We present several key contributions to advance this field: (1) a human-annotated benchmark for DB-ER task, derived from popular text-to-sql benchmarks, (2) a novel data augmentation procedure that leverages automatic annotation of NLQs based on the corresponding SQL queries which are available in popular text-to-SQL benchmarks, (3) a specialized language model based entity recognition model using T5 as a backbone and two down-stream DB-ER tasks: sequence tagging and token classification for fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER tagger with two state-of-the-art NER taggers, and observed better performance in both precision and recall for our model. The ablation evaluation shows that data augmentation boosts precision and recall by over 10%, while fine-tuning of the T5 backbone boosts these metrics by 5-10%.         ",
    "url": "https://arxiv.org/abs/2508.19372",
    "authors": [
      "Zikun Fu",
      "Chen Yang",
      "Kourosh Davoudi",
      "Ken Q. Pu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19379",
    "title": "Robust Recursive Query Parallelism in Graph Database Management Systems",
    "abstract": "           Efficient multi-core parallel processing of recursive join queries is critical for achieving good performance in graph database management systems (GDBMSs). Prior work adopts two broad approaches. First is the state of the art morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes computations at the source node level. Second is to parallelize each iteration of the computation at the frontier level. We show that these approaches can be seen as part of a design space of morsel dispatching policies based on picking different granularities of morsels. We then empirically study the question of which policies parallelize better in practice under a variety of datasets and query workloads that contain one to many source nodes. We show that these two policies can be combined in a hybrid policy that issues morsels both at the source node and frontier levels. We then show that the multi-source breadth-first search optimization from prior work can also be modeled as a morsel dispatching policy that packs multiple source nodes into multi-source morsels. We implement these policies inside a single system, the Kuzu GDBMS, and evaluate them both within Kuzu and across other systems. We show that the hybrid policy captures the behavior of both source morsel-only and frontier morsel-only policies in cases when these approaches parallelize well, and out-perform them on queries when they are limited, and propose it as a robust approach to parallelizing recursive queries. We further show that assigning multi-sources is beneficial, as it reduces the amount of scans, but only when there is enough sources in the query.         ",
    "url": "https://arxiv.org/abs/2508.19379",
    "authors": [
      "Anurag Chakraborty",
      "Semih Saliho\u011flu"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2508.19380",
    "title": "FlipWalker: Jacob's Ladder toy-inspired robot for locomotion across diverse, complex terrain",
    "abstract": "           This paper introduces FlipWalker, a novel underactuated robot locomotion system inspired by Jacob's Ladder illusion toy, designed to traverse challenging terrains where wheeled robots often struggle. Like the Jacob's Ladder toy, FlipWalker features two interconnected segments joined by flexible cables, enabling it to pivot and flip around singularities in a manner reminiscent of the toy's cascading motion. Actuation is provided by motor-driven legs within each segment that push off either the ground or the opposing segment, depending on the robot's current configuration. A physics-based model of the underactuated flipping dynamics is formulated to elucidate the critical design parameters governing forward motion and obstacle clearance or climbing. The untethered prototype weighs 0.78 kg, achieves a maximum flipping speed of 0.2 body lengths per second. Experimental trials on artificial grass, river rocks, and snow demonstrate that FlipWalker's flipping strategy, which relies on ground reaction forces applied normal to the surface, offers a promising alternative to traditional locomotion for navigating irregular outdoor terrain.         ",
    "url": "https://arxiv.org/abs/2508.19380",
    "authors": [
      "Diancheng Li",
      "Nia Ralston",
      "Bastiaan Hagen",
      "Phoebe Tan",
      "Matthew A. Robertson"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.19381",
    "title": "Towards Quantum Machine Learning for Malicious Code Analysis",
    "abstract": "           Classical machine learning (CML) has been extensively studied for malware classification. With the emergence of quantum computing, quantum machine learning (QML) presents a paradigm-shifting opportunity to improve malware detection, though its application in this domain remains largely unexplored. In this study, we investigate two hybrid quantum-classical models -- a Quantum Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN), for malware classification. Both models utilize angle embedding to encode malware features into quantum states. QMLP captures complex patterns through full qubit measurement and data re-uploading, while QCNN achieves faster training via quantum convolution and pooling layers that reduce active qubits. We evaluate both models on five widely used malware datasets -- API-Graph, EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and multiclass classification tasks. Our results show high accuracy for binary classification -- 95-96% on API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class, and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex multiclass tasks, while QCNN offers improved training efficiency at the cost of reduced accuracy.         ",
    "url": "https://arxiv.org/abs/2508.19381",
    "authors": [
      "Jesus Lopez",
      "Saeefa Rubaiyet Nowmi",
      "Viviana Cadena",
      "Mohammad Saidur Rahman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19389",
    "title": "DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting",
    "abstract": "           Accurate long-term traffic forecasting remains a critical challenge in intelligent transportation systems, particularly when predicting high-frequency traffic phenomena such as shock waves and congestion boundaries over extended rollout horizons. Neural operators have recently gained attention as promising tools for modeling traffic flow. While effective at learning function space mappings, they inherently produce smooth predictions that fail to reconstruct high-frequency features such as sharp density gradients which results in rapid error accumulation during multi-step rollout predictions essential for real-time traffic management. To address these fundamental limitations, we introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO) architecture. DETNO leverages a transformer neural operator with cross-attention mechanisms, providing model expressivity and super-resolution, coupled with a diffusion-based refinement component that iteratively reconstructs high-frequency traffic details through progressive denoising. This overcomes the inherent smoothing limitations and rollout instability of standard neural operators. Through comprehensive evaluation on chaotic traffic datasets, our method demonstrates superior performance in extended rollout predictions compared to traditional and transformer-based neural operators, preserving high-frequency components and improving stability over long prediction horizons.         ",
    "url": "https://arxiv.org/abs/2508.19389",
    "authors": [
      "Owais Ahmad",
      "Milad Ramezankhani",
      "Anirudh Deodhar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2508.19393",
    "title": "GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification",
    "abstract": "           Analog subcircuit identification is a core task in analog design, essential for simulation, sizing, and layout. Traditional methods often require extensive human expertise, rule-based encoding, or large labeled datasets. To address these challenges, we propose GENIE-ASI, the first training-free, large language model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI operates in two phases: it first uses in-context learning to derive natural language instructions from a few demonstration examples, then translates these into executable Python code to identify subcircuits in unseen SPICE netlists. In addition, to evaluate LLM-based approaches systematically, we introduce a new benchmark composed of operational amplifier netlists (op-amps) that cover a wide range of subcircuit variants. Experimental results on the proposed benchmark show that GENIE-ASI matches rule-based performance on simple structures (F1-score = 1.0), remains competitive on moderate abstractions (F1-score = 0.81), and shows potential even on complex subcircuits (F1-score = 0.31). These findings demonstrate that LLMs can serve as adaptable, general-purpose tools in analog design automation, opening new research directions for foundation model applications in analog design automation.         ",
    "url": "https://arxiv.org/abs/2508.19393",
    "authors": [
      "Phuoc Pham",
      "Arun Venkitaraman",
      "Chia-Yu Hsieh",
      "Andrea Bonetti",
      "Stefan Uhlich",
      "Markus Leibl",
      "Simon Hofmann",
      "Eisaku Ohbuchi",
      "Lorenzo Servadei",
      "Ulf Schlichtmann",
      "Robert Wille"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19398",
    "title": "Learning Robust Regions of Attraction Using Rollout-Enhanced Physics-Informed Neural Networks with Policy Iteration",
    "abstract": "           The region of attraction is a key metric of the robustness of systems. This paper addresses the numerical solution of the generalized Zubov's equation, which produces a special Lyapunov function characterizing the robust region of attraction for perturbed systems. To handle the highly nonlinear characteristic of the generalized Zubov's equation, we propose a physics-informed neural network framework that employs a policy iteration training scheme with rollout to approximate the viscosity solution. In addition to computing the optimal disturbance during the policy improvement process, we incorporate neural network-generated value estimates as anchor points to facilitate the training procedure to prevent singularities in both low- and high-dimensional systems. Numerical simulations validate the effectiveness of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2508.19398",
    "authors": [
      "Junkai Wang",
      "Yuxuan Zhao",
      "Mi Zhou",
      "Fumin Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.19410",
    "title": "Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks",
    "abstract": "           We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure energy conservation by learning Hamiltonian functions directly from data, existing implementations, often relying on MLPs, cause hypersensitivity to the hyperparameters while exploring complex energy landscapes. Our approach exploits the localized function approximations to better capture high-frequency and multi-scale dynamics, reducing energy drift and improving long-term predictive stability. The networks preserve the symplectic form of Hamiltonian systems, and thus maintain interpretability and physical consistency. After assessing KAR-HNN on four benchmark problems including spring-mass, simple pendulum, two- and three-body problem, we foresee its effectiveness for accurate and stable modeling of realistic physical processes often at high dimensions and with few known parameters.         ",
    "url": "https://arxiv.org/abs/2508.19410",
    "authors": [
      "Zongyu Wu",
      "Ruichen Xu",
      "Luoyao Chen",
      "Georgios Kementzidis",
      "Siyao Wang",
      "Yuefan Deng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2508.19424",
    "title": "MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification",
    "abstract": "           Motivation. Understanding the pan-cancer mutational landscape offers critical insights into the molecular mechanisms underlying tumorigenesis. While patient-level machine learning techniques have been widely employed to identify tumor subtypes, cohort-level clustering, where entire cancer types are grouped based on shared molecular features, has largely relied on classical statistical methods. Results. In this study, we introduce a novel unsupervised contrastive learning framework to cluster 43 cancer types based on coding mutation data derived from the COSMIC database. For each cancer type, we construct two complementary mutation signatures: a gene-level profile capturing nucleotide substitution patterns across the most frequently mutated genes, and a chromosome-level profile representing normalized substitution frequencies across chromosomes. These dual views are encoded using TabNet encoders and optimized via a multi-scale contrastive learning objective (NT-Xent loss) to learn unified cancer-type embeddings. We demonstrate that the resulting latent representations yield biologically meaningful clusters of cancer types, aligning with known mutational processes and tissue origins. Our work represents the first application of contrastive learning to cohort-level cancer clustering, offering a scalable and interpretable framework for mutation-driven cancer subtyping.         ",
    "url": "https://arxiv.org/abs/2508.19424",
    "authors": [
      "Yifan Dou",
      "Adam Khadre",
      "Ruben C Petreaca",
      "Golrokh Mirzaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19428",
    "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)",
    "abstract": "           We present a comprehensive system for addressing Tasks A, B, and C of the LLMs4OL 2025 challenge, which together span the full ontology construction pipeline: term extraction, typing, and taxonomy discovery. Our approach combines retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling -- each tailored to the demands of the respective task. For Task A, we jointly extract domain-specific terms and their ontological types using a retrieval-augmented generation (RAG) pipeline. Training data was reformulated into a document to terms and types correspondence, while test-time inference leverages semantically similar training examples. This single-pass method requires no model finetuning and improves overall performance through lexical augmentation Task B, which involves assigning types to given terms, is handled via a dual strategy. In the few-shot setting (for domains with labeled training data), we reuse the RAG scheme with few-shot prompting. In the zero-shot setting (for previously unseen domains), we use a zero-shot classifier that combines cosine similarity scores from multiple embedding models using confidence-based weighting. In Task C, we model taxonomy discovery as graph inference. Using embeddings of type labels, we train a lightweight cross-attention layer to predict is-a relations by approximating a soft adjacency matrix. These modular, task-specific solutions enabled us to achieve top-ranking results in the official leaderboard across all three tasks. Taken together these strategies showcase the scalability, adaptability, and robustness of LLM-based architectures for ontology learning across heterogeneous domains. Code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2508.19428",
    "authors": [
      "Aleksandra Beliaeva",
      "Temurbek Rahmatullaev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Logic in Computer Science (cs.LO)",
      "Symbolic Computation (cs.SC)"
    ]
  },
  {
    "id": "arXiv:2508.19429",
    "title": "An Iterative Approach for Heterogeneous Multi-Agent Route Planning with Resource Transportation Uncertainty and Temporal Logic Goals",
    "abstract": "           This paper presents an iterative approach for heterogeneous multi-agent route planning in environments with unknown resource distributions. We focus on a team of robots with diverse capabilities tasked with executing missions specified using Capability Temporal Logic (CaTL), a formal framework built on Signal Temporal Logic to handle spatial, temporal, capability, and resource constraints. The key challenge arises from the uncertainty in the initial distribution and quantity of resources in the environment. To address this, we introduce an iterative algorithm that dynamically balances exploration and task fulfillment. Robots are guided to explore the environment, identifying resource locations and quantities while progressively refining their understanding of the resource landscape. At the same time, they aim to maximally satisfy the mission objectives based on the current information, adapting their strategies as new data is uncovered. This approach provides a robust solution for planning in dynamic, resource-constrained environments, enabling efficient coordination of heterogeneous teams even under conditions of uncertainty. Our method's effectiveness and performance are demonstrated through simulated case studies.         ",
    "url": "https://arxiv.org/abs/2508.19429",
    "authors": [
      "Gustavo A. Cardona",
      "Kaier Liang",
      "Cristian-Ioan Vasile"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ]
  },
  {
    "id": "arXiv:2508.19430",
    "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks",
    "abstract": "           Formal verification is crucial for ensuring the robustness of security protocols against adversarial attacks. The Needham-Schroeder protocol, a foundational authentication mechanism, has been extensively studied, including its integration with Physical Layer Security (PLS) techniques such as watermarking and jamming. Recent research has used ProVerif to verify these mechanisms in terms of secrecy. However, the ProVerif-based approach limits the ability to improve understanding of security beyond verification results. To overcome these limitations, we re-model the same protocol using an Isabelle formalism that generates sound animation, enabling interactive and automated formal verification of security protocols. Our modelling and verification framework is generic and highly configurable, supporting both cryptography and PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy and authenticity in four different eavesdropper locations under both passive and active attacks) using our new web interface. Our findings not only successfully reproduce and reinforce previous results on secrecy but also reveal an uncommon but expected outcome: authenticity is preserved across all examined scenarios, even in cases where secrecy is compromised. We have proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and jamming, and our analysis shows that it is secure for deriving a session key with required authentication. These highlight the advantages of our novel approach, demonstrating its robustness in formally verifying security properties beyond conventional methods.         ",
    "url": "https://arxiv.org/abs/2508.19430",
    "authors": [
      "Kangfeng Ye",
      "Roberto Metere",
      "Jim Woodcock",
      "Poonam Yadav"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2508.19441",
    "title": "Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models",
    "abstract": "           Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDE's governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long time integration of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local \"stencil\" states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timesteps' worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented synthetic stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories.         ",
    "url": "https://arxiv.org/abs/2508.19441",
    "authors": [
      "Sanket Jantre",
      "Deepak Akhare",
      "Xiaoning Qian",
      "Nathan M. Urban"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.19445",
    "title": "On Surjectivity of Neural Networks: Can you elicit any behavior from your model?",
    "abstract": "           Given a trained neural network, can any specified output be generated by some input? Equivalently, does the network correspond to a function that is surjective? In generative models, surjectivity implies that any output, including harmful or undesirable content, can in principle be generated by the networks, raising concerns about model safety and jailbreak vulnerabilities. In this paper, we prove that many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective. As corollaries, widely used generative frameworks, including GPT-style transformers and diffusion models with deterministic ODE solvers, admit inverse mappings for arbitrary outputs. By studying surjectivity of these modern and commonly used neural architectures, we contribute a formalism that sheds light on their unavoidable vulnerability to a broad class of adversarial attacks.         ",
    "url": "https://arxiv.org/abs/2508.19445",
    "authors": [
      "Haozhe Jiang",
      "Nika Haghtalab"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.19450",
    "title": "CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection",
    "abstract": "           The Internet of Things (IoT), with its high degree of interconnectivity and limited computational resources, is particularly vulnerable to a wide range of cyber threats. Intrusion detection systems (IDS) have been extensively studied to enhance IoT security, and machine learning-based IDS (ML-IDS) show considerable promise for detecting malicious activity. However, their effectiveness is often constrained by poor adaptability to emerging threats and the issue of catastrophic forgetting during continuous learning. To address these challenges, we propose CITADEL, a self-supervised continual learning framework designed to extract robust representations from benign data while preserving long-term knowledge through optimized memory consolidation mechanisms. CITADEL integrates a tabular-to-image transformation module, a memory-aware masked autoencoder for self-supervised representation learning, and a novelty detection component capable of identifying anomalies without dependence on labeled attack data. Our design enables the system to incrementally adapt to emerging behaviors while retaining its ability to detect previously observed threats. Experiments on multiple intrusion datasets demonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based lifelong anomaly detector (VLAD) in key detection and retention metrics, highlighting its effectiveness in dynamic IoT environments.         ",
    "url": "https://arxiv.org/abs/2508.19450",
    "authors": [
      "Elvin Li",
      "Onat Gungor",
      "Zhengli Shang",
      "Tajana Rosing"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19456",
    "title": "ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification",
    "abstract": "           Minimizing computational overhead in time-series classification, particularly in deep learning models, presents a significant challenge due to the high complexity of model architectures and the large volume of sequential data that must be processed in real time. This challenge is further compounded by adversarial attacks, emphasizing the need for resilient methods that ensure robust performance and efficient model selection. To address this challenge, we propose ReLATE+, a comprehensive framework that detects and classifies adversarial attacks, adaptively selects deep learning models based on dataset-level similarity, and thus substantially reduces retraining costs relative to conventional methods that do not leverage prior knowledge, while maintaining strong performance. ReLATE+ first checks whether the incoming data is adversarial and, if so, classifies the attack type, using this insight to identify a similar dataset from a repository and enable the reuse of the best-performing associated model. This approach ensures strong performance while reducing the need for retraining, and it generalizes well across different domains with varying data distributions and feature spaces. Experiments show that ReLATE+ reduces computational overhead by an average of 77.68%, enhancing adversarial resilience and streamlining robust model selection, all without sacrificing performance, within 2.02% of Oracle.         ",
    "url": "https://arxiv.org/abs/2508.19456",
    "authors": [
      "Cagla Ipek Kocal",
      "Onat Gungor",
      "Tajana Rosing",
      "Baris Aksanli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19458",
    "title": "The Sample Complexity of Membership Inference and Privacy Auditing",
    "abstract": "           A membership-inference attack gets the output of a learning algorithm, and a target individual, and tries to determine whether this individual is a member of the training data or an independent sample from the same distribution. A successful membership-inference attack typically requires the attacker to have some knowledge about the distribution that the training data was sampled from, and this knowledge is often captured through a set of independent reference samples from that distribution. In this work we study how much information the attacker needs for membership inference by investigating the sample complexity-the minimum number of reference samples required-for a successful attack. We study this question in the fundamental setting of Gaussian mean estimation where the learning algorithm is given $n$ samples from a Gaussian distribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate $\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq \\rho^2 d$. Our result shows that for membership inference in this setting, $\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that competes with a fully informed attacker. Our result is the first to show that the attacker sometimes needs many more samples than the training algorithm uses to train the model. This result has significant implications for practice, as all attacks used in practice have a restricted form that uses $O(n)$ samples and cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be underestimating the possibility of membership inference, and better attacks may be possible when information about the distribution is easy to obtain.         ",
    "url": "https://arxiv.org/abs/2508.19458",
    "authors": [
      "Mahdi Haghifam",
      "Adam Smith",
      "Jonathan Ullman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.19472",
    "title": "SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis",
    "abstract": "           Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a persistent and under-addressed threat across software systems, often leading to serious security breaches. Existing detection tools rarely target the diverse subcategories of CWE-200 or provide context-aware analysis of code-level data flows. Aims: This paper aims to present SIExVulTS, a novel vulnerability detection system that integrates transformer-based models with static analysis to identify and verify sensitive information exposure in Java applications. Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface Detection Engine that uses sentence embeddings to identify sensitive variables, strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification Engine that leverages GraphCodeBERT to semantically validate source-to-sink flows. We evaluate SIExVulTS using three curated datasets, including real-world CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31 open-source projects. Results: The Attack Surface Detection Engine achieved an average F1 score greater than 93\\%, the Exposure Analysis Engine achieved an F1 score of 85.71\\%, and the Flow Verification Engine increased precision from 22.61\\% to 87.23\\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs in major Apache projects. Conclusions: The results demonstrate that SIExVulTS is effective and practical for improving software security against sensitive data exposure, addressing limitations of existing tools in detecting and verifying CWE-200 vulnerabilities.         ",
    "url": "https://arxiv.org/abs/2508.19472",
    "authors": [
      "Kyler Katz",
      "Sara Moshtari",
      "Ibrahim Mujhid",
      "Mehdi Mirakhorli",
      "Derek Garcia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19486",
    "title": "Distribution Shift Aware Neural Tabular Learning",
    "abstract": "           Tabular learning transforms raw features into optimized spaces for downstream tasks, but its effectiveness deteriorates under distribution shifts between training and testing data. We formalize this challenge as the Distribution Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature Transformation (SAFT) framework to address it. SAFT reframes tabular learning from a discrete search task into a continuous representation-generation paradigm, enabling differentiable optimization over transformed feature sets. SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant representation via embedding decorrelation and sample reweighting, (ii) flatness-aware generation through suboptimal embedding averaging, and (iii) normalization-based alignment between training and test distributions. Extensive experiments show that SAFT consistently outperforms prior tabular learning methods in terms of robustness, effectiveness, and generalization ability under diverse real-world distribution shifts.         ",
    "url": "https://arxiv.org/abs/2508.19486",
    "authors": [
      "Wangyang Ying",
      "Nanxu Gong",
      "Dongjie Wang",
      "Xinyuan Wang",
      "Arun Vignesh Malarkkan",
      "Vivek Gupta",
      "Chandan K. Reddy",
      "Yanjie Fu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19488",
    "title": "PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense",
    "abstract": "           Cyber defense requires automating defensive decision-making under stealthy, deceptive, and continuously evolving adversarial strategies. The FlipIt game provides a foundational framework for modeling interactions between a defender and an advanced adversary that compromises a system without being immediately detected. In FlipIt, the attacker and defender compete to control a shared resource by performing a Flip action and paying a cost. However, the existing FlipIt frameworks rely on a small number of heuristics or specialized learning techniques, which can lead to brittleness and the inability to adapt to new attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym environment that extends the FlipIt game to allow efficient learning for attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train defender agents equipped to generalize against a range of unknown, potentially adaptive opponents. Our empirical results suggest that Flip-PSRO defenders are $2\\times$ more effective than baselines to generalize to a heuristic attack not exposed in training. In addition, our newly designed ownership-based utility functions ensure that Flip-PSRO defenders maintain a high level of control while optimizing performance.         ",
    "url": "https://arxiv.org/abs/2508.19488",
    "authors": [
      "Xavier Cadet",
      "Simona Boboila",
      "Sie Hendrata Dharmawan",
      "Alina Oprea",
      "Peter Chin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19489",
    "title": "Interactive Graph Visualization and TeamingRecommendation in an Interdisciplinary Project'sTalent Knowledge Graph",
    "abstract": "           Interactive visualization of large scholarly knowledge graphs combined with LLM reasoning shows promise butremains under-explored. We address this gap by developing an interactive visualization system for the Cell Map forAI Talent Knowledge Graph (28,000 experts and 1,179 biomedical datasets). Our approach integrates WebGLvisualization with LLM agents to overcome limitations of traditional tools such as Gephi, particularly for large-scaleinteractive node handling. Key functionalities include responsive exploration, filtering, and AI-drivenrecommendations with justifications. This integration can potentially enable users to effectively identify potentialcollaborators and relevant dataset users within biomedical and AI research communities. The system contributes anovel framework that enhances knowledge graph exploration through intuitive visualization and transparent, LLM-guided recommendations. This adaptable solution extends beyond the CM4AI community to other large knowledgegraphs, improving information representation and decision-making. Demo: this https URL ",
    "url": "https://arxiv.org/abs/2508.19489",
    "authors": [
      "Jiawei Xu",
      "Juichien Chen",
      "Yilin Ye",
      "Zhandos Sembay",
      "Swathi Thaker",
      "Pamela Payne-Foster",
      "Jake Chen",
      "Ying Ding"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2508.19493",
    "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents",
    "abstract": "           Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19493",
    "authors": [
      "Zhixin Lin",
      "Jungang Li",
      "Shidong Pan",
      "Yibo Shi",
      "Yue Yao",
      "Dongliang Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19495",
    "title": "Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks",
    "abstract": "           Ambient intelligence (AmI) is a computing paradigm in which physical environments are embedded with sensing, computation, and communication so they can perceive people and context, decide appropriate actions, and respond autonomously. Realizing AmI at global scale requires sixth generation (6G) wireless networks with capabilities for real time perception, reasoning, and action aligned with human behavior and mobility patterns. We argue that Generative Artificial Intelligence (GenAI) is the creative core of such environments. Unlike traditional AI, GenAI learns data distributions and can generate realistic samples, making it well suited to close key AmI gaps, including generating synthetic sensor and channel data in under observed areas, translating user intent into compact, semantic messages, predicting future network conditions for proactive control, and updating digital twins without compromising privacy. This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models, and generative transformers, and connects them to practical AmI use cases, including spectrum sharing, ultra reliable low latency communication, intelligent security, and context aware digital twins. We also examine how 6G enablers, such as edge and fog computing, IoT device swarms, intelligent reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate distributed GenAI. Finally, we outline open challenges in energy efficient on device training, trustworthy synthetic data, federated generative learning, and AmI specific standardization. We show that GenAI is not a peripheral addition, but a foundational element for transforming 6G from a faster network into an ambient intelligent ecosystem.         ",
    "url": "https://arxiv.org/abs/2508.19495",
    "authors": [
      "Muhammad Ahmed Mohsin",
      "Junaid Ahmad",
      "Muhammad Hamza Nawaz",
      "Muhammad Ali Jamshed"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.19500",
    "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills",
    "abstract": "           This paper identifies and analyzes a novel vulnerability class in Model Context Protocol (MCP) based agent systems. The attack chain describes and demonstrates how benign, individually authorized tasks can be orchestrated to produce harmful emergent behaviors. Through systematic analysis using the MITRE ATLAS framework, we demonstrate how 95 agents tested with access to multiple services-including browser automation, financial analysis, location tracking, and code deployment-can chain legitimate operations into sophisticated attack sequences that extend beyond the security boundaries of any individual service. These red team exercises survey whether current MCP architectures lack cross-domain security measures necessary to detect or prevent a large category of compositional attacks. We present empirical evidence of specific attack chains that achieve targeted harm through service orchestration, including data exfiltration, financial manipulation, and infrastructure compromise. These findings reveal that the fundamental security assumption of service isolation fails when agents can coordinate actions across multiple domains, creating an exponential attack surface that grows with each additional capability. This research provides a barebones experimental framework that evaluate not whether agents can complete MCP benchmark tasks, but what happens when they complete them too well and optimize across multiple services in ways that violate human expectations and safety constraints. We propose three concrete experimental directions using the existing MCP benchmark suite.         ",
    "url": "https://arxiv.org/abs/2508.19500",
    "authors": [
      "David Noever"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19506",
    "title": "Learning Game-Playing Agents with Generative Code Optimization",
    "abstract": "           We present a generative optimization approach for learning game-playing agents, where policies are represented as Python programs and refined using large language models (LLMs). Our method treats decision-making policies as self-evolving code, with current observation as input and an in-game action as output, enabling agents to self-improve through execution traces and natural language feedback with minimal human intervention. Applied to Atari games, our game-playing Python program achieves performance competitive with deep reinforcement learning (RL) baselines while using significantly less training time and much fewer environment interactions. This work highlights the promise of programmatic policy representations for building efficient, adaptable agents capable of complex, long-horizon reasoning.         ",
    "url": "https://arxiv.org/abs/2508.19506",
    "authors": [
      "Zhiyi Kuang",
      "Ryan Rong",
      "YuCheng Yuan",
      "Allen Nie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19507",
    "title": "A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation",
    "abstract": "           In e-commerce, where users face a vast array of possible item choices, recommender systems are vital for helping them discover suitable items they might otherwise overlook. While many recommender systems primarily rely on a user's purchase history, recent multi-behavior recommender systems incorporate various auxiliary user behaviors, such as item clicks and cart additions, to enhance recommendations. Despite their overall performance gains, their effectiveness varies considerably between visited items (i.e., those a user has interacted with through auxiliary behaviors) and unvisited items (i.e., those with which the user has had no such interactions). Specifically, our analysis reveals that (1) existing multi-behavior recommender systems exhibit a significant gap in recommendation quality between the two item types (visited and unvisited items) and (2) achieving strong performance on both types with a single model architecture remains challenging. To tackle these issues, we propose a novel multi-behavior recommender system, MEMBER. It employs a mixture-of-experts framework, with experts designed to recommend the two item types, respectively. Each expert is trained using a self-supervised method specialized for its design goal. In our comprehensive experiments, we show the effectiveness of MEMBER across both item types, achieving up to 65.46\\% performance gain over the best competitor in terms of Hit Ratio@20.         ",
    "url": "https://arxiv.org/abs/2508.19507",
    "authors": [
      "Kyungho Kim",
      "Sunwoo Kim",
      "Geon Lee",
      "Kijung Shin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19511",
    "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity",
    "abstract": "           The automated management of invasive weeds is critical for sustainable agriculture, yet the performance of deep learning models in real-world fields is often compromised by two factors: challenging environmental conditions and the high cost of data annotation. This study tackles both issues through a diagnostic-driven, semi-supervised framework. Using a unique dataset of approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in sugarcane, we first establish strong supervised baselines for classification (ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by interpretability tools, uncovered a pervasive \"shadow bias,\" where models learned to misidentify shadows as vegetation. This diagnostic insight motivated our primary contribution: a semi-supervised pipeline that leverages unlabeled data to enhance model robustness. By training models on a more diverse set of visual information through pseudo-labeling, this framework not only helps mitigate the shadow bias but also provides a tangible boost in recall, a critical metric for minimizing weed escapes in automated spraying systems. To validate our methodology, we demonstrate its effectiveness in a low-data regime on a public crop-weed benchmark. Our work provides a clear and field-tested framework for developing, diagnosing, and improving robust computer vision systems for the complex realities of precision agriculture.         ",
    "url": "https://arxiv.org/abs/2508.19511",
    "authors": [
      "Alzayat Saleh",
      "Shunsuke Hatano",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19532",
    "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation",
    "abstract": "           The code generation capabilities of Large Language Models (LLMs) have advanced applications like tool invocation and problem-solving. However, improving performance in code-related tasks remains challenging due to limited training data that is verifiable with accurate test cases. While Direct Preference Optimization (DPO) has shown promise, existing methods for generating test cases still face limitations. In this paper, we propose a novel approach that splits code snippets into smaller, granular blocks, creating more diverse DPO pairs from the same test cases. Additionally, we introduce the Abstract Syntax Tree (AST) splitting and curriculum training method to enhance the DPO training. Our approach demonstrates significant improvements in code generation tasks, as validated by experiments on benchmark datasets such as HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19532",
    "authors": [
      "Houxing Ren",
      "Zimu Lu",
      "Weikang Shi",
      "Haotian Hou",
      "Yunqiao Yang",
      "Ke Wang",
      "Aojun Zhou",
      "Junting Pan",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.19541",
    "title": "Hybrid ML-RL Approach for Smart Grid Stability Prediction and Optimized Control Strategy",
    "abstract": "           Electrical grids are now much more complex due to the rapid integration of distributed generation and alternative energy sources, which makes forecasting grid stability with optimized control a crucial task for operators. Traditional statistical, physics-based, and ML models can learn the pattern of the grid features, but have limitations in optimal strategy control with instability prediction. This work proposes a hybrid ML-RL framework that leverages ML for rapid stability prediction and RL for dynamic control and optimization. The first stage of this study created a baseline that explored the potential of various ML models for stability prediction. Out of them, the stacking classifiers of several fundamental models show a significant performance in classifying the instability, leading to the second stage, where reinforcement learning algorithms (PPO, A2C, and DQN) optimize power control actions. Experimental results demonstrate that the hybrid ML-RL model effectively stabilizes the grid, achieves rapid convergence, and significantly reduces training time. The integration of ML-based stability classification with RL-based dynamic control enhances decision-making efficiency while lowering computational complexity, making it well-suited for real-time smart grid applications.         ",
    "url": "https://arxiv.org/abs/2508.19541",
    "authors": [
      "Kazi Sifatul Islam",
      "Anandi Dutta",
      "Shivani Mruthyunjaya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.19542",
    "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning",
    "abstract": "           While multimodal large language models (MLLMs) exhibit strong performance on single-video tasks (e.g., video question answering), their ability across multiple videos remains critically underexplored. However, this capability is essential for real-world applications, including multi-camera surveillance and cross-video procedural learning. To bridge this gap, we present CVBench, the first comprehensive benchmark designed to assess cross-video relational reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning three hierarchical tiers: cross-video object association (identifying shared entities), cross-video event association (linking temporal or causal event chains), and cross-video complex reasoning (integrating commonsense and domain knowledge). Built from five domain-diverse video clusters (e.g., sports, life records), the benchmark challenges models to synthesise information across dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought prompting paradigms. Key findings reveal stark performance gaps: even top models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks, compared to the 91% accuracy of human performance. Crucially, our analysis reveals fundamental bottlenecks inherent in current MLLM architectures, notably deficient inter-video context retention and poor disambiguation of overlapping entities. CVBench establishes a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation this http URL data and evaluation code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19542",
    "authors": [
      "Nannan Zhu",
      "Yonghao Dong",
      "Teng Wang",
      "Xueqian Li",
      "Shengjun Deng",
      "Yijia Wang",
      "Zheng Hong",
      "Tiantian Geng",
      "Guo Niu",
      "Hanyan Huang",
      "Xiongfei Yao",
      "Shuaiwei Jiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19547",
    "title": "Improving Recommendation Fairness via Graph Structure and Representation Augmentation",
    "abstract": "           Graph Convolutional Networks (GCNs) have become increasingly popular in recommendation systems. However, recent studies have shown that GCN-based models will cause sensitive information to disseminate widely in the graph structure, amplifying data bias and raising fairness concerns. While various fairness methods have been proposed, most of them neglect the impact of biased data on representation learning, which results in limited fairness improvement. Moreover, some studies have focused on constructing fair and balanced data distributions through data augmentation, but these methods significantly reduce utility due to disruption of user preferences. In this paper, we aim to design a fair recommendation method from the perspective of data augmentation to improve fairness while preserving recommendation utility. To achieve fairness-aware data augmentation with minimal disruption to user preferences, we propose two prior hypotheses. The first hypothesis identifies sensitive interactions by comparing outcomes of performance-oriented and fairness-aware recommendations, while the second one focuses on detecting sensitive features by analyzing feature similarities between biased and debiased representations. Then, we propose a dual data augmentation framework for fair recommendation, which includes two data augmentation strategies to generate fair augmented graphs and feature representations. Furthermore, we introduce a debiasing learning method that minimizes the dependence between the learned representations and sensitive information to eliminate bias. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed framework.         ",
    "url": "https://arxiv.org/abs/2508.19547",
    "authors": [
      "Tongxin Xu",
      "Wenqiang Liu",
      "Chenzhong Bin",
      "Cihan Xiao",
      "Zhixin Zeng",
      "Tianlong Gu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.19558",
    "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking",
    "abstract": "           Embedding models have demonstrated strong performance in tasks like clustering, retrieval, and feature extraction while offering computational advantages over generative models and cross-encoders. Benchmarks such as MTEB have shown that text embeddings from large language models (LLMs) capture rich semantic information, but their ability to reflect code-level functional semantics remains unclear. Existing studies largely focus on code clone detection, which emphasizes syntactic similarity and overlooks functional understanding. In this paper, we focus on the functional consistency of LLM code embeddings, which determines if two code snippets perform the same function regardless of syntactic differences. We propose a novel data synthesis framework called Functionality-Oriented Code Self-Evolution to construct diverse and challenging benchmarks. Specifically, we define code examples across four semantic and syntactic categories and find that existing datasets predominantly capture syntactic properties. Our framework generates four unique variations from a single code instance, providing a broader spectrum of code examples that better reflect functional differences. Extensive experiments on three downstream tasks-code clone detection, code functional consistency identification, and code retrieval-demonstrate that embedding models significantly improve their performance when trained on our evolved datasets. These results highlight the effectiveness and generalization of our data synthesis framework, advancing the functional understanding of code.         ",
    "url": "https://arxiv.org/abs/2508.19558",
    "authors": [
      "Zhuohao Li",
      "Wenqing Chen",
      "Jianxing Yu",
      "Zhichao Lu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2508.19559",
    "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference",
    "abstract": "           Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.         ",
    "url": "https://arxiv.org/abs/2508.19559",
    "authors": [
      "Rongzhi Li",
      "Ruogu Du",
      "Zefang Chu",
      "Sida Zhao",
      "Chunlei Han",
      "Zuocheng Shi",
      "Yiwen Shao",
      "Huanle Han",
      "Long Huang",
      "Zherui Liu",
      "Shufan Liu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19561",
    "title": "Energy-Equidistributed Moving Sampling Physics-informed Neural Networks for Solving Conservative Partial Differential Equations",
    "abstract": "           This paper presents a novel Energy-Equidistributed adaptive sampling framework for multi-dimensional conservative PDEs, introducing both location-based and velocity-based formulations of Energy-Equidistributed moving mesh PDEs (EMMPDEs). The framework utilizes the energy density function as the monitor function, ensuring that mesh adaptation dynamically tracks energy evolution during temporal integration. These theoretical developments are integrated with deep neural networks to establish the Energy-Equidistributed Moving Sampling Physics-Informed Neural Networks (EEMS-PINNs), which integrate physics-informed learning with energy-adaptive mesh optimization. Extensive numerical experiments demonstrate that EEMS-PINNs effectively maintain solution accuracy in long-time simulations while preserving conserved energy. The framework's robustness is further evidenced by its stable performance in non-conservative systems. The code for this paper can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19561",
    "authors": [
      "Qinjiao Gao",
      "Longzhe Xu",
      "Dongjiang Wang",
      "Ran Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.19565",
    "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection",
    "abstract": "           End-to-end object detectors offer a promising NMS-free paradigm for real-time applications, yet their high computational cost remains a significant barrier, particularly for complex scenarios like intersection traffic monitoring. To address this challenge, we propose FlowDet, a high-speed detector featuring a decoupled encoder optimization strategy applied to the DETR architecture. Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to maintain high representational power across extreme scale variations. To rigorously evaluate the model's performance in environments with severe occlusion and high object density, we collected the Intersection-Flow-5k dataset, a new challenging scene for this task. Evaluated on Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by 1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference speed by 16.2%. Our work demonstrates a new path towards building highly efficient and accurate detectors for demanding, real-world perception systems. The Intersection-Flow-5k dataset is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19565",
    "authors": [
      "Yuhang Zhao",
      "Zixing Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19570",
    "title": "Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era",
    "abstract": "           Generative models such as Large Language Models, Diffusion Models, and generative adversarial networks have recently revolutionized the creation of synthetic data, offering scalable solutions to data scarcity, privacy, and annotation challenges in data mining. This tutorial introduces the foundations and latest advances in synthetic data generation, covers key methodologies and practical frameworks, and discusses evaluation strategies and applications. Attendees will gain actionable insights into leveraging generative synthetic data to enhance data mining research and practice. More information can be found on our website: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19570",
    "authors": [
      "Dawei Li",
      "Yue Huang",
      "Ming Li",
      "Tianyi Zhou",
      "Xiangliang Zhang",
      "Huan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19573",
    "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection",
    "abstract": "           Anomaly detection in medical images is challenging due to limited annotations and a domain gap compared to natural images. Existing reconstruction methods often rely on frozen pre-trained encoders, which limits adaptation to domain-specific features and reduces localization accuracy. Prototype-based learning offers interpretability and clustering benefits but suffers from prototype collapse, where few prototypes dominate training, harming diversity and generalization. To address this, we propose a unified framework combining a trainable encoder with prototype-guided reconstruction and a novel Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum branch, enables stable domain-adaptive feature learning. A lightweight Prototype Extractor mines informative normal prototypes to guide the decoder via attention for precise reconstruction. Our loss enforces balanced prototype use through diversity constraints and per-prototype normalization, effectively preventing collapse. Experiments on multiple medical imaging benchmarks show significant improvements in representation quality and anomaly localization, outperforming prior methods. Visualizations and prototype assignment analyses further validate the effectiveness of our anti-collapse mechanism and enhanced interpretability.         ",
    "url": "https://arxiv.org/abs/2508.19573",
    "authors": [
      "Luhu Li",
      "Bowen Lin",
      "Mukhtiar Khan",
      "Shujun Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19576",
    "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding",
    "abstract": "           With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We validate the effectiveness of the proposed RL paradigm through extensive experiments on coding problems. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19576",
    "authors": [
      "Sining Zhoubian",
      "Dan Zhang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19591",
    "title": "A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation",
    "abstract": "           Centralized recommender systems encounter privacy leakage due to the need to collect user behavior and other private data. Hence, federated recommender systems (FedRec) have become a promising approach with an aggregated global model on the server. However, this distributed training paradigm suffers from embedding degradation caused by suboptimal personalization and dimensional collapse, due to the existence of sparse interactions and heterogeneous preferences. To this end, we propose a novel model-agnostic strategy for FedRec to strengthen the personalized embedding utility, which is called Personalized Local-Global Collaboration (PLGC). It is the first research in federated recommendation to alleviate the dimensional collapse issue. Particularly, we incorporate the frozen global item embedding table into local devices. Based on a Neural Tangent Kernel strategy that dynamically balances local and global information, PLGC optimizes personalized representations during forward inference, ultimately converging to user-specific preferences. Additionally, PLGC carries on a contrastive objective function to reduce embedding redundancy by dissolving dependencies between dimensions, thereby improving the backward representation learning process. We introduce PLGC as a model-agnostic personalized training strategy for federated recommendations that can be applied to existing baselines to alleviate embedding degradation. Extensive experiments on five real-world datasets have demonstrated the effectiveness and adaptability of PLGC, which outperforms various baseline algorithms.         ",
    "url": "https://arxiv.org/abs/2508.19591",
    "authors": [
      "Jiakui Shen",
      "Yunqi Mi",
      "Guoshuai Zhao",
      "Jialie Shen",
      "Xueming Qian"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.19593",
    "title": "Generalizing Monocular 3D Object Detection",
    "abstract": "           Monocular 3D object detection (Mono3D) is a fundamental computer vision task that estimates an object's class, 3D position, dimensions, and orientation from a single image. Its applications, including autonomous driving, augmented reality, and robotics, critically rely on accurate 3D environmental understanding. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios, including occlusions, datasets, object sizes, and camera parameters. To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. We address the issue of large object detection, demonstrating that it's not solely a data imbalance or receptive field problem but also a noise sensitivity issue. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D models to unseen camera heights and improve Mono3D generalization in such out-of-distribution settings.         ",
    "url": "https://arxiv.org/abs/2508.19593",
    "authors": [
      "Abhinav Kumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19595",
    "title": "A Lightweight Crowd Model for Robot Social Navigation",
    "abstract": "           Robots operating in human-populated environments must navigate safely and efficiently while minimizing social disruption. Achieving this requires estimating crowd movement to avoid congested areas in real-time. Traditional microscopic models struggle to scale in dense crowds due to high computational cost, while existing macroscopic crowd prediction models tend to be either overly simplistic or computationally intensive. In this work, we propose a lightweight, real-time macroscopic crowd prediction model tailored for human motion, which balances prediction accuracy and computational efficiency. Our approach simplifies both spatial and temporal processing based on the inherent characteristics of pedestrian flow, enabling robust generalization without the overhead of complex architectures. We demonstrate a 3.6 times reduction in inference time, while improving prediction accuracy by 3.1 %. Integrated into a socially aware planning framework, the model enables efficient and socially compliant robot navigation in dynamic environments. This work highlights that efficient human crowd modeling enables robots to navigate dense environments without costly computations.         ",
    "url": "https://arxiv.org/abs/2508.19595",
    "authors": [
      "Maryam Kazemi Eskeri",
      "Thomas Wiedemann",
      "Ville Kyrki",
      "Dominik Baumann",
      "Tomasz Piotr Kucner"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19600",
    "title": "Quantization Robustness to Input Degradations for Object Detection",
    "abstract": "           Post-training quantization (PTQ) is crucial for deploying efficient object detection models, like YOLO, on resource-constrained devices. However, the impact of reduced precision on model robustness to real-world input degradations such as noise, blur, and compression artifacts is a significant concern. This paper presents a comprehensive empirical study evaluating the robustness of YOLO models (nano to extra-large scales) across multiple precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8 (TensorRT). We introduce and evaluate a degradation-aware calibration strategy for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix of clean and synthetically degraded images. Models were benchmarked on the COCO dataset under seven distinct degradation conditions (including various types and levels of noise, blur, low contrast, and JPEG compression) and a mixed-degradation scenario. Results indicate that while Static INT8 TensorRT engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop (~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did not yield consistent, broad improvements in robustness over standard clean-data calibration across most models and degradations. A notable exception was observed for larger model scales under specific noise conditions, suggesting model capacity may influence the efficacy of this calibration approach. These findings highlight the challenges in enhancing PTQ robustness and provide insights for deploying quantized detectors in uncontrolled environments. All code and evaluation tables are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19600",
    "authors": [
      "Toghrul Karimov",
      "Hassan Imani",
      "Allan Kazakov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19608",
    "title": "Autonomous Aerial Manipulation at Arbitrary Pose in SE(3) with Robust Control and Whole-body Planning",
    "abstract": "           Aerial manipulators based on conventional multirotors can conduct manipulation only in small roll and pitch angles due to the underactuatedness of the multirotor base. If the multirotor base is capable of hovering at arbitrary orientation, the robot can freely locate itself at any point in $\\mathsf{SE}(3)$, significantly extending its manipulation workspace and enabling a manipulation task that was originally not viable. In this work, we present a geometric robust control and whole-body motion planning framework for an omnidirectional aerial manipulator (OAM). To maximize the strength of OAM, we first propose a geometric robust controller for a floating base. Since the motion of the robotic arm and the interaction forces during manipulation affect the stability of the floating base, the base should be capable of mitigating these adverse effects while controlling its 6D pose. We then design a two-step optimization-based whole-body motion planner, jointly considering the pose of the floating base and the joint angles of the robotic arm to harness the entire configuration space. The devised two-step approach facilitates real-time applicability and enhances convergence of the optimization problem with non-convex and non-Euclidean search space. The proposed approach enables the base to be stationary at any 6D pose while autonomously carrying out sophisticated manipulation near obstacles without any collision. We demonstrate the effectiveness of the proposed framework through experiments in which an OAM performs grasping and pulling of an object in multiple scenarios, including near $90^\\circ$ and even $180^\\circ$ pitch angles.         ",
    "url": "https://arxiv.org/abs/2508.19608",
    "authors": [
      "Dongjae Lee",
      "Byeongjun Kim",
      "H. Jin Kim"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.19610",
    "title": "The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts",
    "abstract": "           Question-and-answer platforms such as Stack Overflow have become an important way for software developers to share and retrieve knowledge. However, reusing poorly understood code can lead to serious problems, such as bugs or security vulnerabilities. To better understand how code comments affect the perceived helpfulness of Stack Overflow answers, we conducted an online experiment simulating a Stack Overflow environment (n=91). The results indicate that both block and inline comments are perceived as significantly more helpful than uncommented source code. Moreover, novices rated code snippets with block comments as more helpful than those with inline comments. Interestingly, other surface features, such as the position of an answer and its answer score, were considered less important. The content of Stack Overflow has been a major source for training large language models. AI-based coding assistants such as GitHub Copilot, which are based on these models, might change the way Stack Overflow is used. However, our findings have implications beyond this specific platform. First, they may help to improve the relevance of community-driven platforms such as Stack Overflow, which provide human advice and explanations of code solutions, complementing AI-based support for software developers. Second, since chat-based AI tools can be prompted to generate code in different ways, knowing which properties influence perceived helpfulness might lead to targeted prompting strategies to generate more readable code snippets.         ",
    "url": "https://arxiv.org/abs/2508.19610",
    "authors": [
      "Kathrin Figl",
      "Maria Kirchner",
      "Sebastian Baltes",
      "Michael Felderer"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.19612",
    "title": "Symbolic Equation Modeling of Composite Loads: A Kolmogorov-Arnold Network based Learning Approach",
    "abstract": "           With increasing penetration of distributed energy resources installed behind the meter, there is a growing need for adequate modelling of composite loads to enable accurate power system simulation analysis. Existing measurement based load modeling methods either fit fixed-structure physical models, which limits adaptability to evolving load mixes, or employ flexible machine learning methods which are however black boxes and offer limited interpretability. This paper presents a new learning based load modelling method based on Kolmogorov Arnold Networks towards modelling flexibility and interpretability. By actively learning activation functions on edges, KANs automatically derive free form symbolic equations that capture nonlinear relationships among measured variables without prior assumptions about load structure. Case studies demonstrate that the proposed approach outperforms other methods in both accuracy and generalization ability, while uniquely representing composite loads into transparent, interpretable mathematical equations.         ",
    "url": "https://arxiv.org/abs/2508.19612",
    "authors": [
      "Sonam Dorji",
      "Yongkang Sun",
      "Yuchen Zhang",
      "Ghavameddin Nourbakhsh",
      "Yateendra Mishra",
      "Yan Xu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.19633",
    "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection",
    "abstract": "           Rapid LLM advancements heighten fake news risks by enabling the automatic generation of increasingly sophisticated misinformation. Previous detection methods, including fine-tuned small models or LLM-based detectors, often struggle with its dynamically evolving nature. In this work, we propose a novel framework called the Symbolic Adversarial Learning Framework (SALF), which implements an adversarial training paradigm by an agent symbolic learning optimization process, rather than relying on numerical updates. SALF introduces a paradigm where the generation agent crafts deceptive narratives, and the detection agent uses structured debates to identify logical and factual flaws for detection, and they iteratively refine themselves through such adversarial interactions. Unlike traditional neural updates, we represent agents using agent symbolic learning, where learnable weights are defined by agent prompts, and simulate back-propagation and gradient descent by operating on natural language representations of weights, loss, and gradients. Experiments on two multilingual benchmark datasets demonstrate SALF's effectiveness, showing it generates sophisticated fake news that degrades state-of-the-art detection performance by up to 53.4% in Chinese and 34.2% in English on average. SALF also refines detectors, improving detection of refined content by up to 7.7%. We hope our work inspires further exploration into more robust, adaptable fake news detection systems.         ",
    "url": "https://arxiv.org/abs/2508.19633",
    "authors": [
      "Chong Tian",
      "Qirong Ho",
      "Xiuying Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.19641",
    "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
    "abstract": "           Graph-structured data, which captures non-Euclidean relationships and interactions between entities, is growing in scale and complexity. As a result, training state-of-the-art graph machine learning (GML) models have become increasingly resource-intensive, turning these models and data into invaluable Intellectual Property (IP). To address the resource-intensive nature of model training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an efficient solution by leveraging third-party cloud services for model development and management. However, deploying such models in GMLaaS also exposes them to potential threats from attackers. Specifically, while the APIs within a GMLaaS system provide interfaces for users to query the model and receive outputs, they also allow attackers to exploit and steal model functionalities or sensitive training data, posing severe threats to the safety of these GML models and the underlying graph data. To address these challenges, this survey systematically introduces the first taxonomy of threats and defenses at the level of both GML model and graph-structured data. Such a tailored taxonomy facilitates an in-depth understanding of GML IP protection. Furthermore, we present a systematic evaluation framework to assess the effectiveness of IP protection methods, introduce a curated set of benchmark datasets across various domains, and discuss their application scopes and future challenges. Finally, we establish an open-sourced versatile library named PyGIP, which evaluates various attack and defense techniques in GMLaaS scenarios and facilitates the implementation of existing benchmark methods. The library resource can be accessed at: this https URL. We believe this survey will play a fundamental role in intellectual property protection for GML and provide practical recipes for the GML community.         ",
    "url": "https://arxiv.org/abs/2508.19641",
    "authors": [
      "Lincan Li",
      "Bolin Shen",
      "Chenxi Zhao",
      "Yuxiang Sun",
      "Kaixiang Zhao",
      "Shirui Pan",
      "Yushun Dong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19647",
    "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks",
    "abstract": "           Fine-grained action localization in untrimmed sports videos presents a significant challenge due to rapid and subtle motion transitions over short durations. Existing supervised and weakly supervised solutions often rely on extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios. In this work, we introduce a lightweight and unsupervised skeleton-based action localization pipeline that leverages spatio-temporal graph neural representations. Our approach pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with blockwise partitions, enabling it to learn intrinsic motion dynamics without any manual labeling. At inference, we define a novel Action Dynamics Metric (ADM), computed directly from low-dimensional ASTGCN embeddings, which detects motion boundaries by identifying inflection points in its curvature profile. Our method achieves a mean Average Precision (mAP) of 82.66% and average localization latency of 29.09 ms on the DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving footage without retraining, demonstrating its practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.         ",
    "url": "https://arxiv.org/abs/2508.19647",
    "authors": [
      "Bikash Kumar Badatya",
      "Vipul Baghel",
      "Ravi Hegde"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19649",
    "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising",
    "abstract": "           Image denoising is a fundamental challenge in computer vision, with applications in photography and medical imaging. While deep learning-based methods have shown remarkable success, their reliance on specific noise distributions limits generalization to unseen noise types and levels. Existing approaches attempt to address this with extensive training data and high computational resources but they still suffer from overfitting. To address these issues, we conduct image denoising by utilizing dynamically generated kernels via efficient operations. This approach helps prevent overfitting and improves resilience to unseen noise. Specifically, our method leverages a Feature Extraction Module for robust noise-invariant features, Global Statistics and Local Correlation Modules to capture comprehensive noise characteristics and structural correlations. The Kernel Prediction Module then employs these cues to produce pixel-wise varying kernels adapted to local structures, which are then applied iteratively for denoising. This ensures both efficiency and superior restoration quality. Despite being trained on single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse noise types and levels, demonstrating the promise of iterative dynamic filtering for practical image denoising.         ",
    "url": "https://arxiv.org/abs/2508.19649",
    "authors": [
      "Dongjin Kim",
      "Jaekyun Ko",
      "Muhammad Kashif Ali",
      "Tae Hyun Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19651",
    "title": "Scalable Object Detection in the Car Interior With Vision Foundation Models",
    "abstract": "           AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and this http URL analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.         ",
    "url": "https://arxiv.org/abs/2508.19651",
    "authors": [
      "B\u00e1lint M\u00e9sz\u00e1ros",
      "Ahmet Firintepe",
      "Sebastian Schmidt",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19664",
    "title": "A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement",
    "abstract": "           Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics by providing a comprehensive view of the retina. However, it often suffers from quality-degrading factors such as blurring and uneven illumination, which obscure fine details and mask pathological information. While numerous retinal image enhancement methods have been proposed for other fundus imageries, they often fail to address the unique requirements in UWF, particularly the need to preserve pathological details. In this paper, we propose a novel frequency-aware self-supervised learning method for UWF image enhancement. It incorporates frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. An asymmetric channel integration operation is introduced in the former module, so as to combine global and local views by leveraging high- and low-frequency information, ensuring the preservation of fine and broader structural details. In addition, a color preservation unit is proposed in the latter Retinex-based module, to provide multi-scale spatial and frequency information, enabling accurate illumination estimation and correction. Experimental results demonstrate that the proposed work not only enhances visualization quality but also improves disease diagnosis performance by restoring and correcting fine local details and uneven intensity. To the best of our knowledge, this work is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.         ",
    "url": "https://arxiv.org/abs/2508.19664",
    "authors": [
      "Weicheng Liao",
      "Zan Chen",
      "Jianyang Xie",
      "Yalin Zheng",
      "Yuhui Ma",
      "Yitian Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19672",
    "title": "$\\mathcal{C}^1$-approximation with rational functions and rational neural networks",
    "abstract": "           We show that suitably regular functions can be approximated in the $\\mathcal{C}^1$-norm both with rational functions and rational neural networks, including approximation rates with respect to width and depth of the network, and degree of the rational functions. As consequence of our results, we further obtain $\\mathcal{C}^1$-approximation results for rational neural networks with the $\\text{EQL}^\u00f7$ and ParFam architecture, both of which are important in particular in the context of symbolic regression for physical law learning.         ",
    "url": "https://arxiv.org/abs/2508.19672",
    "authors": [
      "Erion Morina",
      "Martin Holler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.19688",
    "title": "SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction",
    "abstract": "           Monocular texture 3D human reconstruction aims to create a complete 3D digital avatar from just a single front-view human RGB image. However, the geometric ambiguity inherent in a single 2D image and the scarcity of 3D human training data are the main obstacles limiting progress in this field. To address these issues, current methods employ prior geometric estimation networks to derive various human geometric forms, such as the SMPL model and normal maps. However, they struggle to integrate these modalities effectively, leading to view inconsistencies, such as facial distortions. To this end, we propose a two-process 3D human reconstruction framework, SAT, which seamlessly learns various prior geometries in a unified manner and reconstructs high-quality textured 3D avatars as the final output. To further facilitate geometry learning, we introduce a Supervisor Feature Regularization module. By employing a multi-view network with the same structure to provide intermediate features as training supervision, these varied geometric priors can be better fused. To tackle data scarcity and further improve reconstruction quality, we also propose an Online Animation Augmentation module. By building a one-feed-forward animation network, we augment a massive number of samples from the original 3D human data online for model training. Extensive experiments on two benchmarks show the superiority of our approach compared to state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2508.19688",
    "authors": [
      "Gangjian Zhang",
      "Jian Shu",
      "Nanjie Yao",
      "Hao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19698",
    "title": "Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators",
    "abstract": "           The rapid advance of deep generative models such as GANs and diffusion networks now produces images that are virtually indistinguishable from genuine photographs, undermining media forensics and biometric security. Supervised detectors quickly lose effectiveness on unseen generators or after adversarial post-processing, while existing unsupervised methods that rely on low-level statistical cues remain fragile. We introduce a physics-inspired, model-agnostic detector that treats synthetic-image identification as a community-detection problem on a sparse weighted graph. Image features are first extracted with pretrained CNNs and reduced to 32 dimensions, each feature vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities are transformed into edge couplings calibrated at the Nishimori temperature, producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum exhibits a characteristic gap when genuine community structure (real images) is present. Synthetic images violate the Nishimori symmetry and therefore lack such gaps. We validate the approach on binary tasks cat versus dog and male versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic counterparts generated by GANs and diffusion models. Without any labeled synthetic data or retraining of the feature extractor, the detector achieves over 94% accuracy. Spectral analysis shows multiple well separated gaps for real image sets and a collapsed spectrum for generated ones. Our contributions are threefold: a novel LDPC graph construction that embeds deep image features, an analytical link between Nishimori temperature RBIM and the Bethe-Hessian spectrum providing a Bayes optimal detection criterion; and a practical, unsupervised synthetic image detector robust to new generative architectures. Future work will extend the framework to video streams and multi-class anomaly detection.         ",
    "url": "https://arxiv.org/abs/2508.19698",
    "authors": [
      "V. S. Usatyuk",
      "D. A. Sapozhnikov",
      "S. I. Egorov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Spectral Theory (math.SP)"
    ]
  },
  {
    "id": "arXiv:2508.19709",
    "title": "Metric spaces of walks and Lipschitz duality on graphs",
    "abstract": "           We study the metric structure of walks on graphs, understood as Lipschitz sequences. To this end, a weighted metric is introduced to handle sequences, enabling the definition of distances between walks based on stepwise vertex distances and weighted norms. We analyze the main properties of these metric spaces, which provides the foundation for the analysis of weaker forms of instruments to measure relative distances between walks: proximities. We provide some representation formulas for such proximities under different assumptions and provide explicit constructions for these cases. The resulting metric framework allows the use of classical tools from metric modeling, such as the extension of Lipschitz functions from subspaces of walks, which permits extending proximity functions while preserving fundamental properties via the mentioned representations. Potential applications include the estimation of proximities and the development of reinforcement learning strategies based on exploratory walks, offering a robust approach to Lipschitz regression on network structures.         ",
    "url": "https://arxiv.org/abs/2508.19709",
    "authors": [
      "R. Arnau",
      "A. Gonz\u00e1lez Cort\u00e9s",
      "E.A. S\u00e1nchez P\u00e9rez",
      "S. Sanjuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)"
    ]
  },
  {
    "id": "arXiv:2508.19730",
    "title": "Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning",
    "abstract": "           The increasing realism and accessibility of deepfakes have raised critical concerns about media authenticity and information integrity. Despite recent advances, deepfake detection models often struggle to generalize beyond their training distributions, particularly when applied to media content found in the wild. In this work, we present a robust video deepfake detection framework with strong generalization that takes advantage of the rich facial representations learned by face foundation models. Our method is built on top of FSFM, a self-supervised model trained on real face data, and is further fine-tuned using an ensemble of deepfake datasets spanning both face-swapping and face-reenactment manipulations. To enhance discriminative power, we incorporate triplet loss variants during training, guiding the model to produce more separable embeddings between real and fake samples. Additionally, we explore attribution-based supervision schemes, where deepfakes are categorized by manipulation type or source dataset, to assess their impact on generalization. Extensive experiments across diverse evaluation benchmarks demonstrate the effectiveness of our approach, especially in challenging real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2508.19730",
    "authors": [
      "Stelios Mylonas",
      "Symeon Papadopoulos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19737",
    "title": "InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections",
    "abstract": "           Graph partitioning (GP), a.k.a. community detection, is a classic problem that divides nodes of a graph into densely-connected blocks. From a perspective of graph signal processing, we find that graph Laplacian with a negative correction can derive graph frequencies beyond the conventional range $[0, 2]$. To explore whether the low-frequency information beyond this range can encode more informative properties about community structures, we propose InfraredGP. It (\\romannumeral1) adopts a spectral GNN as its backbone combined with low-pass filters and a negative correction mechanism, (\\romannumeral2) only feeds random inputs to this backbone, (\\romannumeral3) derives graph embeddings via one feed-forward propagation (FFP) without any training, and (\\romannumeral4) obtains feasible GP results by feeding the derived embeddings to BIRCH. Surprisingly, our experiments demonstrate that based solely on the negative correction mechanism that amplifies low-frequency information beyond $[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard clustering modules (e.g., BIRCH) and obtain high-quality results for GP without any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate InfraredGP for both static and streaming GP, where InfraredGP can achieve much better efficiency (e.g., 16x-23x faster) and competitive quality over various baselines. We have made our code public at this https URL ",
    "url": "https://arxiv.org/abs/2508.19737",
    "authors": [
      "Meng Qin",
      "Weihua Li",
      "Jinqiang Cui",
      "Sen Pei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.19742",
    "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection",
    "abstract": "           Line segment detection in images has been studied for several decades. Existing line segment detectors can be roughly divided into two categories: generic line segment detectors and wireframe line segment detectors. Generic line segment detectors aim to detect all meaningful line segments in images and traditional approaches usually fall into this category. Recent deep learning based approaches are mostly wireframe line segment detectors. They detect only line segments that are geometrically meaningful and have large spatial support. Due to the difference in the aim of design, the performance of generic line segment detectors for the task of wireframe line segment detection won't be satisfactory, and vice versa. In this work, we propose a robust framework that can be used for both generic line segment detection and wireframe line segment detection. The proposed method is an improved version of the Pixel Orientation Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments from edge strength maps, and can be combined with any edge detector. We show in our experiments that by combining the proposed POEv2 with an efficient edge detector, it achieves state-of-the-art performance on three publicly available datasets.         ",
    "url": "https://arxiv.org/abs/2508.19742",
    "authors": [
      "Chenguang Liu",
      "Chisheng Wang",
      "Yuhua Cai",
      "Chuanhua Zhu",
      "Qingquan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19746",
    "title": "SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection",
    "abstract": "           Segment Anything Model (SAM) has demonstrated remarkable capabilities in solving light field salient object detection (LF SOD). However, most existing models tend to neglect the extraction of prompt information under this task. Meanwhile, traditional models ignore the analysis of frequency-domain information, which leads to small objects being overwhelmed by noise. In this paper, we put forward a novel model called self-prompting light field segment anything model (SPLF-SAM), equipped with unified multi-scale feature embedding block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is capable of identifying multiple objects of varying sizes, while MAFA, by learning frequency features, effectively prevents small objects from being overwhelmed by noise. Extensive experiments have demonstrated the superiority of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19746",
    "authors": [
      "Qiyao Xu",
      "Qiming Wu",
      "Xiaowei Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19762",
    "title": "BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions",
    "abstract": "           Pollinator insects such as honeybees and bumblebees are vital to global food production and ecosystem stability, yet their populations are declining due to increasing anthropogenic and environmental stressors. To support scalable, automated pollinator monitoring, we introduce BuzzSet, a new large-scale dataset of high-resolution pollinator images collected in real agricultural field conditions. BuzzSet contains 7856 manually verified and labeled images, with over 8000 annotated instances across three classes: honeybees, bumblebees, and unidentified insects. Initial annotations were generated using a YOLOv12 model trained on external data and refined via human verification using open-source labeling tools. All images were preprocessed into 256~$\\times$~256 tiles to improve the detection of small insects. We provide strong baselines using the RF-DETR transformer-based object detector. The model achieves high F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively, with confusion matrix results showing minimal misclassification between these categories. The unidentified class remains more challenging due to label ambiguity and lower sample frequency, yet still contributes useful insights for robustness evaluation. Overall detection quality is strong, with a best mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object detection, class separation under label noise, and ecological computer vision.         ",
    "url": "https://arxiv.org/abs/2508.19762",
    "authors": [
      "Ahmed Emam",
      "Mohamed Elbassiouny",
      "Julius Miller",
      "Patrick Donworth",
      "Sabine Seidel",
      "Ribana Roscher"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19768",
    "title": "Burst: Collaborative Curation in Connected Social Media Communities",
    "abstract": "           Positive social interactions can occur in groups of many shapes and sizes, spanning from small and private to large and open. However, social media tends to binarize our experiences into either isolated small groups or into large public squares. In this paper, we introduce Burst, a social media design that allows users to share and curate content between many spaces of varied size and composition. Users initially post content to small trusted groups, who can then burst that content, routing it to the groups that would be the best audience. We instantiate this approach into a mobile phone application, and demonstrate through a ten-day field study (N=36) that Burst enabled a participatory curation culture. With this work, we aim to articulate potential new design directions for social media sharing.         ",
    "url": "https://arxiv.org/abs/2508.19768",
    "authors": [
      "Yutong Zhang",
      "Taeuk Kang",
      "Sydney Yeh",
      "Anavi Baddepudi",
      "Lindsay Popowski",
      "Tiziano Piccardi",
      "Michael S. Bernstein"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.19774",
    "title": "The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again",
    "abstract": "           Pickle deserialization vulnerabilities have persisted throughout Python's history, remaining widely recognized yet unresolved. Due to its ability to transparently save and restore complex objects into byte streams, many AI/ML frameworks continue to adopt pickle as the model serialization protocol despite its inherent risks. As the open-source model ecosystem grows, model-sharing platforms such as Hugging Face have attracted massive participation, significantly amplifying the real-world risks of pickle exploitation and opening new avenues for model supply chain poisoning. Although several state-of-the-art scanners have been developed to detect poisoned models, their incomplete understanding of the poisoning surface leaves the detection logic fragile and allows attackers to bypass them. In this work, we present the first systematic disclosure of the pickle-based model poisoning surface from both model loading and risky function perspectives. Our research demonstrates how pickle-based model poisoning can remain stealthy and highlights critical gaps in current scanning solutions. On the model loading surface, we identify 22 distinct pickle-based model loading paths across five foundational AI/ML frameworks, 19 of which are entirely missed by existing scanners. We further develop a bypass technique named Exception-Oriented Programming (EOP) and discover 9 EOP instances, 7 of which can bypass all scanners. On the risky function surface, we discover 133 exploitable gadgets, achieving almost a 100% bypass rate. Even against the best-performing scanner, these gadgets maintain an 89% bypass rate. By systematically revealing the pickle-based model poisoning surface, we achieve practical and robust bypasses against real-world scanners. We responsibly disclose our findings to corresponding vendors, receiving acknowledgments and a $6000 bug bounty.         ",
    "url": "https://arxiv.org/abs/2508.19774",
    "authors": [
      "Tong Liu",
      "Guozhu Meng",
      "Peng Zhou",
      "Zizhuang Deng",
      "Shuaiyin Yao",
      "Kai Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19804",
    "title": "A bag of tricks for real-time Mitotic Figure detection",
    "abstract": "           Mitotic figure (MF) detection in histopathology images is challenging due to large variations in slide scanners, staining protocols, tissue types, and the presence of artifacts. This paper presents a collection of training techniques - a bag of tricks - that enable robust, real-time MF detection across diverse domains. We build on the efficient RTMDet single stage object detector to achieve high inference speed suitable for clinical deployment. Our method addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling, and careful augmentation. Additionally, we employ targeted, hard negative mining on necrotic and debris tissue to reduce false positives. In a grouped 5-fold cross-validation across multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025 challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81, outperforming larger models and demonstrating adaptability to new, unfamiliar domains. The proposed solution offers a practical trade-off between accuracy and speed, making it attractive for real-world clinical adoption.         ",
    "url": "https://arxiv.org/abs/2508.19804",
    "authors": [
      "Christian Marzahl",
      "Brian Napora"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19819",
    "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning",
    "abstract": "           Gradient inversion attacks have garnered attention for their ability to compromise privacy in federated learning. However, many studies consider attacks with the model in inference mode, where training-time behaviors like dropout are disabled and batch normalization relies on fixed statistics. In this work, we systematically analyze how architecture and training behavior affect vulnerability, including the first in-depth study of inference-mode clients, which we show dramatically simplifies inversion. To assess attack feasibility under more realistic conditions, we turn to clients operating in standard training mode. In this setting, we find that successful attacks are only possible when several architectural conditions are met simultaneously: models must be shallow and wide, use skip connections, and, critically, employ pre-activation normalization. We introduce two novel attacks against models in training-mode with varying attacker knowledge, achieving state-of-the-art performance under realistic training conditions. We extend these efforts by presenting the first attack on a production-grade object-detection model. Here, to enable any visibly identifiable leakage, we revert to the lenient inference mode setting and make multiple architectural modifications to increase model vulnerability, with the extent of required changes highlighting the strong inherent robustness of such architectures. We conclude this work by offering the first comprehensive mapping of settings, clarifying which combinations of architectural choices and operational modes meaningfully impact privacy. Our analysis provides actionable insight into when models are likely vulnerable, when they appear robust, and where subtle leakage may persist. Together, these findings reframe how gradient inversion risk should be assessed in future research and deployment scenarios.         ",
    "url": "https://arxiv.org/abs/2508.19819",
    "authors": [
      "Viktor Valadi",
      "Mattias \u00c5kesson",
      "Johan \u00d6stman",
      "Salman Toor",
      "Andreas Hellander"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19830",
    "title": "Gradient Rectification for Robust Calibration under Distribution Shift",
    "abstract": "           Deep neural networks often produce overconfident predictions, undermining their reliability in safety-critical applications. This miscalibration is further exacerbated under distribution shift, where test data deviates from the training distribution due to environmental or acquisition changes. While existing approaches improve calibration through training-time regularization or post-hoc adjustment, their reliance on access to or simulation of target domains limits their practicality in real-world scenarios. In this paper, we propose a novel calibration framework that operates without access to target domain information. From a frequency-domain perspective, we identify that distribution shifts often distort high-frequency visual cues exploited by deep models, and introduce a low-frequency filtering strategy to encourage reliance on domain-invariant features. However, such information loss may degrade In-Distribution (ID) calibration performance. Therefore, we further propose a gradient-based rectification mechanism that enforces ID calibration as a hard constraint during optimization. Experiments on synthetic and real-world shifted datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method significantly improves calibration under distribution shift while maintaining strong in-distribution performance.         ",
    "url": "https://arxiv.org/abs/2508.19830",
    "authors": [
      "Yilin Zhang",
      "Cai Xu",
      "You Wu",
      "Ziyu Guan",
      "Wei Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19842",
    "title": "Symplectic convolutional neural networks",
    "abstract": "           We propose a new symplectic convolutional neural network (CNN) architecture by leveraging symplectic neural networks, proper symplectic decomposition, and tensor techniques. Specifically, we first introduce a mathematically equivalent form of the convolution layer and then, using symplectic neural networks, we demonstrate a way to parameterize the layers of the CNN to ensure that the convolution layer remains symplectic. To construct a complete autoencoder, we introduce a symplectic pooling layer. We demonstrate the performance of the proposed neural network on three examples: the wave equation, the nonlinear Schr\u00f6dinger (NLS) equation, and the sine-Gordon equation. The numerical results indicate that the symplectic CNN outperforms the linear symplectic autoencoder obtained via proper symplectic decomposition.         ",
    "url": "https://arxiv.org/abs/2508.19842",
    "authors": [
      "S\u00fcleyman Y\u0131ld\u0131z",
      "Konrad Janik",
      "Peter Benner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19855",
    "title": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning",
    "abstract": "           Graph retrieval-augmented generation (GraphRAG) has effectively enhanced large language models in complex reasoning by organizing fragmented knowledge into explicitly structured graphs. Prior efforts have been made to improve either graph construction or graph retrieval in isolation, yielding suboptimal performance, especially when domain shifts occur. In this paper, we propose a vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the entire framework as an intricate integration. Specifically, (i) a seed graph schema is introduced to bound the automatic extraction agent with targeted entity types, relations and attribute types, also continuously expanded for scalability over unseen domains; (ii) To obtain higher-level knowledge upon the schema, we develop novel dually-perceived community detection, fusing structural topology with subgraph semantics for comprehensive knowledge organization. This naturally yields a hierarchical knowledge tree that supports both top-down filtering and bottom-up reasoning with community summaries; (iii) An agentic retriever is designed to interpret the same graph schema to transform complex queries into tractable and parallel sub-queries. It iteratively performs reflection for more advanced reasoning; (iv) To alleviate the knowledge leaking problem in pre-trained LLM, we propose a tailored anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures the real performance of the GraphRAG frameworks. Extensive experiments across six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG, remarkably moving the Pareto frontier with up to 90.71% saving of token costs and 16.62% higher accuracy over state-of-the-art baselines. The results indicate our adaptability, allowing seamless domain transfer with minimal intervention on schema.         ",
    "url": "https://arxiv.org/abs/2508.19855",
    "authors": [
      "Junnan Dong",
      "Siyu An",
      "Yifei Yu",
      "Qian-Wen Zhang",
      "Linhao Luo",
      "Xiao Huang",
      "Yunsheng Wu",
      "Di Yin",
      "Xing Sun"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.19862",
    "title": "Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction",
    "abstract": "           Personalized, accurate prediction of aortic aneurysm progression is essential for timely intervention but remains challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN introduces a dual-branch architecture combining a novel local KNN-based convolutional network (KCN) to preserve fine-grained geometric details and a global graph convolutional network (GCN) to capture long-range structural context, overcoming the over-smoothing limitations of deep GCNs. A dedicated condition branch encodes clinical attributes (age, sex) and the target time interval to generate anatomically plausible, temporally controlled predictions, enabling retrospective and prospective modeling. We curated TAAMesh, a new longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive experiments demonstrate that MCMeshGAN consistently outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation. This framework offers a robust step toward clinically deployable, personalized 3D disease trajectory modeling. The source code for MCMeshGAN and the baseline methods is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19862",
    "authors": [
      "Long Chen",
      "Ashiv Patel",
      "Mengyun Qiao",
      "Mohammad Yousuf Salmasi",
      "Salah A. Hammouche",
      "Vasilis Stavrinides",
      "Jasleen Nagi",
      "Soodeh Kalaie",
      "Xiao Yun Xu",
      "Wenjia Bai",
      "Declan P. O'Regan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19864",
    "title": "Self-supervised structured object representation learning",
    "abstract": "           Self-supervised learning (SSL) has emerged as a powerful technique for learning visual representations. While recent SSL approaches achieve strong results in global image understanding, they are limited in capturing the structured representation in scenes. In this work, we propose a self-supervised approach that progressively builds structured visual representations by combining semantic grouping, instance level separation, and hierarchical structuring. Our approach, based on a novel ProtoScale module, captures visual elements across multiple spatial scales. Unlike common strategies like DINO that rely on random cropping and global embeddings, we preserve full scene context across augmented views to improve performance in dense prediction tasks. We validate our method on downstream object detection tasks using a combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results show that our method learns object centric representations that enhance supervised object detection and outperform the state-of-the-art methods, even when trained with limited annotated data and fewer fine-tuning epochs.         ",
    "url": "https://arxiv.org/abs/2508.19864",
    "authors": [
      "Oussama Hadjerci",
      "Antoine Letienne",
      "Mohamed Abbas Hedjazi",
      "Adel Hafiane"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19866",
    "title": "TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations",
    "abstract": "           With the introduction of vehicles with autonomous capabilities on public roads, predicting pedestrian crossing intention has emerged as an active area of research. The task of predicting pedestrian crossing intention involves determining whether pedestrians in the scene are likely to cross the road or not. In this work, we propose TrajFusionNet, a novel transformer-based model that combines future pedestrian trajectory and vehicle speed predictions as priors for predicting crossing intention. TrajFusionNet comprises two branches: a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM branch learns from a sequential representation of the observed and predicted pedestrian trajectory and vehicle speed. Complementarily, the VAM branch enables learning from a visual representation of the predicted pedestrian trajectory by overlaying predicted pedestrian bounding boxes onto scene images. By utilizing a small number of lightweight modalities, TrajFusionNet achieves the lowest total inference time (including model runtime and data preprocessing) among current state-of-the-art approaches. In terms of performance, it achieves state-of-the-art results across the three most commonly used datasets for pedestrian crossing intention prediction.         ",
    "url": "https://arxiv.org/abs/2508.19866",
    "authors": [
      "Fran\u00e7ois G. Landry",
      "Moulay A. Akhloufi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19875",
    "title": "Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network",
    "abstract": "           Sky background subtraction is a critical step in Multi-objective Fiber spectra process. However, current subtraction relies mainly on sky fiber spectra to build Super Sky. These average spectra are lacking in the modeling of the environment surrounding the objects. To address this issue, a sky background estimation model: Sky background building based on Mutual Information (SMI) is proposed. SMI based on mutual information and incremental training approach. It utilizes spectra from all fibers in the plate to estimate the sky background. SMI contains two main networks, the first network applies a wavelength calibration module to extract sky features from spectra, and can effectively solve the feature shift problem according to the corresponding emission position. The second network employs an incremental training approach to maximize mutual information between representations of different spectra to capturing the common component. Then, it minimizes the mutual information between adjoining spectra representations to obtain individual components. This network yields an individual sky background at each location of the object. To verify the effectiveness of the method in this paper, we conducted experiments on the spectra of LAMOST. Results show that SMI can obtain a better object sky background during the observation, especially in the blue end.         ",
    "url": "https://arxiv.org/abs/2508.19875",
    "authors": [
      "Hui Zhang",
      "Jianghui Cai",
      "Haifeng Yang",
      "Ali Luo",
      "Yuqing Yang",
      "Xiao Kong",
      "Zhichao Ding",
      "Lichan Zhou",
      "Qin Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19883",
    "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula",
    "abstract": "           The use of inappropriate language -- such as outdated, exclusionary, or non-patient-centered terms -- medical instructional materials can significantly influence clinical training, patient interactions, and health outcomes. Despite their reputability, many materials developed over past decades contain examples now considered inappropriate by current medical standards. Given the volume of curricular content, manually identifying instances of inappropriate use of language (IUL) and its subcategories for systematic review is prohibitively costly and impractical. To address this challenge, we conduct a first-in-class evaluation of small language models (SLMs) fine-tuned on labeled data and pre-trained LLMs with in-context learning on a dataset containing approximately 500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL classifier, (2) subcategory-specific binary classifiers, (3) a multilabel classifier, and (4) a two-stage hierarchical pipeline for general IUL detection followed by multilabel classification. For LLMs, we consider variations of prompts that include subcategory definitions and/or shots. We found that both LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed by SLMs. While the multilabel classifier performs best on annotated data, supplementing training with unflagged excerpts as negative examples boosts the specific classifiers' AUC by up to 25%, making them most effective models for mitigating harmful language in medical curricula.         ",
    "url": "https://arxiv.org/abs/2508.19883",
    "authors": [
      "Chiman Salavati",
      "Shannon Song",
      "Scott A. Hale",
      "Roberto E. Montenegro",
      "Shiri Dori-Hacohen",
      "Fabricio Murai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.19884",
    "title": "Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have shown remarkable performance in structured data modeling tasks such as node classification. However, mainstream approaches generally rely on a large number of trainable parameters and fixed aggregation rules, making it difficult to adapt to graph data with strong structural heterogeneity and complex feature distributions. This often leads to over-smoothing of node representations and semantic degradation. To address these issues, this paper proposes a parameter-free graph neural network framework based on structural diversity, namely SDGNN (Structural-Diversity Graph Neural Network). The framework is inspired by structural diversity theory and designs a unified structural-diversity message passing mechanism that simultaneously captures the heterogeneity of neighborhood structures and the stability of feature semantics, without introducing additional trainable parameters. Unlike traditional parameterized methods, SDGNN does not rely on complex model training, but instead leverages complementary modeling from both structure-driven and feature-driven perspectives, thereby effectively improving adaptability across datasets and scenarios. Experimental results show that on eight public benchmark datasets and an interdisciplinary PubMed citation network, SDGNN consistently outperforms mainstream GNNs under challenging conditions such as low supervision, class imbalance, and cross-domain transfer. This work provides a new theoretical perspective and general approach for the design of parameter-free graph neural networks, and further validates the importance of structural diversity as a core signal in graph representation learning. To facilitate reproducibility and further research, the full implementation of SDGNN has been released at: this https URL ",
    "url": "https://arxiv.org/abs/2508.19884",
    "authors": [
      "Mingyue Kong",
      "Yinglong Zhang",
      "Chengda Xu",
      "Xuewen Xia",
      "Xing Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19906",
    "title": "Streamlining the Development of Active Learning Methods in Real-World Object Detection",
    "abstract": "           Active learning (AL) for real-world object detection faces computational and reliability challenges that limit practical deployment. Developing new AL methods requires training multiple detectors across iterations to compare against existing approaches. This creates high costs for autonomous driving datasets where the training of one detector requires up to 282 GPU hours. Additionally, AL method rankings vary substantially across validation sets, compromising reliability in safety-critical transportation systems. We introduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses these challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without requiring detector training by measuring similarity between training sets and target domains using object-level features. This enables the elimination of ineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables the selection of representative validation sets for robust evaluation. We validate our similarity-based approach on three autonomous driving datasets (KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with two detector architectures (EfficientDet, YOLOv3). This work is the first to unify AL training and evaluation strategies in object detection based on object similarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object crops, and integrates with existing AL pipelines. This provides a practical framework for deploying AL in real-world applications where computational efficiency and evaluation reliability are critical. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19906",
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein",
      "Michelle Karg",
      "Christian Wirth",
      "Sahin Albayrak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19907",
    "title": "GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs",
    "abstract": "           Given a signed bipartite graph (SBG) G with two disjoint node sets U and V, the goal of link sign prediction is to predict the signs of potential links connecting U and V based on known positive and negative edges in G. The majority of existing solutions towards link sign prediction mainly focus on unipartite signed graphs, which are sub-optimal due to the neglect of node heterogeneity and unique bipartite characteristics of SBGs. To this end, recent studies adapt graph neural networks to SBGs by introducing message-passing schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node pairs. However, the fundamental spectral convolutional operators were originally designed for positive links in unsigned graphs, and thus, are not optimal for inferring missing positive or negative links from known ones in SBGs. Motivated by this, this paper proposes GegenNet, a novel and effective spectral convolutional neural network model for link sign prediction in SBGs. In particular, GegenNet achieves enhanced model capacity and high predictive accuracy through three main technical contributions: (i) fast and theoretically grounded spectral decomposition techniques for node feature initialization; (ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and (iii) multi-layer sign-aware spectral convolutional networks alternating Gegenbauer polynomial filters with positive and negative edges. Our extensive empirical studies reveal that GegenNet can achieve significantly superior performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign prediction compared to 11 strong competitors over 6 benchmark SBG datasets.         ",
    "url": "https://arxiv.org/abs/2508.19907",
    "authors": [
      "Hewen Wang",
      "Renchi Yang",
      "Xiaokui Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.19913",
    "title": "Internally-Convex Drawings of Outerplanar Graphs in Small Area",
    "abstract": "           A well-known result by Kant [Algorithmica, 1996] implies that n-vertex outerplane graphs admit embedding-preserving planar straight-line grid drawings where the internal faces are convex polygons in $O(n^2)$ area. In this paper, we present an algorithm to compute such drawings in $O(n^{1.5})$ area. We also consider outerplanar drawings in which the internal faces are required to be strictly-convex polygons. In this setting, we consider outerplanar graphs whose weak dual is a path and give a drawing algorithm that achieves $\\Theta(nk^2)$ area, where $k$ is the maximum size of an internal facial cycle.         ",
    "url": "https://arxiv.org/abs/2508.19913",
    "authors": [
      "Michael A. Bekos",
      "Giordano Da Lozzo",
      "Fabrizio Frati",
      "Giuseppe Liotta",
      "Antonios Symvonis"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2508.19920",
    "title": "Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks",
    "abstract": "           Recently, researchers have explored control methods that embrace nonlinear dynamic coupling instead of suppressing it. Such designs leverage dynamical coupling for communication between different parts of the robot. Morphological communication refers to when those dynamics can be used as an emergent data bus to facilitate coordination among independent controller modules within the same robot. Previous research with tensegrity-based robot designs has shown that evolutionary learning models that evolve spiking neural networks (SNN) as robot control mechanisms are effective for controlling non-rigid robots. Our own research explores the emergence of morphological communication in an SNN-based simulated soft robot in theEvoGym environment.         ",
    "url": "https://arxiv.org/abs/2508.19920",
    "authors": [
      "Matthew Meek",
      "Guy Tallent",
      "Thomas Breimer",
      "James Gaskell",
      "Abhay Kashyap",
      "Atharv Tekurkar",
      "Jonathan Fischman",
      "Luodi Wang",
      "Viet-Dung Nguyen",
      "John Rieffel"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.19921",
    "title": "An assessment of estimation models and investment gaps for the deployment of high-speed broadband networks in NUTS3 regions to meet the objectives of the European Gigabit Society",
    "abstract": "           This paper analyses the deployment of high speed broadband networks in the European Union (EU). Its aim is to assess the investment required to meet the targets set by the European Commission (EC) for 2025, within the framework of the European Gigabit Society (EGS). This plan aims to ensure the availability and take up of very high capacity fixed and wireless networks, in both urban and rural areas, among households and the main socioeconomic drivers. The estimation model presented here uses a methodology supported by data at the local (NUTS3) level to give a bottom up estimation of the investment gap for each of the EGS objectives, using three different scenarios depending on the mix of wired and wireless technologies offered. The methodology and estimation model used in the paper are examined against other examples and assumptions available in the literature. We also offer a dynamic perspective on the analysis of the evolution of this investment gap over the years 2017 2019, which includes an assessment of the usefulness of these estimation models.         ",
    "url": "https://arxiv.org/abs/2508.19921",
    "authors": [
      "Ferrandis Jesus",
      "Ramos Sergio",
      "Feijoo Claudio"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2508.19924",
    "title": "FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification",
    "abstract": "           Network traffic classification using pre-training models has shown promising results, but existing methods struggle to capture packet structural characteristics, flow-level behaviors, hierarchical protocol semantics, and inter-packet contextual relationships. To address these challenges, we propose FlowletFormer, a BERT-based pre-training model specifically designed for network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware Traffic Representation Model for segmenting traffic into semantically meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining Tasks to enhance both inter-packet and inter-flow learning. Experimental results demonstrate that FlowletFormer significantly outperforms existing methods in the effectiveness of traffic representation, classification accuracy, and few-shot learning capability. Moreover, by effectively integrating domain-specific network knowledge, FlowletFormer shows better comprehension of the principles of network transmission (e.g., stateful connections of TCP), providing a more robust and trustworthy framework for traffic analysis.         ",
    "url": "https://arxiv.org/abs/2508.19924",
    "authors": [
      "Liming Liu",
      "Ruoyu Li",
      "Qing Li",
      "Meijia Hou",
      "Yong Jiang",
      "Mingwei Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19926",
    "title": "FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control",
    "abstract": "           Unified physics-based humanoid controllers are pivotal for robotics and character animation, yet models that excel on gentle, everyday motions still stumble on explosive actions, hampering real-world deployment. We bridge this gap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts), an end-to-end framework composed of frame-accelerated augmentation, a robust base controller, and a residual mixture-of-experts (MoE). Frame-accelerated augmentation exposes the model to high-velocity pose changes by widening inter-frame gaps. The base controller reliably tracks everyday low-dynamic motions, while the residual MoE adaptively allocates additional network capacity to handle challenging high-dynamic actions, significantly enhancing tracking accuracy. In the absence of a public benchmark, we curate the High-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically plausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and lowers global mean per-joint position error by 14.6\\% relative to the baseline, while preserving near-perfect accuracy on low-dynamic motions. These results establish FARM as a new baseline for high-dynamic humanoid control and introduce the first open benchmark dedicated to this challenge. The code and dataset will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.19926",
    "authors": [
      "Tan Jing",
      "Shiting Chen",
      "Yangfan Li",
      "Weisheng Xu",
      "Renjing Xu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.19927",
    "title": "WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution",
    "abstract": "           Transformers have demonstrated promising performance in computer vision tasks, including image super-resolution (SR). The quadratic computational complexity of window self-attention mechanisms in many transformer-based SR methods forces the use of small, fixed windows, limiting the receptive field. In this paper, we propose a new approach by embedding the wavelet transform within a hierarchical transformer framework, called (WaveHiT-SR). First, using adaptive hierarchical windows instead of static small windows allows to capture features across different levels and greatly improve the ability to model long-range dependencies. Secondly, the proposed model utilizes wavelet transforms to decompose images into multiple frequency subbands, allowing the network to focus on both global and local features while preserving structural details. By progressively reconstructing high-resolution images through hierarchical processing, the network reduces computational complexity without sacrificing performance. The multi-level decomposition strategy enables the network to capture fine-grained information in lowfrequency components while enhancing high-frequency textures. Through extensive experimentation, we confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR results, achieving higher efficiency with fewer parameters, lower FLOPs, and faster speeds.         ",
    "url": "https://arxiv.org/abs/2508.19927",
    "authors": [
      "Fayaz Ali",
      "Muhammad Zawish",
      "Steven Davy",
      "Radu Timofte"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19933",
    "title": "Combined Stochastic and Robust Optimization for Electric Autonomous Mobility-on-Demand with Nested Benders Decomposition",
    "abstract": "           The electrification and automation of mobility are reshaping how cities operate on-demand transport systems. Managing Electric Autonomous Mobility-on-Demand (EAMoD) fleets effectively requires coordinating dispatch, rebalancing, and charging decisions under multiple uncertainties, including travel demand, travel time, energy consumption, and charger availability. We address this challenge with a combined stochastic and robust model predictive control (MPC) framework. The framework integrates spatio-temporal Bayesian neural network forecasts with a multi-stage stochastic optimization model, formulated as a large-scale mixed-integer linear program. To ensure real-time applicability, we develop a tailored Nested Benders Decomposition that exploits the scenario tree structure and enables efficient parallelized solution. Stochastic optimization is employed to anticipate demand and infrastructure variability, while robust constraints on energy consumption and travel times safeguard feasibility under worst-case realizations. We evaluate the framework using high-fidelity simulations of San Francisco and Chicago. Compared with deterministic, reactive, and robust baselines, the combined stochastic and robust approach reduces median passenger waiting times by up to 36% and 95th-percentile delays by nearly 20%, while also lowering rebalancing distance by 27% and electricity costs by more than 35%. We also conduct a sensitivity analysis of battery size and vehicle efficiency, finding that energy-efficient vehicles maintain stable performance even with small batteries, whereas less efficient vehicles require larger batteries and greater infrastructure support. Our results emphasize the importance of jointly optimizing predictive control, vehicle capabilities, and infrastructure planning to enable scalable, cost-efficient EAMoD operations.         ",
    "url": "https://arxiv.org/abs/2508.19933",
    "authors": [
      "Sten Elling Tingstad Jacobsen",
      "Bal\u00e1zs Kulcs\u00e1r",
      "Anders Lindman"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.19990",
    "title": "Self-Supervised Pre-Training with Equilibrium Constraints",
    "abstract": "           Self-supervised pre-training using unlabeled data is widely used in machine learning. In this paper, we propose a new self-supervised pre-training approach to dealing with heterogeneous data. Instead of mixing all the data and minimizing the averaged global loss in the conventional way, we impose additional equilibrium constraints to ensure that the models optimizes each source of heterogeneous data to its local optima after $K$-step gradient descent initialized from the model. We formulate this as a bilevel optimization problem, and use the first-order approximation method to solve the problem. We discuss its connection to model-agnostic meta learning (MAML). Experiments are carried out on self-supervised pre-training using multi-domain and multilingual datasets, demonstrating that the proposed approach can significantly improve the adaptivity of the self-supervised pre-trained model for the downstream supervised fine-tuning tasks.         ",
    "url": "https://arxiv.org/abs/2508.19990",
    "authors": [
      "Xiaodong Cui",
      "A F M Saif",
      "Brian Kingsbury",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20032",
    "title": "Pruning Strategies for Backdoor Defense in LLMs",
    "abstract": "           Backdoor attacks are a significant threat to the performance and integrity of pre-trained language models. Although such models are routinely fine-tuned for downstream NLP tasks, recent work shows they remain vulnerable to backdoor attacks that survive vanilla fine-tuning. These attacks are difficult to defend because end users typically lack knowledge of the attack triggers. Such attacks consist of stealthy malicious triggers introduced through subtle syntactic or stylistic manipulations, which can bypass traditional detection and remain in the model, making post-hoc purification essential. In this study, we explore whether attention-head pruning can mitigate these threats without any knowledge of the trigger or access to a clean reference model. To this end, we design and implement six pruning-based strategies: (i) gradient-based pruning, (ii) layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2 sparsification, (iv) randomized ensemble pruning, (v) reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning. Each method iteratively removes the least informative heads while monitoring validation accuracy to avoid over-pruning. Experimental evaluation shows that gradient-based pruning performs best while defending the syntactic triggers, whereas reinforcement learning and Bayesian pruning better withstand stylistic attacks.         ",
    "url": "https://arxiv.org/abs/2508.20032",
    "authors": [
      "Santosh Chapagain",
      "Shah Muhammad Hamdi",
      "Soukaina Filali Boubrahimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20038",
    "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks",
    "abstract": "           Despite advances in improving large language model(LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.         ",
    "url": "https://arxiv.org/abs/2508.20038",
    "authors": [
      "Sheng Liu",
      "Qiang Sheng",
      "Danding Wang",
      "Yang Li",
      "Guang Yang",
      "Juan Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20063",
    "title": "OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations",
    "abstract": "           Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.         ",
    "url": "https://arxiv.org/abs/2508.20063",
    "authors": [
      "Peng-Hao Hsu",
      "Ke Zhang",
      "Fu-En Wang",
      "Tao Tu",
      "Ming-Feng Li",
      "Yu-Lun Liu",
      "Albert Y. C. Chen",
      "Min Sun",
      "Cheng-Hao Kuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20064",
    "title": "Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices",
    "abstract": "           Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the model's performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize.         ",
    "url": "https://arxiv.org/abs/2508.20064",
    "authors": [
      "Philippe Zhang",
      "Weili Jiang",
      "Yihao Li",
      "Jing Zhang",
      "Sarah Matta",
      "Yubo Tan",
      "Hui Lin",
      "Haoshen Wang",
      "Jiangtian Pan",
      "Hui Xu",
      "Laurent Borderie",
      "Alexandre Le Guilcher",
      "B\u00e9atrice Cochener",
      "Chubin Ou",
      "Gwenol\u00e9 Quellec",
      "Mathieu Lamard"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.20066",
    "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence",
    "abstract": "           Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios.         ",
    "url": "https://arxiv.org/abs/2508.20066",
    "authors": [
      "Zheng Li",
      "Yanming Guo",
      "WenZhe Liu",
      "Xueyi Zhang",
      "Zhaoyun Ding",
      "Long Xu",
      "Mingrui Lao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.20076",
    "title": "Anomaly Detection in Networked Bandits",
    "abstract": "           The nodes' interconnections on a social network often reflect their dependencies and information-sharing behaviors. Nevertheless, abnormal nodes, which significantly deviate from most of the network concerning patterns or behaviors, can lead to grave consequences. Therefore, it is imperative to design efficient online learning algorithms that robustly learn users' preferences while simultaneously detecting anomalies. We introduce a novel bandit algorithm to address this problem. Through network knowledge, the method characterizes the users' preferences and residuals of feature information. By learning and analyzing these preferences and residuals, it develops a personalized recommendation strategy for each user and simultaneously detects anomalies. We rigorously prove an upper bound on the regret of the proposed algorithm and experimentally compare it with several state-of-the-art collaborative contextual bandit algorithms on both synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2508.20076",
    "authors": [
      "Xiaotong Cheng",
      "Setareh Maghsudi"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.20083",
    "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning",
    "abstract": "           Retrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \\textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems. In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses.         ",
    "url": "https://arxiv.org/abs/2508.20083",
    "authors": [
      "Yanbo Dai",
      "Zhenlan Ji",
      "Zongjie Li",
      "Kuan Li",
      "Shuai Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.20086",
    "title": "Smart Contract Intent Detection with Pre-trained Programming Language Model",
    "abstract": "           Malicious intent in smart contract development can lead to substantial economic losses. SmartIntentNN is a deep learning model specifically designed to identify unsafe intents in smart contracts. This model integrates the Universal Sentence Encoder, a K-means clustering-based intent highlighting mechanism, and a Bidirectional Long Short-Term Memory network for multi-label classification, achieving an F1 of 0.8633 in distinguishing ten different intent categories. In this study, we present an upgraded version of this model, SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant enhancement in V2 is the incorporation of a BERT-based pre-trained language model, which has been trained on a dataset of 16,000 real smart contracts using a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based multi-label classification network. With an improved F1 of 0.927, V2 demonstrates enhanced performance compared to its predecessor, establishing itself as the state-of-the-art model for smart contract intent detection.         ",
    "url": "https://arxiv.org/abs/2508.20086",
    "authors": [
      "Youwei Huang",
      "Jianwen Li",
      "Sen Fang",
      "Yao Li",
      "Peng Yang",
      "Bin Hu",
      "Tao Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.19303",
    "title": "2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks",
    "abstract": "           Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to their potential for rupture, which is often asymptomatic but can be fatal. Although maximum diameter is commonly used for risk assessment, diameter alone is insufficient as it does not capture the properties of the underlying material of the vessel wall, which play a critical role in determining the risk of rupture. To overcome this limitation, we propose a deep learning-based framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite element simulations, we generate a diverse dataset of displacement fields with their corresponding modulus distributions. We train a model with U-Net architecture and normalized mean squared error (NMSE) to infer the spatial modulus distribution from the axial and lateral components of the displacement fields. This model is evaluated across three experimental domains: digital phantom data from 3D COMSOL simulations, physical phantom experiments using biomechanically distinct vessel models, and clinical ultrasound exams from AAA patients. Our simulated results demonstrate that the proposed deep learning model is able to reconstruct modulus distributions, achieving an NMSE score of 0.73\\%. Similarly, in phantom data, the predicted modular ratio closely matches the expected values, affirming the model's ability to generalize to phantom data. We compare our approach with an iterative method which shows comparable performance but higher computation time. In contrast, the deep learning method can provide quick and effective estimates of tissue stiffness from ultrasound images, which could help assess the risk of AAA rupture without invasive procedures.         ",
    "url": "https://arxiv.org/abs/2508.19303",
    "authors": [
      "Utsav Ratna Tuladhar",
      "Richard Simon",
      "Doran Mix",
      "Michael Richards"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19319",
    "title": "MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction",
    "abstract": "           Accurate sarcopenia diagnosis via ultrasound remains challenging due to subtle imaging cues, limited labeled data, and the absence of clinical context in most models. We propose MedVQA-TREE, a multimodal framework that integrates a hierarchical image interpretation module, a gated feature-level fusion mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision module includes anatomical classification, region segmentation, and graph-based spatial reasoning to capture coarse, mid-level, and fine-grained structures. A gated fusion mechanism selectively integrates visual features with textual queries, while clinical knowledge is retrieved through a UMLS-guided pipeline accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA) and a custom sarcopenia ultrasound dataset. The model achieved up to 99% diagnostic accuracy and outperformed previous state-of-the-art methods by over 10%. These results underscore the benefit of combining structured visual understanding with guided knowledge retrieval for effective AI-assisted diagnosis in sarcopenia.         ",
    "url": "https://arxiv.org/abs/2508.19319",
    "authors": [
      "Pardis Moradbeiki",
      "Nasser Ghadiri",
      "Sayed Jalal Zahabi",
      "Uffe Kock Wiil",
      "Kristoffer Kittelmann Brockhattingen",
      "Ali Ebrahimi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.19327",
    "title": "Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning",
    "abstract": "           Bell's theorem reveals a profound conflict between quantum mechanics and local realism, a conflict we reinterpret through the modern lens of causal inference. We propose and computationally validate a framework where quantum entanglement acts as a \"super-confounding\" resource, generating correlations that violate the classical causal bounds set by Bell's inequalities. This work makes three key contributions: First, we establish a physical hierarchy of confounding (Quantum > Classical) and introduce Confounding Strength (CS) to quantify this effect. Second, we provide a circuit-based implementation of the quantum $\\mathcal{DO}$-calculus to distinguish causality from spurious correlation. Finally, we apply this calculus to a quantum machine learning problem, where causal feature selection yields a statistically significant 11.3% average absolute improvement in model robustness. Our framework bridges quantum foundations and causal AI, offering a new, practical perspective on quantum correlations.         ",
    "url": "https://arxiv.org/abs/2508.19327",
    "authors": [
      "Pilsung Kang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19566",
    "title": "Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks",
    "abstract": "           This work proposes an energy-efficient, learning-based beamforming scheme for integrated sensing and communication (ISAC)-enabled V2X networks. Specifically, we first model the dynamic and uncertain nature of V2X environments as a Markov Decision Process. This formulation allows the roadside unit to generate beamforming decisions based solely on current sensing information, thereby eliminating the need for frequent pilot transmissions and extensive channel state information acquisition. We then develop a deep reinforcement learning (DRL) algorithm to jointly optimize beamforming and power allocation, ensuring both communication throughput and sensing accuracy in highly dynamic scenario. To address the high energy demands of conventional learning-based schemes, we embed spiking neural networks (SNNs) into the DRL framework. Leveraging their event-driven and sparsely activated architecture, SNNs significantly enhance energy efficiency while maintaining robust performance. Simulation results confirm that the proposed method achieves substantial energy savings and superior communication performance, demonstrating its potential to support green and sustainable connectivity in future V2X systems.         ",
    "url": "https://arxiv.org/abs/2508.19566",
    "authors": [
      "Chen Shang",
      "Jiadong Yu",
      "Dinh Thai Hoang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19660",
    "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation",
    "abstract": "           Printed electronics offer a promising alternative for applications beyond silicon-based systems, requiring properties like flexibility, stretchability, conformality, and ultra-low fabrication costs. Despite the large feature sizes in printed electronics, printed neural networks have attracted attention for meeting target application requirements, though realizing complex circuits remains challenging. This work bridges the gap between classification accuracy and area efficiency in printed neural networks, covering the entire processing-near-sensor system design and co-optimization from the analog-to-digital interface-a major area and power bottleneck-to the digital classifier. We propose an automated framework for designing printed Ternary Neural Networks with arbitrary input precision, utilizing multi-objective optimization and holistic approximation. Our circuits outperform existing approximate printed neural networks by 17x in area and 59x in power on average, being the first to enable printed-battery-powered operation with under 5% accuracy loss while accounting for analog-to-digital interfacing costs.         ",
    "url": "https://arxiv.org/abs/2508.19660",
    "authors": [
      "Vojtech Mrazek",
      "Konstantinos Balaskas",
      "Paula Carolina Lozano Duarte",
      "Zdenek Vasicek",
      "Mehdi B. Tahoori",
      "Georgios Zervakis"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.19683",
    "title": "Topological Uncertainty for Anomaly Detection in the Neural-network EoS Inference with Neutron Star Data",
    "abstract": "           We study the performance of the Topological Uncertainty (TU) constructed with a trained feedforward neural network (FNN) for Anomaly Detection. Generally, meaningful information can be stored in the hidden layers of the trained FNN, and the TU implementation is one tractable recipe to extract buried information by means of the Topological Data Analysis. We explicate the concept of the TU and the numerical procedures. Then, for a concrete demonstration of the performance test, we employ the Neutron Star data used for inference of the equation of state (EoS). For the training dataset consisting of the input (Neutron Star data) and the output (EoS parameters), we can compare the inferred EoSs and the exact answers to classify the data with the label $k$. The subdataset with $k=0$ leads to the normal inference for which the inferred EoS approximates the answer well, while the subdataset with $k=1$ ends up with the unsuccessful inference. Once the TU is prepared based on the $k$-labled subdatasets, we introduce the cross-TU to quantify the uncertainty of characterizing the $k$-labeled data with the label $j$. The anomaly or unsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is smaller than that for $j=0$ and $k=1$. In our numerical experiment, for various input data, we calculate the cross-TU and estimate the performance of Anomaly Detection. We find that performance depends on FNN hyperparameters, and the success rate of Anomaly Detection exceeds $90\\%$ in the best case. We finally discuss further potential of the TU application to retrieve the information hidden in the trained FNN.         ",
    "url": "https://arxiv.org/abs/2508.19683",
    "authors": [
      "Kenji Fukushima",
      "Syo Kamata"
    ],
    "subjectives": [
      "Nuclear Theory (nucl-th)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.19751",
    "title": "Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields",
    "abstract": "           Modelling the effects of perturbations on optical fields often requires learning highly oscillatory complex-valued functions. Standard multi-layer perceptrons (MLPs) struggle with this task due to an inherent spectral bias, preventing them from fitting high-frequency sinusoids. To overcome this, we incorporate Fourier features - a set of predefined sinusoids dependent on the perturbation - as an additional network input. This reframes the learning problem from approximating a complex function to finding a linear combination of basis functions. We demonstrate this method by training a Fourier Feature Network to predict the transmission matrix of a multimode fibre under mechanical compression. Compared to a standard MLP, our network reduces prediction error in the output field's amplitude and phase by an order of magnitude, achieving a mean complex correlation of 0.995 with the ground truth, despite using 85% fewer parameters. This approach offers a general and robust method for accurately modelling a wide class of oscillatory physical systems.         ",
    "url": "https://arxiv.org/abs/2508.19751",
    "authors": [
      "Joshua R. Jandrell",
      "Mitchell A. Cox"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2508.19841",
    "title": "Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers",
    "abstract": "           We present a probabilistic, data-driven surrogate model for predicting the radiative properties of nanoparticle embedded scattering media. The model uses conditional normalizing flows, which learn the conditional distribution of optical outputs, including reflectance, absorbance, and transmittance, given input parameters such as the absorption coefficient, scattering coefficient, anisotropy factor, and particle size distribution. We generate training data using Monte Carlo radiative transfer simulations, with optical properties derived from Mie theory. Unlike conventional neural networks, the conditional normalizing flow model yields full posterior predictive distributions, enabling both accurate forecasts and principled uncertainty quantification. Our results demonstrate that this model achieves high predictive accuracy and reliable uncertainty estimates, establishing it as a powerful and efficient surrogate for radiative transfer simulations.         ",
    "url": "https://arxiv.org/abs/2508.19841",
    "authors": [
      "Fahime Seyedheydari",
      "Kevin Conley",
      "Simo S\u00e4rkk\u00e4"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2309.05075",
    "title": "Secure Set-based State Estimation for Safety-Critical Applications under Adversarial Attacks on Sensors",
    "abstract": "           Set-based state estimation provides guaranteed state inclusion certificates that are crucial for the safety verification of dynamical systems. However, when system sensors are subject to cyberattacks, maintaining both safety and security guarantees becomes a fundamental challenge that existing point-based secure state estimation methods cannot adequately address due to their inherent inability to provide state inclusion certificates. This paper introduces a novel approach that simultaneously ensures safety guarantees through guaranteed state inclusion and security guarantees against sensor attacks, without imposing conservative restrictions on system operation. We propose a Secure Set-based State Estimation (S3E) algorithm that maintains the true system state within the estimated set under sensor attacks, provided the initialization set contains the initial state and the system remains observable from the uncompromised sensor subset. The algorithm gives the estimated set as a collection of constrained zonotopes (agreement sets), which can be employed as robust certificates for verifying whether the system adheres to safety constraints. Furthermore, we demonstrate that the estimated set remains unaffected by attack signals of sufficiently large magnitude and also establish sufficient conditions for attack detection, identification, and filtering. This compels the attacker to only inject signals of small magnitudes to evade detection, thus preserving the accuracy of the estimated set. To address the computational complexity of the algorithm, we offer several strategies for complexity-performance trade-offs. The efficacy of the proposed algorithm is illustrated through several examples, including its application to a three-story building model.         ",
    "url": "https://arxiv.org/abs/2309.05075",
    "authors": [
      "M. Umar B. Niazi",
      "Michelle S. Chong",
      "Amr Alanwar",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2402.08441",
    "title": "Latent space configuration for improved generalization in supervised autoencoder neural networks",
    "abstract": "           Autoencoders (AE) are simple yet powerful class of neural networks that compress data by projecting input into low-dimensional latent space (LS). Whereas LS is formed according to the loss function minimization during training, its properties and topology are not controlled directly. In this paper we focus on AE LS properties and propose two methods for obtaining LS with desired topology, called LS configuration. The proposed methods include loss configuration using a geometric loss term that acts directly in LS, and encoder configuration. We show that the former allows to reliably obtain LS with desired configuration by defining the positions and shapes of LS clusters for supervised AE (SAE). Knowing LS configuration allows to define similarity measure in LS to predict labels or estimate similarity for multiple inputs without using decoders or classifiers. We also show that this leads to more stable and interpretable training. We show that SAE trained for clothes texture classification using the proposed method generalizes well to unseen data from LIP, Market1501, and WildTrack datasets without fine-tuning, and even allows to evaluate similarity for unseen classes. We further illustrate the advantages of pre-configured LS similarity estimation with cross-dataset searches and text-based search using a text query without language models.         ",
    "url": "https://arxiv.org/abs/2402.08441",
    "authors": [
      "Nikita Gabdullin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.18235",
    "title": "On the Joint Effect of Culture and Discussion Topics on X (Twitter) Signed Ego Networks",
    "abstract": "           Humans are known to structure social relationships according to certain patterns, such as the Ego Network Model (ENM). These patterns result from our innate cognitive limits and can therefore be observed in the vast majority of large human social groups. Until recently, the main focus of research was the structural characteristics of this model. The main aim of this paper is to complement previous findings with systematic and data-driven analyses on the positive and negative sentiments of social relationships, across different cultures, communities and topics of discussion. A total of 26 datasets were collected for this work. It was found that contrary to previous findings, the influence of culture is not easily ``overwhelmed'' by that of the topic of discussion. However, more specific and polarising topics do lead to noticeable increases in negativity across all cultures. These negativities also appear to be stable across the different levels of the ENM, which contradicts previous hypotheses. Finally, the number of generic topics being discussed between users seems to be a good predictor of the overall positivity of their relationships.         ",
    "url": "https://arxiv.org/abs/2402.18235",
    "authors": [
      "Jack Tacchi",
      "Chiara Boldrini",
      "Andrea Passarella",
      "Marco Conti"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2404.02360",
    "title": "FraGNNet: A Deep Probabilistic Model for Tandem Mass Spectrum Prediction",
    "abstract": "           Compound identification from tandem mass spectrometry (MS/MS) data is a critical step in the analysis of complex mixtures. Typical solutions for the MS/MS spectrum to compound (MS2C) problem involve comparing the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to MS/MS spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted MS/MS spectra. Unfortunately, many existing C2MS models suffer from problems with mass accuracy, generalization, or interpretability. We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately simulate MS/MS spectra with high mass accuracy. Our approach formulates the C2MS problem as learning a distribution over molecule fragments. FraGNNet achieves state-of-the-art performance in terms of prediction error and surpasses existing C2MS models as a tool for retrieval-based MS2C.         ",
    "url": "https://arxiv.org/abs/2404.02360",
    "authors": [
      "Adamo Young",
      "Fei Wang",
      "David S Wishart",
      "Bo Wang",
      "Russell Greiner",
      "Hannes R\u00f6st"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2405.07460",
    "title": "HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models",
    "abstract": "           HONeYBEE (Harmonized ONcologY Biomedical Embedding Encoder) is an open-source framework that integrates multimodal biomedical data for oncology applications. It processes clinical data (structured and unstructured), whole-slide images, radiology scans, and molecular profiles to generate unified patient-level embeddings using domain-specific foundation models and fusion strategies. These embeddings enable survival prediction, cancer-type classification, patient similarity retrieval, and cohort clustering. Evaluated on 11,400+ patients across 33 cancer types from The Cancer Genome Atlas (TCGA), clinical embeddings showed the strongest single-modality performance with 98.5% classification accuracy and 96.4% precision@10 in patient retrieval. They also achieved the highest survival prediction concordance indices across most cancer types. Multimodal fusion provided complementary benefits for specific cancers, improving overall survival prediction beyond clinical features alone. Comparative evaluation of four large language models revealed that general-purpose models like Qwen3 outperformed specialized medical models for clinical text representation, though task-specific fine-tuning improved performance on heterogeneous data such as pathology reports.         ",
    "url": "https://arxiv.org/abs/2405.07460",
    "authors": [
      "Aakash Tripathi",
      "Asim Waqas",
      "Matthew B. Schabath",
      "Yasin Yilmaz",
      "Ghulam Rasool"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2407.01619",
    "title": "TabSketchFM: Sketch-based Tabular Representation Learning for Data Discovery over Data Lakes",
    "abstract": "           Enterprises have a growing need to identify relevant tables in data lakes; e.g. tables that are unionable, joinable, or subsets of each other. Tabular neural models can be helpful for such data discovery tasks. In this paper, we present TabSketchFM, a neural tabular model for data discovery over data lakes. First, we propose novel pre-training: a sketch-based approach to enhance the effectiveness of data discovery in neural tabular models. Second, we finetune the pretrained model for identifying unionable, joinable, and subset table pairs and show significant improvement over previous tabular neural models. Third, we present a detailed ablation study to highlight which sketches are crucial for which tasks. Fourth, we use these finetuned models to perform table search; i.e., given a query table, find other tables in a corpus that are unionable, joinable, or that are subsets of the query. Our results demonstrate significant improvements in F1 scores for search compared to state-of-the-art techniques. Finally, we show significant transfer across datasets and tasks establishing that our model can generalize across different tasks and over different data lakes.         ",
    "url": "https://arxiv.org/abs/2407.01619",
    "authors": [
      "Aamod Khatiwada",
      "Harsha Kokel",
      "Ibrahim Abdelaziz",
      "Subhajit Chaudhury",
      "Julian Dolby",
      "Oktie Hassanzadeh",
      "Zhenhan Huang",
      "Tejaswini Pedapati",
      "Horst Samulowitz",
      "Kavitha Srinivas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2407.07058",
    "title": "An efficient implementation for solving the all pairs minimax path problem in an undirected dense graph",
    "abstract": "           We provide an efficient $ O(n^2) $ implementation for solving the all pairs minimax path problem or widest path problem in an undirected dense graph. It is a code implementation of the Algorithm 4 (MMJ distance by Calculation and Copy) in a previous paper. The distance matrix is also called the all points path distance (APPD). We conducted experiments to test the implementation and algorithm, compared it with several other algorithms for solving the APPD matrix. Result shows Algorithm 4 works good for solving the widest path or minimax path APPD matrix. It can drastically improve the efficiency for computing the APPD matrix. There are several theoretical outcomes which claim the APPD matrix can be solved accurately in $ O(n^2) $ . However, they are impractical because there is no code implementation of these algorithms. It seems Algorithm 4 is the first algorithm that has an actual code implementation for solving the APPD matrix of minimax path or widest path problem in $ O(n^2) $, in an undirected dense graph.         ",
    "url": "https://arxiv.org/abs/2407.07058",
    "authors": [
      "Gangli Liu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2407.15199",
    "title": "Multiple Object Detection and Tracking in Panoramic Videos for Cycling Safety Analysis",
    "abstract": "           Cyclists face a disproportionate risk of injury, yet conventional crash records are too limited to reconstruct the circumstances of incidents or to diagnose risk at the finer spatial and temporal detail needed for targeted interventions. Recently, naturalistic studies have gained traction as a way to capture the complex behavioural and infrastructural factors that contribute to crashes. These approaches typically involve the collection and analysis of video data. A video promising format is panoramic video, which can record 360-degree views around a rider. However, its use is limited by severe distortions, large numbers of small objects and boundary continuity. This study addresses these challenges by proposing a novel three-step framework: (1) enhancing object detection accuracy on panoramic imagery by segmenting and projecting the original 360-degree images into four perspective sub-images, thus reducing distortion; (2) modifying multi-object tracking models to incorporate boundary continuity and object category information for improved tracking consistency; and (3) validating the proposed approach through a real-world application focused on detecting overtaking manoeuvres by vehicles around cyclists. The methodology is evaluated using panoramic videos recorded by cyclists on London's roadways under diverse conditions. Experimental results demonstrate notable improvements over baseline methods, achieving higher average precision across varying image resolutions. Moreover, the enhanced tracking approach yields a 3.0% increase in multi-object tracking accuracy and a 4.6% improvement in identification F-score. The overtaking detection task achieves a high F-score of 0.81, illustrating the practical effectiveness of the proposed method in real-world cycling safety scenarios. The code is available on GitHub (this https URL) to ensure reproducibility.         ",
    "url": "https://arxiv.org/abs/2407.15199",
    "authors": [
      "Jingwei Guo",
      "Yitai Cheng",
      "Meihui Wang",
      "Ilya Ilyankou",
      "Natchapon Jongwiriyanurak",
      "Xiaowei Gao",
      "Nicola Christie",
      "James Haworth"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2407.20648",
    "title": "Leveraging Multi-facet Paths for Heterogeneous Graph Representation Learning",
    "abstract": "           Recent advancements in graph neural networks (GNNs) and heterogeneous GNNs (HGNNs) have advanced node embeddings and relationship learning for various tasks. However, existing methods often rely on domain-specific predefined meta-paths, which are coarse-grained and focus solely on aspects like node type, limiting their ability to capture complex interactions. We introduce MF2Vec, a model that uses multi-faceted (fine-grained) paths instead of predefined meta-paths. MF2Vec extracts paths via random walks and generates multi-faceted vectors, ignoring predefined schemas. This method learns diverse aspects of nodes and their relationships, constructs a homogeneous network, and creates node embeddings for classification, link prediction, and clustering. Extensive experiments show that MF2Vec outperforms existing methods, offering a more flexible and comprehensive framework for analyzing complex networks. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2407.20648",
    "authors": [
      "Jongwoo Kim",
      "Seongyeub Chu",
      "Hyeongmin Park",
      "Bryan Wong",
      "Keejun Han",
      "Mun Yong Yi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.05886",
    "title": "Online-Score-Aided Federated Learning: Taming the Resource Constraints in Wireless Networks",
    "abstract": "           While federated learning (FL) is a widely popular distributed machine learning (ML) strategy that protects data privacy, time-varying wireless network parameters and heterogeneous configurations of the wireless devices pose significant challenges. Although the limited radio and computational resources of the network and the clients, respectively, are widely acknowledged, two critical yet often ignored aspects are (a) wireless devices can only dedicate a small chunk of their limited storage for the FL task and (b) new training samples may arrive in an online manner in many practical wireless applications. Therefore, we propose a new FL algorithm called online-score-aided federated learning (OSAFL), specifically designed to learn tasks relevant to wireless applications under these practical considerations. Since clients' local training steps differ under resource constraints, which may lead to client drift under statistically heterogeneous data distributions, we leverage normalized gradient similarities and exploit weighting clients' updates based on optimized scores that facilitate the convergence rate of the proposed OSAFL algorithm without incurring any communication overheads to the clients or requiring any statistical data information from them. We theoretically show how the new factors, i.e., online score and local data distribution shifts, affect the convergence bound and derive the necessary conditions for a sublinear convergence rate. Our extensive simulation results on two different tasks with multiple popular ML models validate the effectiveness of the proposed OSAFL algorithm compared to modified state-of-the-art FL baselines.         ",
    "url": "https://arxiv.org/abs/2408.05886",
    "authors": [
      "Ferdous Pervej",
      "Minseok Choi",
      "Andreas F. Molisch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.16564",
    "title": "Human-Inspired Computing for Robust and Efficient Audio-Visual Speech Recognition",
    "abstract": "           Humans naturally perform audiovisual speech recognition (AVSR), enhancing the accuracy and robustness by integrating auditory and visual information. Spiking neural networks (SNNs), which mimic the brain's information-processing mechanisms, are well-suited for emulating the human capability of AVSR. Despite their potential, research on SNNs for AVSR is scarce, with most existing audio-visual multimodal methods focused on object or digit recognition. These models simply integrate features from both modalities, neglecting their unique characteristics and interactions. Additionally, they often rely on future information for current processing, which increases recognition latency and limits real-time applicability. Inspired by human speech perception, this paper proposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating three key characteristics: cueing interaction, causal processing and spike activity. For cueing interaction, we propose a visual-cued auditory attention module (VCA2M) that leverages visual cues to guide attention to auditory features. We achieve causal processing by aligning the SNN's temporal dimension with that of visual and auditory features and applying temporal masking to utilize only past and current information. To implement spike activity, in addition to using SNNs, we leverage the event camera to capture lip movement as spikes, mimicking the human retina and providing efficient visual data. We evaluate HI-AVSNN on an audiovisual speech recognition dataset combining the DVS-Lip dataset with its corresponding audio samples. Experimental results demonstrate the superiority of our proposed fusion method, outperforming existing audio-visual SNN fusion methods and achieving a 2.27% improvement in accuracy over the only existing SNN-based AVSR method.         ",
    "url": "https://arxiv.org/abs/2408.16564",
    "authors": [
      "Qianhui Liu",
      "Jiadong Wang",
      "Yang Wang",
      "Xin Yang",
      "Gang Pan",
      "Haizhou Li"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2410.03141",
    "title": "Machine Learning for Asymptomatic Ratoon Stunting Disease Detection With Freely Available Satellite Based Multispectral Imaging",
    "abstract": "           Disease detection in sugarcane, particularly the identification of asymptomatic infectious diseases such as Ratoon Stunting Disease (RSD), is critical for effective crop management. This study employed various machine learning techniques to detect the presence of RSD in different sugarcane varieties, using vegetation indices derived from freely available satellite-based spectral data. Our results show that the Support Vector Machine with a Radial Basis Function Kernel (SVM-RBF) was the most effective algorithm, achieving classification accuracy between 85.64% and 96.55%, depending on the variety. Gradient Boosting and Random Forest also demonstrated high performance achieving accuracy between 83.33% to 96.55%, while Logistic Regression and Quadratic Discriminant Analysis showed variable results across different varieties. The inclusion of sugarcane variety and vegetation indices was important in the detection of RSD. This agreed with what was identified in the current literature. Our study highlights the potential of satellite-based remote sensing as a cost-effective and efficient method for large-scale sugarcane disease detection alternative to traditional manual laboratory testing methods.         ",
    "url": "https://arxiv.org/abs/2410.03141",
    "authors": [
      "Ethan Kane Waters",
      "Carla Chia-ming Chen",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2410.05672",
    "title": "Embedding derivatives and derivative Area operators of Hardy spaces into Lebesgue spaces",
    "abstract": "           We characterize the compactness of embedding derivatives from Hardy space $H^p$ into Lebesgue space $L^q(\\mu)$. We also completely characterize the boundedness and compactness of derivative area operators from $H^p$ into $L^q(\\mathbb{S}_n)$, $0<p, q<\\infty$. Some of the tools used in the proof of the one-dimensional case are not available in higher dimensions, such as the strong factorization of Hardy spaces. Therefore, we need the theory of tent spaces which was established by Coifman, Mayer and Stein in 1985.         ",
    "url": "https://arxiv.org/abs/2410.05672",
    "authors": [
      "Xiaosong Liu",
      "Zengjian Lou",
      "Zixing Yuan",
      "Ruhan Zhao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2412.08896",
    "title": "LV-CadeNet: A Long-View Feature Convolution-Attention Fusion Encoder-Decoder Network for EEG/MEG Spike Analysis",
    "abstract": "           The analysis of interictal epileptiform discharges (IEDs) in magnetoencephalography (MEG) or electroencephalogram (EEG) recordings represents a critical component in the diagnosis of epilepsy. However, manual analysis of these IEDs, which appear as epileptic spikes, from the large amount of MEG/EEG data is labor intensive and requires high expertise. Although automated methods have been developed to address this challenge, current approaches fail to fully emulate clinical experts' diagnostic intelligence in two key aspects: (1) their analysis on the input signals is limited to short temporal windows matching individual spike durations, missing the extended contextual patterns clinicians use to assess significance; and (2) they fail to adequately capture the dipole patterns with simultaneous positive-negative potential distributions across adjacent sensors that serve as clinicians' key diagnostic criterion for IED identification. To bridge this artificial-human intelligence gap, we propose a novel deep learning framework LV-CadeNet that integrates two key innovations: (1) a Long-View morphological feature representation that mimics expert clinicians' comprehensive assessment of both local spike characteristics and long-view contextual information, and (2) a hierarchical Encoder-Decoder NETwork that employs Convolution-Attention blocks for multi-scale spatiotemporal feature learning with progressive abstraction. Extensive evaluations confirm the superior performance of LV-CadeNet, which outperforms six state-of-the-art methods in EEG spike classification on TUEV, the largest public EEG spike dataset. Additionally, LV-CadeNet attains a significant improvement of 13.58% in balanced accuracy over the leading baseline for MEG spike detection on a clinical MEG dataset from Sanbo Brain Hospital, Capital Medical University.         ",
    "url": "https://arxiv.org/abs/2412.08896",
    "authors": [
      "Kuntao Xiao",
      "Xiongfei Wang",
      "Pengfei Teng",
      "Yi Sun",
      "Yong Zhang",
      "Wanli Yang",
      "Zikang Xu",
      "Liang Zhang",
      "Hanyang Dong",
      "Guoming Luan",
      "Shurong Sheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.11668",
    "title": "Online Writer Retrieval with Chinese Handwritten Phrases: A Synergistic Temporal-Frequency Representation Learning Approach",
    "abstract": "           Currently, the prevalence of online handwriting has spurred a critical need for effective retrieval systems to accurately search relevant handwriting instances from specific writers, known as online writer retrieval. Despite the growing demand, this field suffers from a scarcity of well-established methodologies and public large-scale datasets. This paper tackles these challenges with a focus on Chinese handwritten phrases. First, we propose DOLPHIN, a novel retrieval model designed to enhance handwriting representations through synergistic temporal-frequency analysis. For frequency feature learning, we propose the HFGA block, which performs gated cross-attention between the vanilla temporal handwriting sequence and its high-frequency sub-bands to amplify salient writing details. For temporal feature learning, we propose the CAIR block, tailored to promote channel interaction and reduce channel redundancy. Second, to address data deficit, we introduce OLIWER, a large-scale online writer retrieval dataset encompassing over 670,000 Chinese handwritten phrases from 1,731 individuals. Through extensive evaluations, we demonstrate the superior performance of DOLPHIN over existing methods. In addition, we explore cross-domain writer retrieval and reveal the pivotal role of increasing feature alignment in bridging the distributional gap between different handwriting data. Our findings emphasize the significance of point sampling frequency and pressure features in improving handwriting representation quality and retrieval performance. Code and dataset are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.11668",
    "authors": [
      "Peirong Zhang",
      "Lianwen Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.15579",
    "title": "Score-based Generative Diffusion Models for Social Recommendations",
    "abstract": "           With the prevalence of social networks on online platforms, social recommendation has become a vital technique for enhancing personalized recommendations. The effectiveness of social recommendations largely relies on the social homophily assumption, which presumes that individuals with social connections often share similar preferences. However, this foundational premise has been recently challenged due to the inherent complexity and noise present in real-world social networks. In this paper, we tackle the low social homophily challenge from an innovative generative perspective, directly generating optimal user social representations that maximize consistency with collaborative signals. Specifically, we propose the Score-based Generative Model for Social Recommendation (SGSR), which effectively adapts the Stochastic Differential Equation (SDE)-based diffusion models for social recommendations. To better fit the recommendation context, SGSR employs a joint curriculum training strategy to mitigate challenges related to missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach in filtering redundant social information and improving recommendation performance.         ",
    "url": "https://arxiv.org/abs/2412.15579",
    "authors": [
      "Chengyi Liu",
      "Jiahao Zhang",
      "Shijie Wang",
      "Wenqi Fan",
      "Qing Li"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.18221",
    "title": "GIMS: Image Matching System Based on Adaptive Graph Construction and Graph Neural Network",
    "abstract": "           Feature-based image matching has extensive applications in computer vision. Keypoints detected in images can be naturally represented as graph structures, and Graph Neural Networks (GNNs) have been shown to outperform traditional deep learning techniques. Consequently, the paradigm of image matching via GNNs has gained significant prominence in recent academic research. In this paper, we first introduce an innovative adaptive graph construction method that utilizes a filtering mechanism based on distance and dynamic threshold similarity. This method dynamically adjusts the criteria for incorporating new vertices based on the characteristics of existing vertices, allowing for the construction of more precise and robust graph structures while avoiding redundancy. We further combine the vertex processing capabilities of GNNs with the global awareness capabilities of Transformers to enhance the model's representation of spatial and feature information within graph structures. This hybrid model provides a deeper understanding of the interrelationships between vertices and their contributions to the matching process. Additionally, we employ the Sinkhorn algorithm to iteratively solve for optimal matching results. Finally, we validate our system using extensive image datasets and conduct comprehensive comparative experiments. Experimental results demonstrate that our system achieves an average improvement of 3.8x-40.3x in overall matching performance. Additionally, the number of vertices and edges significantly impacts training efficiency and memory usage; therefore, we employ multi-GPU technology to accelerate the training process. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.18221",
    "authors": [
      "Xianfeng Song",
      "Yi Zou",
      "Zheng Shi",
      "Zheng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.12496",
    "title": "Regularity and tailored regularization of Deep Neural Networks, with application to parametric PDEs in uncertainty quantification",
    "abstract": "           In this paper we consider Deep Neural Networks (DNNs) with a smooth activation function as surrogates for high-dimensional functions that are somewhat smooth but costly to evaluate. We consider the standard (non-periodic) DNNs as well as propose a new model of periodic DNNs which are especially suited for a class of periodic target functions when Quasi-Monte Carlo lattice points are used as training points. We study the regularity of DNNs by obtaining explicit bounds on all mixed derivatives with respect to the input parameters. The bounds depend on the neural network parameters as well as the choice of activation function. By imposing restrictions on the network parameters to match the regularity features of the target functions, we prove that DNNs with $N$ tailor-constructed lattice training points can achieve the generalization error (or $L_2$ approximation error) bound ${\\tt tol} + \\mathcal{O}(N^{-r/2})$, where ${\\tt tol}\\in (0,1)$ is the tolerance achieved by the training error in practice, and $r = 1/p^*$, with $p^*$ being the ``summability exponent'' of a sequence that characterises the decay of the input variables in the target functions, and with the implied constant independent of the dimensionality of the input data. We apply our analysis to popular models of parametric elliptic PDEs in uncertainty quantification. In our numerical experiments, we restrict the network parameters during training by adding tailored regularization terms, and we show that for an algebraic equation mimicking the parametric PDE problems the DNNs trained with tailored regularization perform significantly better.         ",
    "url": "https://arxiv.org/abs/2502.12496",
    "authors": [
      "Alexander Keller",
      "Frances Y. Kuo",
      "Dirk Nuyens",
      "Ian H. Sloan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2502.15525",
    "title": "Enhanced Probabilistic Collision Detection for Motion Planning Under Sensing Uncertainty",
    "abstract": "           Probabilistic collision detection (PCD) is essential in motion planning for robots operating in unstructured environments, where considering sensing uncertainty helps prevent damage. Existing PCD methods mainly used simplified geometric models and addressed only position estimation errors. This paper presents an enhanced PCD method with two key advancements: (a) using superquadrics for more accurate shape approximation and (b) accounting for both position and orientation estimation errors to improve robustness under sensing uncertainty. Our method first computes an enlarged surface for each object that encapsulates its observed rotated copies, thereby addressing the orientation estimation errors. Then, the collision probability under the position estimation errors is formulated as a chance-constraint problem that is solved with a tight upper bound. Both the two steps leverage the recently developed normal parameterization of superquadric surfaces. Results show that our PCD method is twice as close to the Monte-Carlo sampled baseline as the best existing PCD method and reduces path length by 30% and planning time by 37%, respectively. A Real2Sim2Real pipeline further validates the importance of considering orientation estimation errors, showing that the collision probability of executing the planned path in simulation is only 2%, compared to 9% and 29% when considering only position estimation errors or no errors at all.         ",
    "url": "https://arxiv.org/abs/2502.15525",
    "authors": [
      "Xiaoli Wang",
      "Sipu Ruan",
      "Xin Meng",
      "Gregory Chirikjian"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2502.19596",
    "title": "Reference-Aligned Retrieval-Augmented Question Answering over Heterogeneous Proprietary Documents",
    "abstract": "           Proprietary corporate documents contain rich domain-specific knowledge, but their overwhelming volume and disorganized structure make it difficult even for employees to access the right information when needed. For example, in the automotive industry, vehicle crash-collision tests, each costing hundreds of thousands of dollars, produce highly detailed documentation. However, retrieving relevant content during decision-making remains time-consuming due to the scale and complexity of the material. While Retrieval-Augmented Generation (RAG)-based Question Answering (QA) systems offer a promising solution, building an internal RAG-QA system poses several challenges: (1) handling heterogeneous multi-modal data sources, (2) preserving data confidentiality, and (3) enabling traceability between each piece of information in the generated answer and its original source document. To address these, we propose a RAG-QA framework for internal enterprise use, consisting of: (1) a data pipeline that converts raw multi-modal documents into a structured corpus and QA pairs, (2) a fully on-premise, privacy-preserving architecture, and (3) a lightweight reference matcher that links answer segments to supporting content. Applied to the automotive domain, our system improves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16), and helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scale ratings from both human and LLM judge.         ",
    "url": "https://arxiv.org/abs/2502.19596",
    "authors": [
      "Nayoung Choi",
      "Grace Byun",
      "Andrew Chung",
      "Ellie S. Paek",
      "Shinsun Lee",
      "Jinho D. Choi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.20336",
    "title": "A posteriori certification for neural networks approximations to PDEs",
    "abstract": "           We propose rigorous lower and upper bounds for neural network (NN) approximations to PDEs by efficiently computing the Riesz representations of suitable extension and restrictions of the NN residual towards geometrically simpler domains, which are either embedded or enveloping the original domain. Error bounds are proven and detailed for elliptic as well as parabolic problems. Numerical experiments show the good quantitative behaviour of the derived upper and lower error bounds.         ",
    "url": "https://arxiv.org/abs/2502.20336",
    "authors": [
      "Lewin Ernst",
      "Nikolaos Rekatsinas",
      "Karsten Urban"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.06591",
    "title": "NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural Networks",
    "abstract": "           Fault tolerance in Deep Neural Networks (DNNs) deployed on resource-constrained systems presents unique challenges for high-accuracy applications with strict timing requirements. Memory bit-flips can severely degrade DNN accuracy, while traditional protection approaches like Triple Modular Redundancy (TMR) often sacrifice accuracy to maintain reliability, creating a three-way dilemma between reliability, accuracy, and timeliness. We introduce NAPER, a novel protection approach that addresses this challenge through ensemble learning. Unlike conventional redundancy methods, NAPER employs heterogeneous model redundancy, where diverse models collectively achieve higher accuracy than any individual model. This is complemented by an efficient fault detection mechanism and a real-time scheduler that prioritizes meeting deadlines by intelligently scheduling recovery operations without interrupting inference. Our evaluations demonstrate NAPER's superiority: 40% faster inference in both normal and fault conditions, maintained accuracy 4.2% higher than TMR-based strategies, and guaranteed uninterrupted operation even during fault recovery. NAPER effectively balances the competing demands of accuracy, reliability, and timeliness in real-time DNN applications         ",
    "url": "https://arxiv.org/abs/2504.06591",
    "authors": [
      "Rian Adam Rajagede",
      "Muhammad Husni Santriaji",
      "Muhammad Arya Fikriansyah",
      "Hilal Hudan Nuha",
      "Yanjie Fu",
      "Yan Solihin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.11195",
    "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
    "abstract": "           Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional vision models. Existing defense techniques typically rely on adversarial fine-tuning during training, which requires labeled data and lacks of flexibility for downstream tasks. To address these limitations, we propose robust test-time prompt tuning (R-TPT), which mitigates the impact of adversarial attacks during the inference stage. We first reformulate the classic marginal entropy objective by eliminating the term that introduces conflicts under adversarial conditions, retaining only the pointwise entropy minimization. Furthermore, we introduce a plug-and-play reliability-based weighted ensembling strategy, which aggregates useful information from reliable augmented views to strengthen the defense. R-TPT enhances defense against adversarial attacks without requiring labeled training data while offering high flexibility for inference tasks. Extensive experiments on widely used benchmarks with various attacks demonstrate the effectiveness of R-TPT. The code is available in this https URL.         ",
    "url": "https://arxiv.org/abs/2504.11195",
    "authors": [
      "Lijun Sheng",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.11717",
    "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents",
    "abstract": "           Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. Environmental prompt injection attacks manipulate the environment to induce the web agent to perform a specific, attacker-chosen action--denoted as the target action. However, existing attacks suffer from limited effectiveness or stealthiness, or are impractical in real-world settings. In this work, we propose EnvInjection, a new attack that addresses these limitations. Our attack adds a perturbation to the raw pixel values of the rendered webpage. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the target action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple webpage datasets shows that EnvInjection is highly effective and significantly outperforms existing baselines.         ",
    "url": "https://arxiv.org/abs/2505.11717",
    "authors": [
      "Xilong Wang",
      "John Bloch",
      "Zedian Shao",
      "Yuepeng Hu",
      "Shuyan Zhou",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.22474",
    "title": "Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis",
    "abstract": "           Long-term forecasting of multivariate urban data poses a significant challenge due to the complex spatiotemporal dependencies inherent in such datasets. This paper presents DST, a novel multivariate time-series forecasting model that integrates graph attention and temporal convolution within a Graph Neural Network (GNN) to effectively capture spatial and temporal dependencies, respectively. To enhance model performance, we apply a decomposition-based preprocessing step that isolates trend, seasonal, and residual components of the time series, enabling the learning of distinct graph structures for different time-series components. Extensive experiments on real-world urban datasets, including electricity demand, weather metrics, carbon intensity, and air pollution, demonstrate the effectiveness of DST across a range of forecast horizons, from several days to one month. Specifically, our approach achieves an average improvement of 2.89% to 9.10% in long-term forecasting accuracy over state-of-the-art time-series forecasting models.         ",
    "url": "https://arxiv.org/abs/2505.22474",
    "authors": [
      "Amirhossein Sohrabbeig",
      "Omid Ardakanian",
      "Petr Musilek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.24595",
    "title": "BinConv: A Neural Architecture for Ordinal Encoding in Time-Series Forecasting",
    "abstract": "           Recent work in time series forecasting has explored reformulating regression as a classification task. By discretizing the continuous target space into bins and predicting over a fixed set of classes, these approaches benefit from more stable training, improved uncertainty modeling, and compatibility with modern deep learning architectures. However, most existing methods rely on one-hot encoding, which ignores the inherent ordinal structure of the target values. As a result, they fail to convey information about the relative distance between predicted and true values during training. In this paper, we address this limitation by applying \\textbf{Cumulative Binary Encoding} (CBE), a monotonic binary representation that transforms both model inputs and outputs. CBE implicitly preserves ordinal and magnitude information, allowing models to learn distance aware representations while operating within a classification framework. To leverage CBE effectively, we propose \\textbf{BinConv}, a fully convolutional neural network architecture designed for probabilistic forecasting. We demonstrate that standard fully connected layers are not only less computationally efficient than convolutional layers when used with CBE, but also degrade forecasting performance. Our experiments on standard benchmark datasets show that BinConv achieves superior performance compared to widely used baselines in both point and probabilistic forecasting, while requiring fewer parameters and enabling faster training.         ",
    "url": "https://arxiv.org/abs/2505.24595",
    "authors": [
      "Andrei Chernov",
      "Vitaliy Pozdnyakov",
      "Ilya Makarov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.06530",
    "title": "Breaking the Gaussian Barrier: Residual-PAC Privacy for Automatic Privatization",
    "abstract": "           The Probably Approximately Correct (PAC) Privacy framework [46] provides a powerful instance-based methodology to preserve privacy in complex data-driven systems. Existing PAC Privacy algorithms (we call them Auto-PAC) rely on a Gaussian mutual information upper bound. However, we show that the upper bound obtained by these algorithms is tight if and only if the perturbed mechanism output is jointly Gaussian with independent Gaussian noise. We propose two approaches for addressing this issue. First, we introduce two tractable post-processing methods for Auto-PAC, based on Donsker-Varadhan representation and sliced Wasserstein distances. However, the result still leaves wasted privacy budget. To address this issue more fundamentally, we introduce Residual-PAC (R-PAC) Privacy, an f-divergence-based measure to quantify privacy that remains after adversarial inference. To implement R-PAC Privacy in practice, we propose a Stackelberg Residual-PAC (SR-PAC) privatization mechanism, a game-theoretic framework that selects optimal noise distributions through convex bilevel optimization. Our approach achieves efficient privacy budget utilization for arbitrary data distributions and naturally composes when multiple mechanisms access the dataset. Through extensive experiments, we demonstrate that SR-PAC consistently obtains a better privacy-utility tradeoff than both PAC and differential privacy baselines.         ",
    "url": "https://arxiv.org/abs/2506.06530",
    "authors": [
      "Tao Zhang",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.07666",
    "title": "ProARD: progressive adversarial robustness distillation: provide wide range of robust students",
    "abstract": "           Adversarial Robustness Distillation (ARD) has emerged as an effective method to enhance the robustness of lightweight deep neural networks against adversarial attacks. Current ARD approaches have leveraged a large robust teacher network to train one robust lightweight student. However, due to the diverse range of edge devices and resource constraints, current approaches require training a new student network from scratch to meet specific constraints, leading to substantial computational costs and increased CO2 emissions. This paper proposes Progressive Adversarial Robustness Distillation (ProARD), enabling the efficient one-time training of a dynamic network that supports a diverse range of accurate and robust student networks without requiring retraining. We first make a dynamic deep neural network based on dynamic layers by encompassing variations in width, depth, and expansion in each design stage to support a wide range of architectures. Then, we consider the student network with the largest size as the dynamic teacher network. ProARD trains this dynamic network using a weight-sharing mechanism to jointly optimize the dynamic teacher network and its internal student networks. However, due to the high computational cost of calculating exact gradients for all the students within the dynamic network, a sampling mechanism is required to select a subset of students. We show that random student sampling in each iteration fails to produce accurate and robust students.         ",
    "url": "https://arxiv.org/abs/2506.07666",
    "authors": [
      "Seyedhamidreza Mousavi",
      "Seyedali Mousavi",
      "Masoud Daneshtalab"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.11066",
    "title": "CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval",
    "abstract": "           Code retrieval is essential in modern software development, as it boosts code reuse and accelerates debugging. However, current benchmarks primarily emphasize functional relevance while neglecting critical dimensions of software quality. Motivated by this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four key dimensions: correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained quality annotations for 42,725 queries and 134,907 code snippets in 11 programming languages, and is accompanied by two quality-centric evaluation metrics: Pairwise Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23 retrieval models, covering both open-source and proprietary systems, and find that even top-performing models frequently fail to distinguish buggy or insecure code from their more robust counterparts. Furthermore, we conduct preliminary investigations into training methods that explicitly encourage retrievers to recognize code quality. Using synthetic datasets, we demonstrate promising improvements in quality-aware metrics across various models, without sacrificing semantic relevance. Downstream code generation experiments further validate the effectiveness of our approach. Overall, our work highlights the importance of integrating quality signals into code retrieval systems, laying the groundwork for more trustworthy and robust software development tools.         ",
    "url": "https://arxiv.org/abs/2506.11066",
    "authors": [
      "Jiahui Geng",
      "Fengyu Cai",
      "Shaobo Cui",
      "Qing Li",
      "Liangwei Chen",
      "Chenyang Lyu",
      "Haonan Li",
      "Derui Zhu",
      "Walter Pretschner",
      "Heinz Koeppl",
      "Fakhri Karray"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.14799",
    "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust",
    "abstract": "           Recent advances in AI has made automated analysis of complex media content at scale possible while generating actionable insights regarding character representation along such dimensions as gender and age. Past works focused on quantifying representation from audio/video/text using AI models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are those to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these open questions by proposing a new AI-based character representation tool and performing a thorough user study. Our tool has two components: (i) An analytics extraction model based on the Contrastive Language Image Pretraining (CLIP) foundation model that analyzes visual screen data to quantify character representation across age and gender; (ii) A visualization component effectively designed for presenting the analytics to lay audience. The user study seeks empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We found that participants were able to understand the analytics in our visualizations, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and the user study data can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.14799",
    "authors": [
      "Evdoxia Taka",
      "Debadyuti Bhattacharya",
      "Joanne Garde-Hansen",
      "Sanjay Sharma",
      "Tanaya Guha"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17632",
    "title": "Pixel-Optimization-Free Patch Attack on Stereo Depth Estimation",
    "abstract": "           Stereo Depth Estimation (SDE) is essential for scene perception in vision-based systems such as autonomous driving. Prior work shows SDE is vulnerable to pixel-optimization attacks, but these methods are limited to digital, static, and view-specific settings, making them impractical. This raises a central question: how to design deployable, adaptive, and transferable attacks under realistic constraints? We present two contributions to answer it. First, we build a unified framework that extends pixel-optimization attacks to four stereo-matching stages: feature extraction, cost-volume construction, cost aggregation, and disparity regression. Through systematic evaluation across nine SDE models with realistic constraints like photometric consistency, we show existing attacks suffer from poor transferability. Second, we propose PatchHunter, the first pixel-optimization-free attack. PatchHunter casts patch generation as a search in a structured space of visual patterns that disrupt core SDE assumptions, and uses a reinforcement learning policy to discover effective and transferable patterns efficiently. We evaluate PatchHunter on three levels: autonomous driving dataset, high-fidelity simulator, and real-world deployment. On KITTI, PatchHunter outperforms pixel-level attacks in both effectiveness and black-box transferability. Tests in CARLA and on vehicles with industrial-grade stereo cameras confirm robustness to physical variations. Even under challenging conditions such as low lighting, PatchHunter achieves a D1-all error above 0.4, while pixel-level attacks remain near 0.         ",
    "url": "https://arxiv.org/abs/2506.17632",
    "authors": [
      "Hangcheng Liu",
      "Xu Kuang",
      "Xingshuo Han",
      "Xingwan Wu",
      "Haoran Ou",
      "Shangwei Guo",
      "Xingyi Huang",
      "Tao Xiang",
      "Tianwei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.18088",
    "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
    "abstract": "           Simulation-based data synthesis has emerged as a powerful paradigm for advancing real-world robotic manipulation. Yet existing datasets remain insufficient for robust bimanual manipulation due to (1) the lack of scalable task generation methods and (2) oversimplified simulation environments. We present RoboTwin 2.0, a scalable framework for automated, large-scale generation of diverse and realistic data, together with unified evaluation protocols for dual-arm manipulation. At its core is RoboTwin-OD, an object library of 731 instances across 147 categories with semantic and manipulation-relevant annotations. Building on this, we design an expert data synthesis pipeline that leverages multimodal language models (MLLMs) and simulation-in-the-loop refinement to automatically generate task-level execution code. To improve sim-to-real transfer, RoboTwin 2.0 applies structured domain randomization along five axes: clutter, lighting, background, tabletop height, and language, enhancing data diversity and policy robustness. The framework is instantiated across 50 dual-arm tasks and five robot embodiments. Empirically, it yields a 10.9% gain in code generation success rate. For downstream policy learning, a VLA model trained with synthetic data plus only 10 real demonstrations achieves a 367% relative improvement over the 10-demo baseline, while zero-shot models trained solely on synthetic data obtain a 228% gain. These results highlight the effectiveness of RoboTwin 2.0 in strengthening sim-to-real transfer and robustness to environmental variations. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation. Project Page: this https URL, Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2506.18088",
    "authors": [
      "Tianxing Chen",
      "Zanxin Chen",
      "Baijun Chen",
      "Zijian Cai",
      "Yibin Liu",
      "Zixuan Li",
      "Qiwei Liang",
      "Xianliang Lin",
      "Yiheng Ge",
      "Zhenyu Gu",
      "Weiliang Deng",
      "Yubin Guo",
      "Tian Nian",
      "Xuanbing Xie",
      "Qiangyu Chen",
      "Kailun Su",
      "Tianling Xu",
      "Guodong Liu",
      "Mengkang Hu",
      "Huan-ang Gao",
      "Kaixuan Wang",
      "Zhixuan Liang",
      "Yusen Qin",
      "Xiaokang Yang",
      "Ping Luo",
      "Yao Mu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2506.20828",
    "title": "Practical and Accurate Local Edge Differentially Private Graph Algorithms",
    "abstract": "           The rise of massive networks across diverse domains necessitates sophisticated graph analytics, often involving sensitive data and raising privacy concerns. This paper addresses these challenges using local differential privacy (LDP), which enforces privacy at the individual level, where no third-party entity is trusted, unlike centralized models that assume a trusted curator. We introduce novel LDP algorithms for two fundamental graph statistics: k-core decomposition and triangle counting. Our approach leverages input-dependent private graph properties, specifically the degeneracy and maximum degree of the graph, to improve theoretical utility. Unlike prior methods, our error bounds are determined by the maximum degree rather than the total number of edges, resulting in significantly tighter guarantees. For triangle counting, we improve upon the work of Imola, Murakami, and Chaudhury [USENIX Security `21, `22], which bounds error in terms of edge count. Instead, our algorithm achieves bounds based on graph degeneracy by leveraging a private out-degree orientation, a refined variant of Eden et al.'s randomized response technique [ICALP `23], and a novel analysis, yielding stronger guarantees than prior work. Beyond theoretical gains, we are the first to evaluate local DP algorithms in a distributed simulation, unlike prior work tested on a single processor. Experiments on real-world graphs show substantial accuracy gains: our k-core decomposition achieves errors within 3x of exact values, far outperforming the 131x error in the baseline of Dhulipala et al. [FOCS `22]. Our triangle counting algorithm reduces multiplicative approximation errors by up to six orders of magnitude, while maintaining competitive runtime.         ",
    "url": "https://arxiv.org/abs/2506.20828",
    "authors": [
      "Pranay Mundra",
      "Charalampos Papamanthou",
      "Julian Shun",
      "Quanquan C. Liu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2507.05622",
    "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective",
    "abstract": "           The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2507.05622",
    "authors": [
      "Shuo Shao",
      "Yiming Li",
      "Mengren Zheng",
      "Zhiyang Hu",
      "Yukun Chen",
      "Boheng Li",
      "Yu He",
      "Junfeng Guo",
      "Dacheng Tao",
      "Zhan Qin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2507.16190",
    "title": "LABNet: A Lightweight Attentive Beamforming Network for Ad-hoc Multichannel Microphone Invariant Real-Time Speech Enhancement",
    "abstract": "           Multichannel speech enhancement (SE) aims to restore clean speech from noisy measurements by leveraging spatiotemporal signal features. In ad-hoc array conditions, microphone invariance (MI) requires systems to handle different microphone numbers and array geometries. From a practical perspective, multichannel recordings inevitably increase the computational burden for edge-device applications, highlighting the necessity of lightweight and efficient deployments. In this work, we propose a lightweight attentive beamforming network (LABNet) to integrate MI in a low-complexity real-time SE system. We design a three-stage framework for efficient intra-channel modeling and inter-channel interaction. A cross-channel attention module is developed to aggregate features from each channel selectively. Experimental results demonstrate our LABNet achieves impressive performance with ultra-light resource overhead while maintaining the MI, indicating great potential for ad-hoc array processing. The code is available:this https URL ",
    "url": "https://arxiv.org/abs/2507.16190",
    "authors": [
      "Haoyin Yan",
      "Jie Zhang",
      "Chengqian Jiang",
      "Shuang Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.16224",
    "title": "LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D object detection",
    "abstract": "           Existing LiDAR-Camera fusion methods have achieved strong results in 3D object detection. To address the sparsity of point clouds, previous approaches typically construct spatial pseudo point clouds via depth completion as auxiliary input and adopts a proposal-refinement framework to generate detection results. However, introducing pseudo points inevitably brings noise, potentially resulting in inaccurate predictions. Considering the differing roles and reliability levels of each modality, we propose LDRFusion, a novel Lidar-dominant two-stage refinement framework for multi-sensor fusion. The first stage soley relies on LiDAR to produce accurately localized proposals, followed by a second stage where pseudo point clouds are incorporated to detect challenging instances. The instance-level results from both stages are subsequently merged. To further enhance the representation of local structures in pseudo point clouds, we present a hierarchical pseudo point residual encoding module, which encodes neighborhood sets using both feature and positional residuals. Experiments on the KITTI dataset demonstrate that our framework consistently achieves strong performance across multiple categories and difficulty levels.         ",
    "url": "https://arxiv.org/abs/2507.16224",
    "authors": [
      "Jijun Wang",
      "Yan Wu",
      "Yujian Mo",
      "Junqiao Zhao",
      "Jun Yan",
      "Yinghao Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.16887",
    "title": "Revisiting Pre-trained Language Models for Vulnerability Detection",
    "abstract": "           The rapid advancement of pre-trained language models (PLMs) has demonstrated promising results for various code-related tasks. However, their effectiveness in detecting real-world vulnerabilities remains a critical challenge. % for the security community. While existing empirical studies evaluate PLMs for vulnerability detection (VD), their inadequate consideration in data preparation, evaluation setups, and experimental settings undermines the accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD, an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and large-scale PLMs using newly constructed datasets. Specifically, we compare the performance of PLMs under both fine-tuning and prompt engineering, assess their effectiveness and generalizability across various training and testing settings, and analyze their robustness against code normalization, abstraction, and semantic-preserving transformations. Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks designed to capture the syntactic and semantic patterns of code outperform both general-purpose PLMs and those solely pre-trained or fine-tuned on large code corpora. However, these models face notable challenges in real-world scenarios, such as difficulties in detecting vulnerabilities with complex dependencies, handling perturbations introduced by code normalization and abstraction, and identifying semantic-preserving vulnerable code transformations. Also, the truncation caused by the limited context windows of PLMs can lead to a non-negligible amount of labeling errors. This study underscores the importance of thorough evaluations of model performance in practical scenarios and outlines future directions to help enhance the effectiveness of PLMs for realistic VD applications.         ",
    "url": "https://arxiv.org/abs/2507.16887",
    "authors": [
      "Youpeng Li",
      "Weiliang Qi",
      "Xuyu Wang",
      "Fuxun Yu",
      "Xinda Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.03252",
    "title": "Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion",
    "abstract": "           Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust 3D object detection tasks. Existing methods often rely on the score matching from 3D boxes or pre-trained diffusion priors. However, they typically require multi-step iterations in inference, which limits efficiency. To address this, we propose a Robust single-stage fully Sparse 3D object Detection Network with a Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in latent feature spaces through lightweight denoising networks like multi-level denoising autoencoders (DAEs). This enables RSDNet to effectively understand scene distributions under multi-level perturbations, achieving robust and reliable detection. Meanwhile, we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise samples and targets, enhancing RSDNet robustness to multiple perturbations. Furthermore, a semantic-geometric conditional guidance is introduced to perceive the object boundaries and shapes, alleviating the center feature missing problem in sparse representations, enabling RSDNet to perform in a fully sparse detection pipeline. Moreover, the detachable denoising network design of DLF enables RSDNet to perform single-step detection in inference, further enhancing detection efficiency. Extensive experiments on public benchmarks show that RSDNet can outperform existing methods, achieving state-of-the-art detection.         ",
    "url": "https://arxiv.org/abs/2508.03252",
    "authors": [
      "Wentao Qu",
      "Guofeng Mei",
      "Jing Wang",
      "Yujiao Wu",
      "Xiaoshui Huang",
      "Liang Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.03337",
    "title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration",
    "abstract": "           The practical application of Multimodal Large Language Models (MLLMs) to Video Question Answering (Video-QA) is severely hindered by the high token cost of processing numerous video frames. While increasing the number of sampled frames is a common strategy, we observe a \"less is more\" phenomenon where excessive frames can paradoxically degrade performance due to context dilution. Concurrently, state-of-the-art keyframe selection methods, while effective, still yield significant temporal redundancy, which we term 'visual echoes'. To address these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel post-processing method that intelligently prunes the selected keyframes. AFP employs an adaptive hierarchical clustering algorithm on a fused ResNet-50 and CLIP feature space to identify and merge these echoes into single representatives. To compensate for information loss, we then introduce a lightweight, text-based semantic graph that provides critical context with minimal token overhead. Conducting extensive experiments on the LongVideoBench and VideoMME benchmarks across multiple leading MLLMs, our full approach demonstrates a drastic reduction in required frames by up to 86.9% and total input tokens by up to 83.2%. Crucially, by providing a concise, high-quality set of frames, our method not only enhances efficiency but often improves accuracy over baselines that use more frames. The code will be released upon publication.         ",
    "url": "https://arxiv.org/abs/2508.03337",
    "authors": [
      "Shaoguang Wang",
      "Ziyang Chen",
      "Yijie Xu",
      "Weiyu Guo",
      "Hui Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13732",
    "title": "Self-Organizing Agent Network for LLM-based Workflow Automation",
    "abstract": "           Recent multi-agent frameworks built upon large language models (LLMs) have demonstrated remarkable capabilities in complex task planning. However, in real-world enterprise environments, business workflows are typically composed through modularization and reuse of numerous subprocesses, resulting in intricate workflows characterized by lengthy and deeply nested execution paths. Such complexity poses significant challenges for LLM-driven orchestration, as extended reasoning chains and state-space explosions severely impact planning effectiveness and the proper sequencing of tool invocations. Therefore, developing an orchestration method with controllable structures capable of handling multi-layer nesting becomes a critical issue. To address this, we propose a novel structure-driven orchestration framework Self-Organizing Agent Network (SOAN). SOAN incrementally builds a formalized agent network by identifying and encapsulating structural units as independent agents, enhancing modularity and clarity in orchestration. Extensive evaluations were performed using multiple benchmarks as well as a real-world enterprise workflow dataset. Experimental results demonstrate that SOAN significantly outperforms state-of-the-art methods in terms of adaptability, fault tolerance, and execution efficiency.         ",
    "url": "https://arxiv.org/abs/2508.13732",
    "authors": [
      "Yiming Xiong",
      "Jian Wang",
      "Bing Li",
      "Yuhan Zhu",
      "Yuqi Zhao"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.14086",
    "title": "EEGDM: EEG Representation Learning via Generative Diffusion Model",
    "abstract": "           While electroencephalogram (EEG) has been a crucial tool for monitoring the brain and diagnosing neurological disorders (e.g., epilepsy), learning meaningful representations from raw EEG signals remains challenging due to limited annotations and high signal variability. Recently, EEG foundation models (FMs) have shown promising potential by adopting transformer architectures and self-supervised pre-training methods from large language models (e.g., masked prediction) to learn representations from diverse EEG data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large models often incurred high computational costs during both training and inference, with only marginal performance improvements as the model size increases. In this work, we proposed an EEG representation learning framework building upon Generative Diffusion Model (EEGDM). Specifically, we developed a structured state-space model for diffusion pretraining (SSMDP) to better capture the temporal dynamics of EEG signals and trained the model using a Denoising Diffusion Probabilistic Model. Subsequently, the resulting latent EEG representations were then used for downstream classification tasks via our proposed latent fusion transformer (LFT). To evaluate our method, we used multi-event datasets covering both interictal epileptiform discharges and seizure detection, and compared EEGDM with current state-of-the-art approaches, including EEG FMs. Empirical results showed that our method outperformed the existing methods. These findings suggested that EEGDM offered a promising alternative to current FMs. Our code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.14086",
    "authors": [
      "Jia Hong Puah",
      "Sim Kuan Goh",
      "Ziwei Zhang",
      "Zixuan Ye",
      "Chow Khuen Chan",
      "Kheng Seang Lim",
      "Si Lei Fong",
      "Kok Sin Woon",
      "Cuntai Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.14500",
    "title": "DGenCTR: Towards a Universal Generative Paradigm for Click-Through Rate Prediction via Discrete Diffusion",
    "abstract": "           Recent advances in generative models have inspired the field of recommender systems to explore generative approaches, but most existing research focuses on sequence generation, a paradigm ill-suited for click-through rate (CTR) prediction. CTR models critically depend on a large number of cross-features between the target item and the user to estimate the probability of clicking on the item, and discarding these cross-features will significantly impair model performance. Therefore, to harness the ability of generative models to understand data distributions and thereby alleviate the constraints of traditional discriminative models in label-scarce space, diverging from the item-generation paradigm of sequence generation methods, we propose a novel sample-level generation paradigm specifically designed for the CTR task: a two-stage Discrete Diffusion-Based Generative CTR training framework (DGenCTR). This two-stage framework comprises a diffusion-based generative pre-training stage and a CTR-targeted supervised fine-tuning stage for CTR. Finally, extensive offline experiments and online A/B testing conclusively validate the effectiveness of our framework.         ",
    "url": "https://arxiv.org/abs/2508.14500",
    "authors": [
      "Moyu Zhang",
      "Yun Chen",
      "Yujun Jin",
      "Jinxin Hu",
      "Yu Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.15031",
    "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
    "abstract": "           Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.15031",
    "authors": [
      "Kaixiang Zhao",
      "Lincan Li",
      "Kaize Ding",
      "Neil Zhenqiang Gong",
      "Yue Zhao",
      "Yushun Dong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.15387",
    "title": "DIO: Refining Mutual Information and Causal Chain to Enhance Machine Abstract Reasoning Ability",
    "abstract": "           Despite the outstanding performance of current deep learning models across various domains, their fundamental bottleneck in abstract reasoning remains unresolved. To address this challenge, the academic community has introduced Raven's Progressive Matrices (RPM) problems as an authoritative benchmark for evaluating the abstract reasoning capabilities of deep learning algorithms, with a focus on core intelligence dimensions such as abstract reasoning, pattern recognition, and complex problem-solving. Therefore, this paper centers on solving RPM problems, aiming to contribute to enhancing the abstract reasoning abilities of machine intelligence. Firstly, this paper adopts a ``causal chain modeling'' perspective to systematically analyze the complete causal chain in RPM tasks: image $\\rightarrow$ abstract attributes $\\rightarrow$ progressive attribute patterns $\\rightarrow$ pattern consistency $\\rightarrow$ correct answer. Based on this analysis, the network architecture of the baseline model DIO is designed. However, experiments reveal that the optimization objective formulated for DIO, namely maximizing the variational lower bound of mutual information between the context and the correct option, fails to enable the model to genuinely acquire the predefined human reasoning logic. This is attributed to two main reasons: the tightness of the lower bound significantly impacts the effectiveness of mutual information maximization, and mutual information, as a statistical measure, does not capture the causal relationship between subjects and objects. To overcome these limitations, this paper progressively proposes three improvement methods:         ",
    "url": "https://arxiv.org/abs/2508.15387",
    "authors": [
      "Ruizhuo Song",
      "Beiming Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.15495",
    "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion",
    "abstract": "           Code completion is a prominent application of Large Language Models (LLMs) in software engineering. Due to the near real-time response requirements of this task, base models with small to medium-sized parameters are typically employed, supplemented by various optimization and post-training techniques. However, these optimization methods often have trade-offs, leading to a seesaw effect where performance improvements on certain datasets or metrics are accompanied by degradations on others -- sometimes even falling below the baseline model's performance. This paper proposes SynthCoder, a model that integrates leading industry practices to achieve state-of-the-art performance on the Fill-in-the-Middle (FIM) code completion task. In specific, we first construct a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with heuristics that simulate developer behavior. Then we enrich our training corpus with cross-file contextual information using the BM25 algorithm and call graphs, enhancing the model's ability to perform code completion in both file-level and repository-level scenarios. As the last step, we employ a two-stage training process using the Seed-Coder-8B-Base as the base model. First, we fine-tune the model using Curriculum Learning technology. Following this, we perform alignment using Direct Preference Optimization (DPO) with preference pairs generated through Rejection Sampling. Experimental results demonstrate that our final model excels on mainstream repository-level code completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and CoLT. Furthermore, our carefully curated training set effectively mitigates the model's tendency to just repeat existing code, a common issue existing in various code completion models.         ",
    "url": "https://arxiv.org/abs/2508.15495",
    "authors": [
      "Dongjun Yu",
      "Xiao Yan",
      "Zhenrui Li",
      "Jipeng Xiao",
      "Haochuan He",
      "Yongda Yu",
      "Hao Zhang",
      "Guoping Rong",
      "Xiaobo Huang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.16021",
    "title": "X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents",
    "abstract": "           State-sponsored trolls, malicious actors who deploy sophisticated linguistic manipulation in coordinated information campaigns, posing threats to online discourse integrity. While Large Language Models (LLMs) achieve strong performance on general natural language processing (NLP) tasks, they struggle with subtle propaganda detection and operate as ``black boxes'', providing no interpretable insights into manipulation strategies. This paper introduces X-Troll, a novel framework that bridges this gap by integrating explainable adapter-based LLMs with expert-derived linguistic knowledge to detect state-sponsored trolls and provide human-readable explanations for its decisions. X-Troll incorporates appraisal theory and propaganda analysis through specialized LoRA adapters, using dynamic gating to capture campaign-specific discourse patterns in coordinated information operations. Experiments on real-world data demonstrate that our linguistically-informed approach shows strong performance compared with both general LLM baselines and existing troll detection models in accuracy while providing enhanced transparency through expert-grounded explanations that reveal the specific linguistic strategies used by state-sponsored actors. X-Troll source code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.16021",
    "authors": [
      "Lin Tian",
      "Xiuzhen Zhang",
      "Maria Myung-Hee Kim",
      "Jennifer Biggs",
      "Marian-Andrei Rizoiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.16024",
    "title": "Wavelet-Space Super-Resolution Network for Rendering Pipelines",
    "abstract": "           We investigate the use of wavelet-space feature decomposition in neural super-resolution for rendering pipelines. Building on neural upscaling frameworks, we introduce a wavelet-domain representation that separates low-frequency and high-frequency details before reconstruction, enabling the network to better preserve fine textures while maintaining structural consistency. Unlike RGB-space regression, our approach leverages the stationary wavelet transform (SWT) to avoid spatial down-sampling, ensuring alignment across subbands and preserving shift invariance. The model predicts wavelet coefficients conditioned on spatial G-buffers and temporally warped history frames, which are then recombined through inverse wavelet synthesis. We conduct a comprehensive ablation study across wavelet families, transform types, and architectural variants, showing that incorporating SWT improves PSNR by 1.5 dB and reduces LPIPS by 17% on average, with only a modest relative runtime overhead. Taken together, our results suggest that wavelet-domain representations a principled path toward higher-quality super-resolution in graphics applications.         ",
    "url": "https://arxiv.org/abs/2508.16024",
    "authors": [
      "Prateek Poudel",
      "Prashant Aryal",
      "Kirtan Kunwar",
      "Navin Nepal",
      "Dinesh Baniya Kshatri"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.18463",
    "title": "Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling",
    "abstract": "           Detecting anomalies in surveillance footage is inherently challenging due to their unpredictable and context-dependent nature. This work introduces a novel context-aware zero-shot anomaly detection framework that identifies abnormal events without exposure to anomaly examples during training. The proposed hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context. TimeSformer serves as the vision backbone to extract rich spatial-temporal features, while DPC forecasts future representations to identify temporal deviations. Furthermore, a CLIP-based semantic stream enables concept-level anomaly detection through context-specific text prompts. These components are jointly trained using InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic representations. A context-gating mechanism further enhances decision-making by modulating predictions with scene-aware cues or global video features. By integrating predictive modeling with vision-language understanding, the system can generalize to previously unseen behaviors in complex environments. This framework bridges the gap between temporal reasoning and semantic context in zero-shot anomaly detection for surveillance. The code for this research has been made available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.18463",
    "authors": [
      "Md. Rashid Shahriar Khan",
      "Md. Abrar Hasan",
      "Mohammod Tareq Aziz Justice"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.18473",
    "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing",
    "abstract": "           While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models. We propose a multiple-testing-inspired method to solve the hallucination detection problem, and provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2508.18473",
    "authors": [
      "Jiawei Li",
      "Akshayaa Magesh",
      "Venugopal V. Veeravalli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.18911",
    "title": "Enhancing Model Privacy in Federated Learning with Random Masking and Quantization",
    "abstract": "           The primary goal of traditional federated learning is to protect data privacy by enabling distributed edge devices to collaboratively train a shared global model while keeping raw data decentralized at local clients. The rise of large language models (LLMs) has introduced new challenges in distributed systems, as their substantial computational requirements and the need for specialized expertise raise critical concerns about protecting intellectual property (IP). This highlights the need for a federated learning approach that can safeguard both sensitive data and proprietary models. To tackle this challenge, we propose FedQSN, a federated learning approach that leverages random masking to obscure a subnetwork of model parameters and applies quantization to the remaining parameters. Consequently, the server transmits only a privacy-preserving proxy of the global model to clients during each communication round, thus enhancing the model's confidentiality. Experimental results across various models and tasks demonstrate that our approach not only maintains strong model performance in federated learning settings but also achieves enhanced protection of model parameters compared to baseline methods.         ",
    "url": "https://arxiv.org/abs/2508.18911",
    "authors": [
      "Zhibo Xu",
      "Jianhao Zhu",
      "Jingwen Xu",
      "Changze Lv",
      "Zisu Huang",
      "Xiaohua Wang",
      "Muling Wu",
      "Qi Qian",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.19009",
    "title": "FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning",
    "abstract": "           Heterogeneous Federated Learning (HFL) has gained attention for its ability to accommodate diverse models and heterogeneous data across clients. Prototype-based HFL methods emerge as a promising solution to address statistical heterogeneity and privacy challenges, paving the way for new advancements in HFL research. This method focuses on sharing only class-representative prototypes among heterogeneous clients. However, these prototypes are often aggregated on the server using weighted averaging, leading to sub-optimal global knowledge; these cause the shrinking of aggregated prototypes, which negatively affects the model performance in scenarios when models are heterogeneous and data distributions are extremely non-IID. We propose FedProtoKD in a Heterogeneous Federated Learning setting, using an enhanced dual-knowledge distillation mechanism to improve the system performance with clients' logits and prototype feature representation. We aim to resolve the prototype margin-shrinking problem using a contrastive learning-based trainable server prototype by leveraging a class-wise adaptive prototype margin. Furthermore, we assess the importance of public samples using the closeness of the sample's prototype to its class representative prototypes, which enhances learning performance. FedProtoKD achieved average improvements of 1.13% up to 34.13% accuracy across various settings and significantly outperforms existing state-of-the-art HFL methods.         ",
    "url": "https://arxiv.org/abs/2508.19009",
    "authors": [
      "Md Anwar Hossen",
      "Fatema Siddika",
      "Wensheng Zhang",
      "Anuj Sharma",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2212.12028",
    "title": "Deep Learning of Semi-Competing Risk Data via a New Neural Expectation-Maximization Algorithm",
    "abstract": "           Prognostication for lung cancer, a leading cause of mortality, remains a complex task, as it needs to quantify the associations of risk factors and health events spanning a patient's entire life. One challenge is that an individual's disease course involves non-terminal (e.g., disease progression) and terminal (e.g., death) events, which form semi-competing relationships. Our motivation comes from the Boston Lung Cancer Study, a large lung cancer survival cohort, which investigates how risk factors influence a patient's disease trajectory. Following developments in the prediction of time-to-event outcomes with neural networks, deep learning has become a focal area for the development of risk prediction methods in survival analysis. However, limited work has been done to predict multi-state or semi-competing risk outcomes, where a patient may experience adverse events such as disease progression prior to death. We propose a novel neural expectation-maximization algorithm to bridge the gap between classical statistical approaches and machine learning. Our algorithm enables estimation of the non-parametric baseline hazards of each state transition, risk functions of predictors, and the degree of dependence among different transitions, via a multi-task deep neural network with transition-specific sub-architectures. We apply our method to the Boston Lung Cancer Study and investigate the impact of clinical and genetic predictors on disease progression and mortality.         ",
    "url": "https://arxiv.org/abs/2212.12028",
    "authors": [
      "Stephen Salerno",
      "Yi Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.01245",
    "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules",
    "abstract": "           Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.         ",
    "url": "https://arxiv.org/abs/2404.01245",
    "authors": [
      "Xiang Li",
      "Feng Ruan",
      "Huiyuan Wang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.13868",
    "title": "Robust Detection of Watermarks for Large Language Models Under Human Edits",
    "abstract": "           Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \\textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.         ",
    "url": "https://arxiv.org/abs/2411.13868",
    "authors": [
      "Xiang Li",
      "Feng Ruan",
      "Huiyuan Wang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.07208",
    "title": "Towards Understanding of Frequency Dependence on Sound Event Detection",
    "abstract": "           In this work, we conduct an in-depth analysis of two frequency-dependent methods for sound event detection (SED): FilterAugment and frequency dynamic convolution (FDY conv). The goal is to better understand their characteristics and behaviors in the context of SED. While SED has been rapidly advancing through the adoption of various deep learning techniques from other pattern recognition fields, such adopted techniques are often not suitable for SED. To address this issue, two frequency-dependent SED methods were previously proposed: FilterAugment, a data augmentation randomly weighting frequency bands, and FDY conv, an architecture applying frequency adaptive convolution kernels. These methods have demonstrated superior performance in SED, and we aim to further analyze their detailed effectiveness and characteristics in SED. We compare class-wise performance to find out specific pros and cons of FilterAugment and FDY conv. We apply Gradient-weighted Class Activation Mapping (Grad-CAM), which highlights time-frequency region that is more inferred by the model, on SED models with and without frequency masking and two types of FilterAugment to observe their detailed characteristics. We propose simpler frequency dependent convolution methods and compare them with FDY conv to further understand which components of FDY conv affects SED performance. Lastly, we apply PCA to show how FDY conv adapts dynamic kernel across frequency dimensions on different sound event classes. The results and discussions demonstrate that frequency dependency plays a significant role in sound event detection and further confirms the effectiveness of frequency dependent methods on SED.         ",
    "url": "https://arxiv.org/abs/2502.07208",
    "authors": [
      "Hyeonuk Nam",
      "Seong-Hu Kim",
      "Deokki Min",
      "Byeong-Yun Ko",
      "Yong-Hwa Park"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2506.06087",
    "title": "Multilevel neural simulation-based inference",
    "abstract": "           Neural simulation-based inference (SBI) is a popular set of methods for Bayesian inference when models are only available in the form of a simulator. These methods are widely used in the sciences and engineering, where writing down a likelihood can be significantly more challenging than constructing a simulator. However, the performance of neural SBI can suffer when simulators are computationally expensive, thereby limiting the number of simulations that can be performed. In this paper, we propose a novel approach to neural SBI which leverages multilevel Monte Carlo techniques for settings where several simulators of varying cost and fidelity are available. We demonstrate through both theoretical analysis and extensive experiments that our method can significantly enhance the accuracy of SBI methods given a fixed computational budget.         ",
    "url": "https://arxiv.org/abs/2506.06087",
    "authors": [
      "Yuga Hikida",
      "Ayush Bharti",
      "Niall Jeffrey",
      "Fran\u00e7ois-Xavier Briol"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2508.01799",
    "title": "Contrastive Multi-Task Learning with Solvent-Aware Augmentation for Drug Discovery",
    "abstract": "           Accurate prediction of protein-ligand interactions is essential for computer-aided drug discovery. However, existing methods often fail to capture solvent-dependent conformational changes and lack the ability to jointly learn multiple related tasks. To address these limitations, we introduce a pre-training method that incorporates ligand conformational ensembles generated under diverse solvent conditions as augmented input. This design enables the model to learn both structural flexibility and environmental context in a unified manner. The training process integrates molecular reconstruction to capture local geometry, interatomic distance prediction to model spatial relationships, and contrastive learning to build solvent-invariant molecular representations. Together, these components lead to significant improvements, including a 3.7% gain in binding affinity prediction, an 82% success rate on the PoseBusters Astex docking benchmarks, and an area under the curve of 97.1% in virtual screening. The framework supports solvent-aware, multi-task modeling and produces consistent results across benchmarks. A case study further demonstrates sub-angstrom docking accuracy with a root-mean-square deviation of 0.157 angstroms, offering atomic-level insight into binding mechanisms and advancing structure-based drug design.         ",
    "url": "https://arxiv.org/abs/2508.01799",
    "authors": [
      "Jing Lan",
      "Hexiao Ding",
      "Hongzhao Chen",
      "Yufeng Jiang",
      "Nga-Chun Ng",
      "Gerald W.Y. Cheng",
      "Zongxi Li",
      "Jing Cai",
      "Liang-ting Lin",
      "Jung Sun Yoo"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.16557",
    "title": "Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution",
    "abstract": "           Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD's generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.         ",
    "url": "https://arxiv.org/abs/2508.16557",
    "authors": [
      "Tainyi Zhang",
      "Zheng-Peng Duan",
      "Peng-Tao Jiang",
      "Bo Li",
      "Ming-Ming Cheng",
      "Chun-Le Guo",
      "Chongyi Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.16895",
    "title": "Quantum State Fidelity for Functional Neural Network Construction",
    "abstract": "           Neuroscientists face challenges in analyzing high-dimensional neural recording data of dense functional networks. Without ground-truth reference data, finding the best algorithm for recovering neurologically relevant networks remains an open question. We implemented hybrid quantum algorithms to construct functional networks and compared them with the results of documented classical techniques. We demonstrated that our quantum state fidelity methods can provide competitive alternatives to classical metrics by revealing distinct functional networks. Our results suggest that quantum computing offers a viable and potentially advantageous alternative for data-driven modeling in neuroscience, underscoring its broader applicability in high-dimensional graph inference and complex system analysis.         ",
    "url": "https://arxiv.org/abs/2508.16895",
    "authors": [
      "Skylar Chan",
      "Wilson Smith",
      "Kyla Gabriel"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Metric Geometry (math.MG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  }
]