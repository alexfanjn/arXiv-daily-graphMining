[
  {
    "id": "arXiv:2508.13156",
    "title": "EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code",
    "abstract": "           Large Language Models (LLMs) have demonstrated great potential in automating the generation of Verilog hardware description language code for hardware design. This automation is critical to reducing human effort in the complex and error-prone process of hardware design. However, existing approaches predominantly rely on human intervention and fine-tuning using curated datasets, limiting their scalability in automated design workflows. Although recent iterative search techniques have emerged, they often fail to explore diverse design solutions and may underperform simpler approaches such as repeated prompting. To address these limitations, we introduce EvoVerilog, a novel framework that combines the reasoning capabilities of LLMs with evolutionary algorithms to automatically generate and refine Verilog code. EvoVerilog utilizes a multiobjective, population-based search strategy to explore a wide range of design possibilities without requiring human intervention. Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively. Furthermore, the framework showcases its ability to explore diverse designs by simultaneously generating a variety of functional Verilog code while optimizing resource utilization.         ",
    "url": "https://arxiv.org/abs/2508.13156",
    "authors": [
      "Ping Guo",
      "Yiting Wang",
      "Wanghao Ye",
      "Yexiao He",
      "Ziyao Wang",
      "Xiaopeng Dai",
      "Ang Li",
      "Qingfu Zhang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13174",
    "title": "AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining",
    "abstract": "           Formula alpha mining, which generates predictive signals from financial data, is critical for quantitative investment. Although various algorithmic approaches-such as genetic programming, reinforcement learning, and large language models-have significantly expanded the capacity for alpha discovery, systematic evaluation remains a key challenge. Existing evaluation metrics predominantly include backtesting and correlation-based measures. Backtesting is computationally intensive, inherently sequential, and sensitive to specific strategy parameters. Correlation-based metrics, though efficient, assess only predictive ability and overlook other crucial properties such as temporal stability, robustness, diversity, and interpretability. Additionally, the closed-source nature of most existing alpha mining models hinders reproducibility and slows progress in this field. To address these issues, we propose AlphaEval, a unified, parallelizable, and backtest-free evaluation framework for automated alpha mining models. AlphaEval assesses the overall quality of generated alphas along five complementary dimensions: predictive power, stability, robustness to market perturbations, financial logic, and diversity. Extensive experiments across representative alpha mining algorithms demonstrate that AlphaEval achieves evaluation consistency comparable to comprehensive backtesting, while providing more comprehensive insights and higher efficiency. Furthermore, AlphaEval effectively identifies superior alphas compared to traditional single-metric screening approaches. All implementations and evaluation tools are open-sourced to promote reproducibility and community engagement.         ",
    "url": "https://arxiv.org/abs/2508.13174",
    "authors": [
      "Hongjun Ding",
      "Binqi Chen",
      "Jinsheng Huang",
      "Taian Guo",
      "Zhengyang Mao",
      "Guoyi Shao",
      "Lutong Zou",
      "Luchen Liu",
      "Ming Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2508.13181",
    "title": "Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices",
    "abstract": "           Atrial fibrillation (AF) is a common arrhythmia and major risk factor for cardiovascular complications. While commercially available devices and supporting Artificial Intelligence (AI) algorithms exist for reliable detection of AF, the scaling of this technology to the amount of people who need this diagnosis is still a major challenge. This paper presents a novel wearable device, designed specifically for the early and reliable detection of AF. We present an FPGA-based patch-style wearable monitor with embedded deep learning-based AF detection. Operating with 3.8mW system power, which is 1-3 orders of magnitude lower than the state-of-the-art, the device enables continuous AF detection for over three weeks while achieving 95% accuracy, surpassing cardiologist-level performance. A key innovation is the combination of energy-efficient hardware-software co-design and optimized power management through the application of hardware-aware neural architecture search. This advancement represents a significant step toward scalable, reliable, and sustainable AF monitoring.         ",
    "url": "https://arxiv.org/abs/2508.13181",
    "authors": [
      "Dominik Loroch",
      "Johannes Feldmann",
      "Vladimir Rybalkin",
      "Norbert Wehn"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.13187",
    "title": "Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection",
    "abstract": "           Homelessness is a persistent social challenge, impacting millions worldwide. Over 770,000 people experienced homelessness in the U.S. in 2024. Social stigmatization is a significant barrier to alleviation, shifting public perception, and influencing policymaking. Given that online and city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases. This research contributes to alleviating homelessness by acting on public opinion. It introduces novel methods, building on natural language processing (NLP) and large language models (LLMs), to identify and measure PEH social bias expressed in digital spaces. We present a new, manually-annotated multi-modal dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across 10 U.S. cities. This unique dataset provides evidence of the typologies of homelessness bias described in the literature. In order to scale up and automate the detection of homelessness bias online, we evaluate LLMs as classifiers. We applied both zero-shot and few-shot classification techniques to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1, Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are significant inconsistencies in local LLM zero-shot classification, the in-context learning classification scores of local LLMs approach the classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT when averaging across all categories. This work aims to raise awareness about the pervasive bias against PEH, develop new indicators to inform policy, and ultimately enhance the fairness and ethical application of Generative AI technologies.         ",
    "url": "https://arxiv.org/abs/2508.13187",
    "authors": [
      "Jonathan A. Karr Jr.",
      "Benjamin F. Herbst",
      "Ting Hua",
      "Matthew Hauenstein",
      "Georgina Curto",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.13205",
    "title": "YOLO11-CR: a Lightweight Convolution-and-Attention Framework for Accurate Fatigue Driving Detection",
    "abstract": "           Driver fatigue detection is of paramount importance for intelligent transportation systems due to its critical role in mitigating road traffic accidents. While physiological and vehicle dynamics-based methods offer accuracy, they are often intrusive, hardware-dependent, and lack robustness in real-world environments. Vision-based techniques provide a non-intrusive and scalable alternative, but still face challenges such as poor detection of small or occluded objects and limited multi-scale feature modeling. To address these issues, this paper proposes YOLO11-CR, a lightweight and efficient object detection model tailored for real-time fatigue detection. YOLO11-CR introduces two key modules: the Convolution-and-Attention Fusion Module (CAFM), which integrates local CNN features with global Transformer-based context to enhance feature expressiveness; and the Rectangular Calibration Module (RCM), which captures horizontal and vertical contextual information to improve spatial localization, particularly for profile faces and small objects like mobile phones. Experiments on the DSM dataset demonstrated that YOLO11-CR achieves a precision of 87.17%, recall of 83.86%, mAP@50 of 88.09%, and mAP@50-95 of 55.93%, outperforming baseline models significantly. Ablation studies further validate the effectiveness of the CAFM and RCM modules in improving both sensitivity and localization accuracy. These results demonstrate that YOLO11-CR offers a practical and high-performing solution for in-vehicle fatigue monitoring, with strong potential for real-world deployment and future enhancements involving temporal modeling, multi-modal data integration, and embedded optimization.         ",
    "url": "https://arxiv.org/abs/2508.13205",
    "authors": [
      "Zhebin Jin",
      "Ligang Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2508.13216",
    "title": "Strategies for training point distributions in physics-informed neural networks",
    "abstract": "           Physics-informed neural networks approach the approximation of differential equations by directly incorporating their structure and given conditions in a loss function. This enables conditions like, e.g., invariants to be easily added during the modelling phase. In addition, the approach can be considered as mesh free and can be utilised to compute solutions on arbitrary grids after the training phase. Therefore, physics-informed neural networks are emerging as a promising alternative to solving differential equations with methods from numerical mathematics. However, their performance highly depends on a large variety of factors. In this paper, we systematically investigate and evaluate a core component of the approach, namely the training point distribution. We test two ordinary and two partial differential equations with five strategies for training data generation and shallow network architectures, with one and two hidden layers. In addition to common distributions, we introduce sine-based training points, which are motivated by the construction of Chebyshev nodes. The results are challenged by using certain parameter combinations like, e.g., random and fixed-seed weight initialisation for reproducibility. The results show the impact of the training point distributions on the solution accuracy and we find evidence that they are connected to the characteristics of the differential equation.         ",
    "url": "https://arxiv.org/abs/2508.13216",
    "authors": [
      "Santosh Humagain",
      "Toni Schneidereit"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13219",
    "title": "Deep Graph Neural Point Process For Learning Temporal Interactive Networks",
    "abstract": "           Learning temporal interaction networks(TIN) is previously regarded as a coarse-grained multi-sequence prediction problem, ignoring the network topology structure influence. This paper addresses this limitation and a Deep Graph Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node Aggregation Layer captures topological structures to generate static representation for users and items, while the Self Attentive Layer dynamically updates embeddings over time. By incorporating both dynamic and static embeddings into the event intensity function and optimizing the model via maximum likelihood estimation, DGNPP predicts events and occurrence time effectively. Experimental evaluations on three public datasets demonstrate that DGNPP achieves superior performance in event prediction and time prediction tasks with high efficiency, significantly outperforming baseline models and effectively mitigating the limitations of prior approaches.         ",
    "url": "https://arxiv.org/abs/2508.13219",
    "authors": [
      "Su Chen",
      "Xiaohua Qi",
      "Xixun Lin",
      "Yanmin Shang",
      "Xiaolin Xu",
      "Yangxi Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13223",
    "title": "MIRAGE: Towards AI-Generated Image Detection in the Wild",
    "abstract": "           The spreading of AI-generated images (AIGI), driven by advances in generative AI, poses a significant threat to information security and public trust. Existing AIGI detectors, while effective against images in clean laboratory settings, fail to generalize to in-the-wild scenarios. These real-world images are noisy, varying from ``obviously fake\" images to realistic ones derived from multiple generative models and further edited for quality control. We address in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is constructed from two sources: (1) a large corpus of Internet-sourced AIGI verified by human experts, and (2) a synthesized dataset created through the collaboration between multiple expert generators, closely simulating the realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a vision-language model with heuristic-to-analytic reasoning, a reflective reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a supervised-fine-tuning cold start, followed by a reinforcement learning stage. By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is able to provide either a quick judgment or a more robust and accurate conclusion, effectively balancing inference speed and performance. Extensive experiments show that our model leads state-of-the-art detectors by 5% and 10% on Mirage and the public benchmark, respectively. The benchmark and code will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2508.13223",
    "authors": [
      "Cheng Xia",
      "Manxi Lin",
      "Jiexiang Tan",
      "Xiaoxiong Du",
      "Yang Qiu",
      "Junjun Zheng",
      "Xiangheng Kong",
      "Yuning Jiang",
      "Bo Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13224",
    "title": "A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education",
    "abstract": "           This paper studies an application of a recurrent neural network to clustering method for the S-P chart: a binary data set used widely in education. As the number of students increases, the S-P chart becomes hard to handle. In order to classify the large chart into smaller charts, we present a simple clustering method based on the network dynamics. In the method, the network has multiple fixed points and basins of attraction give clusters corresponding to small S-P charts. In order to evaluate the clustering performance, we present an important feature quantity: average caution index that characterizes singularity of students answer oatterns. Performing fundamental experiments, effectiveness of the method is confirmed.         ",
    "url": "https://arxiv.org/abs/2508.13224",
    "authors": [
      "Mizuki Ohira",
      "Toshimichi Saito"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2508.13229",
    "title": "RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning",
    "abstract": "           Vision-Language Models (VLMs) struggle with complex image annotation tasks, such as emotion classification and context-driven object detection, which demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses solely on annotation outcomes, ignoring underlying rationales, while Visual Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought (CoTs) due to the absence of high-quality, verified CoTs during pre-training. We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement learning-driven \"annotation-reasoning-annotation\" closed-loop generates visually grounded, logically consistent CoTs by verifying their ability to reconstruct original annotations without direct leakage. The Inspire and Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement fine-tuning to produce interpretable reasoning and accurate annotations, achieving Expertise in complex visual tasks. Evaluated on complex and simple image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and Visual-RFT, achieving robust performance and enhanced explainability. RISE offers a self-supervised solution for advancing VLM reasoning without requiring manually annotated CoTs.         ",
    "url": "https://arxiv.org/abs/2508.13229",
    "authors": [
      "Suhang Hu",
      "Wei Hu",
      "Yuhang Su",
      "Fan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13231",
    "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System",
    "abstract": "           Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.         ",
    "url": "https://arxiv.org/abs/2508.13231",
    "authors": [
      "Yunhua Fang",
      "Rui Xie",
      "Asad Ul Haq",
      "Linsen Ma",
      "Kaoutar El Maghraoui",
      "Naigang Wang",
      "Meng Wang",
      "Liu Liu",
      "Tong Zhang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2508.13236",
    "title": "Uncertainty-Aware Learning Policy for Reliable Pulmonary Nodule Detection on Chest X-Ray",
    "abstract": "           Early detection and rapid intervention of lung cancer are crucial. Nonetheless, ensuring an accurate diagnosis is challenging, as physicians' ability to interpret chest X-rays varies significantly depending on their experience and degree of fatigue. Although medical AI has been rapidly advancing to assist in diagnosis, physicians' trust in such systems remains limited, preventing widespread clinical adoption. This skepticism fundamentally stems from concerns about its diagnostic uncertainty. In clinical diagnosis, physicians utilize extensive background knowledge and clinical experience. In contrast, medical AI primarily relies on repetitive learning of the target lesion to generate diagnoses based solely on that data. In other words, medical AI does not possess sufficient knowledge to render a diagnosis, leading to diagnostic uncertainty. Thus, this study suggests an Uncertainty-Aware Learning Policy that can address the issue of knowledge deficiency by learning the physicians' background knowledge alongside the Chest X-ray lesion information. We used 2,517 lesion-free images and 656 nodule images, all obtained from Ajou University Hospital. The proposed model attained 92% (IoU 0.2 / FPPI 2) with a 10% enhancement in sensitivity compared to the baseline model while also decreasing entropy as a measure of uncertainty by 0.2.         ",
    "url": "https://arxiv.org/abs/2508.13236",
    "authors": [
      "Hyeonjin Choi",
      "Jinse Kim",
      "Dong-yeon Yoo",
      "Ju-sung Sun",
      "Jung-won Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13250",
    "title": "Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information",
    "abstract": "           In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing users' information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13250",
    "authors": [
      "Zeyu Zhang",
      "Yang Zhang",
      "Haoran Tan",
      "Rui Li",
      "Xu Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.13257",
    "title": "ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models",
    "abstract": "           In modern Very Large Scale Integrated (VLSI) circuit design flow, the Register-Transfer Level (RTL) stage presents a critical opportunity for timing optimization. Addressing timing violations at this early stage is essential, as modern systems demand higher speeds, where even minor timing violations can lead to functional failures or system crashes. However, traditional timing optimization heavily relies on manual expertise, requiring engineers to iteratively analyze timing reports and debug. To automate this process, this paper proposes ViTAD, a method that efficiently analyzes the root causes of timing violations and dynamically generates targeted repair strategies. Specifically, we first parse Verilog code and timing reports to construct a Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation path analysis and use large language models (LLMs) to infer the root causes of violations. Finally, by analyzing the causes of violations, we selectively retrieve relevant debugging knowledge from a domain-specific knowledge base to generate customized repair solutions. To evaluate the effectiveness of our method, we construct a timing violation dataset based on real-world open-source projects. This dataset contains 54 cases of violations. Experimental results show that our method achieves a 73.68% success rate in repairing timing violations, while the baseline using only LLM is 54.38%. Our method improves the success rate by 19.30%.         ",
    "url": "https://arxiv.org/abs/2508.13257",
    "authors": [
      "Wenhao Lv",
      "Yingjie Xia",
      "Xiyuan Chen",
      "Li Kuang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13280",
    "title": "CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification",
    "abstract": "           Estimating disease severity from endoscopic images is essential in assessing ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to grade inflammation. However, MES classification remains challenging due to label noise from inter-observer variability and the ordinal nature of the score, which standard models often ignore. We propose CLoE, a curriculum learning framework that accounts for both label reliability and ordinal structure. Image quality, estimated via a lightweight model trained on Boston Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation confidence to order samples from easy (clean) to hard (noisy). This curriculum is further combined with ResizeMix augmentation to improve robustness. Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and Transformers, show that CLoE consistently improves performance over strong supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches 82.5\\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These results highlight the potential of difficulty-aware training strategies for improving ordinal classification under label uncertainty. Code will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13280",
    "authors": [
      "Zeynep Ozdemir",
      "Hacer Yalim Keles",
      "Omer Ozgur Tanriover"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13295",
    "title": "Time Profile of U.S. Neighborhoods: Datasets of Time Use at Social Infrastructure Places",
    "abstract": "           Social infrastructure plays a critical role in shaping neighborhood well-being by fostering social and cultural interaction, enabling service provision, and encouraging exposure to diverse environments. Despite the growing knowledge of its spatial accessibility, time use at social infrastructure places is underexplored due to the lack of a spatially resolved national dataset. We address this gap by developing scalable Social-Infrastructure Time Use measures (STU) that capture length and depth of engagement, activity diversity, and spatial inequality, supported by first-of-their-kind datasets spanning multiple geographic scales from census tracts to metropolitan areas. Our datasets leverage anonymized and aggregated foot traffic data collected between 2019 and 2024 across 49 continental U.S. states. The data description reveals variances in STU across time, space, and differing neighborhood sociodemographic characteristics. Validation demonstrates generally robust population representation, consistent with established national survey findings while revealing more nuanced patterns. Future analyses could link STU with public health outcomes and environmental factors to inform targeted interventions aimed at enhancing population well-being and guiding social infrastructure planning and usage.         ",
    "url": "https://arxiv.org/abs/2508.13295",
    "authors": [
      "Yan Wang",
      "Ziyi Guo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2508.13309",
    "title": "DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples",
    "abstract": "           Numerous techniques have been proposed for generating adversarial examples in white-box settings under strict Lp-norm constraints. However, such norm-bounded examples often fail to align well with human perception, and only recently have a few methods begun specifically exploring perceptually aligned adversarial examples. Moreover, it remains unclear whether insights from Lp-constrained attacks can be effectively leveraged to improve perceptual efficacy. In this paper, we introduce DAASH, a fully differentiable meta-attack framework that generates effective and perceptually aligned adversarial examples by strategically composing existing Lp-based attack methods. DAASH operates in a multi-stage fashion: at each stage, it aggregates candidate adversarial examples from multiple base attacks using learned, adaptive weights and propagates the result to the next stage. A novel meta-loss function guides this process by jointly minimizing misclassification loss and perceptual distortion, enabling the framework to dynamically modulate the contribution of each base attack throughout the stages. We evaluate DAASH on adversarially trained models across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on Lp-constrained based methods, DAASH significantly outperforms state-of-the-art perceptual attacks such as AdvAD -- achieving higher attack success rates (e.g., 20.63\\% improvement) and superior visual quality, as measured by SSIM, LPIPS, and FID (improvements $\\approx$ of 11, 0.015, and 5.7, respectively). Furthermore, DAASH generalizes well to unseen defenses, making it a practical and strong baseline for evaluating robustness without requiring handcrafted adaptive attacks for each new defense.         ",
    "url": "https://arxiv.org/abs/2508.13309",
    "authors": [
      "Abdullah Al Nomaan Nafi",
      "Habibur Rahaman",
      "Zafaryab Haider",
      "Tanzim Mahfuz",
      "Fnu Suya",
      "Swarup Bhunia",
      "Prabuddha Chakraborty"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13328",
    "title": "A Dual-Attention Graph Network for fMRI Data Classification",
    "abstract": "           Understanding the complex neural activity dynamics is crucial for the development of the field of neuroscience. Although current functional MRI classification approaches tend to be based on static functional connectivity or cannot capture spatio-temporal relationships comprehensively, we present a new framework that leverages dynamic graph creation and spatiotemporal attention mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in this research dynamically infers functional brain connectivity in each time interval using transformer-based attention mechanisms, enabling the model to selectively focus on crucial brain regions and time segments. By constructing time-varying graphs that are then processed with Graph Convolutional Networks (GCNs) and transformers, our method successfully captures both localized interactions and global temporal dependencies. Evaluated on the subset of ABIDE dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint modeling of dynamic connectivity and spatio-temporal context for fMRI classification. The core novelty arises from (1) attention-driven dynamic graph creation that learns temporal brain region interactions and (2) hierarchical spatio-temporal feature fusion through GCNtransformer fusion.         ",
    "url": "https://arxiv.org/abs/2508.13328",
    "authors": [
      "Amirali Arbab",
      "Zeinab Davarani",
      "Mehran Safayani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13362",
    "title": "Adaptive Conformal Prediction Intervals Over Trajectory Ensembles",
    "abstract": "           Future trajectories play an important role across domains such as autonomous driving, hurricane forecasting, and epidemic modeling, where practitioners commonly generate ensemble paths by sampling probabilistic models or leveraging multiple autoregressive predictors. While these trajectories reflect inherent uncertainty, they are typically uncalibrated. We propose a unified framework based on conformal prediction that transforms sampled trajectories into calibrated prediction intervals with theoretical coverage guarantees. By introducing a novel online update step and an optimization step that captures inter-step dependencies, our method can produce discontinuous prediction intervals around each trajectory, naturally capture temporal dependencies, and yield sharper, more adaptive uncertainty estimates.         ",
    "url": "https://arxiv.org/abs/2508.13362",
    "authors": [
      "Ruipu Li",
      "Daniel Menacho",
      "Alexander Rodr\u00edguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13365",
    "title": "Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection",
    "abstract": "           The recent trend towards utilisation of reasoning models has improved the performance of Large Language Models (LLMs) across many tasks which involve logical steps. One linguistic task that could benefit from this framing is idiomaticity detection, as a potentially idiomatic expression must first be understood before it can be disambiguated and serves as a basis for reasoning. In this paper, we explore how reasoning capabilities in LLMs affect idiomaticity detection performance and examine the effect of model size. We evaluate, as open source representative models, the suite of DeepSeek-R1 distillation models ranging from 1.5B to 70B parameters across four idiomaticity detection datasets. We find the effect of reasoning to be smaller and more varied than expected. For smaller models, producing chain-of-thought (CoT) reasoning increases performance from Math-tuned intermediate models, but not to the levels of the base models, whereas larger models (14B, 32B, and 70B) show modest improvements. Our in-depth analyses reveal that larger models demonstrate good understanding of idiomaticity, successfully producing accurate definitions of expressions, while smaller models often fail to output the actual meaning. For this reason, we also experiment with providing definitions in the prompts of smaller models, which we show can improve performance in some cases.         ",
    "url": "https://arxiv.org/abs/2508.13365",
    "authors": [
      "Dylan Phelps",
      "Rodrigo Wilkens",
      "Edward Gow-Smith",
      "Thomas Pickard",
      "Maggie Mi",
      "Aline Villavicencio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.13375",
    "title": "State & Geopolitical Censorship on Twitter (X): Detection & Impact Analysis of Withheld Content",
    "abstract": "           State and geopolitical censorship on Twitter, now X, has been turning into a routine, raising concerns about the boundaries between criminal content and freedom of speech. One such censorship practice, withholding content in a particular state has renewed attention due to Elon Musk's apparent willingness to comply with state demands. In this study, we present the first quantitative analysis of the impact of state censorship by withholding on social media using a dataset in which two prominent patterns emerged: Russian accounts censored in the EU for spreading state-sponsored narratives, and Turkish accounts blocked within Turkey for promoting militant propaganda. We find that censorship has little impact on posting frequency but significantly reduces likes and retweets by 25%, and follower growth by 90%-especially when the censored region aligns with the account's primary audience. Meanwhile, some Russian accounts continue to experience growth as their audience is outside the withholding jurisdictions. We develop a user-level binary classifier with a transformer backbone and temporal aggregation strategies, aiming to predict whether an account is likely to be withheld. Through an ablation study, we find that tweet content is the primary signal in predicting censorship, while tweet metadata and profile features contribute marginally. Our best model achieves an F1 score of 0.73 and an AUC of 0.83. This work informs debates on platform governance, free speech, and digital repression.         ",
    "url": "https://arxiv.org/abs/2508.13375",
    "authors": [
      "Yusuf M\u00fccahit \u00c7etinkaya",
      "Tu\u011frulcan Elmas"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.13379",
    "title": "Low-Cost Sensing and Classification for Early Stress and Disease Detection in Avocado Plants",
    "abstract": "           With rising demands for efficient disease and salinity management in agriculture, early detection of plant stressors is crucial, particularly for high-value crops like avocados. This paper presents a comprehensive evaluation of low-cost sensors deployed in the field for early stress and disease detection in avocado plants. Our monitoring system was deployed across 72 plants divided into four treatment categories within a greenhouse environment, with data collected over six months. While leaf temperature and conductivity measurements, widely used metrics for controlled settings, were found unreliable in field conditions due to environmental interference and positioning challenges, leaf spectral measurements produced statistically significant results when combined with our machine learning approach. For soil data analysis, we developed a two-level hierarchical classifier that leverages domain knowledge about treatment characteristics, achieving 75-86\\% accuracy across different avocado genotypes and outperforming conventional machine learning approaches by over 20\\%. In addition, performance evaluation on an embedded edge device demonstrated the viability of our approach for resource-constrained environments, with reasonable computational efficiency while maintaining high classification accuracy. Our work bridges the gap between theoretical potential and practical application of low-cost sensors in agriculture and offers insights for developing affordable, scalable monitoring systems.         ",
    "url": "https://arxiv.org/abs/2508.13379",
    "authors": [
      "Abdulrahman Bukhari",
      "Bullo Mamo",
      "Mst Shamima Hossain",
      "Ziliang Zhang",
      "Mohsen Karimi",
      "Daniel Enright",
      "Patricia Manosalva",
      "Hyoseung Kim"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.13387",
    "title": "SPANER: Shared Prompt Aligner for Multimodal Semantic Representation",
    "abstract": "           Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have significantly improved performance on downstream tasks such as few-shot retrieval. However, most existing approaches focus on task-specific gains while neglecting the structure of the multimodal embedding space. As a result, modality-specific representations often remain isolated, limiting cross-modal generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a modality-agnostic PEFT framework designed to embed inputs from diverse modalities into a unified semantic space. At its core, SPANER employs a shared prompt mechanism that acts as a conceptual anchor, enabling semantically related instances to converge spatially regardless of modality. This shared prompt design is inherently extensible, supporting the seamless integration of additional modalities, such as audio, without altering the core architecture. Through comprehensive experiments across vision-language and audio-visual benchmarks, SPANER demonstrates competitive few-shot retrieval performance while preserving high semantic coherence in the learned embedding space. Our results highlight the importance of aligning embedding structures, rather than merely tuning adapter weights, for scalable multimodal learning.         ",
    "url": "https://arxiv.org/abs/2508.13387",
    "authors": [
      "Thye Shan Ng",
      "Caren Soyeon Han",
      "Eun-Jung Holden"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13394",
    "title": "CASPER: Concept-integrated Sparse Representation for Scientific Retrieval",
    "abstract": "           The exponential growth of scientific literature has made it increasingly difficult for researchers to keep up with the literature. In an attempt to alleviate this problem, we propose CASPER, a sparse retrieval model for scientific search that utilizes tokens and keyphrases as representation units (i.e. dimensions in the sparse embedding space), enabling it to represent queries and documents with research concepts and match them at both granular and conceptual levels. To overcome the lack of suitable training data, we propose mining training data by leveraging scholarly references (i.e. signals that capture how research concepts of papers are expressed in different settings), including titles, citation contexts, author-assigned keyphrases, and co-citations. CASPER outperforms strong dense and sparse retrieval baselines on eight scientific retrieval benchmarks. Moreover, we demonstrate that through simple post-processing, CASPER can be effectively used for the keyphrase generation tasks, achieving competitive performance with the established CopyRNN while producing more diverse keyphrases and being nearly four times faster.         ",
    "url": "https://arxiv.org/abs/2508.13394",
    "authors": [
      "Lam Thanh Do",
      "Linh Van Nguyen",
      "David Fu",
      "Kevin Chen-Chuan Chang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2508.13397",
    "title": "Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU",
    "abstract": "           Large inter-GPU all-reduce operations, prevalent throughout deep learning, are bottlenecked by communication costs. Emerging heterogeneous architectures are comprised of complex nodes, often containing $4$ GPUs and dozens to hundreds of CPU cores per node. Parallel applications are typically accelerated on the available GPUs, using only a single CPU core per GPU while the remaining cores sit idle. This paper presents novel optimizations to large GPU-aware all-reduce operations, extending lane-aware reductions to the GPUs, and notably using multiple CPU cores per GPU to accelerate these operations. These multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta supercomputer. Finally, the approach is extended to NVIDIA's and AMD's collective communication libraries, achieving speedup of up to $1.77$x and $1.71$x, respectively, across $2$ state-of-the-art supercomputers.         ",
    "url": "https://arxiv.org/abs/2508.13397",
    "authors": [
      "Michael Adams",
      "Amanda Bienz"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.13402",
    "title": "Robust Live Streaming over LEO Satellite Constellations: Measurement, Analysis, and Handover-Aware Adaptation",
    "abstract": "           Live streaming has experienced significant growth recently. Yet this rise in popularity contrasts with the reality that a substantial segment of the global population still lacks Internet access. The emergence of Low Earth orbit Satellite Networks (LSNs), such as SpaceX's Starlink and Amazon's Project Kuiper, presents a promising solution to fill this gap. Nevertheless, our measurement study reveals that existing live streaming platforms may not be able to deliver a smooth viewing experience on LSNs due to frequent satellite handovers, which lead to frequent video rebuffering events. Current state-of-the-art learning-based Adaptive Bitrate (ABR) algorithms, even when trained on LSNs' network traces, fail to manage the abrupt network variations associated with satellite handovers effectively. To address these challenges, for the first time, we introduce Satellite-Aware Rate Adaptation (SARA), a versatile and lightweight middleware that can seamlessly integrate with various ABR algorithms to enhance the performance of live streaming over LSNs. SARA intelligently modulates video playback speed and furnishes ABR algorithms with insights derived from the distinctive network characteristics of LSNs, thereby aiding ABR algorithms in making informed bitrate selections and effectively minimizing rebuffering events that occur during satellite handovers. Our extensive evaluation shows that SARA can effectively reduce the rebuffering time by an average of $39.41\\%$ and slightly improve latency by $0.65\\%$ while only introducing an overall loss in bitrate by $0.13\\%$.         ",
    "url": "https://arxiv.org/abs/2508.13402",
    "authors": [
      "Hao Fang",
      "Haoyuan Zhao",
      "Jianxin Shi",
      "Miao Zhang",
      "Guanzhen Wu",
      "Yi Ching Chou",
      "Feng Wang",
      "Jiangchuan Liu"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2508.13406",
    "title": "Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp",
    "abstract": "           This study presents a quantitative framework for evaluating the spatial concordance between clinically defined seizure onset zones (SOZs) and statistically anomalous channels identified through time-frequency analysis of chirp events. The proposed pipeline employs a two-step methodology: (1) Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with adaptive neighborhood selection identifies anomalous channels based on spectro-temporal features of chirp (Onset frequency, offset frequency, and temporal duration); and (2) Spatial Correlation Analysis, which computes both exact co-occurrence metrics and weighted index similarity, incorporating hemispheric congruence and electrode proximity. Key findings demonstrate that the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects outliers, with index matching (weighted by channel proximity) outperforming exact matching in SOZ localization. Performance metrics (precision, recall, F1) were highest for seizure-free patients (Index Precision mean: 0.903) and those with successful surgical outcomes (Index Precision mean: 0.865), whereas failure cases exhibited lower concordance (Index Precision mean: 0.460). The key takeaway is that chirp-based outlier detection, combined with weighted spatial metrics, provides a complementary method for SOZ localization, particularly in patients with successful surgical outcomes.         ",
    "url": "https://arxiv.org/abs/2508.13406",
    "authors": [
      "Nooshin Bahador",
      "Milad Lankarany"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13411",
    "title": "Decentralized Contextual Bandits with Network Adaptivity",
    "abstract": "           We consider contextual linear bandits over networks, a class of sequential decision-making problems where learning occurs simultaneously across multiple locations and the reward distributions share structural similarities while also exhibiting local differences. While classical contextual bandits assume either fully centralized data or entirely isolated learners, much remains unexplored in networked environments when information is partially shared. In this paper, we address this gap by developing two network-aware Upper Confidence Bound (UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information sharing guided by dynamically updated network weights. Our approach decompose learning into global and local components and as a result allow agents to benefit from shared structure without full synchronization. Both algorithms incur lighter communication costs compared to a fully centralized setting as agents only share computed summaries regarding the homogeneous features. We establish regret bounds showing that our methods reduce the learning complexity associated with the shared structure from $O(N)$ to sublinear $O(\\sqrt{N})$, where $N$ is the size of the network. The two algorithms reveal complementary strengths: NetLinUCB excels in low-noise regimes with fine-grained heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance contexts. We further demonstrate the effectiveness of our methods across simulated pricing environments compared to standard benchmarks.         ",
    "url": "https://arxiv.org/abs/2508.13411",
    "authors": [
      "Chuyun Deng",
      "Huiwen Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2508.13425",
    "title": "When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks",
    "abstract": "           Secure aggregation is a common technique in federated learning (FL) for protecting data privacy from both curious internal entities (clients or server) and external adversaries (eavesdroppers). However, in dynamic and resource-constrained environments such as low Earth orbit (LEO) satellite networks, traditional secure aggregation methods fall short in two aspects: (1) they assume continuous client availability while LEO satellite visibility is intermittent and irregular; (2) they consider privacy in each communication round but have overlooked the possible privacy leakage through multiple rounds. To address these limitations, we propose LTP-FLEO, an asynchronous FL framework that preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO introduces (i) privacy-aware satellite partitioning, which groups satellites based on their predictable visibility to the server and enforces joint participation; (ii) model age balancing, which mitigates the adverse impact of stale model updates; and (iii) fair global aggregation, which treats satellites of different visibility durations in an equitable manner. Theoretical analysis and empirical validation demonstrate that LTP-FLEO effectively safeguards both model and data privacy across multi-round training, promotes fairness in line with satellite contributions, accelerates global convergence, and achieves competitive model accuracy.         ",
    "url": "https://arxiv.org/abs/2508.13425",
    "authors": [
      "Mohamed Elmahallawy",
      "Tie Luo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.13435",
    "title": "SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer",
    "abstract": "           Directed graphs are widely used to model asymmetric relationships in real-world systems. However, existing directed graph neural networks often struggle to jointly capture directional semantics and global structural patterns due to their isotropic aggregation mechanisms and localized filtering mechanisms. To address this limitation, this paper proposes SVDformer, a novel framework that synergizes SVD and Transformer architecture for direction-aware graph representation learning. SVDformer first refines singular value embeddings through multi-head self-attention, adaptively enhancing critical spectral components while suppressing high-frequency noise. This enables learnable low-pass/high-pass graph filtering without requiring spectral kernels. Furthermore, by treating singular vectors as directional projection bases and singular values as scaling factors, SVDformer uses the Transformer to model multi-scale interactions between incoming/outgoing edge patterns through attention weights, thereby explicitly preserving edge directionality during feature propagation. Extensive experiments on six directed graph benchmarks demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and direction-aware baselines on node classification tasks, establishing a new paradigm for learning representations on directed graphs.         ",
    "url": "https://arxiv.org/abs/2508.13435",
    "authors": [
      "Jiayu Fang",
      "Zhiqi Shao",
      "S T Boris Choy",
      "Junbin Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13459",
    "title": "Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms",
    "abstract": "           The ``Last Mile Challenge'' has long been considered an important, yet unsolved, challenge for autonomous vehicles, public service robots, and delivery robots. A central issue in this challenge is the ability of robots to navigate constrained and cluttered environments (e.g., doorways, hallways, corridor intersections), often while competing for space with other robots and humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs are tightly coupled, high-agency interactions that arise within general multi-robot navigation (MRN) scenarios. They are identified through certain distinct characteristics and require specialized metrics to evaluate them. Traditional navigation approaches designed for MRN do not perform well in SMGs, which has led to focused research on dedicated SMG solvers (navigation methods specialized to navigate in SMGs), which has flourished in recent years. However, publications on SMG navigation research make different assumptions (on centralized versus decentralized, observability, communication, cooperation, etc.), and have different objective functions (safety versus liveness). These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult to establish appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. Such ad-hoc representation of the field also presents a barrier to new researchers wanting to start research in this area. SMG navigation research requires its own taxonomy, definitions, and evaluation protocols to guide effective research moving forward. This survey is the first to catalog SMG solvers using a well-defined and unified taxonomy and to classify existing methods accordingly.         ",
    "url": "https://arxiv.org/abs/2508.13459",
    "authors": [
      "Rohan Chandra",
      "Shubham Singh",
      "Abhishek Jha",
      "Dannon Andrade",
      "Hriday Sainathuni",
      "Katia Sycara"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.13469",
    "title": "Fundamentals of Next-generation Network Planning",
    "abstract": "           The fifth-generation (5G) of cellular communications is expected to be deployed in the next years to support a wide range of services with different demands of peak data rates, latency and quality of experience (QoE). To support higher data rates and latency requirements third-generation partnership project (3GPP) has introduced numerology and bandwidth parts (BWPs), via new radio (NR) for service-tailored resource allocation. Legacy 4G networks have generated extensive data, which combined with crowd-sourced LTE infrastructure insights, enables identification of high-traffic 5G deployment area (5GDA) for planning new services. Given the mission-critical nature of 5G services, QoE is a big challenge for MNOs to guarantee peak data rates for a defined percentage of time. This work studies the fundamentals of 5G network planning methods that reconciles coverage-capacity trade-offs through balanced radio network dimensioning (RND), leveraging pragmatic NR modeling, and data-driven strategies to minimize deployment costs and reduce cost-per-bit.         ",
    "url": "https://arxiv.org/abs/2508.13469",
    "authors": [
      "M. Umar Khan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.13471",
    "title": "MINR: Efficient Implicit Neural Representations for Multi-Image Encoding",
    "abstract": "           Implicit Neural Representations (INRs) aim to parameterize discrete signals through implicit continuous functions. However, formulating each image with a separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to computational and storage inefficiencies when encoding multi-images. To address this issue, we propose MINR, sharing specific layers to encode multi-image efficiently. We first compare the layer-wise weight distributions for several trained INRs and find that corresponding intermediate layers follow highly similar distribution patterns. Motivated by this, we share these intermediate layers across multiple images while preserving the input and output layers as input-specific. In addition, we design an extra novel projection layer for each image to capture its unique features. Experimental results on image reconstruction and super-resolution tasks demonstrate that MINR can save up to 60\\% parameters while maintaining comparable performance. Particularly, MINR scales effectively to handle 100 images, maintaining an average peak signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones proves the robustness of the proposed MINR.         ",
    "url": "https://arxiv.org/abs/2508.13471",
    "authors": [
      "Wenyong Zhou",
      "Taiqiang Wu",
      "Zhengwu Liu",
      "Yuxin Cheng",
      "Chen Zhang",
      "Ngai Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13474",
    "title": "Electromagnetic Signal Modulation Recognition based on Subgraph Embedding Learning",
    "abstract": "           Automatic Modulation Recognition (AMR) detects modulation schemes of received signals for further processing of signals without any priori information, which is critically important for civil spectrum regulation, information countermea sures, and communication security. Due to the powerful feature extraction and classification capabilities of Deep Learning (DL), DL-based AMR algorithms have achieved excellent performance gains compared with traditional modulation detection algorithms. However, all existing DL-based AMR algorithms, to the best of our knowledge, are designed for specific channels and systems, because data dimension of the used training dataset is fixed. To this end, we takes the first step to propose a Subgraph Embedding Learning (SEL) structure to address the classical AMR problem, and the proposed algorithm is called SEL-AMR. Our algorithm treats the communication system as a subgraph and uses the relationship between samples to smooth the effects brought by noise and different channels to extract robust features. Thus, the proposed SEL-AMR algorithm can adapt to any dynamic channels and systems. We use 5 public real datasets and a small amount of simulation data to evaluate our SEL-AMR algorithm. Experimental results reveal that SEL-AMR can well adapt to different channels and systems, and always outperforms the state of-the-art algorithms by improving up to 20% macro-average recognition precision and 30% recognition accuracy.         ",
    "url": "https://arxiv.org/abs/2508.13474",
    "authors": [
      "Bojun Zhang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.13478",
    "title": "Distribution-Aware Hadamard Quantization for Hardware-Efficient Implicit Neural Representations",
    "abstract": "           Implicit Neural Representations (INRs) encode discrete signals using Multi-Layer Perceptrons (MLPs) with complex activation functions. While INRs achieve superior performance, they depend on full-precision number representation for accurate computation, resulting in significant hardware overhead. Previous INR quantization approaches have primarily focused on weight quantization, offering only limited hardware savings due to the lack of activation quantization. To fully exploit the hardware benefits of quantization, we propose DHQ, a novel distribution-aware Hadamard quantization scheme that targets both weights and activations in INRs. Our analysis shows that the weights in the first and last layers have distributions distinct from those in the intermediate layers, while the activations in the last layer differ significantly from those in the preceding layers. Instead of customizing quantizers individually, we utilize the Hadamard transformation to standardize these diverse distributions into a unified bell-shaped form, supported by both empirical evidence and theoretical analysis, before applying a standard quantizer. To demonstrate the practical advantages of our approach, we present an FPGA implementation of DHQ that highlights its hardware efficiency. Experiments on diverse image reconstruction tasks show that DHQ outperforms previous quantization methods, reducing latency by 32.7\\%, energy consumption by 40.1\\%, and resource utilization by up to 98.3\\% compared to full-precision counterparts.         ",
    "url": "https://arxiv.org/abs/2508.13478",
    "authors": [
      "Wenyong Zhou",
      "Jiachen Ren",
      "Taiqiang Wu",
      "Yuxin Cheng",
      "Zhengwu Liu",
      "Ngai Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13480",
    "title": "Generating the Spanning Trees of Series-Parallel Graphs up to Graph Automorphism",
    "abstract": "           In this paper, we investigate the problem of generating the spanning trees of a graph $G$ up to the automorphisms or \"symmetries\" of $G$. After introducing and surveying this problem for general input graphs, we present algorithms that fully solve the case of series-parallel graphs, under two standard definitions. We first show how to generate the nonequivalent spanning trees of a oriented series-parallel graph $G$ in output-linear time, where both terminals of $G$ have been individually distinguished (i.e. applying an automorphism that exchanges the terminals produces a different series-parallel graph). Subsequently, we show how to adapt these oriented algorithms to the case of semioriented series-parallel graphs, where we still have a set of two distinguished terminals but neither has been designated as a source or sink. Finally, we discuss the case of unoriented series-parallel graphs, where no terminals have been distinguished and present a few observations and open questions relating to them. The algorithms we present generate the nonequivalent spanning trees of $G$ but never explicitly compute the automorphism group of $G$, revealing how the recursive structure of $G$'s automorphism group mirrors that of its spanning trees.         ",
    "url": "https://arxiv.org/abs/2508.13480",
    "authors": [
      "Mithra Karamchedu",
      "Lucas Bang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2508.13481",
    "title": "Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations",
    "abstract": "           Implicit Neural Representations (INRs) encode discrete signals in a continuous manner using neural networks, demonstrating significant value across various multimedia applications. However, the vulnerability of INRs presents a critical challenge for their real-world deployments, as the network weights might be subjected to unavoidable perturbations. In this work, we investigate the robustness of INRs for the first time and find that even minor perturbations can lead to substantial performance degradation in the quality of signal reconstruction. To mitigate this issue, we formulate the robustness problem in INRs by minimizing the difference between loss with and without weight perturbations. Furthermore, we derive a novel robust loss function to regulate the gradient of the reconstruction loss with respect to weights, thereby enhancing the robustness. Extensive experiments on reconstruction tasks across multiple modalities demonstrate that our method achieves up to a 7.5~dB improvement in peak signal-to-noise ratio (PSNR) values compared to original INRs under noisy conditions.         ",
    "url": "https://arxiv.org/abs/2508.13481",
    "authors": [
      "Wenyong Zhou",
      "Yuxin Cheng",
      "Zhengwu Liu",
      "Taiqiang Wu",
      "Chen Zhang",
      "Ngai Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13485",
    "title": "CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving",
    "abstract": "           4D radar-based object detection has garnered great attention for its robustness in adverse weather conditions and capacity to deliver rich spatial information across diverse driving scenarios. Nevertheless, the sparse and noisy nature of 4D radar point clouds poses substantial challenges for effective perception. To address the limitation, we present CORENet, a novel cross-modal denoising framework that leverages LiDAR supervision to identify noise patterns and extract discriminative features from raw 4D radar data. Designed as a plug-and-play architecture, our solution enables seamless integration into voxel-based detection frameworks without modifying existing pipelines. Notably, the proposed method only utilizes LiDAR data for cross-modal supervision during training while maintaining full radar-only operation during inference. Extensive evaluation on the challenging Dual-Radar dataset, which is characterized by elevated noise level, demonstrates the effectiveness of our framework in enhancing detection robustness. Comprehensive experiments validate that CORENet achieves superior performance compared to existing mainstream approaches.         ",
    "url": "https://arxiv.org/abs/2508.13485",
    "authors": [
      "Fuyang Liu",
      "Jilin Mei",
      "Fangyuan Mao",
      "Chen Min",
      "Yan Xing",
      "Yu Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13488",
    "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments",
    "abstract": "           Loop closure detection is important for simultaneous localization and mapping (SLAM), which associates current observations with historical keyframes, achieving drift correction and global relocalization. However, a falsely detected loop can be fatal, and this is especially difficult in repetitive environments where appearance-based features fail due to the high similarity. Therefore, verification of a loop closure is a critical step in avoiding false positive detections. Existing works in loop closure verification predominantly focus on learning invariant appearance features, neglecting the prior knowledge of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter, we propose ROVER, a loop closure verification method that leverages the historical trajectory as a prior constraint to reject false loops in challenging repetitive environments. For each loop candidate, it is first used to estimate the robot trajectory with pose-graph optimization. This trajectory is then submitted to a scoring scheme that assesses its compliance with the trajectory without the loop, which we refer to as the trajectory prior, to determine if the loop candidate should be accepted. Benchmark comparisons and real-world experiments demonstrate the effectiveness of the proposed method. Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify its robustness and efficiency. Our source code and self-collected dataset are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13488",
    "authors": [
      "Jingwen Yu",
      "Jiayi Yang",
      "Anjun Hu",
      "Jiankun Wang",
      "Ping Tan",
      "Hong Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13490",
    "title": "DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing",
    "abstract": "           A primary challenge in using neural networks to approximate nonlinear dynamical systems governed by partial differential equations (PDEs) is transforming these systems into a suitable format, especially when dealing with non-linearizable dynamics or the need for infinite-dimensional spaces for linearization. This paper introduces DyMixOp, a novel neural operator framework for PDEs that integrates insights from complex dynamical systems to address this challenge. Grounded in inertial manifold theory, DyMixOp transforms infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent space, establishing a structured foundation that maintains essential nonlinear interactions and enhances physical interpretability. A key innovation is the Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in turbulence. This transformation effectively captures both fine-scale details and nonlinear interactions, while mitigating spectral bias commonly found in existing neural operators. The framework is further strengthened by a dynamics-informed architecture that connects multiple LGM layers to approximate linear and nonlinear dynamics, reflecting the temporal evolution of dynamical systems. Experimental results across diverse PDE benchmarks demonstrate that DyMixOp achieves state-of-the-art performance, significantly reducing prediction errors, particularly in convection-dominated scenarios reaching up to 86.7\\%, while maintaining computational efficiency and scalability.         ",
    "url": "https://arxiv.org/abs/2508.13490",
    "authors": [
      "Pengyu Lai",
      "Yixiao Chen",
      "Hui Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chaotic Dynamics (nlin.CD)"
    ]
  },
  {
    "id": "arXiv:2508.13512",
    "title": "CountingStars: Low-overhead Network-wide Measurement in LEO Mega-constellation Networks",
    "abstract": "           The high mobility of satellites in Low Earth Orbit (LEO) mega-constellations induces a highly dynamic network topology, leading to many problems like frequent service disruptions. To mitigate this, Packet-based Load Balancing (PBLB) is employed. However, this paradigm shift introduces two critical challenges for network measurement stemming from the requirement for port-level granularity: memory inflation and severe hash collisions. To tackle these challenges, we propose CountingStars, a low-overhead network-wide measurement architecture. In the ground controller, CountingStars builds a digital twins system to accurately predict the future network topology. This allows ground controller to generate and distribute collision-free hash seeds to satellites in advance. On the satellite, we introduce a port aggregation data structure that decouples the unique flow identifier from its multi-port counter and updates it through efficient bit operations, solving the memory inflation caused by PBLB. Simulation results show that the memory usage of CountingStars is reduced by 70\\% on average, and the relative error of measurement is reduced by 90\\% on average. Implementation on FPGA shows its prospect to deploy in real system.         ",
    "url": "https://arxiv.org/abs/2508.13512",
    "authors": [
      "Xiyuan Liu",
      "Guano Liu",
      "Xiucheng Tian",
      "Wenting Wei"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2508.13517",
    "title": "Heterogeneous Influence Maximization in User Recommendation",
    "abstract": "           User recommendation systems enhance user engagement by encouraging users to act as inviters to interact with other users (invitees), potentially fostering information propagation. Conventional recommendation methods typically focus on modeling interaction willingness. Influence-Maximization (IM) methods focus on identifying a set of users to maximize the information propagation. However, existing methods face two significant challenges. First, recommendation methods fail to unleash the candidates' spread capability. Second, IM methods fail to account for the willingness to interact. To solve these issues, we propose two models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to unleash the dissemination potential of user recommendation systems. HeteroIM fills the gap between the IM method and the recommendation task, improving interaction willingness and maximizing spread coverage. The HeteroIR introduces a two-stage framework to estimate the spread profits. The HeteroIM incrementally selects the most influential invitee to recommend and rerank based on the number of reverse reachable (RR) sets containing inviters and invitees. RR set denotes a set of nodes that can reach a target via propagation. Extensive experiments show that HeteroIR and HeteroIM significantly outperform the state-of-the-art baselines with the p-value < 0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online gaming platforms and gained an 8.5\\% and 10\\% improvement in the online A/B test, respectively. Implementation codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13517",
    "authors": [
      "Hongru Hou",
      "Jiachen Sun",
      "Wenqing Lin",
      "Wendong Bi",
      "Xiangrong Wang",
      "Deqing Yang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2508.13522",
    "title": "DDoS Attacks in Cloud Computing: Detection and Prevention",
    "abstract": "           DDoS attacks are one of the most prevalent and harmful cybersecurity threats faced by organizations and individuals today. In recent years, the complexity and frequency of DDoS attacks have increased significantly, making it challenging to detect and mitigate them effectively. The study analyzes various types of DDoS attacks, including volumetric, protocol, and application layer attacks, and discusses the characteristics, impact, and potential targets of each type. It also examines the existing techniques used for DDoS attack detection, such as packet filtering, intrusion detection systems, and machine learning-based approaches, and their strengths and limitations. Moreover, the study explores the prevention techniques employed to mitigate DDoS attacks, such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the effectiveness of each approach and its suitability for different types of attacks and environments. In conclusion, this study provides a comprehensive overview of the different types of DDoS attacks, their detection, and prevention techniques. It aims to provide insights and guidelines for organizations and individuals to enhance their cybersecurity posture and protect against DDoS attacks.         ",
    "url": "https://arxiv.org/abs/2508.13522",
    "authors": [
      "Zain Ahmad",
      "Musab Ahmad",
      "Bilal Ahmad"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2508.13544",
    "title": "FLAIR: Frequency- and Locality-Aware Implicit Neural Representations",
    "abstract": "           Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity, spatial localization, and sparse representations, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is RC-GAUSS, a novel activation designed for explicit frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform (DWT) to compute energy scores and explicitly guide frequency information to the network. Our method consistently outperforms existing INRs in 2D image representation and restoration, as well as 3D reconstruction.         ",
    "url": "https://arxiv.org/abs/2508.13544",
    "authors": [
      "Sukhun Ko",
      "Dahyeon Kye",
      "Kyle Min",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13546",
    "title": "GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering",
    "abstract": "           Foveated rendering significantly reduces computational demands in virtual reality applications by concentrating rendering quality where users focus their gaze. Current approaches require expensive hardware-based eye tracking systems, limiting widespread adoption due to cost, calibration complexity, and hardware compatibility constraints. This paper presents GazeProphet, a software-only approach for predicting gaze locations in VR environments without requiring dedicated eye tracking hardware. The approach combines a Spherical Vision Transformer for processing 360-degree VR scenes with an LSTM-based temporal encoder that captures gaze sequence patterns. A multi-modal fusion network integrates spatial scene features with temporal gaze dynamics to predict future gaze locations with associated confidence estimates. Experimental evaluation on a comprehensive VR dataset demonstrates that GazeProphet achieves a median angular error of 3.83 degrees, outperforming traditional saliency-based baselines by 24% while providing reliable confidence calibration. The approach maintains consistent performance across different spatial regions and scene types, enabling practical deployment in VR systems without additional hardware requirements. Statistical analysis confirms the significance of improvements across all evaluation metrics. These results show that software-only gaze prediction can work for VR foveated rendering, making this performance boost more accessible to different VR platforms and apps.         ",
    "url": "https://arxiv.org/abs/2508.13546",
    "authors": [
      "Farhaan Ebadulla",
      "Chiraag Mudlapur",
      "Gaurav BV"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13548",
    "title": "CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics",
    "abstract": "           Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public health threat within hospitals as well as long-term care facilities. Better understanding of MRSA risks, evaluation of interventions and forecasting MRSA rates are important public health problems. Existing forecasting models rely on statistical or neural network approaches, which lack epidemiological interpretability, and have limited performance. Mechanistic epidemic models are difficult to calibrate and limited in incorporating diverse datasets. We present CALYPSO, a hybrid framework that integrates neural networks with mechanistic metapopulation models to capture the spread dynamics of infectious diseases (i.e., MRSA) across healthcare and community settings. Our model leverages patient-level insurance claims, commuting data, and healthcare transfer patterns to learn region- and time-specific parameters governing MRSA spread. This enables accurate, interpretable forecasts at multiple spatial resolutions (county, healthcare facility, region, state) and supports counterfactual analyses of infection control policies and outbreak risks. We also show that CALYPSO improves statewide forecasting performance by over 4.5% compared to machine learning baselines, while also identifying high-risk regions and cost-effective strategies for allocating infection prevention resources.         ",
    "url": "https://arxiv.org/abs/2508.13548",
    "authors": [
      "Rituparna Datta",
      "Jiaming Cui",
      "Gregory R. Madden",
      "Anil Vullikanti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13552",
    "title": "Collapsing ROC approach for risk prediction research on both common and rare variants",
    "abstract": "           Risk prediction that capitalizes on emerging genetic findings holds great promise for improving public health and clinical care. However, recent risk prediction research has shown that predictive tests formed on existing common genetic loci, including those from genome-wide association studies, have lacked sufficient accuracy for clinical use. Because most rare variants on the genome have not yet been studied for their role in risk prediction, future disease prediction discoveries should shift toward a more comprehensive risk prediction strategy that takes into account both common and rare variants. We are proposing a collapsing receiver operating characteristic CROC approach for risk prediction research on both common and rare variants. The new approach is an extension of a previously developed forward ROC FROC approach, with additional procedures for handling rare variants. The approach was evaluated through the use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction model built on all SNPs gained more accuracy AUC = 0.605 than one built on common variants alone AUC = 0.585. We further evaluated the performance of two approaches by gradually reducing the number of common variants in the analysis. We found that the CROC method attained more accuracy than the FROC method when the number of common variants in the data decreased. In an extreme scenario, when there are only rare variants in the data, the CROC reached an AUC value of 0.603, whereas the FROC had an AUC value of 0.524.         ",
    "url": "https://arxiv.org/abs/2508.13552",
    "authors": [
      "Changshuai Wei",
      "Qing Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2508.13561",
    "title": "Prediction of Hospital Associated Infections During Continuous Hospital Stays",
    "abstract": "           The US Centers for Disease Control and Prevention (CDC), in 2019, designated Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial resistance threat. The risk of acquiring MRSA and suffering life-threatening consequences due to it remains especially high for hospitalized patients due to a unique combination of factors, including: co-morbid conditions, immuno suppression, antibiotic use, and risk of contact with contaminated hospital workers and equipment. In this paper, we present a novel generative probabilistic model, GenHAI, for modeling sequences of MRSA test results outcomes for patients during a single hospitalization. This model can be used to answer many important questions from the perspectives of hospital administrators for mitigating the risk of MRSA infections. Our model is based on the probabilistic programming paradigm, and can be used to approximately answer a variety of predictive, causal, and counterfactual questions. We demonstrate the efficacy of our model by comparing it against discriminative and generative machine learning models using two real-world datasets.         ",
    "url": "https://arxiv.org/abs/2508.13561",
    "authors": [
      "Rituparna Datta",
      "Methun Kamruzzaman",
      "Eili Y. Klein",
      "Gregory R Madden",
      "Xinwei Deng",
      "Anil Vullikanti",
      "Parantapa Bhattacharya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13562",
    "title": "Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose Inverse Kinematics",
    "abstract": "           In 3D human pose and shape estimation, SMPLify remains a robust baseline that solves inverse kinematics (IK) through iterative optimization. However, its high computational cost limits its practicality. Recent advances across domains have shown that replacing iterative optimization with data-driven neural networks can achieve significant runtime improvements without sacrificing accuracy. Motivated by this trend, we propose Learnable SMPLify, a neural framework that replaces the iterative fitting process in SMPLify with a single-pass regression model. The design of our framework targets two core challenges in neural IK: data construction and generalization. To enable effective training, we propose a temporal sampling strategy that constructs initialization-target pairs from sequential frames. To improve generalization across diverse motions and unseen poses, we propose a human-centric normalization scheme and residual learning to narrow the solution space. Learnable SMPLify supports both sequential inference and plug-in post-processing to refine existing image-based estimators. Extensive experiments demonstrate that our method establishes itself as a practical and simple baseline: it achieves nearly 200x faster runtime compared to SMPLify, generalizes well to unseen 3DPW and RICH, and operates in a model-agnostic manner when used as a plug-in tool on LucidAction. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13562",
    "authors": [
      "Yuchen Yang",
      "Linfeng Dong",
      "Wei Wang",
      "Zhihang Zhong",
      "Xiao Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13581",
    "title": "Security-as-a-Function for IDS/IPS in Softwarized Network and Applications to 5G Network Systems",
    "abstract": "           The service-based architecture of 5G network allows network operators to place virtualized network functions on commodity hardware, unlike the traditional vendor-specific hardware-based functionalities. However, it expands the security vulnerabilities and threats to the 5G network. While there exist several theoretical studies on network function placement and service routing, a few focused on the security aspects of the 5G network systems. This paper focuses on safeguarding the 5G core network systems from DoS and DDoS attacks by placing intrusion detection and prevention systems (IDS-IPS) as virtualized network functions following the 5G standalone architecture. To ensure the virtualized placement of IDS-IPS, first, we provide thorough virtual machine (VM)-based and containerized implementation details and evaluate the network performance with two scenarios, IDS and IPS, in the presence of TCP and UDP applications. Second, we apply the VM-based implementation of IDS-IPS on a softwarized 5G core network and study the network performances. The experiment results on network throughput, latency, and packet drop reveal that the softwarized IDS-IPS can meet the QoS requirements of 5G applications, while safeguarding the network from DoS and DDoS attacks.         ",
    "url": "https://arxiv.org/abs/2508.13581",
    "authors": [
      "Shivank Malik",
      "Samaresh Bera"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2508.13596",
    "title": "A Generalized Learning Framework for Self-Supervised Contrastive Learning",
    "abstract": "           Self-supervised contrastive learning (SSCL) has recently demonstrated superiority in multiple downstream tasks. In this paper, we generalize the standard SSCL methods to a Generalized Learning Framework (GLF) consisting of two parts: the aligning part and the constraining part. We analyze three existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be unified under GLF with different choices of the constraining part. We further propose empirical and theoretical analyses providing two insights into designing the constraining part of GLF: intra-class compactness and inter-class separability, which measure how well the feature space preserves the class information of the inputs. However, since SSCL can not use labels, it is challenging to design a constraining part that satisfies these properties. To address this issue, we consider inducing intra-class compactness and inter-class separability by iteratively capturing the dynamic relationship between anchor and other samples and propose a plug-and-play method called Adaptive Distribution Calibration (ADC) to ensure that samples that are near or far from the anchor point in the original input space are closer or further away from the anchor point in the feature space. Both the theoretical analysis and the empirical evaluation demonstrate the superiority of ADC.         ",
    "url": "https://arxiv.org/abs/2508.13596",
    "authors": [
      "Lingyu Si",
      "Jingyao Wang",
      "Wenwen Qiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13604",
    "title": "Scalable Sensor Placement for Cyclic Networks with Observability Guarantees: Application to Water Distribution Networks",
    "abstract": "           Optimal sensor placement is essential for state estimation and effective network monitoring. As known in the literature, this problem becomes particularly challenging in large-scale undirected or bidirected cyclic networks with parametric uncertainties, such as water distribution networks (WDNs), where pipe resistance and demand patterns are often unknown. Motivated by the challenges of cycles, parametric uncertainties, and scalability, this paper proposes a sensor placement algorithm that guarantees structural observability for cyclic and acyclic networks with parametric uncertainties. By leveraging a graph-based strategy, the proposed method efficiently addresses the computational complexities of large-scale networks. To demonstrate the algorithm's effectiveness, we apply it to several EPANET benchmark WDNs. Most notably, the developed algorithm solves the sensor placement problem with guaranteed structured observability for the L-town WDN with 1694 nodes and 124 cycles in under 0.1 seconds.         ",
    "url": "https://arxiv.org/abs/2508.13604",
    "authors": [
      "J.J.H. van Gemert",
      "V. Breschi",
      "D.R. Yntema",
      "K.J. Keesman",
      "M. Lazar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.13607",
    "title": "Bounding Causal Effects and Counterfactuals",
    "abstract": "           Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics. All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface.         ",
    "url": "https://arxiv.org/abs/2508.13607",
    "authors": [
      "Tobias Maringgele"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2508.13625",
    "title": "Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models",
    "abstract": "           Large models, renowned for superior performance, outperform smaller ones even without billion-parameter scales. While mobile network servers have ample computational resources to support larger models than client devices, privacy constraints prevent clients from directly sharing their raw data. Federated Learning (FL) enables decentralized clients to collaboratively train a shared model by exchanging model parameters instead of transmitting raw data. Yet, it requires a uniform model architecture and multiple communication rounds, which neglect resource heterogeneity, impose heavy computational demands on clients, and increase communication overhead. To address these challenges, we propose FedOL, to construct a larger and more comprehensive server model in one-shot settings (i.e., in a single communication round). Instead of model parameter sharing, FedOL employs knowledge distillation, where clients only exchange model prediction outputs on an unlabeled public dataset. This reduces communication overhead by transmitting compact predictions instead of full model weights and enables model customization by allowing heterogeneous model architectures. A key challenge in this setting is that client predictions may be biased due to skewed local data distributions, and the lack of ground-truth labels in the public dataset further complicates reliable learning. To mitigate these issues, FedOL introduces a specialized objective function that iteratively refines pseudo-labels and the server model, improving learning reliability. To complement this, FedOL incorporates a tailored pseudo-label generation and knowledge distillation strategy that effectively integrates diverse knowledge. Simulation results show that FedOL significantly outperforms existing baselines, offering a cost-effective solution for mobile networks where clients possess valuable private data but limited computational resources.         ",
    "url": "https://arxiv.org/abs/2508.13625",
    "authors": [
      "Wenxuan Ye",
      "Xueli An",
      "Onur Ayan",
      "Junfan Wang",
      "Xueqiang Yan",
      "Georg Carle"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13633",
    "title": "Text2Weight: Bridging Natural Language and Neural Network Weight Spaces",
    "abstract": "           How far are we really from automatically generating neural networks? While neural network weight generation shows promise, current approaches struggle with generalization to unseen tasks and practical application exploration. To address this, we propose T2W, a diffusion transformer framework that generates task-specific weights conditioned on natural language descriptions. T2W hierarchically processes network parameters into uniform blocks, integrates text embeddings from CLIP via a prior attention mechanism, and employs adversarial training with weight-space augmentation to enhance generalization. Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability to produce high-quality weights for unseen tasks, outperforming optimization-based initialization and enabling novel applications such as weight enhancement and text-guided model fusion. Our work bridges textual semantics with weight-space dynamics, supported by an open-source dataset of text-weight pairs, advancing the practicality of generative models in neural network parameter synthesis. Our code is available on Github.         ",
    "url": "https://arxiv.org/abs/2508.13633",
    "authors": [
      "Bowen Tian",
      "Wenshuo Chen",
      "Zexi Li",
      "Songning Lai",
      "Jiemin Wu",
      "Yutao Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13634",
    "title": "V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task",
    "abstract": "           Precise localization of GUI elements is crucial for the development of GUI agents. Traditional methods rely on bounding box or center-point regression, neglecting spatial interaction uncertainty and visual-semantic hierarchies. Recent methods incorporate attention mechanisms but still face two key issues: (1) ignoring processing background regions causes attention drift from the desired area, and (2) uniform labeling fails to distinguish between center and edges of the target UI element, leading to click imprecision. Inspired by how humans visually process and interact with GUI elements, we propose the Valley-to-Peak (V2P) method to address these issues. To mitigate background distractions, V2P introduces a suppression attention mechanism that minimizes the model's focus on irrelevant regions to highlight the intended region. For the issue of center-edge distinction, V2P applies a Fitts' Law-inspired approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight gradually decreases from the center towards the edges. The weight distribution follows a Gaussian function, with the variance determined by the target's size. Consequently, V2P effectively isolates the target area and teaches the model to concentrate on the most essential point of the UI element. The model trained by V2P achieves the performance with 92.3% and 50.5% on two benchmarks ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's contribution, highlighting V2P's generalizability for precise GUI grounding tasks.         ",
    "url": "https://arxiv.org/abs/2508.13634",
    "authors": [
      "Jikai Chen",
      "Long Chen",
      "Dong Wang",
      "Leilei Gan",
      "Chenyi Zhuang",
      "Jinjie Gu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13644",
    "title": "Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems",
    "abstract": "           Accurately assessing software vulnerabilities is essential for effective prioritization and remediation. While various scoring systems exist to support this task, their differing goals, methodologies and outputs often lead to inconsistent prioritization decisions. This work provides the first large-scale, outcome-linked empirical comparison of four publicly available vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS), the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit Prediction Scoring System (EPSS), and the Exploitability Index. We use a dataset of 600 real-world vulnerabilities derived from four months of Microsoft's Patch Tuesday disclosures to investigate the relationships between these scores, evaluate how they support vulnerability management task, how these scores categorize vulnerabilities across triage tiers, and assess their ability to capture the real-world exploitation risk. Our findings reveal significant disparities in how scoring systems rank the same vulnerabilities, with implications for organizations relying on these metrics to make data-driven, risk-based decisions. We provide insights into the alignment and divergence of these systems, highlighting the need for more transparent and consistent exploitability, risk, and severity assessments.         ",
    "url": "https://arxiv.org/abs/2508.13644",
    "authors": [
      "Viktoria Koscinski",
      "Mark Nelson",
      "Ahmet Okutan",
      "Robert Falso",
      "Mehdi Mirakhorli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.13656",
    "title": "AutoMPC: A Code Generator for MPC-based Automated Driving",
    "abstract": "           Model Predictive Control (MPC) is a powerful technique to control nonlinear, multi-input multi-output systems subject to input and state constraints. It is now a standard tool for trajectory tracking control of automated vehicles. As such it has been used in many research and development projects. However, MPC faces several challenges to be integrated into industrial production vehicles. The most important ones are its high computational demands and the complexity of implementation. The software packages AutoMPC aims to address both of these challenges. It builds on a robustified version of an active set algorithm for Nonlinear MPC. The algorithm is embedded into a framework for vehicle trajectory tracking, which makes it easy to used, yet highly customizable. Automatic code generation transforms the selections into a standalone, computationally efficient C-code file with static memory allocation. As such it can be readily deployed on a wide range of embedded platforms, e.g., based on Matlab/Simulink or Robot Operating System (ROS). Compared to a previous version of the code, the vehicle model and the numerical integration method can be manually specified, besides basic algorithm parameters. All of this information and all specifications are directly baked into the generated C-code. The algorithm is suitable driving scenarios at low or high speeds, even drifting, and supports direction changes. Multiple simulation scenarios show the versatility and effectiveness of the AutoMPC code, with the guarantee of a feasible solution, a high degree of robustness, and computational efficiency.         ",
    "url": "https://arxiv.org/abs/2508.13656",
    "authors": [
      "Georg Schildbach",
      "Jasper Pflughaupt"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Mathematical Software (cs.MS)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.13657",
    "title": "In-Context Decision Making for Optimizing Complex AutoML Pipelines",
    "abstract": "           Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13657",
    "authors": [
      "Amir Rezaei Balef",
      "Katharina Eggensperger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13663",
    "title": "Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints",
    "abstract": "           Methods for query answering over incomplete knowledge graphs retrieve entities that are likely to be answers, which is particularly useful when such answers cannot be reached by direct graph traversal due to missing edges. However, existing approaches have focused on queries formalized using first-order-logic. In practice, many real-world queries involve constraints that are inherently vague or context-dependent, such as preferences for attributes or related categories. Addressing this gap, we introduce the problem of query answering with soft constraints. We propose a Neural Query Reranker (NQR) designed to adjust query answer scores by incorporating soft constraints without disrupting the original answers to a query. NQR operates interactively, refining answers based on incremental examples of preferred and non-preferred entities. We extend existing QA benchmarks by generating datasets with soft constraints. Our experiments demonstrate that NQR can capture soft constraints while maintaining robust query answering performance.         ",
    "url": "https://arxiv.org/abs/2508.13663",
    "authors": [
      "Daniel Daza",
      "Alberto Bernardi",
      "Luca Costabello",
      "Christophe Gueret",
      "Masoud Mansoury",
      "Michael Cochez",
      "Martijn Schut"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13666",
    "title": "The Hidden Cost of Readability: How Code Formatting Silently Consumes Your LLM Budget",
    "abstract": "           Source code is usually formatted with elements like indentation and newlines to improve readability for human developers. However, these visual aids do not seem to be beneficial for large language models (LLMs) in the same way since the code is processed as a linear sequence of tokens. Furthermore, these additional tokens can lead to increased computational costs and longer response times for LLMs. If such formatting elements are non-essential to LLMs, we can reduce such costs by removing them from the code. To figure out the role played by formatting elements, we conduct a comprehensive empirical study to evaluate the impact of code formatting on LLM performance and efficiency. Through large-scale experiments on Fill-in-the-Middle Code Completion tasks across four programming languages (Java, Python, C++, C\\#) and ten LLMs-including both commercial and open-source models-we systematically analyze token count and performance when formatting elements are removed. Key findings indicate that LLMs can maintain performance across formatted code and unformatted code, achieving an average input token reduction of 24.5\\% with negligible output token reductions. This makes code format removal a practical optimization strategy for improving LLM efficiency. Further exploration reveals that both prompting and fine-tuning LLMs can lead to significant reductions (up to 36.1\\%) in output code length without compromising correctness. To facilitate practical applications, we develop a bidirectional code transformation tool for format processing, which can be seamlessly integrated into existing LLM inference workflows, ensuring both human readability and LLM efficiency.         ",
    "url": "https://arxiv.org/abs/2508.13666",
    "authors": [
      "Dangfeng Pan",
      "Zhensu Sun",
      "Cenyuan Zhang",
      "David Lo",
      "Xiaoning Du"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.13669",
    "title": "DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction",
    "abstract": "           The automated extraction of complete and precise road network graphs from remote sensing imagery remains a critical challenge in geospatial computer vision. Segmentation-based approaches, while effective in pixel-level recognition, struggle to maintain topology fidelity after vectorization postprocessing. Graph-growing methods build more topologically faithful graphs but suffer from computationally prohibitive iterative ROI cropping. Graph-generating methods first predict global static candidate road network vertices, and then infer possible edges between vertices. They achieve fast topology-aware inference, but limits the dynamic insertion of vertices. To address these challenges, we propose DeH4R, a novel hybrid model that combines graph-generating efficiency and graph-growing dynamics. This is achieved by decoupling the task into candidate vertex detection, adjacent vertex prediction, initial graph contruction, and graph expansion. This architectural innovation enables dynamic vertex (edge) insertions while retaining fast inference speed and enhancing both topology fidelity and spatial consistency. Comprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate state-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA graph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while being approximately 10 $\\times$ faster. The code will be made publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.13669",
    "authors": [
      "Dengxian Gong",
      "Shunping Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13673",
    "title": "Multi-Plasticity Synergy with Adaptive Mechanism Assignment for Training Spiking Neural Networks",
    "abstract": "           Spiking Neural Networks (SNNs) are promising brain-inspired models known for low power consumption and superior potential for temporal processing, but identifying suitable learning mechanisms remains a challenge. Despite the presence of multiple coexisting learning strategies in the brain, current SNN training methods typically rely on a single form of synaptic plasticity, which limits their adaptability and representational capability. In this paper, we propose a biologically inspired training framework that incorporates multiple synergistic plasticity mechanisms for more effective SNN training. Our method enables diverse learning algorithms to cooperatively modulate the accumulation of information, while allowing each mechanism to preserve its own relatively independent update dynamics. We evaluated our approach on both static image and dynamic neuromorphic datasets to demonstrate that our framework significantly improves performance and robustness compared to conventional learning mechanism models. This work provides a general and extensible foundation for developing more powerful SNNs guided by multi-strategy brain-inspired learning.         ",
    "url": "https://arxiv.org/abs/2508.13673",
    "authors": [
      "Yuzhe Liu",
      "Xin Deng",
      "Qiang Yu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13675",
    "title": "Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks",
    "abstract": "           Knowledge Graphs are used for various purposes, including business applications, biomedical analyses, or digital twins in industry 4.0. In this paper, we investigate knowledge graphs describing household actions, which are beneficial for controlling household robots and analyzing video footage. In the latter case, the information extracted from videos is notoriously incomplete, and completing the knowledge graph for enhancing the situational picture is essential. In this paper, we show that, while a standard link prediction problem, situational knowledge graphs have special characteristics that render many link prediction algorithms not fit for the job, and unable to outperform even simple baselines.         ",
    "url": "https://arxiv.org/abs/2508.13675",
    "authors": [
      "Mariam Arustashvili",
      "J\u00f6rg Deigm\u00f6ller",
      "Heiko Paulheim"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13676",
    "title": "MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model",
    "abstract": "           To maintain the company's talent pool, recruiters need to continuously search for resumes from third-party websites (e.g., LinkedIn, Indeed). However, fetched resumes are often incomplete and inaccurate. To improve the quality of third-party resumes and enrich the company's talent pool, it is essential to conduct duplication detection between the fetched resumes and those already in the company's talent pool. Such duplication detection is challenging due to the semantic complexity, structural heterogeneity, and information incompleteness of resume texts. To this end, we propose MHSNet, an multi-level identity verification framework that fine-tunes BGE-M3 using contrastive learning. With the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and dense representations for resumes, enabling the computation of corresponding multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts (MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental results verify the effectiveness of MHSNet         ",
    "url": "https://arxiv.org/abs/2508.13676",
    "authors": [
      "Yu Li",
      "Zulong Chen",
      "Wenjian Xu",
      "Hong Wen",
      "Yipeng Yu",
      "Man Lung Yiu",
      "Yuyu Yin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13679",
    "title": "Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond",
    "abstract": "           Heavy-tailed bandits have been extensively studied since the seminal work of \\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits, enabling efficient learning with both a large number of arms and heavy-tailed noises, have recently attracted significant attention \\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}. However, prior studies focus almost exclusively on stochastic regimes, with few exceptions limited to the special case of heavy-tailed multi-armed bandits (MABs) \\citep{Huang0H22,ChengZ024,Chen2024uniINF}. In this work, we propose a general framework for adversarial heavy-tailed bandit problems, which performs follow-the-regularized-leader (FTRL) over the loss estimates shifted by a bonus function. Via a delicate setup of the bonus function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm for heavy-tailed MABs, which does not require the truncated non-negativity assumption and achieves an $\\widetilde{O}(T^{\\frac{1}{\\varepsilon}})$ worst-case regret in the adversarial regime as well as an $\\widetilde{O}(\\log T)$ gap-dependent regret in the stochastic regime. We then extend our framework to the linear case, proposing the first algorithm for adversarial heavy-tailed linear bandits with finite arm sets. This algorithm achieves an $\\widetilde{O}(d^{\\frac{1}{2}}T^{\\frac{1}{\\varepsilon}})$ regret, matching the best-known worst-case regret bound in stochastic regimes. Moreover, we propose a general data-dependent learning rate, termed \\textit{heavy-tailed noise aware stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW regret bounds for general heavy-tailed bandit problems once certain conditions are satisfied. By using HT-SPM and, in particular, a variance-reduced linear loss estimator, we obtain the first BOBW result for heavy-tailed linear bandits.         ",
    "url": "https://arxiv.org/abs/2508.13679",
    "authors": [
      "Canzhe Zhao",
      "Shinji Ito",
      "Shuai Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13716",
    "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning",
    "abstract": "           Graph Neural Networks (GNNs) have shown remarkable capabilities in processing graph-structured data prevalent in various real-world applications. However, the scalability of full-batch GNN training becomes severely limited by high communication overhead and load imbalance in distributed environments. In this paper, we present CaPGNN, a novel framework for efficient parallel full-batch GNN training on single-server with multi-GPU, designed specifically to reduce redundant inter-GPU communication and balance computational workloads. We propose a joint adaptive caching algorithm that leverages both CPU and GPU memory to significantly reduce the repetitive transmission of vertex features across partitions. Additionally, we introduce a resource-aware graph partitioning algorithm that adjusts subgraph sizes dynamically according to the heterogeneous computational and communication capacities of GPUs. Extensive experiments on large-scale benchmark datasets demonstrate that CaPGNN effectively reduces communication costs by up to 96% and accelerates GNN training by up to 12.7 times compared to state-of-the-art approaches. Our results highlight the potential of adaptive caching and resource-aware partitioning to facilitate scalable, efficient, and practical deployment of full-batch GNN training in distributed computing environments.         ",
    "url": "https://arxiv.org/abs/2508.13716",
    "authors": [
      "Xianfeng Song",
      "Yi Zou",
      "Zheng Shi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.13729",
    "title": "Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings",
    "abstract": "           Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability. We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.         ",
    "url": "https://arxiv.org/abs/2508.13729",
    "authors": [
      "Hanna Herasimchyk",
      "Alhassan Abdelhalim",
      "S\u00f6ren Laue",
      "Michaela Regneri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13730",
    "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions",
    "abstract": "           Federated Learning (FL) is an emerging distributed machine learning paradigm enabling multiple clients to train a global model collaboratively without sharing their raw data. While FL enhances data privacy by design, it remains vulnerable to various security and privacy threats. This survey provides a comprehensive overview of more than 200 papers regarding the state-of-the-art attacks and defense mechanisms developed to address these challenges, categorizing them into security-enhancing and privacy-preserving techniques. Security-enhancing methods aim to improve FL robustness against malicious behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same time, privacy-preserving techniques focus on protecting sensitive data through cryptographic approaches, differential privacy, and secure aggregation. We critically analyze the strengths and limitations of existing methods, highlight the trade-offs between privacy, security, and model performance, and discuss the implications of non-IID data distributions on the effectiveness of these defenses. Furthermore, we identify open research challenges and future directions, including the need for scalable, adaptive, and energy-efficient solutions operating in dynamic and heterogeneous FL environments. Our survey aims to guide researchers and practitioners in developing robust and privacy-preserving FL systems, fostering advancements safeguarding collaborative learning frameworks' integrity and confidentiality.         ",
    "url": "https://arxiv.org/abs/2508.13730",
    "authors": [
      "Daniel M. Jimenez-Gutierrez",
      "Yelizaveta Falkouskaya",
      "Jose L. Hernandez-Ramos",
      "Aris Anagnostopoulos",
      "Ioannis Chatzigiannakis",
      "Andrea Vitaletti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2508.13732",
    "title": "Self-Organizing Agent Network for LLM-based Workflow Automation",
    "abstract": "           Recent multi-agent frameworks built upon large language models (LLMs) have demonstrated remarkable capabilities in complex task planning. However, in real-world enterprise environments, business workflows are typically composed through modularization and reuse of numerous subprocesses, resulting in intricate workflows characterized by lengthy and deeply nested execution paths. Such complexity poses significant challenges for LLM-driven orchestration, as extended reasoning chains and state-space explosions severely impact planning effectiveness and the proper sequencing of tool invocations. Therefore, developing an orchestration method with controllable structures capable of handling multi-layer nesting becomes a critical issue. To address this, we propose a novel structure-driven orchestration framework Self-Organizing Agent Network (SOAN). SOAN incrementally builds a formalized agent network by identifying and encapsulating structural units as independent agents, enhancing modularity and clarity in orchestration. Extensive evaluations were performed using multiple benchmarks as well as a real-world enterprise workflow dataset. Experimental results demonstrate that SOAN significantly outperforms state-of-the-art methods in terms of adaptability, fault tolerance, and execution efficiency.         ",
    "url": "https://arxiv.org/abs/2508.13732",
    "authors": [
      "Yiming Xiong",
      "Jian Wang",
      "Bing Li",
      "Yuhan Zhu",
      "Yuqi Zhao"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.13739",
    "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance",
    "abstract": "           Targeted adversarial attacks are essential for proactively identifying security flaws in Vision-Language Models before real-world deployment. However, current methods perturb images to maximize global similarity with the target text or reference image at the encoder level, collapsing rich visual semantics into a single global vector. This limits attack granularity, hindering fine-grained manipulations such as modifying a car while preserving its background. Furthermore, these methods largely overlook the projector module, a critical semantic bridge between the visual encoder and the language model in VLMs, thereby failing to disrupt the full vision-language alignment pipeline within VLMs and limiting attack effectiveness. To address these issues, we propose the Intermediate Projector Guided Attack (IPGA), the first method to attack using the intermediate stage of the projector module, specifically the widely adopted Q-Former, which transforms global image embeddings into fine-grained visual features. This enables more precise control over adversarial perturbations by operating on semantically meaningful visual tokens rather than a single global representation. Specifically, IPGA leverages the Q-Former pretrained solely on the first vision-language alignment stage, without LLM fine-tuning, which improves both attack effectiveness and transferability across diverse VLMs. Furthermore, we propose Residual Query Alignment (RQA) to preserve unrelated visual content, thereby yielding more controlled and precise adversarial manipulations. Extensive experiments show that our attack method consistently outperforms existing methods in both standard global image captioning tasks and fine-grained visual question-answering tasks in black-box environment. Additionally, IPGA successfully transfers to multiple commercial VLMs, including Google Gemini and OpenAI GPT.         ",
    "url": "https://arxiv.org/abs/2508.13739",
    "authors": [
      "Yiming Cao",
      "Yanjie Li",
      "Kaisheng Liang",
      "Yuni Lai",
      "Bin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13743",
    "title": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA",
    "abstract": "           Large language models (LLMs), while increasingly used in domains requiring factual rigor, often display a troubling behavior: sycophancy, the tendency to align with user beliefs regardless of correctness. This tendency is reinforced by preference-based alignment techniques that optimize for user satisfaction but can undermine truthfulness. While relatively benign in casual dialogue, sycophancy poses serious risks in high-stakes settings such as scientific question answering (QA), where model outputs may shape collaborative reasoning, decision-making, and knowledge formation. Despite its importance, this phenomenon remains underexamined in factual QA contexts. We address this gap by introducing a unified evaluation framework to quantify the impact of sycophantic context on model behavior in scientific QA, measuring how much user-imposed social pressure distorts model outputs. The framework incorporates adversarial prompting setups and targeted metrics, such as misleading resistance and sycophancy resistance, that capture a model's ability to maintain factual consistency under misleading cues. Systematic evaluations across open-source and proprietary models reveal pervasive sycophantic tendencies, driven more by alignment strategy than by model size. To mitigate this issue, we propose Pressure-Tune, a lightweight post-training method that fine-tunes models on synthetic adversarial dialogues paired with chain-of-thought rationales. These rationales reject user misinformation while reinforcing factual commitments. Experiments on challenging scientific QA benchmarks show that Pressure-Tune significantly enhances sycophancy resistance without compromising accuracy or responsiveness to valid feedback, offering a practical pathway toward more truthful and principled model behavior.         ",
    "url": "https://arxiv.org/abs/2508.13743",
    "authors": [
      "Kaiwei Zhang",
      "Qi Jia",
      "Zijian Chen",
      "Wei Sun",
      "Xiangyang Zhu",
      "Chunyi Li",
      "Dandan Zhu",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.13757",
    "title": "COMPASS: A Multi-Dimensional Benchmark for Evaluating Code Generation in Large Language Models",
    "abstract": "           Current code generation benchmarks focus primarily on functional correctness while overlooking two critical aspects of real-world programming: algorithmic efficiency and code quality. We introduce COMPASS (COdility's Multi-dimensional Programming ASSessment), a comprehensive evaluation framework that assesses code generation across three dimensions: correctness, efficiency, and quality. COMPASS consists of 50 competitive programming problems from real Codility competitions, providing authentic human baselines from 393,150 submissions. Unlike existing benchmarks that treat algorithmically inefficient solutions identically to optimal ones provided they pass test cases, COMPASS systematically evaluates runtime efficiency and code quality using industry-standard analysis tools. Our evaluation of three leading reasoning-enhanced models, Anthropic Claude Opus 4, Google Gemini 2.5 Pro, and OpenAI O4-Mini-High, reveals that models achieving high correctness scores do not necessarily produce efficient algorithms or maintainable code. These findings highlight the importance of evaluating more than just correctness to truly understand the real-world capabilities of code generation models. COMPASS serves as a guiding framework, charting a path for future research toward AI systems that are robust, reliable, and ready for production use.         ",
    "url": "https://arxiv.org/abs/2508.13757",
    "authors": [
      "James Meaden",
      "Micha\u0142 Jarosz",
      "Piotr Jod\u0142owski",
      "Grigori Melnik"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13768",
    "title": "MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment",
    "abstract": "           Large Language Models have shown growing ability to generate fluent and coherent texts that are highly similar to the writing style of humans. Current detectors for Machine-Generated Text (MGT) perform well when they are trained and tested in the same domain but generalize poorly to unseen domains, due to domain shift between data from different sources. In this work, we propose MGT-Prism, an MGT detection method from the perspective of the frequency domain for better domain generalization. Our key insight stems from analyzing text representations in the frequency domain, where we observe consistent spectral patterns across diverse domains, while significant discrepancies in magnitude emerge between MGT and human-written texts (HWTs). The observation initiates the design of a low frequency domain filtering module for filtering out the document-level features that are sensitive to domain shift, and a dynamic spectrum alignment strategy to extract the task-specific and domain-invariant features for improving the detector's performance in domain generalization. Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test datasets across three domain-generalization scenarios.         ",
    "url": "https://arxiv.org/abs/2508.13768",
    "authors": [
      "Shengchao Liu",
      "Xiaoming Liu",
      "Chengzhengxu Li",
      "Zhaohan Zhang",
      "Guoxin Ma",
      "Yu Lan",
      "Shuai Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.13783",
    "title": "Encoding Optimization for Low-Complexity Spiking Neural Network Equalizers in IM/DD Systems",
    "abstract": "           Neural encoding parameters for spiking neural networks (SNNs) are typically set heuristically. We propose a reinforcement learning-based algorithm to optimize them. Applied to an SNN-based equalizer and demapper in an IM/DD system, the method improves performance while reducing computational load and network size.         ",
    "url": "https://arxiv.org/abs/2508.13783",
    "authors": [
      "Eike-Manuel Edelmann",
      "Alexander von Bank",
      "Laurent Schmalen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2508.13786",
    "title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer",
    "abstract": "           Controllable text-to-audio generation aims to synthesize audio from textual descriptions while satisfying user-specified constraints, including event types, temporal sequences, and onset and offset timestamps. This enables precise control over both the content and temporal structure of the generated audio. Despite recent progress, existing methods still face inherent trade-offs among accurate temporal localization, open-vocabulary scalability, and practical efficiency. To address these challenges, we propose DegDiT, a novel dynamic event graph-guided diffusion transformer framework for open-vocabulary controllable audio generation. DegDiT encodes the events in the description as structured dynamic graphs. The nodes in each graph are designed to represent three aspects: semantic features, temporal attributes, and inter-event connections. A graph transformer is employed to integrate these nodes and produce contextualized event embeddings that serve as guidance for the diffusion model. To ensure high-quality and diverse training data, we introduce a quality-balanced data selection pipeline that combines hierarchical event annotation with multi-criteria quality scoring, resulting in a curated dataset with semantic diversity. Furthermore, we present consensus preference optimization, facilitating audio generation through consensus among multiple reward signals. Extensive experiments on AudioCondition, DESED, and AudioTime datasets demonstrate that DegDiT achieves state-of-the-art performances across a variety of objective and subjective evaluation metrics.         ",
    "url": "https://arxiv.org/abs/2508.13786",
    "authors": [
      "Yisu Liu",
      "Chenxing Li",
      "Wanqian Zhang",
      "Wenfu Wang",
      "Meng Yu",
      "Ruibo Fu",
      "Zheng Lin",
      "Weiping Wang",
      "Dong Yu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13806",
    "title": "Reinforcement Learning-based Adaptive Path Selection for Programmable Networks",
    "abstract": "           This work presents a proof-of-concept implementation of a distributed, in-network reinforcement learning (IN-RL) framework for adaptive path selection in programmable networks. By combining Stochastic Learning Automata (SLA) with real-time telemetry data collected via In-Band Network Telemetry (INT), the proposed system enables local, data-driven forwarding decisions that adapt dynamically to congestion conditions. The system is evaluated on a Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how our SLA-based mechanism converges to effective path selections and adapts to shifting network conditions at line rate.         ",
    "url": "https://arxiv.org/abs/2508.13806",
    "authors": [
      "Jos\u00e9 Eduardo Zerna Torres",
      "Marios Avgeris",
      "Chrysa Papagianni",
      "Gergely Pongr\u00e1cz",
      "Istv\u00e1n G\u00f3dor",
      "Paola Grosso"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13808",
    "title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images",
    "abstract": "           Neural Radiance Fields (NeRF) has gained significant attention for its prominent implicit 3D representation and realistic novel view synthesis capabilities. Available works unexceptionally employ straight-line volume rendering, which struggles to handle sophisticated lightpath scenarios and introduces geometric ambiguities during training, particularly evident when processing motion-blurred images. To address these challenges, this work proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit lightpath modeling in real-world environments. By unifying six common light propagation phenomena through an in-scattering representation, we establish a new scattering-aware volume rendering pipeline adaptable to complex lightpaths. Additionally, we introduce an adaptive learning strategy that enables autonomous determining of scattering directions and sampling intervals to capture finer object details. The proposed network jointly optimizes NeRF parameters, scattering parameters, and camera motions to recover fine-grained scene representations from blurry images. Comprehensive evaluations demonstrate that it effectively handles complex real-world scenarios, outperforming state-of-the-art approaches in generating high-fidelity images with accurate geometric details.         ",
    "url": "https://arxiv.org/abs/2508.13808",
    "authors": [
      "Nan Luo",
      "Chenglin Ye",
      "Jiaxu Li",
      "Gang Liu",
      "Bo Wan",
      "Di Wang",
      "Lupeng Liu",
      "Jun Xiao"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13812",
    "title": "Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation",
    "abstract": "           State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural networks (SNNs), which largely rely on extending FGSM and PGD frameworks, face a critical limitation: substantial attack latency from multi-timestep processing, rendering them infeasible for practical real-time applications. This inefficiency stems from their design as direct extensions of ANN paradigms, which fail to exploit key SNN properties. In this paper, we propose the timestep-compressed attack (TCA), a novel framework that significantly reduces attack latency. TCA introduces two components founded on key insights into SNN behavior. First, timestep-level backpropagation (TLBP) is based on our finding that global temporal information in backpropagation to generate perturbations is not critical for an attack's success, enabling per-timestep evaluation for early stopping. Second, adversarial membrane potential reuse (A-MPR) is motivated by the observation that initial timesteps are inefficiently spent accumulating membrane potential, a warm-up phase that can be pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the CIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the required attack latency by up to 56.6% and 57.1% compared to SOTA methods in white-box and black-box settings, respectively, while maintaining a comparable attack success rate.         ",
    "url": "https://arxiv.org/abs/2508.13812",
    "authors": [
      "Donghwa Kang",
      "Doohyun Kim",
      "Sang-Ki Ko",
      "Jinkyu Lee",
      "Hyeongboo Baek",
      "Brent ByungHoon Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.13819",
    "title": "Structural and Connectivity Patterns in the Maven Central Software Dependency Network",
    "abstract": "           Understanding the structural characteristics and connectivity patterns of large-scale software ecosystems is critical for enhancing software reuse, improving ecosystem resilience, and mitigating security risks. In this paper, we investigate the Maven Central ecosystem, one of the largest repositories of Java libraries, by applying network science techniques to its dependency graph. Leveraging the Goblin framework, we extracted a sample consisting of the top 5,000 highly connected artifacts based on their degree centrality and then performed breadth-first search (BFS) expansion from each selected artifact as a seed node, traversing the graph outward to capture all libraries and releases reachable those seed nodes. This sampling strategy captured the immediate structural context surrounding these libraries resulted in a curated graph comprising of 1.3 million nodes and 20.9 million edges. We conducted a comprehensive analysis of this graph, computing degree distributions, betweenness centrality, PageRank centrality, and connected components graph-theoretic metrics. Our results reveal that Maven Central exhibits a highly interconnected, scale-free, and small-world topology, characterized by a small number of infrastructural hubs that support the majority of projects. Further analysis using PageRank and betweenness centrality shows that these hubs predominantly consist of core ecosystem infrastructure, including testing frameworks and general-purpose utility libraries. While these hubs facilitate efficient software reuse and integration, they also pose systemic risks; failures or vulnerabilities affecting these critical nodes can have widespread and cascading impacts throughout the ecosystem.         ",
    "url": "https://arxiv.org/abs/2508.13819",
    "authors": [
      "Daniel Ogenrwot",
      "John Businge",
      "Shaikh Arifuzzaman"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.13825",
    "title": "Energy Management and Wake-up for IoT Networks Powered by Energy Harvesting",
    "abstract": "           The rapid growth of the Internet of Things (IoT) presents sustainability challenges such as increased maintenance requirements and overall higher energy consumption. This motivates self-sustainable IoT ecosystems based on Energy Harvesting (EH). This paper treats IoT deployments in which IoT devices (IoTDs) rely solely on EH to sense and transmit information about events/alarms to a base station (BS). The objective is to effectively manage the duty cycling of the IoTDs to prolong battery life and maximize the relevant data sent to the BS. The BS can also wake up specific IoTDs if extra information about an event is needed upon initial detection. We propose a K-nearest neighbors (KNN)-based duty cycling management to optimize energy efficiency and detection accuracy by considering spatial correlations among IoTDs' activity and their EH process. We evaluate machine learning approaches, including reinforcement learning (RL) and decision transformers (DT), to maximize information captured from events while managing energy consumption. Significant improvements over the state-ofthe-art approaches are obtained in terms of energy saving by all three proposals, KNN, RL, and DT. Moreover, the RL-based solution approaches the performance of a genie-aided benchmark as the number of IoTDs increases.         ",
    "url": "https://arxiv.org/abs/2508.13825",
    "authors": [
      "David Ernesto Ruiz-Guirola",
      "Samuel Montejo-Sanchez",
      "Israel Leyva-Mayorga",
      "Zhu Han",
      "Petar Popovski",
      "Onel L. A. Lopez"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.13853",
    "title": "FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks",
    "abstract": "           Federated Learning (FL) can be vulnerable to attacks, such as model poisoning, where adversaries send malicious local weights to compromise the global model. Federated Unlearning (FU) is emerging as a solution to address such vulnerabilities by selectively removing the influence of detected malicious contributors on the global model without complete retraining. However, unlike typical FU scenarios where clients are trusted and cooperative, applying FU with malicious and possibly colluding clients is challenging because their collaboration in unlearning their data cannot be assumed. This work presents FedUP, a lightweight FU algorithm designed to efficiently mitigate malicious clients' influence by pruning specific connections within the attacked model. Our approach achieves efficiency by relying only on clients' weights from the last training round before unlearning to identify which connections to inhibit. Isolating malicious influence is non-trivial due to overlapping updates from benign and malicious clients. FedUP addresses this by carefully selecting and zeroing the highest magnitude weights that diverge the most between the latest updates from benign and malicious clients while preserving benign information. FedUP is evaluated under a strong adversarial threat model, where up to 50%-1 of the clients could be malicious and have full knowledge of the aggregation process. We demonstrate the effectiveness, robustness, and efficiency of our solution through experiments across IID and Non-IID data, under label-flipping and backdoor attacks, and by comparing it with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces malicious influence, lowering accuracy on malicious data to match that of a model retrained from scratch while preserving performance on benign data. FedUP achieves effective unlearning while consistently being faster and saving storage compared to the SOTA.         ",
    "url": "https://arxiv.org/abs/2508.13853",
    "authors": [
      "Nicol\u00f2 Romandini",
      "Cristian Borcea",
      "Rebecca Montanari",
      "Luca Foschini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13859",
    "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
    "abstract": "           Symbolic regression encompasses a family of search algorithms that aim to discover the best fitting function for a set of data without requiring an a priori specification of the model structure. The most successful and commonly used technique for symbolic regression is Genetic Programming (GP), an evolutionary search method that evolves a population of mathematical expressions through the mechanism of natural selection. In this work we analyze the efficiency of the evolutionary search in GP and show that many points in the search space are re-visited and re-evaluated multiple times by the algorithm, leading to wasted computational effort. We address this issue by introducing a caching mechanism based on the Zobrist hash, a type of hashing frequently used in abstract board games for the efficient construction and subsequent update of transposition tables. We implement our caching approach using the open-source framework Operon and demonstrate its performance on a selection of real-world regression problems, where we observe up to 34\\% speedups without any detrimental effects on search quality. The hashing approach represents a straightforward way to improve runtime performance while also offering some interesting possibilities for adjusting search strategy based on cached information.         ",
    "url": "https://arxiv.org/abs/2508.13859",
    "authors": [
      "Bogdan Burlacu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2508.13878",
    "title": "RICO: Two Realistic Benchmarks and an In-Depth Analysis for Incremental Learning in Object Detection",
    "abstract": "           Incremental Learning (IL) trains models sequentially on new data without full retraining, offering privacy, efficiency, and scalability. IL must balance adaptability to new data with retention of old knowledge. However, evaluations often rely on synthetic, simplified benchmarks, obscuring real-world IL performance. To address this, we introduce two Realistic Incremental Object Detection Benchmarks (RICO): Domain RICO (D-RICO) features domain shifts with a fixed class set, and Expanding-Classes RICO (EC-RICO) integrates new domains and classes per IL step. Built from 14 diverse datasets covering real and synthetic domains, varying conditions (e.g., weather, time of day), camera sensors, perspectives, and labeling policies, both benchmarks capture challenges absent in existing evaluations. Our experiments show that all IL methods underperform in adaptability and retention, while replaying a small amount of previous data already outperforms all methods. However, individual training on the data remains superior. We heuristically attribute this gap to weak teachers in distillation, single models' inability to manage diverse tasks, and insufficient plasticity. Our code will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2508.13878",
    "authors": [
      "Matthias Neuwirth-Trapp",
      "Maarten Bieshaar",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13891",
    "title": "Forecasting Smog Events Using ConvLSTM: A Spatio-Temporal Approach for Aerosol Index Prediction in South Asia",
    "abstract": "           The South Asian Smog refers to the recurring annual air pollution events marked by high contaminant levels, reduced visibility, and significant socio-economic impacts, primarily affecting the Indo-Gangetic Plains (IGP) from November to February. Over the past decade, increased air pollution sources such as crop residue burning, motor vehicles, and changing weather patterns have intensified these smog events. However, real-time forecasting systems for increased particulate matter concentrations are still not established at regional scale. The Aerosol Index, closely tied to smog formation and a key component in calculating the Air Quality Index (AQI), reflects particulate matter concentrations. This study forecasts aerosol events using Sentinel-5P air constituent data (2019-2023) and a Convolutional Long-Short Term Memory (ConvLSTM) neural network, which captures spatial and temporal correlations more effectively than previous models. Using the Ultraviolet (UV) Aerosol Index at 340-380 nm as the predictor, results show the Aerosol Index can be forecasted at five-day intervals with a Mean Squared Error of ~0.0018, loss of ~0.3995, and Structural Similarity Index of ~0.74. While effective, the model can be improved by integrating additional data and refining its architecture.         ",
    "url": "https://arxiv.org/abs/2508.13891",
    "authors": [
      "Taimur Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13899",
    "title": "SCRNet: Spatial-Channel Regulation Network for Medical Ultrasound Image Segmentation",
    "abstract": "           Medical ultrasound image segmentation presents a formidable challenge in the realm of computer vision. Traditional approaches rely on Convolutional Neural Networks (CNNs) and Transformer-based methods to address the intricacies of medical image segmentation. Nevertheless, inherent limitations persist, as CNN-based methods tend to disregard long-range dependencies, while Transformer-based methods may overlook local contextual information. To address these deficiencies, we propose a novel Feature Aggregation Module (FAM) designed to process two input features from the preceding layer. These features are seamlessly directed into two branches of the Convolution and Cross-Attention Parallel Module (CCAPM) to endow them with different roles in each of the two branches to help establish a strong connection between the two input features. This strategy enables our module to focus concurrently on both long-range dependencies and local contextual information by judiciously merging convolution operations with cross-attention mechanisms. Moreover, by integrating FAM within our proposed Spatial-Channel Regulation Module (SCRM), the ability to discern salient regions and informative features warranting increased attention is enhanced. Furthermore, by incorporating the SCRM into the encoder block of the UNet architecture, we introduce a novel framework dubbed Spatial-Channel Regulation Network (SCRNet). The results of our extensive experiments demonstrate the superiority of SCRNet, which consistently achieves state-of-the-art (SOTA) performance compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2508.13899",
    "authors": [
      "Weixin Xu",
      "Ziliang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13921",
    "title": "DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts",
    "abstract": "           Image degradation caused by complex lighting conditions such as low-light and backlit scenarios is commonly encountered in real-world environments, significantly affecting image quality and downstream vision tasks. Most existing methods focus on a single type of illumination degradation and lack the ability to handle diverse lighting conditions in a unified manner. To address this issue, we propose a dual-illumination enhancement framework called DIME-Net. The core of our method is a Mixture-of-Experts illumination estimator module, where a sparse gating mechanism adaptively selects suitable S-curve expert networks based on the illumination characteristics of the input image. By integrating Retinex theory, this module effectively performs enhancement tailored to both low-light and backlit images. To further correct illumination-induced artifacts and color distortions, we design a damage restoration module equipped with Illumination-Aware Cross Attention and Sequential-State Global Attention mechanisms. In addition, we construct a hybrid illumination dataset, MixBL, by integrating existing datasets, allowing our model to achieve robust illumination adaptability through a single training process. Experimental results show that DIME-Net achieves competitive performance on both synthetic and real-world low-light and backlit datasets without any retraining. These results demonstrate its generalization ability and potential for practical multimedia applications under diverse and complex illumination conditions.         ",
    "url": "https://arxiv.org/abs/2508.13921",
    "authors": [
      "Ziang Wang",
      "Xiaoqin Wang",
      "Dingyi Wang",
      "Qiang Li",
      "Shushan Qiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13953",
    "title": "ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features",
    "abstract": "           In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble). While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page this https URL ",
    "url": "https://arxiv.org/abs/2508.13953",
    "authors": [
      "A.J.W. de Vink",
      "Natalia Amat-Lefort",
      "Lifeng Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2508.13975",
    "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation",
    "abstract": "           This contribution is concerned with the following issue: can pretrained large language models (LLMs) be refined and customized to the point where they become virtual assistants helping experts with the effective use of a simulation tool? In this case study, the ``simulation tool'' considered is PyChrono, an open source multi-physics dynamics engine for multibody systems. We present a framework for refining and customizing both open- and closed-source LLMs to harness the power of AI in generating scripts that perform PyChrono virtual experiments. We refine and customize several classes of LLMs through a process that leads to a quantifiable improvement in the quality of the generated PyChrono simulation scripts. These scripts can range from simple single-pendulum simulations to complex virtual experiments involving full vehicles on deformable terrain. While the generated scripts are rarely perfect, they often serve as strong starting points for the user to modify and improve on. Additionally, the LLM can answer specific API questions about the simulator, or recommend modeling approaches. The framework discussed is general and can be applied to lower the entry barrier for simulation tools associated with other application domains.         ",
    "url": "https://arxiv.org/abs/2508.13975",
    "authors": [
      "Jingquan Wang",
      "Andrew Negrut",
      "Harry Zhang",
      "Khailanii Slaton",
      "Shu Wang",
      "Radu Serban",
      "Jinlong Wu",
      "Dan Negrut"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13982",
    "title": "The Social Context of Human-Robot Interactions",
    "abstract": "           The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term \"social context\" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term \"social context\". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.         ",
    "url": "https://arxiv.org/abs/2508.13982",
    "authors": [
      "Sydney Thompson",
      "Kate Candon",
      "Marynel V\u00e1zquez"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2508.13983",
    "title": "OmViD: Omni-supervised active learning for video action detection",
    "abstract": "           Video action detection requires dense spatio-temporal annotations, which are both challenging and expensive to obtain. However, real-world videos often vary in difficulty and may not require the same level of annotation. This paper analyzes the appropriate annotation types for each sample and their impact on spatio-temporal video action detection. It focuses on two key aspects: 1) how to obtain varying levels of annotation for videos, and 2) how to learn action detection from different annotation types. The study explores video-level tags, points, scribbles, bounding boxes, and pixel-level masks. First, a simple active learning strategy is proposed to estimate the necessary annotation type for each video. Then, a novel spatio-temporal 3D-superpixel approach is introduced to generate pseudo-labels from these annotations, enabling effective training. The approach is validated on UCF101-24 and JHMDB-21 datasets, significantly cutting annotation costs with minimal performance loss.         ",
    "url": "https://arxiv.org/abs/2508.13983",
    "authors": [
      "Aayush Rana",
      "Akash Kumar",
      "Vibhav Vineet",
      "Yogesh S Rawat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13995",
    "title": "Self-Supervised Sparse Sensor Fusion for Long Range Perception",
    "abstract": "           Outside of urban hubs, autonomous cars and trucks have to master driving on intercity highways. Safe, long-distance highway travel at speeds exceeding 100 km/h demands perception distances of at least 250 m, which is about five times the 50-100m typically addressed in city driving, to allow sufficient planning and braking margins. Increasing the perception ranges also allows to extend autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks, which need a longer planning horizon due to their high inertia. However, most existing perception approaches focus on shorter ranges and rely on Bird's Eye View (BEV) representations, which incur quadratic increases in memory and compute costs as distance grows. To overcome this limitation, we built on top of a sparse representation and introduced an efficient 3D encoding of multi-modal and temporal features, along with a novel self-supervised pre-training scheme that enables large-scale learning from unlabeled camera-LiDAR data. Our approach extends perception distances to 250 meters and achieves an 26.6% improvement in mAP in object detection and a decrease of 30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods, reaching distances up to 250 meters. Project Page: this https URL ",
    "url": "https://arxiv.org/abs/2508.13995",
    "authors": [
      "Edoardo Palladin",
      "Samuel Brucker",
      "Filippo Ghilotti",
      "Praveen Narayanan",
      "Mario Bijelic",
      "Felix Heide"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.14004",
    "title": "GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks",
    "abstract": "           Quantized neural networks can be viewed as a chain of noisy channels, where rounding in each layer reduces capacity as bit-width shrinks; the floating-point (FP) checkpoint sets the maximum input rate. We track capacity dynamics as the average bit-width decreases and identify resulting quantization bottlenecks by casting fine-tuning as a smooth, constrained optimization problem. Our approach employs a fully differentiable Straight-Through Estimator (STE) with learnable bit-width, noise scale and clamp bounds, and enforces a target bit-width via an exterior-point penalty; mild metric smoothing (via distillation) stabilizes training. Despite its simplicity, the method attains competitive accuracy down to the extreme W1A1 setting while retaining the efficiency of STE.         ",
    "url": "https://arxiv.org/abs/2508.14004",
    "authors": [
      "Sergey Salishev",
      "Ian Akhremchik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2508.14005",
    "title": "ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery",
    "abstract": "           Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a non-invasive window into large-scale neural dynamics by measuring blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can be modeled as interactions among Regions of Interest (ROIs), which are grouped into functional communities based on their underlying roles in brain function. Emerging evidence suggests that connectivity patterns within and between these communities are particularly sensitive to ASD-related alterations. Effectively capturing these patterns and identifying interactions that deviate from typical development is essential for improving ASD diagnosis and enabling biomarker discovery. In this work, we introduce ASDFormer, a Transformer-based architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to capture neural signatures associated with ASD. By integrating multiple specialized expert branches with attention mechanisms, ASDFormer adaptively emphasizes different brain regions and connectivity patterns relevant to autism. This enables both improved classification performance and more interpretable identification of disorder-related biomarkers. Applied to the ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and reveals robust insights into functional connectivity disruptions linked to ASD, highlighting its potential as a tool for biomarker discovery.         ",
    "url": "https://arxiv.org/abs/2508.14005",
    "authors": [
      "Mohammad Izadi",
      "Mehran Safayani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.14013",
    "title": "Efficient Knowledge Graph Unlearning with Zeroth-order Information",
    "abstract": "           Due to regulations like the Right to be Forgotten, there is growing demand for removing training data and its influence from models. Since full retraining is costly, various machine unlearning methods have been proposed. In this paper, we firstly present an efficient knowledge graph (KG) unlearning algorithm. We remark that KG unlearning is nontrivial due to the distinctive structure of KG and the semantic relations between entities. Also, unlearning by estimating the influence of removed components incurs significant computational overhead when applied to large-scale knowledge graphs. To this end, we define an influence function for KG unlearning and propose to approximate the model's sensitivity without expensive computation of first-order and second-order derivatives for parameter updates. Specifically, we use Taylor expansion to estimate the parameter changes caused by data removal. Given that the first-order gradients and second-order derivatives dominate the computational load, we use the Fisher matrices and zeroth-order optimization to approximate the inverse-Hessian vector product without constructing the computational graphs. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art graph unlearning baselines significantly in terms of unlearning efficiency and unlearning quality. Our code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.14013",
    "authors": [
      "Yang Xiao",
      "Ruimeng Ye",
      "Bohan Liu",
      "Xiaolong Ma",
      "Bo Hui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.14015",
    "title": "Backdooring Self-Supervised Contrastive Learning by Noisy Alignment",
    "abstract": "           Self-supervised contrastive learning (CL) effectively learns transferable representations from unlabeled data containing images or image-text pairs but suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary can inject poisoned images into pretraining datasets, causing compromised CL encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs, however, achieve limited efficacy due to their dependence on fragile implicit co-occurrence between backdoor and target object and inadequate suppression of discriminative features in backdoored images. We propose Noisy Alignment (NA), a DPCL method that explicitly suppresses noise components in poisoned images. Inspired by powerful training-controllable CL attacks, we identify and extract the critical objective of noisy alignment, adapting it effectively into data-poisoning scenarios. Our method implements noisy alignment by strategically manipulating contrastive learning's random cropping mechanism, formulating this process as an image layout optimization problem with theoretically derived optimal parameters. The resulting method is simple yet effective, achieving state-of-the-art performance compared to existing DPCLs, while maintaining clean-data accuracy. Furthermore, Noisy Alignment demonstrates robustness against common backdoor defenses. Codes can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.14015",
    "authors": [
      "Tuo Chen",
      "Jie Gui",
      "Minjing Dong",
      "Ju Jia",
      "Lanting Fang",
      "Jian Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.14017",
    "title": "Analog computation with transcriptional networks",
    "abstract": "           Transcriptional networks represent one of the most extensively studied types of systems in synthetic biology. Although the completeness of transcriptional networks for digital logic is well-established, *analog* computation plays a crucial role in biological systems and offers significant potential for synthetic biology applications. While transcriptional circuits typically rely on cooperativity and highly non-linear behavior of transcription factors to regulate *production* of proteins, they are often modeled with simple linear *degradation* terms. In contrast, general analog dynamics require both non-linear positive as well as negative terms, seemingly necessitating control over not just transcriptional (i.e., production) regulation but also the degradation rates of transcription factors. Surprisingly, we prove that controlling transcription factor production (i.e., transcription rate) without explicitly controlling degradation is mathematically complete for analog computation, achieving equivalent capabilities to systems where both production and degradation are programmable. We demonstrate our approach on several examples including oscillatory and chaotic dynamics, analog sorting, memory, PID controller, and analog extremum seeking. Our result provides a systematic methodology for engineering novel analog dynamics using synthetic transcriptional networks without the added complexity of degradation control and informs our understanding of the capabilities of natural transcriptional circuits. We provide a compiler, in the form of a Python package that can take any system of polynomial ODEs and convert it to an equivalent transcriptional network implementing the system *exactly*, under appropriate conditions.         ",
    "url": "https://arxiv.org/abs/2508.14017",
    "authors": [
      "David Doty",
      "Mina Latifi",
      "David Soloveichick"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2508.14041",
    "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
    "abstract": "           LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: this https URL ",
    "url": "https://arxiv.org/abs/2508.14041",
    "authors": [
      "Chin-Yang Lin",
      "Cheng Sun",
      "Fu-En Yang",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13188",
    "title": "Colon Polyps Detection from Colonoscopy Images Using Deep Learning",
    "abstract": "           Colon polyps are precursors to colorectal cancer, a leading cause of cancer-related mortality worldwide. Early detection is critical for improving patient outcomes. This study investigates the application of deep learning-based object detection for early polyp identification using colonoscopy images. We utilize the Kvasir-SEG dataset, applying extensive data augmentation and splitting the data into training (80\\%), validation (20\\% of training), and testing (20\\%) sets. Three variants of the YOLOv5 architecture (YOLOv5s, YOLOv5m, YOLOv5l) are evaluated. Experimental results show that YOLOv5l outperforms the other variants, achieving a mean average precision (mAP) of 85.1\\%, with the highest average Intersection over Union (IoU) of 0.86. These findings demonstrate that YOLOv5l provides superior detection performance for colon polyp localization, offering a promising tool for enhancing colorectal cancer screening accuracy.         ",
    "url": "https://arxiv.org/abs/2508.13188",
    "authors": [
      "Md Al Amin",
      "Bikash Kumar Paul"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13207",
    "title": "Utilizing the RAIN method and Graph SAGE Model to Identify Effective Drug Combinations for Gastric Neoplasm Treatment",
    "abstract": "           Background: Gastric neoplasm, primarily adenocarcinoma, is an aggressive cancer with high mortality, often diagnosed late, leading to complications like metastasis. Effective drug combinations are vital to address disease heterogeneity, enhance efficacy, reduce resistance, and improve patient outcomes. Methods: The RAIN method integrated Graph SAGE to propose drug combinations, using a graph model with p-value-weighted edges connecting drugs, genes, and proteins. NLP and systematic literature review (PubMed, Scopus, etc.) validated proposed drugs, followed by network meta-analysis to assess efficacy, implemented in Python. Results: Oxaliplatin, fluorouracil, and trastuzumab were identified as effective, supported by 61 studies. Fluorouracil alone had a p-value of 0.0229, improving to 0.0099 with trastuzumab, and 0.0069 for the triple combination, indicating superior efficacy. Conclusion: The RAIN method, combining AI and network meta-analysis, effectively identifies optimal drug combinations for gastric neoplasm, offering a promising strategy to enhance treatment outcomes and guide health policy.         ",
    "url": "https://arxiv.org/abs/2508.13207",
    "authors": [
      "S. Z. Pirasteh",
      "Ali A. Kiaei",
      "Mahnaz Bush",
      "Sabra Moghadam",
      "Raha Aghaei",
      "Behnaz Sadeghigol"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.13253",
    "title": "Automated Cervical Cancer Detection through Visual Inspection with Acetic Acid in Resource-Poor Settings with Lightweight Deep Learning Models Deployed on an Android Device",
    "abstract": "           Cervical cancer is among the most commonly occurring cancer among women and claims a huge number of lives in low and middle-income countries despite being relatively easy to treat. Several studies have shown that public screening programs can bring down cervical cancer incidence and mortality rates significantly. While several screening tests are available, visual inspection with acetic acid (VIA) presents itself as the most viable option for low-resource settings due to the affordability and simplicity of performing the test. VIA requires a trained medical professional to interpret the test and is subjective in nature. Automating VIA using AI eliminates subjectivity and would allow shifting of the task to less trained health workers. Task shifting with AI would help further expedite screening programs in low-resource settings. In our work, we propose a lightweight deep learning algorithm that includes EfficientDet-Lite3 as the Region of Interest (ROI) detector and a MobileNet- V2 based model for classification. These models would be deployed on an android-based device that can operate remotely and provide almost instant results without the requirement of highly-trained medical professionals, labs, sophisticated infrastructure, or internet connectivity. The classification model gives an accuracy of 92.31%, a sensitivity of 98.24%, and a specificity of 88.37% on the test dataset and presents itself as a promising automated low-resource screening approach.         ",
    "url": "https://arxiv.org/abs/2508.13253",
    "authors": [
      "Leander Melroy Maben",
      "Keerthana Prasad",
      "Shyamala Guruvare",
      "Vidya Kudva",
      "P C Siddalingaswamy"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.13559",
    "title": "Physics-Informed Neural Networks for Programmable Origami Metamaterials with Controlled Deployment",
    "abstract": "           Origami-inspired structures provide unprecedented opportunities for creating lightweight, deployable systems with programmable mechanical responses. However, their design remains challenging due to complex nonlinear mechanics, multistability, and the need for precise control of deployment forces. Here, we present a physics-informed neural network (PINN) framework for both forward prediction and inverse design of conical Kresling origami (CKO) without requiring pre-collected training data. By embedding mechanical equilibrium equations directly into the learning process, the model predicts complete energy landscapes with high accuracy while minimizing non-physical artifacts. The inverse design routine specifies both target stable-state heights and separating energy barriers, enabling freeform programming of the entire energy curve. This capability is extended to hierarchical CKO assemblies, where sequential layer-by-layer deployment is achieved through programmed barrier magnitudes. Finite element simulations and experiments on physical prototypes validate the designed deployment sequences and barrier ratios, confirming the robustness of the approach. This work establishes a versatile, data-free route for programming complex mechanical energy landscapes in origami-inspired metamaterials, offering broad potential for deployable aerospace systems, morphing structures, and soft robotic actuators.         ",
    "url": "https://arxiv.org/abs/2508.13559",
    "authors": [
      "Sukheon Kang",
      "Youngkwon Kim",
      "Jinkyu Yang",
      "Seunghwa Ryu"
    ],
    "subjectives": [
      "Soft Condensed Matter (cond-mat.soft)",
      "Artificial Intelligence (cs.AI)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2508.13710",
    "title": "Optimizing Region of Interest Selection for Effective Embedding in Video Steganography Based on Genetic Algorithms",
    "abstract": "           With the widespread use of the internet, there is an increasing need to ensure the security and privacy of transmitted data. This has led to an intensified focus on the study of video steganography, which is a technique that hides data within a video cover to avoid detection. The effectiveness of any steganography method depends on its ability to embed data without altering the original video quality while maintaining high efficiency. This paper proposes a new method to video steganography, which involves utilizing a Genetic Algorithm (GA) for identifying the Region of Interest (ROI) in the cover video. The ROI is the area in the video that is the most suitable for data embedding. The secret data is encrypted using the Advanced Encryption Standard (AES), which is a widely accepted encryption standard, before being embedded into the cover video, utilizing up to 10% of the cover video. This process ensures the security and confidentiality of the embedded data. The performance metrics for assessing the proposed method are the Peak Signal to Noise Ratio (PSNR) and the encoding and decoding time. The results show that the proposed method has a high embedding capacity and efficiency, with a PSNR ranging between 64 and 75 dBs, which indicates that the embedded data is almost indistinguishable from the original video. Additionally, the method can encode and decode data quickly, making it efficient for real time applications.         ",
    "url": "https://arxiv.org/abs/2508.13710",
    "authors": [
      "Nizheen A. Ali",
      "Ramadhan J. Mstafa"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2508.13936",
    "title": "MMIS-Net for Retinal Fluid Segmentation and Detection",
    "abstract": "           Purpose: Deep learning methods have shown promising results in the segmentation, and detection of diseases in medical images. However, most methods are trained and tested on data from a single source, modality, organ, or disease type, overlooking the combined potential of other available annotated data. Numerous small annotated medical image datasets from various modalities, organs, and diseases are publicly available. In this work, we aim to leverage the synergistic potential of these datasets to improve performance on unseen data. Approach: To this end, we propose a novel algorithm called MMIS-Net (MultiModal Medical Image Segmentation Network), which features Similarity Fusion blocks that utilize supervision and pixel-wise similarity knowledge selection for feature map fusion. Additionally, to address inconsistent class definitions and label contradictions, we created a one-hot label space to handle classes absent in one dataset but annotated in another. MMIS-Net was trained on 10 datasets encompassing 19 organs across 2 modalities to build a single model. Results: The algorithm was evaluated on the RETOUCH grand challenge hidden test set, outperforming large foundation models for medical image segmentation and other state-of-the-art algorithms. We achieved the best mean Dice score of 0.83 and an absolute volume difference of 0.035 for the fluids segmentation task, as well as a perfect Area Under the Curve of 1 for the fluid detection task. Conclusion: The quantitative results highlight the effectiveness of our proposed model due to the incorporation of Similarity Fusion blocks into the network's backbone for supervision and similarity knowledge selection, and the use of a one-hot label space to address label class inconsistencies and contradictions.         ",
    "url": "https://arxiv.org/abs/2508.13936",
    "authors": [
      "Nchongmaje Ndipenocha",
      "Alina Mirona",
      "Kezhi Wanga",
      "Yongmin Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.13959",
    "title": "Adversarially robust quantum state learning and testing",
    "abstract": "           Quantum state learning is a fundamental problem in physics and computer science. As near-term quantum devices are error-prone, it is important to design error-resistant algorithms. Apart from device errors, other unexpected factors could also affect the algorithm, such as careless human read-out error, or even a malicious hacker deliberately altering the measurement results. Thus, we want our algorithm to work even in the worst case when things go against our favor. We consider the practical setting of single-copy measurements and propose the $\\gamma$-adversarial corruption model where an imaginary adversary can arbitrarily change $\\gamma$-fraction of the measurement outcomes. This is stronger than the $\\gamma$-bounded SPAM noise model, where the post-measurement state changes by at most $\\gamma$ in trace distance. Under our stronger model of corruption, we design an algorithm using non-adaptive measurements that can learn an unknown rank-$r$ state up to $\\tilde{O}(\\gamma\\sqrt{r})$ in trace distance, provided that the number of copies is sufficiently large. We further prove an information-theoretic lower bound of $\\Omega(\\gamma\\sqrt{r})$ for non-adaptive measurements, demonstrating the optimality of our algorithm. Our upper and lower bounds also hold for quantum state testing, where the goal is to test whether an unknown state is equal to a given state or far from it. Our results are intriguingly optimistic and pessimistic at the same time. For general states, the error is dimension-dependent and $\\gamma\\sqrt{d}$ in the worst case, meaning that only corrupting a very small fraction ($1/\\sqrt{d}$) of the outcomes could totally destroy any non-adaptive learning algorithm. However, for constant-rank states that are useful in many quantum algorithms, it is possible to achieve dimension-independent error, even in the worst-case adversarial setting.         ",
    "url": "https://arxiv.org/abs/2508.13959",
    "authors": [
      "Maryam Aliakbarpour",
      "Vladimir Braverman",
      "Nai-Hui Chia",
      "Yuhan Liu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2508.13967",
    "title": "A PC Algorithm for Max-Linear Bayesian Networks",
    "abstract": "           Max-linear Bayesian networks (MLBNs) are a relatively recent class of structural equation models which arise when the random variables involved have heavy-tailed distributions. Unlike most directed graphical models, MLBNs are typically not faithful to d-separation and thus classical causal discovery algorithms such as the PC algorithm or greedy equivalence search can not be used to accurately recover the true graph structure. In this paper, we begin the study of constraint-based discovery algorithms for MLBNs given an oracle for testing conditional independence in the true, unknown graph. We show that if the oracle is given by the $\\ast$-separation criteria in the true graph, then the PC algorithm remains consistent despite the presence of additional CI statements implied by $\\ast$-separation. We also introduce a new causal discovery algorithm named \"PCstar\" which assumes faithfulness to $C^\\ast$-separation and is able to orient additional edges which cannot be oriented with only d- or $\\ast$-separation.         ",
    "url": "https://arxiv.org/abs/2508.13967",
    "authors": [
      "Carlos Am\u00e9ndola",
      "Benjamin Hollering",
      "Francesco Nowell"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Combinatorics (math.CO)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2010.11563",
    "title": "Advancing Toward Robust and Scalable Fingerprint Orientation Estimation: From Gradients to Deep Learning",
    "abstract": "           The study identifies a clear evolution from traditional methods to more advanced machine learning approaches. Current algorithms face persistent challenges, including degraded image quality, damaged ridge structures, and background noise, which impact performance. To overcome these limitations, future research must focus on developing efficient algorithms with lower computational complexity while maintaining robust performance across varied conditions. Hybrid methods that combine the simplicity and efficiency of gradient-based techniques with the adaptability and robustness of machine learning are particularly promising for advancing fingerprint recognition systems. Fingerprint orientation estimation plays a crucial role in improving the reliability and accuracy of biometric systems. This study highlights the limitations of current approaches and underscores the importance of designing next-generation algorithms that can operate efficiently across diverse application domains. By addressing these challenges, future developments could enhance the scalability, reliability, and applicability of biometric systems, paving the way for broader use in security and identification technologies.         ",
    "url": "https://arxiv.org/abs/2010.11563",
    "authors": [
      "Amit Kumar Trivedi",
      "Jasvinder Pal Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.06925",
    "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines",
    "abstract": "           Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchronous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to 50%; benefiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer based language models. For a GPT-2 model with 1.3 billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by 1.16x-2.34x over the state-of-the-art synchronous and asynchronous pipeline approaches.         ",
    "url": "https://arxiv.org/abs/2107.06925",
    "authors": [
      "Shigang Li",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2205.04981",
    "title": "Characterizing Community Formation in Response to Extreme Weather Events through Human Mobility Networks",
    "abstract": "           Community formation in socio-spatial human networks is one of the important mechanisms for mitigating hazard impacts of extreme weather events. Research is scarce regarding latent network characteristics shaping community formation in human mobility networks during natural disasters. Here, we examined human mobility networks in Harris County, Texas, in the context of the managed power outage forced by 2021 Winter Storm Uri to detect communities and to evaluate latent characteristics in those communities. We examined three characteristics in the communities formed within human mobility networks: hazard-exposure heterophily, socio-demographic homophily, and social-connectedness strength. The results show that population movements were shaped by socio-demographic homophily, heterophilic hazard exposure, and social connectedness strength. Our results also indicate that a community encompassing more high-impact areas would motivate population movements to areas with weaker social connectedness. Our findings reveal important characteristics shaping community formation in human mobility networks in hazard response. Specific to managed power outages, formed communities are spatially co-located, underscoring a best management practice to avoid prolonged power outages among areas within communities, thus improving hazard exposure heterophily. The findings have implications for power utility operators to account for the characteristics of socio-spatial human networks when determining the patterns of managed power outages.         ",
    "url": "https://arxiv.org/abs/2205.04981",
    "authors": [
      "Cheng-Chun Lee",
      "Junwei Ma",
      "Kai Yin",
      "Siri Namburi",
      "Xin Xiao",
      "Ali Mostafavi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2303.15564",
    "title": "Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder",
    "abstract": "           Deep neural networks are vulnerable to backdoor attacks, where an adversary manipulates the model behavior through overlaying images with special triggers. Existing backdoor defense methods often require accessing a few validation data and model parameters, which is impractical in many real-world applications, e.g., when the model is provided as a cloud service. In this paper, we address the practical task of blind backdoor defense at test time, in particular for local attacks and black-box models. The true label of every test image needs to be recovered on the fly from a suspicious model regardless of image benignity. We consider test-time image purification that incapacitates local triggers while keeping semantic contents intact. Due to diverse trigger patterns and sizes, the heuristic trigger search can be unscalable. We circumvent such barrier by leveraging the strong reconstruction power of generative models, and propose Blind Defense with Masked AutoEncoder (BDMAE). BDMAE detects possible local triggers using image structural similarity and label consistency between the test image and MAE restorations. The detection results are then refined by considering trigger topology. Finally, we fuse MAE restorations adaptively into a purified image for making prediction. Extensive experiments under different backdoor settings validate its effectiveness and generalizability.         ",
    "url": "https://arxiv.org/abs/2303.15564",
    "authors": [
      "Tao Sun",
      "Lu Pang",
      "Weimin Lyu",
      "Chao Chen",
      "Haibin Ling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.02625",
    "title": "Diffusion Noise Feature: Accurate and Fast Generated Image Detection",
    "abstract": "           Generative models now produce images with such stunning realism that they can easily deceive the human eye. While this progress unlocks vast creative potential, it also presents significant risks, such as the spread of misinformation. Consequently, detecting generated images has become a critical research challenge. However, current detection methods are often plagued by low accuracy and poor generalization. In this paper, to address these limitations and enhance the detection of generated images, we propose a novel representation, Diffusion Noise Feature (DNF). Derived from the inverse process of diffusion models, DNF effectively amplifies the subtle, high-frequency artifacts that act as fingerprints of artificial generation. Our key insight is that real and generated images exhibit distinct DNF signatures, providing a robust basis for differentiation. By training a simple classifier such as ResNet-50 on DNF, our approach achieves remarkable accuracy, robustness, and generalization in detecting generated images, including those from unseen generators or with novel content. Extensive experiments across four training datasets and five test sets confirm that DNF establishes a new state-of-the-art in generated image detection. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2312.02625",
    "authors": [
      "Yichi Zhang",
      "Xiaogang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.07846",
    "title": "Rethinking Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising",
    "abstract": "           Blind-spot networks (BSN) have been prevalent neural architectures in self-supervised image denoising (SSID). However, most existing BSNs are conducted with convolution layers. Although transformers have shown the potential to overcome the limitations of convolutions in many image restoration tasks, the attention mechanisms may violate the blind-spot requirement, thereby restricting their applicability in BSN. To this end, we propose to analyze and redesign the channel and spatial attentions to meet the blind-spot requirement. Specifically, channel self-attention may leak the blind-spot information in multi-scale architectures, since the downsampling shuffles the spatial feature into channel dimensions. To alleviate this problem, we divide the channel into several groups and perform channel attention separately. For spatial selfattention, we apply an elaborate mask to the attention matrix to restrict and mimic the receptive field of dilated convolution. Based on the redesigned channel and window attentions, we build a Transformer-based Blind-Spot Network (TBSN), which shows strong local fitting and global perspective abilities. Furthermore, we introduce a knowledge distillation strategy that distills TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-theart SSID methods.         ",
    "url": "https://arxiv.org/abs/2404.07846",
    "authors": [
      "Junyi Li",
      "Zhilu Zhang",
      "Wangmeng Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2405.16873",
    "title": "ContrastAlign: Toward Robust BEV Feature Alignment via Contrastive Learning for Multi-Modal 3D Object Detection",
    "abstract": "           In the field of 3D object detection tasks, fusing heterogeneous features from LiDAR and camera sensors into a unified Bird's Eye View (BEV) representation is a widely adopted paradigm. However, existing methods often suffer from imprecise sensor calibration, leading to feature misalignment in LiDAR-camera BEV fusion. Moreover, such inaccuracies cause errors in depth estimation for the camera branch, aggravating misalignment between LiDAR and camera BEV features. In this work, we propose a novel ContrastAlign approach that utilizes contrastive learning to enhance the alignment of heterogeneous modalities, thereby improving the robustness of the fusion process. Specifically, our approach comprises three key components: (1) the L-Instance module, which extracts LiDAR instance features within the LiDAR BEV features; (2) the C-Instance module, which predicts camera instance features through Region of Interest (RoI) pooling on the camera BEV features; (3) the InstanceFusion module, which employs contrastive learning to generate consistent instance features across heFterogeneous modalities. Subsequently, we use graph matching to calculate the similarity between the neighboring camera instance features and the similarity instance features to complete the alignment of instance features. Our method achieves SOTA performance, with an mAP of 71.5%, surpassing GraphBEV by 1.4% on the nuScenes val set. Importantly, our method excels BEVFusion under conditions with spatial & temporal misalignment noise, improving mAP by 1.4% and 11.1% on nuScenes dataset. Notably, on the Argoverse2 dataset, ContrastAlign outperforms GraphBEV by 1.0% in mAP, indicating that the farther the distance, the more severe the feature misalignment and the more effective.         ",
    "url": "https://arxiv.org/abs/2405.16873",
    "authors": [
      "Ziying Song",
      "Hongyu Pan",
      "Feiyang Jia",
      "Yongchang Zhang",
      "Lin Liu",
      "Lei Yang",
      "Shaoqing Xu",
      "Peiliang Wu",
      "Caiyan Jia",
      "Zheng Zhang",
      "Yadan Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.07829",
    "title": "Disentangled Representation Learning with the Gromov-Monge Gap",
    "abstract": "           Learning disentangled representations from unlabelled data is a fundamental challenge in machine learning. Solving it may unlock other problems, such as generalization, interpretability, or fairness. Although remarkably challenging to solve in theory, disentanglement is often achieved in practice through prior matching. Furthermore, recent works have shown that prior matching approaches can be enhanced by leveraging geometrical considerations, e.g., by learning representations that preserve geometric features of the data, such as distances or angles between points. However, matching the prior while preserving geometric features is challenging, as a mapping that fully preserves these features while aligning the data distribution with the prior does not exist in general. To address these challenges, we introduce a novel approach to disentangled representation learning based on quadratic optimal transport. We formulate the problem using Gromov-Monge maps that transport one distribution onto another with minimal distortion of predefined geometric features, preserving them as much as can be achieved. To compute such maps, we propose the Gromov-Monge-Gap (GMG), a regularizer quantifying whether a map moves a reference distribution with minimal geometry distortion. We demonstrate the effectiveness of our approach for disentanglement across four standard benchmarks, outperforming other methods leveraging geometric considerations.         ",
    "url": "https://arxiv.org/abs/2407.07829",
    "authors": [
      "Th\u00e9o Uscidda",
      "Luca Eyring",
      "Karsten Roth",
      "Fabian Theis",
      "Zeynep Akata",
      "Marco Cuturi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.09578",
    "title": "Unsupervised Anomaly Detection Using Diffusion Trend Analysis for Display Inspection",
    "abstract": "           Reconstruction-based anomaly detection via denoising diffusion model has limitations in determining appropriate noise parameters that can degrade anomalies while preserving normal characteristics. Also, normal regions can fluctuate considerably during reconstruction, resulting in false detection. In this paper, we propose a method to detect anomalies by analysis of reconstruction trend depending on the degree of degradation, effectively solving the both problems that impede practical application in display inspection.         ",
    "url": "https://arxiv.org/abs/2407.09578",
    "authors": [
      "Eunwoo Kim",
      "Un Yang",
      "Cheol Lae Roh",
      "Stefano Ermon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06516",
    "title": "Exposing Barriers to Flexibility Aggregation in Unbalanced Distribution Networks",
    "abstract": "           The increasing integration of distributed energy resources (DER) offers new opportunities for distribution system operators (DSO) to improve network operation through flexibility services. To utilise flexible resources, various DER flexibility aggregation methods have been proposed, such as the concept of aggregated P-Q flexibility areas. Yet, many existing studies assume perfect coordination among DER and rely on single-phase power flow analysis, thus overlooking barriers to flexibility aggregation in real unbalanced systems. To quantify the impact of these barriers, this paper proposes a three-phase optimal power flow (OPF) framework for P-Q flexibility assessment, implemented as an open-source Julia tool this http URL. The framework explicitly accounts for voltage unbalance and imperfect coordination among DER in low voltage (LV) distribution networks. Simulations on an illustrative 5-bus system and a real 221-bus LV network in the UK reveal that over 30% of the theoretical aggregated flexibility potential can be lost due to phase unbalance and lack of coordination across phases. These findings highlight the need for improved flexibility aggregation tools applicable to real unbalanced distribution networks.         ",
    "url": "https://arxiv.org/abs/2408.06516",
    "authors": [
      "Andrey Churkin",
      "Wangwei Kong",
      "Pierluigi Mancarella",
      "Eduardo A. Mart\u00ednez Cese\u00f1a"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.16568",
    "title": "AxLSTMs: learning self-supervised audio representations with xLSTMs",
    "abstract": "           While the transformer has emerged as the eminent neural architecture, several independent lines of research have emerged to address its limitations. Recurrent neural approaches have observed a lot of renewed interest, including the extended long short-term memory (xLSTM) architecture, which reinvigorates the original LSTM. However, while xLSTMs have shown competitive performance compared to the transformer, their viability for learning self-supervised general-purpose audio representations has not been evaluated. This work proposes Audio xLSTM (AxLSTM), an approach for learning audio representations from masked spectrogram patches in a self-supervised setting. Pretrained on the AudioSet dataset, the proposed AxLSTM models outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by up to 25% in relative performance across a set of ten diverse downstream tasks while having up to 45% fewer parameters.         ",
    "url": "https://arxiv.org/abs/2408.16568",
    "authors": [
      "Sarthak Yadav",
      "Sergios Theodoridis",
      "Zheng-Hua Tan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2410.03877",
    "title": "FDR-SVM: A Federated Distributionally Robust Support Vector Machine via a Mixture of Wasserstein Balls Ambiguity Set",
    "abstract": "           We study a federated classification problem over a network of multiple clients and a central server, in which each client's local data remains private and is subject to uncertainty in both the features and labels. To address these uncertainties, we develop a novel Federated Distributionally Robust Support Vector Machine (FDR-SVM), robustifying the classification boundary against perturbations in local data distributions. Specifically, the data at each client is governed by a unique true distribution that is unknown. To handle this heterogeneity, we develop a novel Mixture of Wasserstein Balls (MoWB) ambiguity set, naturally extending the classical Wasserstein ball to the federated setting. We then establish theoretical guarantees for our proposed MoWB, deriving an out-of-sample performance bound and showing that its design preserves the separability of the FDR-SVM optimization problem. Next, we rigorously derive two algorithms that solve the FDR-SVM problem and analyze their convergence behavior as well as their worst-case time complexity. We evaluate our algorithms on industrial data and various UCI datasets, whereby we demonstrate that they frequently outperform existing state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2410.03877",
    "authors": [
      "Michael Ibrahim",
      "Heraldo Rozas",
      "Nagi Gebraeel",
      "Weijun Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.18424",
    "title": "A Causal Graph-Enhanced Gaussian Process Regression for Modeling Engine-out NOx",
    "abstract": "           The stringent regulatory requirements on nitrogen oxides (NOx) emissions from diesel compression ignition engines require accurate and reliable models for real-time monitoring and diagnostics. Although traditional methods such as physical sensors and virtual engine control module (ECM) sensors provide essential data, they are only used for estimation. Ubiquitous literature primarily focuses on deterministic models with little emphasis on capturing the various uncertainties. The lack of probabilistic frameworks restricts the applicability of these models for robust diagnostics. The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOx emissions using Gaussian process regression. Our approach is as follows. We employ three variants of Gaussian process models: the first with a standard radial basis function kernel with input window, the second incorporating a deep kernel using convolutional neural networks to capture temporal dependencies, and the third enriching the deep kernel with a causal graph derived via graph convolutional networks. The causal graph embeds physics knowledge into the learning process. All models are compared against a virtual ECM sensor using both quantitative and qualitative metrics. We conclude that our model provides an improvement in predictive performance when using an input window and a deep kernel structure. Even more compelling is the further enhancement achieved by the incorporation of a causal graph into the deep kernel. These findings are corroborated across different verification and validation datasets.         ",
    "url": "https://arxiv.org/abs/2410.18424",
    "authors": [
      "Shrenik Zinage",
      "Ilias Bilionis",
      "Peter Meckl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.09965",
    "title": "Exploiting structural observability and graph colorability for optimal sensor placement in water distribution networks",
    "abstract": "           Water distribution networks (WDNs) are critical systems for our society and detecting leakages is important for minimizing losses and water waste. This makes optimal sensor placement for leakage detection very relevant. Existing sensor placement methods rely on simulation-based scenarios, often lacking structure and generalizability, or depend on the knowledge of specific parameters of the WDN as well as on initial sensor data for linearization and demand estimation. Motivated by this, this paper investigates the observability of an entire WDN, based on structural observability theory. This allows us to establish the conditions for the observability of the WDN model, independently of parameter uncertainties. Additionally, a sensor placement algorithm is proposed that leverages such observability conditions and graph theory and accounts for the industrial and material costs. To demonstrate the effectiveness of our approach, we apply it to a hydraulic-transient WDN model.         ",
    "url": "https://arxiv.org/abs/2412.09965",
    "authors": [
      "J.J.H. van Gemert",
      "V. Breschi",
      "D.R. Yntema",
      "K.J. Keesman",
      "M. Lazar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2412.10436",
    "title": "Benchmarking Federated Learning for Semantic Datasets: Federated Scene Graph Generation",
    "abstract": "           Federated learning (FL) enables decentralized training while preserving data privacy, yet existing FL benchmarks address relatively simple classification tasks, where each sample is annotated with a one-hot label. However, little attention has been paid to demonstrating an FL benchmark that handles complicated semantics, where each sample encompasses diverse semantic information, such as relations between objects. Because the existing benchmarks are designed to distribute data in a narrow view of a single semantic, managing the complicated semantic heterogeneity across clients when formalizing FL benchmarks is non-trivial. In this paper, we propose a benchmark process to establish an FL benchmark with controllable semantic heterogeneity across clients: two key steps are (i) data clustering with semantics and (ii) data distributing via controllable semantic heterogeneity across clients. As a proof of concept, we construct a federated PSG benchmark, demonstrating the efficacy of the existing PSG methods in an FL setting with controllable semantic heterogeneity of scene graphs. We also present the effectiveness of our benchmark by applying robust federated learning algorithms to data heterogeneity to show increased performance. To our knowledge, this is the first benchmark framework that enables federated learning and its evaluation for multi-semantic vision tasks under the controlled semantic heterogeneity. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.10436",
    "authors": [
      "SeungBum Ha",
      "Taehwan Lee",
      "Jiyoun Lim",
      "Sung Whan Yoon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.20439",
    "title": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation",
    "abstract": "           Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets.         ",
    "url": "https://arxiv.org/abs/2412.20439",
    "authors": [
      "Wangyu Wu",
      "Xianglin Qiu",
      "Siqi Song",
      "Zhenhong Chen",
      "Xiaowei Huang",
      "Fei Ma",
      "Jimin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.00838",
    "title": "Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation",
    "abstract": "           Current optical flow methods exploit the stable appearance of frame (or RGB) data to establish robust correspondences across time. Event cameras, on the other hand, provide high-temporal-resolution motion cues and excel in challenging scenarios. These complementary characteristics underscore the potential of integrating frame and event data for optical flow estimation. However, most cross-modal approaches fail to fully utilize the complementary advantages, relying instead on simply stacking information. This study introduces a novel approach that uses a spatially dense modality to guide the aggregation of the temporally dense event modality, achieving effective cross-modal fusion. Specifically, we propose an event-enhanced frame representation that preserves the rich texture of frames and the basic structure of events. We use the enhanced representation as the guiding modality and employ events to capture temporally dense motion information. The robust motion features derived from the guiding modality direct the aggregation of motion information from events. To further enhance fusion, we propose a transformer-based module that complements sparse event motion features with spatially rich frame information and enhances global information propagation. Additionally, a mix-fusion encoder is designed to extract comprehensive spatiotemporal contextual features from both modalities. Extensive experiments on the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our framework. Leveraging the complementary strengths of frames and events, our method achieves leading performance on the DSEC-Flow dataset. Compared to the event-only model, frame guidance improves accuracy by 10\\%. Furthermore, it outperforms the state-of-the-art fusion-based method with a 4\\% accuracy gain and a 45\\% reduction in inference time.         ",
    "url": "https://arxiv.org/abs/2501.00838",
    "authors": [
      "Qianang Zhou",
      "Junhui Hou",
      "Meiyi Yang",
      "Yongjian Deng",
      "Youfu Li",
      "Junlin Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.01239",
    "title": "High-Order Tensor Regression in Sparse Convolutional Neural Networks",
    "abstract": "           This article presents a generic approach to convolution that significantly differs from conventional methodologies in the current Machine Learning literature. The approach, in its mathematical aspects, proved to be clear and concise, particularly when high-order tensors are involved. In this context, a rational theory of regression in neural networks is developed, as a framework for a generic view of sparse convolutional neural networks, the primary focus of this study. As a direct outcome, the classic Backpropagation Algorithm is redefined to align with this rational tensor-based approach and presented in its simplest, most generic form.         ",
    "url": "https://arxiv.org/abs/2501.01239",
    "authors": [
      "Roberto Dias Algarte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.03666",
    "title": "Hybrid Machine Learning Model with a Constrained Action Space for Trajectory Prediction",
    "abstract": "           Trajectory prediction is crucial to advance autonomous driving, improving safety, and efficiency. Although end-to-end models based on deep learning have great potential, they often do not consider vehicle dynamic limitations, leading to unrealistic predictions. To address this problem, this work introduces a novel hybrid model that combines deep learning with a kinematic motion model. It is able to predict object attributes such as acceleration and yaw rate and generate trajectories based on them. A key contribution is the incorporation of expert knowledge into the learning objective of the deep learning model. This results in the constraint of the available action space, thus enabling the prediction of physically feasible object attributes and trajectories, thereby increasing safety and robustness. The proposed hybrid model facilitates enhanced interpretability, thereby reinforcing the trustworthiness of deep learning methods and promoting the development of safe planning solutions. Experiments conducted on the publicly available real-world Argoverse dataset demonstrate realistic driving behaviour, with benchmark comparisons and ablation studies showing promising results.         ",
    "url": "https://arxiv.org/abs/2501.03666",
    "authors": [
      "Alexander Fertig",
      "Lakshman Balasubramanian",
      "Michael Botsch"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.06561",
    "title": "Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for Mid-term Human Mobility Prediction",
    "abstract": "           Predicting individual mobility patterns is crucial across various applications. While current methods mainly focus on predicting the next location for personalized services like recommendations, they often fall short in supporting broader applications such as traffic management and epidemic control, which require longer period forecasts of human mobility. This study addresses mid-term mobility prediction, aiming to capture daily travel patterns and forecast trajectories for the upcoming day or week. We propose a novel Multi-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to efficiently extract spatial and temporal information by decoupling daily trajectories into distinct location-duration chains. Our approach employs a hierarchical encoder to model multi-scale temporal patterns, including daily recurrence and weekly periodicity, and utilizes a transformer-based decoder to globally attend to predicted information in the location or duration chain. Additionally, we introduce a spatial heterogeneous graph learner to capture multi-scale spatial relationships, enhancing semantic-rich representations. Extensive experiments, including statistical physics analysis, are conducted on large-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay Area, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to epidemic modeling in Boston, MSTDP significantly outperforms the best-performing baseline, achieving a remarkable 62.8% reduction in MAE for cumulative new cases.         ",
    "url": "https://arxiv.org/abs/2501.06561",
    "authors": [
      "Zongyuan Huang",
      "Weipeng Wang",
      "Shaoyu Huang",
      "Marta C. Gonzalez",
      "Yaohui Jin",
      "Yanyan Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.08306",
    "title": "Environmental Feature Engineering and Statistical Validation for ML-Based Path Loss Prediction",
    "abstract": "           Wireless communications rely on path loss modeling, which is most effective when it includes the physical details of the propagation environment. Acquiring this data has historically been challenging, but geographic information systems data is becoming increasingly available with higher resolution and accuracy. Access to such details enables propagation models to more accurately predict coverage and account for interference in wireless deployments. Machine learning-based modeling can significantly support this effort, with feature based approaches allowing for accurate, efficient, and scalable propagation modeling. Building on previous work, we introduce an extended set of features that improves prediction accuracy while, most importantly, proving model generalization through rigorous statistical assessment and the use of test set holdouts.         ",
    "url": "https://arxiv.org/abs/2501.08306",
    "authors": [
      "Jonathan Ethier",
      "Mathieu Chateauvert",
      "Ryan G. Dempsey",
      "Alexis Bose"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.02386",
    "title": "Edge Correlations and Link Prediction in Growing Hypergraphs",
    "abstract": "           We propose a generative model of temporally-evolving hypergraphs in which hyperedges form via noisy copying of previous hyperedges. Our proposed model reproduces several stylized facts from many empirical hypergraphs, is learnable from data, and defines a likelihood over a complete hypergraph rather than ego-based or other sub-hypergraphs. Analyzing our model, we derive descriptions of node degree, edge size, and edge intersection size distributions in terms of the model parameters. We also show several features of empirical hypergraphs which are and are not successfully captured by our model. We provide a scalable stochastic expectation maximization algorithm with which we can fit our model to hypergraph data sets with millions of nodes and edges. Finally, we assess our model on a hypergraph link prediction task, finding that an instantiation of our model with just 11 parameters can achieve competitive predictive performance with large neural networks.         ",
    "url": "https://arxiv.org/abs/2502.02386",
    "authors": [
      "Xie He",
      "Philip S. Chodrow",
      "Peter J. Mucha"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2502.11664",
    "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
    "abstract": "           Rotary Position Embedding (RoPE) has shown strong performance in text-based Large Language Models (LLMs), but extending it to video remains a challenge due to the intricate spatiotemporal structure of video frames. Existing adaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions separately but suffer from two major limitations: positional bias in attention distribution and disruptions in video-text transitions. To overcome these issues, we propose Video Rotary Position Embedding (VRoPE), a novel positional encoding method tailored for Video-LLMs. Specifically, we introduce a more balanced encoding strategy that mitigates attention biases, ensuring a more uniform distribution of spatial focus. Additionally, our approach restructures positional indices to ensure a smooth transition between video and text tokens. Extensive experiments on different models demonstrate that VRoPE consistently outperforms previous RoPE variants, achieving significant improvements in video understanding, temporal reasoning, and retrieval tasks. Code will be available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.11664",
    "authors": [
      "Zikang Liu",
      "Longteng Guo",
      "Yepeng Tang",
      "Tongtian Yue",
      "Junxian Cai",
      "Kai Ma",
      "Qingbin Liu",
      "Xi Chen",
      "Jing Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.08549",
    "title": "GoAI: Enhancing AI Students' Learning Paths and Idea Generation via Graph of AI Ideas",
    "abstract": "           With the rapid advancement of artificial intelligence technology, AI students are confronted with a significant \"information-to-innovation\" gap: they must navigate through the rapidly expanding body of literature, trace the development of a specific research field, and synthesize various techniques into feasible innovative concepts. An additional critical step for students is to identify the necessary prerequisite knowledge and learning paths. Although many approaches based on large language models (LLMs) can summarize the content of papers and trace the development of a field through citations, these methods often overlook the prerequisite knowledge involved in the papers and the rich semantic information embedded in the citation relationships between papers. Such information reveals how methods are interrelated, built upon, extended, or challenged. To address these limitations, we propose GoAI, a tool for constructing educational knowledge graphs from AI research papers that leverages these graphs to plan personalized learning paths and support creative ideation. The nodes in the knowledge graph we have built include papers and the prerequisite knowledge, such as concepts, skills, and tools, that they involve; the edges record the semantic information of citations. When a student queries a specific paper, a beam search-based path search method can trace the current development trends of the field from the queried paper and plan a learning path toward cutting-edge objectives. The integrated Idea Studio guides students to clarify problem statements, compare alternative designs, and provide formative feedback on novelty, clarity, feasibility, and alignment with learning objectives.         ",
    "url": "https://arxiv.org/abs/2503.08549",
    "authors": [
      "Xian Gao",
      "Zongyun Zhang",
      "Ting Liu",
      "Yuzhuo Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.10428",
    "title": "Langevin Monte-Carlo Provably Learns Depth Two Neural Nets at Any Size and Data",
    "abstract": "           In this work, we will establish that the Langevin Monte-Carlo algorithm can learn depth-2 neural nets of any size and for any data and we give non-asymptotic convergence rates for it. We achieve this via showing that in q-Renyi divergence, the iterates of Langevin Monte Carlo converge to the Gibbs distribution of Frobenius norm regularized losses for any of these nets, when using smooth activations and in both classification and regression settings. Most critically, the amount of regularization needed for our results is independent of the size of the net. This result achieves a synthesis of several recent observations about isoperimetry conditions under which LMC converges and that two-layer neural loss functions can always be regularized by a certain constant amount such that they satisfy the Villani conditions, and thus their Gibbs measures satisfy a Poincare inequality.         ",
    "url": "https://arxiv.org/abs/2503.10428",
    "authors": [
      "Dibyakanti Kumar",
      "Samyak Jha",
      "Anirbit Mukherjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2503.12339",
    "title": "Augmented Adversarial Trigger Learning",
    "abstract": "           Gradient optimization-based adversarial attack methods automate the learning of adversarial triggers to generate jailbreak prompts or leak system prompts. In this work, we take a closer look at the optimization objective of adversarial trigger learning and propose ATLA: Adversarial Trigger Learning with Augmented objectives. ATLA improves the negative log-likelihood loss used by previous studies into a weighted loss formulation that encourages the learned adversarial triggers to optimize more towards response format tokens. This enables ATLA to learn an adversarial trigger from just one query-response pair and the learned trigger generalizes well to other similar queries. We further design a variation to augment trigger optimization with an auxiliary loss that suppresses evasive responses. We showcase how to use ATLA to learn adversarial suffixes jailbreaking LLMs and to extract hidden system prompts. Empirically we demonstrate that ATLA consistently outperforms current state-of-the-art techniques, achieving nearly 100% success in attacking while requiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high generalization to unseen queries and transfer well to new LLMs. We released our code this https URL ",
    "url": "https://arxiv.org/abs/2503.12339",
    "authors": [
      "Zhe Wang",
      "Yanjun Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.13911",
    "title": "Incorporating Attributes and Multi-Scale Structures for Heterogeneous Graph Contrastive Learning",
    "abstract": "           Heterogeneous graphs (HGs) are composed of multiple types of nodes and edges, making it more effective in capturing the complex relational structures inherent in the real world. However, in real-world scenarios, labeled data is often difficult to obtain, which limits the applicability of semi-supervised approaches. Self-supervised learning aims to enable models to automatically learn useful features from data, effectively addressing the challenge of limited labeling data. In this paper, we propose a novel contrastive learning framework for heterogeneous graphs (ASHGCL), which incorporates three distinct views, each focusing on node attributes, high-order and low-order structural information, respectively, to effectively capture attribute information, high-order structures, and low-order structures for node representation learning. Furthermore, we introduce an attribute-enhanced positive sample selection strategy that combines both structural information and attribute information, effectively addressing the issue of sampling bias. Extensive experiments on four real-world datasets show that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks.         ",
    "url": "https://arxiv.org/abs/2503.13911",
    "authors": [
      "Ruobing Jiang",
      "Yacong Li",
      "Haobing Liu",
      "Yanwei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.00735",
    "title": "Reinforcement learning for robust dynamic metabolic control",
    "abstract": "           Dynamic metabolic control allows key metabolic fluxes to be modulated in real time, enhancing bioprocess flexibility and expanding available optimization degrees of freedom. This is achieved, e.g., via targeted modulation of metabolic enzyme expression. However, identifying optimal dynamic control policies is challenging due to the generally high-dimensional solution space and the need to manage metabolic burden and cytotoxic effects arising from inducible enzyme expression. The task is further complicated by stochastic dynamics, which reduce bioprocess reproducibility. We propose a reinforcement learning framework} to derive optimal policies by allowing an agent (the controller) to interact with a surrogate dynamic model. To promote robustness, we apply domain randomization, enabling the controller to generalize across uncertainties. When transferred to an experimental system, the agent can in principle continue fine-tuning the policy. Our framework provides an alternative to conventional model-based control such as model predictive control, which requires model differentiation with respect to decision variables; often impractical for complex stochastic, nonlinear, stiff, and piecewise-defined dynamics. In contrast, our approach relies on forward integration of the model, thereby simplifying the task. We demonstrate the framework in two $\\textit{Escherichia coli}$ bioprocesses: dynamic control of acetyl-CoA carboxylase for fatty-acid synthesis and of adenosine triphosphatase for lactate synthesis.         ",
    "url": "https://arxiv.org/abs/2504.00735",
    "authors": [
      "Sebasti\u00e1n Espinel-R\u00edos",
      "River Walser",
      "Dongda Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05747",
    "title": "SEA-LION: Southeast Asian Languages in One Network",
    "abstract": "           Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.         ",
    "url": "https://arxiv.org/abs/2504.05747",
    "authors": [
      "Raymond Ng",
      "Thanh Ngan Nguyen",
      "Yuli Huang",
      "Ngee Chia Tai",
      "Wai Yi Leong",
      "Wei Qi Leong",
      "Xianbin Yong",
      "Jian Gang Ngui",
      "Yosephine Susanto",
      "Nicholas Cheng",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Adithya Venkatadri Hulagadri",
      "Kok Wai Teng",
      "Yeo Yeow Tong",
      "Bryan Siow",
      "Wei Yi Teo",
      "Wayne Lau",
      "Choon Meng Tan",
      "Brandon Ong",
      "Zhi Hao Ong",
      "Jann Railey Montalan",
      "Adwin Chan",
      "Sajeban Antonyrex",
      "Ren Lee",
      "Esther Choa",
      "David Ong Tat-Wee",
      "Bing Jie Darius Liu",
      "William Chandra Tjhi",
      "Erik Cambria",
      "Leslie Teo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07058",
    "title": "Residual weighted physics informed neural network for forward and inverse problems of low grade brain tumors",
    "abstract": "           A low-grade tumor is a slow-growing tumor with a lower likelihood of spreading compared to high-grade tumors. Mathematical modeling using partial differential equations (PDEs) plays a crucial role in describing tumor behavior, growth, and progression. This study employs the Burgess and extended Fisher Kolmogorov equations to investigate the dynamics of low-grade brain tumors. We introduce a new method, termed the Residual Weighted Physics-Informed Neural Network (RW PINN), designed to enhance the accuracy of PINN-based algorithms. The proposed approach assigns a weight proportional to the PDE residual, scales it accordingly, and incorporates the weighted residual into the loss function to achieve more efficient and stable convergence. Using this novel RW PINN algorithm, we develop an automated numerical solver for these models and explore their application in solving forward and inverse problems in brain tumor dynamics. The study demonstrates that RW PINN based algorithms provide advanced methodologies for modeling brain tumor dynamics by integrating deep learning with physics-informed principles. Additionally, we establish generalized error bounds in terms of training and quadrature errors, and derive the convergence and stability of the neural network for both models. Numerical experiments confirm the accuracy and efficiency of the algorithms in nonlinear cases, and a statistical analysis of the results is presented. To further validate the effectiveness of our methodology, we also implemented PINN based forward and inverse frameworks for low-grade brain tumors and performed a comparative analysis with the proposed approach.         ",
    "url": "https://arxiv.org/abs/2504.07058",
    "authors": [
      "K. Murari",
      "P. Roul",
      "S. Sundar"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.10486",
    "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar Relighting",
    "abstract": "           Creating relightable and animatable human avatars from monocular videos is a rising research topic with a range of applications, e.g. virtual reality, sports, and video games. Previous works utilize neural fields together with physically based rendering (PBR), to estimate geometry and disentangle appearance properties of human avatars. However, one drawback of these methods is the slow rendering speed due to the expensive Monte Carlo ray tracing. To tackle this problem, we proposed to distill the knowledge from implicit neural fields (teacher) to explicit 2D Gaussian splatting (student) representation to take advantage of the fast rasterization property of Gaussian splatting. To avoid ray-tracing, we employ the split-sum approximation for PBR appearance. We also propose novel part-wise ambient occlusion probes for shadow computation. Shadow prediction is achieved by querying these probes only once per pixel, which paves the way for real-time relighting of avatars. These techniques combined give high-quality relighting results with realistic shadow effects. Our experiments demonstrate that the proposed student model achieves comparable or even better relighting results with our teacher model while being 370 times faster at inference time, achieving a 67 FPS rendering speed.         ",
    "url": "https://arxiv.org/abs/2504.10486",
    "authors": [
      "Zeren Jiang",
      "Shaofei Wang",
      "Siyu Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.12332",
    "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
    "abstract": "           Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.12332",
    "authors": [
      "Qianyue Hu",
      "Junyan Wu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.17972",
    "title": "MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings",
    "abstract": "           Feature engineering for generalized seizure detection models remains a significant challenge. Recently proposed models show variable performance depending on the training data and remain ineffective at accurately distinguishing artifacts from seizure data. In this study, we propose a novel end-to-end model, \"Multiresolutional EEGWaveNet (MR-EEGWaveNet),\" which efficiently distinguishes seizure events from background electroencephalogram (EEG) and artifacts/noise by capturing both temporal dependencies across different time frames and spatial relationships between channels. The model has three modules: convolution, feature extraction, and predictor. The convolution module extracts features through depth-wise and spatio-temporal convolution. The feature extraction module individually reduces the feature dimension extracted from EEG segments and their sub-segments. Subsequently, the extracted features are concatenated into a single vector for classification using a fully connected classifier called the predictor module. In addition, an anomaly score-based post-classification processing technique is introduced to reduce the false-positive rates of the model. Experimental results are reported and analyzed using different parameter settings and datasets (Siena (public) and Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the conventional non-multiresolution approach, improving the F1 scores from 0.177 to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9% and 20.62%, respectively.         ",
    "url": "https://arxiv.org/abs/2505.17972",
    "authors": [
      "Kazi Mahmudul Hassan",
      "Xuyang Zhao",
      "Hidenori Sugano",
      "Toshihisa Tanaka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.18499",
    "title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning",
    "abstract": "           Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erd\u00f5s, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erd\u00f5s, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully. Our implementation is open-sourced at this https URL, with models and datasets hosted on Hugging Face collections this https URL for broader accessibility.         ",
    "url": "https://arxiv.org/abs/2505.18499",
    "authors": [
      "Xiaojun Guo",
      "Ang Li",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Yisen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.18637",
    "title": "Neural Coding Is Not Always Semantic: Toward the Standardized Coding Workflow in Semantic Communications",
    "abstract": "           Semantic communication, leveraging advanced deep learning techniques, emerges as a new paradigm that meets the requirements of next-generation wireless networks. However, current semantic communication systems, which employ neural coding for feature extraction from raw data, have not adequately addressed the fundamental question: Is general feature extraction through deep neural networks sufficient for understanding semantic meaning within raw data in semantic communication? This article is thus motivated to clarify two critical aspects: semantic understanding and general semantic representation. This article presents a standardized definition on semantic coding, an extensive neural coding scheme for general semantic representation that clearly represents underlying data semantics based on contextual modeling. With these general semantic representations obtained, both human- and machine-centric end-to-end data transmission can be achieved through only minimal specialized modifications, such as fine-tuning and regularization. This article contributes to establishing a commonsense that semantic communication extends far beyond mere feature transmission, focusing instead on conveying compact semantic representations through context-aware coding schemes.         ",
    "url": "https://arxiv.org/abs/2505.18637",
    "authors": [
      "Hai-Long Qin",
      "Jincheng Dai",
      "Sixian Wang",
      "Xiaoqi Qin",
      "Shuo Shao",
      "Kai Niu",
      "Wenjun Xu",
      "Ping Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.24688",
    "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration",
    "abstract": "           Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution. The code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.24688",
    "authors": [
      "Qinglin Zhu",
      "Runcong Zhao",
      "Hanqi Yan",
      "Yulan He",
      "Yudong Chen",
      "Lin Gui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.10459",
    "title": "Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Weighted Intermediate Feature Divergence",
    "abstract": "           Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose security challenges to hyperspectral image (HSI) classification based on DNNs. Numerous adversarial attack methods have been designed in the domain of natural images. However, different from natural images, HSIs contains high-dimensional rich spectral information, which presents new challenges for generating adversarial examples. Based on the specific characteristics of HSIs, this paper proposes a novel method to enhance the transferability of the adversarial examples for HSI classification using 3D structure-invariant transformation and weighted intermediate feature divergence. While keeping the HSIs structure invariant, the proposed method divides the image into blocks in both spatial and spectral dimensions. Then, various transformations are applied on each block to increase input diversity and mitigate the overfitting to substitute models. Moreover, a weighted intermediate feature divergence loss is also designed by leveraging the differences between the intermediate features of original and adversarial examples. It constrains the perturbation direction by enlarging the feature maps of the original examples, and assigns different weights to different feature channels to destroy the features that have a greater impact on HSI classification. Extensive experiments demonstrate that the adversarial examples generated by the proposed method achieve more effective adversarial transferability on three public HSI datasets. Furthermore, the method maintains robust attack performance even under defense strategies.         ",
    "url": "https://arxiv.org/abs/2506.10459",
    "authors": [
      "Chun Liu",
      "Bingqian Zhu",
      "Tao Xu",
      "Zheng Zheng",
      "Zheng Li",
      "Wei Yang",
      "Zhigang Han",
      "Jiayao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2506.12352",
    "title": "Efficient Network Automatic Relevance Determination",
    "abstract": "           We propose Network Automatic Relevance Determination (NARD), an extension of ARD for linearly probabilistic models, to simultaneously model sparse relationships between inputs $X \\in \\mathbb R^{d \\times N}$ and outputs $Y \\in \\mathbb R^{m \\times N}$, while capturing the correlation structure among the $Y$. NARD employs a matrix normal prior which contains a sparsity-inducing parameter to identify and discard irrelevant features, thereby promoting sparsity in the model. Algorithmically, it iteratively updates both the precision matrix and the relationship between $Y$ and the refined inputs. To mitigate the computational inefficiencies of the $\\mathcal O(m^3 + d^3)$ cost per iteration, we introduce Sequential NARD, which evaluates features sequentially, and a Surrogate Function Method, leveraging an efficient approximation of the marginal likelihood and simplifying the calculation of determinant and inverse of an intermediate matrix. Combining the Sequential update with the Surrogate Function method further reduces computational costs. The computational complexity per iteration for these three methods is reduced to $\\mathcal O(m^3+p^3)$, $\\mathcal O(m^3 + d^2)$, $\\mathcal O(m^3+p^2)$, respectively, where $p \\ll d$ is the final number of features in the model. Our methods demonstrate significant improvements in computational efficiency with comparable performance on both synthetic and real-world datasets.         ",
    "url": "https://arxiv.org/abs/2506.12352",
    "authors": [
      "Hongwei Zhang",
      "Ziqi Ye",
      "Xinyuan Wang",
      "Xin Guo",
      "Zenglin Xu",
      "Yuan Cheng",
      "Zixin Hu",
      "Yuan Qi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.15746",
    "title": "Neural Cellular Automata for ARC-AGI",
    "abstract": "           Cellular automata and their differentiable counterparts, Neural Cellular Automata (NCA), are highly expressive and capable of surprisingly complex behaviors. This paper explores how NCAs perform when applied to tasks requiring precise transformations and few-shot generalization, using the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that challenges their capabilities in ways not previously explored. Specifically, this paper uses gradient-based training to learn iterative update rules that transform input grids into their outputs from the training examples and apply them to the test inputs. Results suggest that gradient-trained NCA models are a promising and efficient approach to a range of abstract grid-based tasks from ARC. Along with discussing the impacts of various design modifications and training constraints, this work examines the behavior and properties of NCAs applied to ARC to give insights for broader applications of self-organizing systems.         ",
    "url": "https://arxiv.org/abs/2506.15746",
    "authors": [
      "Kevin Xu",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.18678",
    "title": "MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation",
    "abstract": "           Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on this https URL.         ",
    "url": "https://arxiv.org/abs/2506.18678",
    "authors": [
      "Tianchen Deng",
      "Guole Shen",
      "Xun Chen",
      "Shenghai Yuan",
      "Hongming Shen",
      "Guohao Peng",
      "Zhenyu Wu",
      "Jingchuan Wang",
      "Lihua Xie",
      "Danwei Wang",
      "Hesheng Wang",
      "Weidong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.22807",
    "title": "FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition",
    "abstract": "           Electroencephalography (EEG) serves as a reliable and objective signal for emotion recognition in affective brain-computer interfaces, offering unique advantages through its high temporal resolution and ability to capture authentic emotional states that cannot be consciously controlled. However, cross-subject generalization remains a fundamental challenge due to individual variability, cognitive traits, and emotional responses. We propose FreqDGT, a frequency-adaptive dynamic graph transformer that systematically addresses these limitations through an integrated framework. FreqDGT introduces frequency-adaptive processing (FAP) to dynamically weight emotion-relevant frequency bands based on neuroscientific evidence, employs adaptive dynamic graph learning (ADGL) to learn input-specific brain connectivity patterns, and implements multi-scale temporal disentanglement network (MTDN) that combines hierarchical temporal transformers with adversarial feature disentanglement to capture both temporal dynamics and ensure cross-subject robustness. Comprehensive experiments demonstrate that FreqDGT significantly improves cross-subject emotion recognition accuracy, confirming the effectiveness of integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical modeling while ensuring robustness to individual differences. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.22807",
    "authors": [
      "Yueyang Li",
      "Shengyu Gong",
      "Weiming Zeng",
      "Nizhuan Wang",
      "Wai Ting Siok"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.00235",
    "title": "Minimum Selective Subset on Some Graph Classes",
    "abstract": "           In a connected simple graph G = (V(G),E(G)), each vertex is assigned a color from the set of colors C={1, 2,..., c}. The set of vertices V(G) is partitioned as V_1, V_2, ... ,V_c, where all vertices in V_j share the same color j. A subset S of V(G) is called Selective Subset if, for every vertex v in V(G), and if v is in V_j, at least one of its nearest neighbors in (S union (V(G)\\ V_j)) has the same color as v. The Minimum Selective Subset (MSS) problem seeks to find a selective subset of minimum size. The problem was first introduced by Wilfong in 1991 for a set of points in the Euclidean plane, where two major problems, MCS (Minimum Consistent Subset) and MSS, were proposed. In graph algorithms, the only known result is that the MSS problem is NP-complete, as shown in 2018. Beyond this, no further progress has been made to date. In contrast, the MCS problem has been widely studied in various graph classes over the years. Therefore, in this work, we also extend the algorithmic study of MSS on various graph classes. We first present a log(n)-approximation algorithm for general graphs with n vertices and regardless of the number of colors. We also show that the problem remains NP-complete in planar graphs when restricted to just two colors.. Finally, we provide polynomial-time algorithms for computing optimal solutions in trees and unit interval graphs for any number of colors.         ",
    "url": "https://arxiv.org/abs/2507.00235",
    "authors": [
      "Bubai Manna"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2507.08348",
    "title": "Content-Oblivious Leader Election in 2-Edge-Connected Networks",
    "abstract": "           Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 & Distributed Computing 2023) studied fully-defective asynchronous networks, where communication channels may suffer an extreme form of alteration errors, rendering messages completely corrupted. The model is equivalent to content-oblivious computation, where nodes communicate solely via pulses. They showed that if the network is 2-edge-connected, then any algorithm for a noiseless setting can be simulated in the fully-defective setting; otherwise, no non-trivial computation is possible in the fully-defective setting. However, their simulation requires a predesignated leader, which they conjectured to be necessary for any non-trivial content-oblivious task. In this work, we present two results: General 2-edge-connected topologies: First, we show an asynchronous content-oblivious leader election algorithm that quiescently terminates in any 2-edge-connected network with message complexity $O(m \\cdot N \\cdot \\mathsf{ID}_{\\min})$, where $m$ is the number of edges, $N$ is a known upper bound on the number of nodes, and $\\mathsf{ID}_{\\min}$ is the smallest $\\mathsf{ID}$. Combined with the above simulation, this result shows that whenever a size bound $N$ is known, any noiseless algorithm can be simulated in the fully-defective model without a preselected leader, fully refuting the conjecture. Unoriented rings: We then show that the knowledge of $N$ can be dropped in unoriented ring topologies by presenting a quiescently terminating election algorithm with message complexity $O(n \\cdot \\mathsf{ID}_{\\max})$ that matches the previous bound. Consequently, this result constitutes a strict improvement over the previous leader election in oriented rings by Frei, Gelles, Ghazy, and Nolin (DISC 2024) and shows that, on rings, fully-defective and noiseless communication are computationally equivalent, with no additional assumptions.         ",
    "url": "https://arxiv.org/abs/2507.08348",
    "authors": [
      "J\u00e9r\u00e9mie Chalopin",
      "Yi-Jun Chang",
      "Lyuting Chen",
      "Giuseppe A. Di Luna",
      "Haoran Zhou"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2507.09214",
    "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline",
    "abstract": "           3D detection technology is widely used in the field of autonomous driving, with its application scenarios gradually expanding from enclosed highways to open conventional roads. For rare anomaly categories that appear on the road, 3D detection models trained on closed sets often misdetect or fail to detect anomaly objects. To address this risk, it is necessary to enhance the generalization ability of 3D detection models for targets of arbitrary shapes and to possess the capability to filter out anomalies. The generalization of 3D detection is limited by two factors: the coupled training of 2D and 3D, and the insufficient diversity in the scale distribution of training samples. This paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm, which decouples the training strategy of 3D and 2D to release the generalization ability for arbitrary 3D foreground detection, and proposes an anomaly scoring algorithm based on foreground confidence prediction, achieving target-level anomaly scoring. In order to further verify and enhance the generalization of anomaly detection, we use a 3D rendering method to synthesize two augmented reality binocular stereo 3D detection datasets which named KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories as extra training data to address the sparse sample distribution issue. Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not used in training to simulate zero-shot scenarios in real-world settings, solely for evaluating 3D anomaly detection. Finally, the performance of the algorithm and the dataset is verified in the experiments. (Code and dataset can be obtained at this https URL).         ",
    "url": "https://arxiv.org/abs/2507.09214",
    "authors": [
      "Shiyi Mu",
      "Zichong Gu",
      "Hanqi Lyu",
      "Yilin Gao",
      "Shugong Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2507.10461",
    "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
    "abstract": "           Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.         ",
    "url": "https://arxiv.org/abs/2507.10461",
    "authors": [
      "Tao Tang",
      "Chengxu Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2507.14647",
    "title": "Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer",
    "abstract": "           We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3: mean opinion score (MOS) prediction for speech with multiple sampling frequencies (SFs). Our submitted model integrates an SF-independent (SFI) convolutional layer into a self-supervised learning (SSL) model to achieve SFI speech feature extraction for MOS prediction. We present some strategies to improve the MOS prediction performance of our model: distilling knowledge from a pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset. Our submission to the AMC 2025 Track 3 ranked the first in one evaluation metric and the fourth in the final ranking. We also report the results of our ablation study to investigate essential factors of our model.         ",
    "url": "https://arxiv.org/abs/2507.14647",
    "authors": [
      "Go Nishikawa",
      "Wataru Nakata",
      "Yuki Saito",
      "Kanami Imamura",
      "Hiroshi Saruwatari",
      "Tomohiko Nakamura"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2507.19046",
    "title": "Dynamics-Informed Reservoir Computing with Visibility Graphs",
    "abstract": "           Accurate prediction of complex and nonlinear time series remains a challenging problem across engineering and scientific disciplines. Reservoir computing (RC) offers a computationally efficient alternative to traditional deep learning by training only the read-out layer while employing a randomly structured and fixed reservoir network. Despite its advantages, the largely random reservoir graph architecture often results in suboptimal and oversized networks with poorly understood dynamics. Addressing this issue, we propose a novel Dynamics-Informed Reservoir Computing (DyRC) framework that systematically infers the reservoir network structure directly from the input training sequence. This work proposes to employ the visibility graph (VG) technique, which converts time series data into networks by representing measurement points as nodes linked by mutual visibility. The reservoir network is constructed by directly adopting the VG network from a training data sequence, leveraging the parameter-free visibility graph approach to avoid expensive hyperparameter tuning. This process results in a reservoir that is directly informed by the specific dynamics of the prediction task under study. We assess the DyRC-VG method through prediction tasks involving the canonical nonlinear Duffing oscillator, evaluating prediction accuracy and consistency. Compared to an Erd\u0151s-R\u00e9nyi (ER) graph of the same size, spectral radius, and fixed density, we observe higher prediction quality and more consistent performance over repeated implementations in the DyRC-VG. An ER graph with density matched to the DyRC-VG can in some conditions outperform both approaches.         ",
    "url": "https://arxiv.org/abs/2507.19046",
    "authors": [
      "Charlotte Geier",
      "Rasha Shanaz",
      "Merten Stender"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.01702",
    "title": "Explicit Function-Correcting Code Constructions for Lee Metric Channels",
    "abstract": "           Function-Correcting Codes (FCCs) are a novel class of codes designed to protect function evaluations of messages against errors while minimizing redundancy. A theoretical framework for systematic FCCs to channels matched to the Lee metric has been studied recently, which introduced function-correcting Lee codes (FCLCs) and also derived upper and lower bounds on their optimal redundancy. In this paper, we first propose a Plotkin-like bound for irregular Lee-distance codes. We then construct explicit FCLCs for specific classes of functions, including the Lee weight, Lee weight distribution, modular sum, and locally bounded function. For these functions, lower bounds on redundancy are obtained, and our constructions are shown to be optimal in certain cases. Finally, a comparative analysis with classical Lee error-correcting codes and codes correcting errors in function values, demonstrates that FCLCs can significantly reduce redundancy while preserving function correctness.         ",
    "url": "https://arxiv.org/abs/2508.01702",
    "authors": [
      "Hareesh K.",
      "Rashid Ummer N.T.",
      "B. Sundar Rajan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2508.05064",
    "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding",
    "abstract": "           Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.         ",
    "url": "https://arxiv.org/abs/2508.05064",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Yehor Karpichev",
      "Brandon Haworth",
      "Homayoun Najjaran"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.05068",
    "title": "Automatic Image Colorization with Convolutional Neural Networks and Generative Adversarial Networks",
    "abstract": "           Image colorization, the task of adding colors to grayscale images, has been the focus of significant research efforts in computer vision in recent years for its various application areas such as color restoration and automatic animation colorization [15, 1]. The colorization problem is challenging as it is highly ill-posed with two out of three image dimensions lost, resulting in large degrees of freedom. However, semantics of the scene as well as the surface texture could provide important cues for colors: the sky is typically blue, the clouds are typically white and the grass is typically green, and there are huge amounts of training data available for learning such priors since any colored image could serve as a training data point [20]. Colorization is initially formulated as a regression task[5], which ignores the multi-modal nature of color prediction. In this project, we explore automatic image colorization via classification and adversarial learning. We will build our models on prior works, apply modifications for our specific scenario and make comparisons.         ",
    "url": "https://arxiv.org/abs/2508.05068",
    "authors": [
      "Changyuan Qiu",
      "Hangrui Cao",
      "Qihan Ren",
      "Ruiyu Li",
      "Yuqing Qiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2508.06189",
    "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration",
    "abstract": "           With the acceleration of urbanization, criminal behavior in public scenes poses an increasingly serious threat to social security. Traditional anomaly detection methods based on feature recognition struggle to capture high-level behavioral semantics from historical information, while generative approaches based on Large Language Models (LLMs) often fail to meet real-time requirements. To address these challenges, we propose MA-CBP, a criminal behavior prediction framework based on multi-agent asynchronous collaboration. This framework transforms real-time video streams into frame-level semantic descriptions, constructs causally consistent historical summaries, and fuses adjacent image frames to perform joint reasoning over long- and short-term contexts. The resulting behavioral decisions include key elements such as event subjects, locations, and causes, enabling early warning of potential criminal activity. In addition, we construct a high-quality criminal behavior dataset that provides multi-scale language supervision, including frame-level, summary-level, and event-level semantic annotations. Experimental results demonstrate that our method achieves superior performance on multiple datasets and offers a promising solution for risk warning in urban public safety scenarios.         ",
    "url": "https://arxiv.org/abs/2508.06189",
    "authors": [
      "Cheng Liu",
      "Daou Zhang",
      "Tingxu Liu",
      "Yuhan Wang",
      "Jinyang Chen",
      "Yuexuan Li",
      "Xinying Xiao",
      "Chenbo Xin",
      "Ziru Wang",
      "Weichao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.06627",
    "title": "Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Records",
    "abstract": "           Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2508.06627",
    "authors": [
      "Mosbah Aouad",
      "Anirudh Choudhary",
      "Awais Farooq",
      "Steven Nevers",
      "Lusine Demirkhanyan",
      "Bhrandon Harris",
      "Suguna Pappu",
      "Christopher Gondi",
      "Ravishankar Iyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.06634",
    "title": "Dual-Head Physics-Informed Graph Decision Transformer for Distribution System Restoration",
    "abstract": "           Driven by recent advances in sensing and computing, deep reinforcement learning (DRL) technologies have shown great potential for addressing distribution system restoration (DSR) under uncertainty. However, their data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit their ability to handle scenarios that require long-term temporal dependencies or few-shot and zero-shot decision making. Emerging Decision Transformers (DTs), which leverage causal transformers for sequence modeling in DRL tasks, offer a promising alternative. However, their reliance on return-to-go (RTG) cloning and limited generalization capacity restricts their effectiveness in dynamic power system environments. To address these challenges, we introduce an innovative Dual-Head Physics-informed Graph Decision Transformer (DH-PGDT) that integrates physical modeling, structural reasoning, and subgoal-based guidance to enable scalable and robust DSR even in zero-shot or few-shot scenarios. DH-PGDT features a dual-head physics-informed causal transformer architecture comprising Guidance Head, which generates subgoal representations, and Action Head, which uses these subgoals to generate actions independently of RTG. It also incorporates an operational constraint-aware graph reasoning module that encodes power system topology and operational constraints to generate a confidence-weighted action vector for refining DT trajectories. This design effectively improves generalization and enables robust adaptation to unseen scenarios. While this work focuses on DSR, the underlying computing model of the proposed PGDT is broadly applicable to sequential decision making across various power system operations and other complex engineering domains.         ",
    "url": "https://arxiv.org/abs/2508.06634",
    "authors": [
      "Hong Zhao",
      "Jin Wei-Kocsis",
      "Adel Heidari Akhijahani",
      "Karen L Butler-Purry"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2508.10040",
    "title": "Exploring Content and Social Connections of Fake News with Explainable Text and Graph Learning",
    "abstract": "           The global spread of misinformation and concerns about content trustworthiness have driven the development of automated fact-checking systems. Since false information often exploits social media dynamics such as \"likes\" and user networks to amplify its reach, effective solutions must go beyond content analysis to incorporate these factors. Moreover, simply labelling content as false can be ineffective or even reinforce biases such as automation and confirmation bias. This paper proposes an explainable framework that combines content, social media, and graph-based features to enhance fact-checking. It integrates a misinformation classifier with explainability techniques to deliver complete and interpretable insights supporting classification decisions. Experiments demonstrate that multimodal information improves performance over single modalities, with evaluations conducted on datasets in English, Spanish, and Portuguese. Additionally, the framework's explanations were assessed for interpretability, trustworthiness, and robustness with a novel protocol, showing that it effectively generates human-understandable justifications for its predictions.         ",
    "url": "https://arxiv.org/abs/2508.10040",
    "authors": [
      "V\u00edtor N. Louren\u00e7o",
      "Aline Paes",
      "Tillman Weyde"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.10594",
    "title": "FreeGAD: A Training-Free yet Effective Approach for Graph Anomaly Detection",
    "abstract": "           Graph Anomaly Detection (GAD) aims to identify nodes that deviate from the majority within a graph, playing a crucial role in applications such as social networks and e-commerce. Despite the current advancements in deep learning-based GAD, existing approaches often suffer from high deployment costs and poor scalability due to their complex and resource-intensive training processes. Surprisingly, our empirical findings suggest that the training phase of deep GAD methods, commonly perceived as crucial, may actually contribute less to anomaly detection performance than expected. Inspired by this, we propose FreeGAD, a novel training-free yet effective GAD method. Specifically, it leverages an affinity-gated residual encoder to generate anomaly-aware representations. Meanwhile, FreeGAD identifies anchor nodes as pseudo-normal and anomalous guides, followed by calculating anomaly scores through anchor-guided statistical deviations. Extensive experiments demonstrate that FreeGAD achieves superior anomaly detection performance, efficiency, and scalability on multiple benchmark datasets from diverse domains, without any training or iterative optimization.         ",
    "url": "https://arxiv.org/abs/2508.10594",
    "authors": [
      "Yunfeng Zhao",
      "Yixin Liu",
      "Shiyuan Li",
      "Qingfeng Chen",
      "Yu Zheng",
      "Shirui Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.10747",
    "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning",
    "abstract": "           Generalized planning using deep reinforcement learning (RL) combined with graph neural networks (GNNs) has shown promising results in various symbolic planning domains described by PDDL. However, existing approaches typically represent planning states as fully connected graphs, leading to a combinatorial explosion in edge information and substantial sparsity as problem scales grow, especially evident in large grid-based environments. This dense representation results in diluted node-level information, exponentially increases memory requirements, and ultimately makes learning infeasible for larger-scale problems. To address these challenges, we propose a sparse, goal-aware GNN representation that selectively encodes relevant local relationships and explicitly integrates spatial features related to the goal. We validate our approach by designing novel drone mission scenarios based on PDDL within a grid world, effectively simulating realistic mission execution environments. Our experimental results demonstrate that our method scales effectively to larger grid sizes previously infeasible with dense graph representations and substantially improves policy generalization and success rates. Our findings provide a practical foundation for addressing realistic, large-scale generalized planning tasks.         ",
    "url": "https://arxiv.org/abs/2508.10747",
    "authors": [
      "Sangwoo Jeon",
      "Juchul Shin",
      "Gyeong-Tae Kim",
      "YeonJe Cho",
      "Seongwoo Kim"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2508.11987",
    "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
    "abstract": "           Future prediction is a complex task for LLM agents, requiring a high level of analytical thinking, information gathering, contextual understanding, and decision-making under uncertainty. Agents must not only gather and interpret vast amounts of dynamic information but also integrate diverse data sources, weigh uncertainties, and adapt predictions based on emerging trends, just as human experts do in fields like politics, economics, and finance. Despite its importance, no large-scale benchmark exists for evaluating agents on future prediction, largely due to challenges in handling real-time updates and retrieving timely, accurate answers. To address this, we introduce $\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically designed for LLM agents performing future prediction tasks. FutureX is the largest and most diverse live benchmark for future prediction, supporting real-time daily updates and eliminating data contamination through an automated pipeline for question gathering and answer collection. We evaluate 25 LLM/agent models, including those with reasoning, search capabilities, and integration of external tools such as the open-source Deep Research Agent and closed-source Deep Research models. This comprehensive evaluation assesses agents' adaptive reasoning and performance in dynamic environments. Additionally, we provide in-depth analyses of agents' failure modes and performance pitfalls in future-oriented tasks, including the vulnerability to fake web pages and the temporal validity. Our goal is to establish a dynamic, contamination-free evaluation standard that drives the development of LLM agents capable of performing at the level of professional human analysts in complex reasoning and predictive thinking.         ",
    "url": "https://arxiv.org/abs/2508.11987",
    "authors": [
      "Zhiyuan Zeng",
      "Jiashuo Liu",
      "Siyuan Chen",
      "Tianci He",
      "Yali Liao",
      "Jinpeng Wang",
      "Zaiyuan Wang",
      "Yang Yang",
      "Lingyue Yin",
      "Mingren Yin",
      "Zhenwei Zhu",
      "Tianle Cai",
      "Zehui Chen",
      "Jiecao Chen",
      "Yantao Du",
      "Xiang Gao",
      "Jiacheng Guo",
      "Liang Hu",
      "Jianpeng Jiao",
      "Xiangsheng Li",
      "Jingkai Liu",
      "Shuang Ni",
      "Zhoufutu Wen",
      "Ge Zhang",
      "Kaiyuan Zhang",
      "Xin Zhou",
      "Jose Blanchet",
      "Xipeng Qiu",
      "Mengdi Wang",
      "Wenhao Huang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.11991",
    "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network",
    "abstract": "           The automation of logic circuit design enhances chip performance, energy efficiency, and reliability, and is widely applied in the field of Electronic Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent, optimize, and verify the functional characteristics of digital circuits, enhancing the efficiency of EDA this http URL to the complex structure and large scale of nodes in real-world AIGs, accurate modeling is challenging, leading to existing work lacking the ability to jointly model functional and structural characteristics, as well as insufficient dynamic information propagation this http URL address the aforementioned challenges, we propose this http URL, AIGer consists of two components: 1) Node logic feature initialization embedding component and 2) AIGs feature learning network this http URL node logic feature initialization embedding component projects logic nodes, such as AND and NOT, into independent semantic spaces, to enable effective node embedding for subsequent this http URL upon this, the AIGs feature learning network component employs a heterogeneous graph convolutional network, designing dynamic relationship weight matrices and differentiated information aggregation approaches to better represent the original structure and information of this http URL combination of these two components enhances AIGer's ability to jointly model functional and structural characteristics and improves its message passing capability. Experimental results indicate that AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of 33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the best-performing models.         ",
    "url": "https://arxiv.org/abs/2508.11991",
    "authors": [
      "Weihao Sun"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12219",
    "title": "C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis",
    "abstract": "           This study presents a deep learning-based optimization of YOLOv11 for cotton disease detection, developing an intelligent monitoring system. Three key challenges are addressed: (1) low precision in early spot detection (35% leakage rate for sub-5mm2 spots), (2) performance degradation in field conditions (25% accuracy drop), and (3) high error rates (34.7%) in multi-disease scenarios. The proposed solutions include: C2PSA module for enhanced small-target feature extraction; Dynamic category weighting to handle sample imbalance; Improved data augmentation via Mosaic-MixUp scaling. Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0% improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS. The mobile-deployed system enables real-time disease monitoring and precision treatment in agricultural applications.         ",
    "url": "https://arxiv.org/abs/2508.12219",
    "authors": [
      "Kaiyuan Wang",
      "Jixing Liu",
      "Xiaobo Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2508.12393",
    "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph",
    "abstract": "           The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.         ",
    "url": "https://arxiv.org/abs/2508.12393",
    "authors": [
      "Duzhen Zhang",
      "Zixiao Wang",
      "Zhong-Zhi Li",
      "Yahan Yu",
      "Shuncheng Jia",
      "Jiahua Dong",
      "Haotian Xu",
      "Xing Wu",
      "Yingying Zhang",
      "Tielin Zhang",
      "Jie Yang",
      "Xiuying Chen",
      "Le Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12410",
    "title": "SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes",
    "abstract": "           Liver Cirrhosis plays a critical role in the prognosis of chronic liver disease. Early detection and timely intervention are critical in significantly reducing mortality rates. However, the intricate anatomical architecture and diverse pathological changes of liver tissue complicate the accurate detection and characterization of lesions in clinical settings. Existing methods underutilize the spatial anatomical details in volumetric MRI data, thereby hindering their clinical effectiveness and explainability. To address this challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to model the spatial relationships within the complex anatomical structures of MRI volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba), SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and combines anatomical information from the sagittal, coronal, and axial planes to construct a global spatial context representation, enabling efficient volumetric segmentation of pathological liver structures. Furthermore, we introduce the Spatial Reverse Attention module (SRMA), designed to progressively refine cirrhotic details in the segmentation map, utilizing both the coarse segmentation map and hierarchical encoding features. Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation. Our code is available for public: this https URL.         ",
    "url": "https://arxiv.org/abs/2508.12410",
    "authors": [
      "Jun Zeng",
      "Yannan Huang",
      "Elif Keles",
      "Halil Ertugrul Aktas",
      "Gorkem Durak",
      "Nikhil Kumar Tomar",
      "Quoc-Huy Trinh",
      "Deepak Ranjan Nayak",
      "Ulas Bagci",
      "Debesh Jha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12649",
    "title": "ChangePrism: Visualizing the Essence of Code Changes",
    "abstract": "           Understanding the changes made by developers when they submit a pull request and/or perform a commit on a repository is a crucial activity in software maintenance and evolution. The common way to review changes relies on examining code diffs, where textual differences between two file versions are highlighted in red and green to indicate additions and deletions of lines. This can be cumbersome for developers, making it difficult to obtain a comprehensive overview of all changes in a commit. Moreover, certain types of code changes can be particularly significant and may warrant differentiation from standard modifications to enhance code comprehension. We present a novel visualization approach supported by a tool named ChangePrism, which provides a way to better understand code changes. The tool comprises two components: extraction, which retrieves code changes and relevant information from the git history, and visualization, which offers both general and detailed views of code changes in commits. The general view provides an overview of different types of code changes across commits, while the detailed view displays the exact changes in the source code for each commit.         ",
    "url": "https://arxiv.org/abs/2508.12649",
    "authors": [
      "Lei Chen",
      "Michele Lanza",
      "Shinpei Hayashi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2508.12672",
    "title": "Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering",
    "abstract": "           Federated Learning (FL) enables collaborative model training across multiple clients without sharing private data. We consider FL scenarios wherein FL clients are subject to adversarial (Byzantine) attacks, while the FL server is trusted (honest) and has a trustworthy side dataset. This may correspond to, e.g., cases where the server possesses trusted data prior to federation, or to the presence of a trusted client that temporarily assumes the server role. Our approach requires only two honest participants, i.e., the server and one client, to function effectively, without prior knowledge of the number of malicious clients. Theoretical analysis demonstrates bounded optimality gaps even under strong Byzantine attacks. Experimental results show that our algorithm significantly outperforms standard and robust FL baselines such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack strategies including label flipping, sign flipping, and Gaussian noise addition across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.         ",
    "url": "https://arxiv.org/abs/2508.12672",
    "authors": [
      "Emmanouil Kritharakis",
      "Dusan Jakovetic",
      "Antonios Makris",
      "Konstantinos Tserpes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12794",
    "title": "Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision",
    "abstract": "           Transportation influence health by shaping exposure to physical activity, air pollution and injury risk. Comparative data on cycling and motorcycling behaviours is scarce, particularly at a global scale. Street view imagery, such as Google Street View (GSV), combined with computer vision, is a valuable resource for efficiently capturing travel behaviour data. This study demonstrates a novel approach using deep learning on street view images to estimate cycling and motorcycling levels across diverse cities worldwide. We utilized data from 185 global cities. The data on mode shares of cycling and motorcycling estimated using travel surveys or censuses. We used GSV images to detect cycles and motorcycles in sampled locations, using 8000 images per city. The YOLOv4 model, fine-tuned using images from six cities, achieved a mean average precision of 89% for detecting cycles and motorcycles. A global prediction model was developed using beta regression with city-level mode shares as outcome, with log transformed explanatory variables of counts of GSV-detected images with cycles and motorcycles, while controlling for population density. We found strong correlations between GSV motorcycle counts and motorcycle mode share (0.78) and moderate correlations between GSV cycle counts and cycling mode share (0.51). Beta regression models predicted mode shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling, achieving median absolute errors (MDAE) of 1.3% and 1.4%, respectively. Scatterplots demonstrated consistent prediction accuracy, though cities like Utrecht and Cali were outliers. The model was applied to 60 cities globally for which we didn't have recent mode share data. We provided estimates for some cities in the Middle East, Latin America and East Asia. With computer vision, GSV images capture travel modes and activity, providing insights alongside traditional data sources.         ",
    "url": "https://arxiv.org/abs/2508.12794",
    "authors": [
      "Kyriaki",
      "Kokka",
      "Rahul Goel",
      "Ali Abbas",
      "Kerry A. Nice",
      "Luca Martial",
      "SM Labib",
      "Rihuan Ke",
      "Carola Bibiane Sch\u00f6nlieb",
      "James Woodcock"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12832",
    "title": "Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds",
    "abstract": "           The widespread adoption of convolutional neural networks (CNNs) in resource-constrained scenarios has driven the development of Machine Learning as a Service (MLaaS) system. However, this approach is susceptible to privacy leakage, as the data sent from the client to the untrusted cloud server often contains sensitive information. Existing CNN privacy-preserving schemes, while effective in ensuring data confidentiality through homomorphic encryption and secret sharing, face efficiency bottlenecks, particularly in convolution operations. In this paper, we propose a novel verifiable privacy-preserving scheme tailored for CNN convolutional layers. Our scheme enables efficient encryption and decryption, allowing resource-constrained clients to securely offload computations to the untrusted cloud server. Additionally, we present a verification mechanism capable of detecting the correctness of the results with a success probability of at least $1-\\frac{1}{\\left|Z\\right|}$. Extensive experiments conducted on 10 datasets and various CNN models demonstrate that our scheme achieves speedups ranging $26 \\times$ ~ $\\ 87\\times$ compared to the original plaintext model while maintaining accuracy.         ",
    "url": "https://arxiv.org/abs/2508.12832",
    "authors": [
      "Jinyu Lu",
      "Xinrong Sun",
      "Yunting Tao",
      "Tong Ji",
      "Fanyu Kong",
      "Guoqiang Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2508.12839",
    "title": "HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms",
    "abstract": "           With the rapid proliferation of streaming services, network load exhibits highly time-varying and bursty behavior, posing serious challenges for maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms (CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS and profitability, accurate load forecasting remains challenging under traffic surges. Existing methods either minimize mean absolute error, resulting in underprovisioning and potential Service Level Agreement (SLA) violations during peak periods, or adopt conservative overprovisioning strategies, which mitigate SLA risks at the expense of increased resource expenditure. To address this dilemma, we propose HRS, a hybrid representation framework with scheduling awareness that integrates numerical and image-based representations to better capture extreme load dynamics. We further introduce a Scheduling-Aware Loss (SAL) that captures the asymmetric impact of prediction errors, guiding predictions that better support scheduling decisions. Extensive experiments on four real-world datasets demonstrate that HRS consistently outperforms ten baselines and achieves state-of-the-art performance, reducing SLA violation rates by 63.1% and total profit loss by 32.3%.         ",
    "url": "https://arxiv.org/abs/2508.12839",
    "authors": [
      "Tiancheng Zhang",
      "Cheng Zhang",
      "Shuren Liu",
      "Xiaofei Wang",
      "Shaoyuan Huang",
      "Wenyu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2508.12946",
    "title": "Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade",
    "abstract": "           In this paper we report on first insights from interviews with teachers and students on using social robots in computer science class in sixth grade. Our focus is on learning about requirements and potential applications. We are particularly interested in getting both perspectives, the teachers' and the learners' view on how robots could be used and what features they should or should not have. Results show that teachers as well as students are very open to robots in the classroom. However, requirements are partially quite heterogeneous among the groups. This leads to complex design challenges which we discuss at the end of this paper.         ",
    "url": "https://arxiv.org/abs/2508.12946",
    "authors": [
      "Ann-Sophie L. Schenk",
      "Stefan Schiffer",
      "Heqiu Song"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2508.12992",
    "title": "MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in Moving Target Selection across Complex Scenarios",
    "abstract": "           Moving target selection in multimedia interactive systems faces unprecedented challenges as users increasingly interact across diverse and dynamic contexts-from live streaming in moving vehicles to VR gaming in varying environments. Existing approaches rely on probabilistic models that relate endpoint distribution to target properties such as size and speed. However, these methods require substantial training data for each new context and lack transferability across scenarios, limiting their practical deployment in diverse multimedia environments where rich multimodal contextual information is readily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian Networks), which addresses these problems by combining classical statistical modeling with a context-aware multimodal method. MAGNeT dynamically fuses pre-fitted Ternary-Gaussian models from various scenarios based on real-time contextual cues, enabling effective adaptation with minimal training data while preserving model interpretability. We conduct experiments on self-constructed 2D and 3D moving target selection datasets under in-vehicle vibration conditions. Extensive experiments demonstrate that MAGNeT achieves lower error rates with few-shot samples by applying context-aware fusion of Gaussian experts from multi-factor conditions.         ",
    "url": "https://arxiv.org/abs/2508.12992",
    "authors": [
      "Xiangxian Li",
      "Yawen Zheng",
      "Baiqiao Zhang",
      "Yijia Ma",
      "Xianhui Cao",
      "Juan Liu",
      "Yulong Bian",
      "Jin Huang",
      "Chenglei Yang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2508.13092",
    "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog",
    "abstract": "           Timely detection of hardware vulnerabilities during the early design stage is critical for reducing remediation costs. Existing early detection techniques often require specialized security expertise, limiting their usability. Recent efforts have explored the use of large language models (LLMs) for Verilog vulnerability detection. However, LLMs struggle to capture the structure in Verilog code, resulting in inconsistent detection results. To this end, we propose VerilogLAVD, the first LLM-aided graph traversal rule generation approach for Verilog vulnerability detection. Our approach introduces the Verilog Property Graph (VeriPG), a unified representation of Verilog code. It combines syntactic features extracted from the abstract syntax tree (AST) with semantic information derived from control flow and data dependency graphs. We leverage LLMs to generate VeriPG-based detection rules from Common Weakness Enumeration (CWE) descriptions. These rules guide the rule executor that traversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we build a dataset collected from open-source repositories and synthesized data. In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types, VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with external knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27, respectively.         ",
    "url": "https://arxiv.org/abs/2508.13092",
    "authors": [
      "Xiang Long",
      "Yingjie Xia",
      "Xiyuan Chen",
      "Li Kuang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.02532",
    "title": "Superposition detection and QMA with non-collapsing measurements",
    "abstract": "           We prove that QMA where the verifier may also make a single non-collapsing measurement is equal to NEXP, resolving an open question of Aaronson. We show this is a corollary to a modified proof of QMA+ = NEXP [arXiv:2306.13247]. At the core of many results inspired by Blier and Tapp [arXiv:0709.0738] is an unphysical property testing problem deciding whether a quantum state is close to an element of a fixed basis.         ",
    "url": "https://arxiv.org/abs/2403.02532",
    "authors": [
      "Roozbeh Bassirian",
      "Kunal Marwaha"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2411.11464",
    "title": "Parallel Network Reconstruction with Multi-directional Regularization",
    "abstract": "           Reconstructing large-scale latent networks from observed dynamics is crucial for understanding complex systems. However, the existing methods based on compressive sensing are often rendered infeasible in practice by prohibitive computational and memory costs. To address this challenge, we introduce a new distributed computing framework for efficient large-scale network reconstruction with parallel computing, namely PALMS (Parallel Adaptive Lasso with Multi-directional Signals). The core idea of PALMS is to decompose the complex global problem by partitioning network nodes, enabling the parallel estimation of sub-networks across multiple computing units. This strategy substantially reduces the computational complexity and storage requirements of classic methods. By using the adaptive multi-directional regularization on each computing unit, we also establish the consistency of PALMS estimator theoretically. Extensive simulation studies and empirical analyses on several large-scale real-world networks validate the computational efficiency and robust reconstruction accuracy of our approach.         ",
    "url": "https://arxiv.org/abs/2411.11464",
    "authors": [
      "Zhaoyu Xing",
      "Wei Zhong"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2501.01437",
    "title": "On the reconstruction limits of complex networks",
    "abstract": "           Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we take a first-principles approach and build on our earlier definition of reconstructability-the fraction of structural information recoverable from data. We relate this quantity to the true data-generating (TDG) process and delineate an information-theoretic reconstruction limit, i.e., the upper bound of the mutual information between the true underlying graph and any graph reconstructed from observations. These concepts lead us to a principled numerical method to assess the validity of empirically reconstructed networks, based on model selection and a quantity we introduce: the reconstruction index. This index approximates the reconstructability from data, quantifies the variability of the reconstructed network ensemble, and is shown to predict reconstruction error without requiring knowledge of the true underlying network. We characterize this method and test it on empirical time series and networks.         ",
    "url": "https://arxiv.org/abs/2501.01437",
    "authors": [
      "Charles Murphy",
      "Simon Lizotte",
      "Fran\u00e7ois Thibault",
      "Vincent Thibeault",
      "Patrick Desrosiers",
      "Antoine Allard"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Information Theory (cs.IT)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2506.19094",
    "title": "Accurate identification of communication between multiple interacting neural populations",
    "abstract": "           Neural recording technologies now enable simultaneous recording of population activity across many brain regions, motivating the development of data-driven models of communication between brain regions. However, existing models can struggle to disentangle the sources that influence recorded neural populations, leading to inaccurate portraits of inter-regional communication. Here, we introduce Multi-Region Latent Factor Analysis via Dynamical Systems (MR-LFADS), a sequential variational autoencoder designed to disentangle inter-regional communication, inputs from unobserved regions, and local neural population dynamics. We show that MR-LFADS outperforms existing approaches at identifying communication across dozens of simulations of task-trained multi-region networks. When applied to large-scale electrophysiology, MR-LFADS predicts brain-wide effects of circuit perturbations that were held out during model fitting. These validations on synthetic and real neural data position MR-LFADS as a promising tool for discovering principles of brain-wide information processing.         ",
    "url": "https://arxiv.org/abs/2506.19094",
    "authors": [
      "Belle Liu",
      "Jacob Sacks",
      "Matthew D. Golub"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2507.03341",
    "title": "UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis",
    "abstract": "           Functional ultrasound (fUS) is a neuroimaging technique known for its high spatiotemporal resolution, enabling non-invasive observation of brain activity through neurovascular coupling. Despite its potential in clinical applications such as neonatal monitoring and intraoperative guidance, the development of fUS faces challenges related to data scarcity and limitations in generating realistic fUS images. This paper explores the use of a generative adversarial network (GAN) framework tailored for fUS image synthesis. The proposed method incorporates architectural enhancements, including feature enhancement modules and normalization techniques, aiming to improve the fidelity and physiological plausibility of generated images. The study evaluates the performance of the framework against existing generative models, demonstrating its capability to produce high-quality fUS images under various experimental conditions. Additionally, the synthesized images are assessed for their utility in downstream tasks, showing improvements in classification accuracy when used for data augmentation. Experimental results are based on publicly available fUS datasets, highlighting the framework's effectiveness in addressing data limitations.         ",
    "url": "https://arxiv.org/abs/2507.03341",
    "authors": [
      "Zhuo Li",
      "Xuhang Chen",
      "Shuqiang Wang",
      "Bin Yuan",
      "Nou Sotheany",
      "Ngeth Rithea"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2508.08025",
    "title": "Flagifying the Dowker Complex",
    "abstract": "           The Dowker complex $\\mathrm{D}_{R}(X,Y)$ is a simplicial complex capturing the topological interplay between two finite sets $X$ and $Y$ under some relation $R\\subseteq X\\times Y$. While its definition is asymmetric, the famous Dowker duality states that $\\mathrm{D}_{R}(X,Y)$ and $\\mathrm{D}_{R}(Y,X)$ have homotopy equivalent geometric realizations. We introduce the Dowker-Rips complex $\\mathrm{DR}_{R}(X,Y)$, defined as the flagification of the Dowker complex or, equivalently, as the maximal simplicial complex whose $1$-skeleton coincides with that of $\\mathrm{D}_{R}(X,Y)$. This is motivated by applications in topological data analysis, since as a flag complex, the Dowker-Rips complex is less expensive to compute than the Dowker complex. While the Dowker duality does not hold for Dowker-Rips complexes in general, we show that one still has that $\\mathrm{H}_{i}(\\mathrm{DR}_{R}(X,Y))\\cong\\mathrm{H}_{i}(\\mathrm{DR}_{R}(Y,X))$ for $i=0,1$. We further show that this weakened duality extends to the setting of persistent homology, and quantify the ``failure\" of the Dowker duality in homological dimensions higher than $1$ by means of interleavings. This makes the Dowker-Rips complex a less expensive, approximate version of the Dowker complex that is usable in topological data analysis. Indeed, we provide a Python implementation of the Dowker-Rips complex and, as an application, we show that it can be used as a drop-in replacement for the Dowker complex in a tumor microenvironment classification pipeline. In that pipeline, using the Dowker-Rips complex leads to increase in speed while retaining classification performance.         ",
    "url": "https://arxiv.org/abs/2508.08025",
    "authors": [
      "Marius Huber",
      "Patrick Schnider"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2508.12063",
    "title": "Generalized invariants meet constitutive neural networks: A novel framework for hyperelastic materials",
    "abstract": "           The major challenge in determining a hyperelastic model for a given material is the choice of invariants and the selection how the strain energy function depends functionally on these invariants. Here we introduce a new data-driven framework that simultaneously discovers appropriate invariants and constitutive models for isotropic incompressible hyperelastic materials. Our approach identifies both the most suitable invariants in a class of generalized invariants and the corresponding strain energy function directly from experimental observations. Unlike previous methods that rely on fixed invariant choices or sequential fitting procedures, our method integrates the discovery process into a single neural network architecture. By looking at a continuous family of possible invariants, the model can flexibly adapt to different material behaviors. We demonstrate the effectiveness of this approach using popular benchmark datasets for rubber and brain tissue. For rubber, the method recovers a stretch-dominated formulation consistent with classical models. For brain tissue, it identifies a formulation sensitive to small stretches, capturing the nonlinear shear response characteristic of soft biological matter. Compared to traditional and neural-network-based models, our framework provides improved predictive accuracy and interpretability across a wide range of deformation states. This unified strategy offers a robust tool for automated and physically meaningful model discovery in hyperelasticity.         ",
    "url": "https://arxiv.org/abs/2508.12063",
    "authors": [
      "Denisa Martonov\u00e1",
      "Alain Goriely",
      "Ellen Kuhl"
    ],
    "subjectives": [
      "Soft Condensed Matter (cond-mat.soft)",
      "Artificial Intelligence (cs.AI)"
    ]
  }
]