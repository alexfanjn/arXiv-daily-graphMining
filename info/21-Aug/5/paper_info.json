[
  {
    "id": "arXiv:2108.01716",
    "title": "A well-conditioned direct PinT algorithm for first- and second-order  evolutionary equations",
    "abstract": "In this paper, we propose a direct parallel-in-time (PinT) algorithm for time-dependent problems with first- or second-order derivative. We use a second-order boundary value method as the time integrator that leads to a tridiagonal time discretization matrix. Instead of solving the corresponding all-at-once system iteratively, we diagonalize the time discretization matrix, which yields a direct parallel implementation across all time levels. A crucial issue on this methodology is how the condition number of the eigenvector matrix $V$ grows as $n$ is increased, where $n$ is the number of time levels. A large condition number leads to large roundoff error in the diagonalization procedure, which could seriously pollute the numerical accuracy. Based on a novel connection between the characteristic equation and the Chebyshev polynomials, we present explicit formulas for computing $V$ and $V^{-1}$, by which we prove that $\\mathrm{Cond}_2(V)=\\mathcal{O}(n^{2})$. This implies that the diagonalization process is well-conditioned and the roundoff error only increases moderately as $n$ grows and thus, compared to other direct PinT algorithms, a much larger $n$ can be used to yield satisfactory parallelism. Numerical results on parallel machine are given to support our findings, where over 60 times speedup is achieved with 256 cores. ",
    "url": "https://arxiv.org/abs/2108.01716",
    "authors": [
      "Jun Liu",
      "Xiang-Sheng Wang",
      "Shu-Lin Wu",
      "Tao Zhou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2108.02040",
    "title": "Convergence of gradient descent for learning linear neural networks",
    "abstract": "We study the convergence properties of gradient descent for training deep linear neural networks, i.e., deep matrix factorizations, by extending a previous analysis for the related gradient flow. We show that under suitable conditions on the step sizes gradient descent converges to a critical point of the loss function, i.e., the square loss in this article. Furthermore, we demonstrate that for almost all initializations gradient descent converges to a global minimum in the case of two layers. In the case of three or more layers we show that gradient descent converges to a global minimum on the manifold matrices of some fixed rank, where the rank cannot be determined a priori. ",
    "url": "https://arxiv.org/abs/2108.02040",
    "authors": [
      "Gabin Maxime Nguegnang",
      "Holger Rauhut",
      "Ulrich Terstiege"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2108.01836",
    "title": "Blind and neural network-guided convolutional beamformer for joint  denoising, dereverberation, and source separation",
    "abstract": "This paper proposes an approach for optimizing a Convolutional BeamFormer (CBF) that can jointly perform denoising (DN), dereverberation (DR), and source separation (SS). First, we develop a blind CBF optimization algorithm that requires no prior information on the sources or the room acoustics, by extending a conventional joint DR and SS method. For making the optimization computationally tractable, we incorporate two techniques into the approach: the Source-Wise Factorization (SW-Fact) of a CBF and the Independent Vector Extraction (IVE). To further improve the performance, we develop a method that integrates a neural network(NN) based source power spectra estimation with CBF optimization by an inverse-Gamma prior. Experiments using noisy reverberant mixtures reveal that our proposed method with both blind and NN-guided scenarios greatly outperforms the conventional state-of-the-art NN-supported mask-based CBF in terms of the improvement in automatic speech recognition and signal distortion reduction performance. ",
    "url": "https://arxiv.org/abs/2108.01836",
    "authors": [
      "Tomohiro Nakatani",
      "Rintaro Ikeshita",
      "Keisuke Kinoshita",
      "Hiroshi Sawada",
      "Shoko Araki"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2108.01995",
    "title": "Robustness of convolutional neural networks to physiological ECG noise",
    "abstract": "The electrocardiogram (ECG) is one of the most widespread diagnostic tools in healthcare and supports the diagnosis of cardiovascular disorders. Deep learning methods are a successful and popular technique to detect indications of disorders from an ECG signal. However, there are open questions around the robustness of these methods to various factors, including physiological ECG noise. In this study we generate clean and noisy versions of an ECG dataset before applying Symmetric Projection Attractor Reconstruction (SPAR) and scalogram image transformations. A pretrained convolutional neural network is trained using transfer learning to classify these image transforms. For the clean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were 0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the noisy ECG datasets. Notably, when the network trained on clean data was used to classify the noisy datasets, performance decreases of up to 0.18 in F1 scores were seen. However, when the network trained on the noisy data was used to classify the clean dataset, the performance decrease was less than 0.05. We conclude that physiological ECG noise impacts classification using deep learning methods and careful consideration should be given to the inclusion of noisy ECG signals in the training data when developing supervised networks for ECG classification. ",
    "url": "https://arxiv.org/abs/2108.01995",
    "authors": [
      "J. Venton",
      "P. M. Harris",
      "A. Sundar",
      "N. A. S. Smith",
      "P. J. Aston"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2103.06025",
    "title": "Several ways to achieve robustness when solving wave propagation  problems",
    "abstract": " Title: Several ways to achieve robustness when solving wave propagation  problems ",
    "url": "https://arxiv.org/abs/2103.06025",
    "authors": [
      "Niall Bootland",
      "Victorita Dolean",
      "Pierre Jolivet",
      "Fr\u00e9d\u00e9ric Nataf",
      "St\u00e9phane Operto",
      "Pierre-Henri Tournier"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2107.04296",
    "title": "Differentially private training of neural networks with Langevin  dynamics for calibrated predictive uncertainty",
    "abstract": " Comments: Accepted to the ICML 2021 Theory and Practice of Differential Privacy Workshop ",
    "url": "https://arxiv.org/abs/2107.04296",
    "authors": [
      "Moritz Knolle",
      "Alexander Ziller",
      "Dmitrii Usynin",
      "Rickmer Braren",
      "Marcus R. Makowski",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.08805",
    "title": "How does a robot's social credibility relate to its perceived  trustworthiness?",
    "abstract": " Comments: Position paper presented at SCRITA, a workshop at IEEE RO-MAN 2021: this https URL ",
    "url": "https://arxiv.org/abs/2107.08805",
    "authors": [
      "Patrick Holthaus"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  }
]