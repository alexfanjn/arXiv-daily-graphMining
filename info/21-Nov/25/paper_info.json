[
  {
    "id": "arXiv:2111.12123",
    "title": "MICS : Multi-steps, Inverse Consistency and Symmetric deep learning  registration network",
    "abstract": "Deformable registration consists of finding the best dense correspondence between two different images. Many algorithms have been published, but the clinical application was made difficult by the high calculation time needed to solve the optimisation problem. Deep learning overtook this limitation by taking advantage of GPU calculation and the learning process. However, many deep learning methods do not take into account desirable properties respected by classical algorithms. In this paper, we present MICS, a novel deep learning algorithm for medical imaging registration. As registration is an ill-posed problem, we focused our algorithm on the respect of different properties: inverse consistency, symmetry and orientation conservation. We also combined our algorithm with a multi-step strategy to refine and improve the deformation grid. While many approaches applied registration to brain MRI, we explored a more challenging body localisation: abdominal CT. Finally, we evaluated our method on a dataset used during the Learn2Reg challenge, allowing a fair comparison with published methods. ",
    "url": "https://arxiv.org/abs/2111.12123",
    "authors": [
      "Th\u00e9o Estienne",
      "Maria Vakalopoulou",
      "Enzo Battistella",
      "Theophraste Henry",
      "Marvin Lerousseau",
      "Amaury Leroy",
      "Nikos Paragios",
      "Eric Deutsch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.12143",
    "title": "Critical initialization of wide and deep neural networks through partial  Jacobians: general theory and applications to LayerNorm",
    "abstract": "Deep neural networks are notorious for defying theoretical treatment. However, when the number of parameters in each layer tends to infinity the network function is a Gaussian process (GP) and quantitatively predictive description is possible. Gaussian approximation allows to formulate criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we describe a new way to diagnose (both theoretically and empirically) this criticality. To that end, we introduce partial Jacobians of a network, defined as derivatives of preactivations in layer $l$ with respect to preactivations in layer $l_0<l$. These quantities are particularly useful when the network architecture involves many different layers. We discuss various properties of the partial Jacobians such as their scaling with depth and relation to the neural tangent kernel (NTK). We derive the recurrence relations for the partial Jacobians and utilize them to analyze criticality of deep MLP networks with (and without) LayerNorm. We find that the normalization layer changes the optimal values of hyperparameters and critical exponents. We argue that LayerNorm is more stable when applied to preactivations, rather than activations due to larger correlation depth. ",
    "url": "https://arxiv.org/abs/2111.12143",
    "authors": [
      "Darshil Doshi",
      "Tianyu He",
      "Andrey Gromov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "High Energy Physics - Theory (hep-th)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2111.12282",
    "title": "Self-orthogonality matrix and Reed-Muller code",
    "abstract": "Kim et al. (2021) gave a method to embed a given binary $[n,k]$ code $\\mathcal{C}$ $(k = 3, 4)$ into a self-orthogonal code of the shortest length which has the same dimension $k$ and minimum distance $d' \\ge d(\\mathcal{C})$. We extends this result for $k=5$ and $6$ by proposing a new method related to a special matrix, called the self-orthogonality matrix $SO_k$, obtained by shortnening a Reed-Muller code $\\mathcal{R}(2,k)$. Furthermore, we disprove partially the conjecture (Kim et al. (2021)) by showing that if $31 \\le n \\le 256$ and $n\\equiv 14,22,29 \\pmod{31}$, then there exist optimal $[n,5]$ codes which are self-orthogonal. We also construct optimal self-orthogonal $[n,6]$ codes when $41 \\le n \\le 256$ satisfies $n \\ne 46, 54, 61$ and $n \\not\\equiv 7, 14, 22, 29, 38, 45, 53, 60 \\pmod{63}$. ",
    "url": "https://arxiv.org/abs/2111.12282",
    "authors": [
      "Jon-Lark Kim",
      "Whan-Hyuk Choi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2111.12296",
    "title": "Spatial-context-aware deep neural network for multi-class image  classification",
    "abstract": "Multi-label image classification is a fundamental but challenging task in computer vision. Over the past few decades, solutions exploring relationships between semantic labels have made great progress. However, the underlying spatial-contextual information of labels is under-exploited. To tackle this problem, a spatial-context-aware deep neural network is proposed to predict labels taking into account both semantic and spatial information. This proposed framework is evaluated on Microsoft COCO and PASCAL VOC, two widely used benchmark datasets for image multi-labelling. The results show that the proposed approach is superior to the state-of-the-art solutions on dealing with the multi-label image classification problem. ",
    "url": "https://arxiv.org/abs/2111.12296",
    "authors": [
      "Jialu Zhang",
      "Qian Zhang",
      "Jianfeng Ren",
      "Yitian Zhao",
      "Jiang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2111.12305",
    "title": "Thundernna: a white box adversarial attack",
    "abstract": "The existing work shows that the neural network trained by naive gradient-based optimization method is prone to adversarial attacks, adds small malicious on the ordinary input is enough to make the neural network wrong. At the same time, the attack against a neural network is the key to improving its robustness. The training against adversarial examples can make neural networks resist some kinds of adversarial attacks. At the same time, the adversarial attack against a neural network can also reveal some characteristics of the neural network, a complex high-dimensional non-linear function, as discussed in previous work. In This project, we develop a first-order method to attack the neural network. Compare with other first-order attacks, our method has a much higher success rate. Furthermore, it is much faster than second-order attacks and multi-steps first-order attacks. ",
    "url": "https://arxiv.org/abs/2111.12305",
    "authors": [
      "Linfeng Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.12406",
    "title": "Auto robust relative radiometric normalization via latent change noise  modelling",
    "abstract": "Relative radiometric normalization(RRN) of different satellite images of the same terrain is necessary for change detection, object classification/segmentation, and map-making tasks. However, traditional RRN models are not robust, disturbing by object change, and RRN models precisely considering object change can not robustly obtain the no-change set. This paper proposes auto robust relative radiometric normalization methods via latent change noise modeling. They utilize the prior knowledge that no change points possess small-scale noise under relative radiometric normalization and that change points possess large-scale radiometric noise after radiometric normalization, combining the stochastic expectation maximization method to quickly and robustly extract the no-change set to learn the relative radiometric normalization mapping functions. This makes our model theoretically grounded regarding the probabilistic theory and mathematics deduction. Specifically, when we select histogram matching as the relative radiometric normalization learning scheme integrating with the mixture of Gaussian noise(HM-RRN-MoG), the HM-RRN-MoG model achieves the best performance. Our model possesses the ability to robustly against clouds/fogs/changes. Our method naturally generates a robust evaluation indicator for RRN that is the no-change set root mean square error. We apply the HM-RRN-MoG model to the latter vegetation/water change detection task, which reduces the radiometric contrast and NDVI/NDWI differences on the no-change set, generates consistent and comparable results. We utilize the no-change set into the building change detection task, efficiently reducing the pseudo-change and boosting the precision. ",
    "url": "https://arxiv.org/abs/2111.12406",
    "authors": [
      "Shiqi Liu",
      "Lu Wang",
      "Jie Lian",
      "Ting chen",
      "Cong Liu",
      "Xuchen Zhan",
      "Jintao Lu",
      "Jie Liu",
      "Ting Wang",
      "Dong Geng",
      "Hongwei Duan",
      "Yuze Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.12606",
    "title": "Deep metric learning improves lab of origin prediction of genetically  engineered plasmids",
    "abstract": "Genome engineering is undergoing unprecedented development and is now becoming widely available. To ensure responsible biotechnology innovation and to reduce misuse of engineered DNA sequences, it is vital to develop tools to identify the lab-of-origin of engineered plasmids. Genetic engineering attribution (GEA), the ability to make sequence-lab associations, would support forensic experts in this process. Here, we propose a method, based on metric learning, that ranks the most likely labs-of-origin whilst simultaneously generating embeddings for plasmid sequences and labs. These embeddings can be used to perform various downstream tasks, such as clustering DNA sequences and labs, as well as using them as features in machine learning models. Our approach employs a circular shift augmentation approach and is able to correctly rank the lab-of-origin $90\\%$ of the time within its top 10 predictions - outperforming all current state-of-the-art approaches. We also demonstrate that we can perform few-shot-learning and obtain $76\\%$ top-10 accuracy using only $10\\%$ of the sequences. This means, we outperform the previous CNN approach using only one-tenth of the data. We also demonstrate that we are able to extract key signatures in plasmid sequences for particular labs, allowing for an interpretable examination of the model's outputs. ",
    "url": "https://arxiv.org/abs/2111.12606",
    "authors": [
      "Igor M. Soares",
      "Fernando H. F. Camargo",
      "Adriano Marques",
      "Oliver M. Crook"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2111.12631",
    "title": "EAD: an ensemble approach to detect adversarial examples from the hidden  features of deep neural networks",
    "abstract": "One of the key challenges in Deep Learning is the definition of effective strategies for the detection of adversarial examples. To this end, we propose a novel approach named Ensemble Adversarial Detector (EAD) for the identification of adversarial examples, in a standard multiclass classification scenario. EAD combines multiple detectors that exploit distinct properties of the input instances in the internal representation of a pre-trained Deep Neural Network (DNN). Specifically, EAD integrates the state-of-the-art detectors based on Mahalanobis distance and on Local Intrinsic Dimensionality (LID) with a newly introduced method based on One-class Support Vector Machines (OSVMs). Although all constituting methods assume that the greater the distance of a test instance from the set of correctly classified training instances, the higher its probability to be an adversarial example, they differ in the way such distance is computed. In order to exploit the effectiveness of the different methods in capturing distinct properties of data distributions and, accordingly, efficiently tackle the trade-off between generalization and overfitting, EAD employs detector-specific distance scores as features of a logistic regression classifier, after independent hyperparameters optimization. We evaluated the EAD approach on distinct datasets (CIFAR-10, CIFAR-100 and SVHN) and models (ResNet and DenseNet) and with regard to four adversarial attacks (FGSM, BIM, DeepFool and CW), also by comparing with competing approaches. Overall, we show that EAD achieves the best AUROC and AUPR in the large majority of the settings and comparable performance in the others. The improvement over the state-of-the-art, and the possibility to easily extend EAD to include any arbitrary set of detectors, pave the way to a widespread adoption of ensemble approaches in the broad field of adversarial example detection. ",
    "url": "https://arxiv.org/abs/2111.12631",
    "authors": [
      "Francesco Craighero",
      "Fabrizio Angaroni",
      "Fabio Stella",
      "Chiara Damiani",
      "Marco Antoniotti",
      "Alex Graudenzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2111.12689",
    "title": "A stacked deep convolutional neural network to predict the remaining  useful life of a turbofan engine",
    "abstract": "This paper presents the data-driven techniques and methodologies used to predict the remaining useful life (RUL) of a fleet of aircraft engines that can suffer failures of diverse nature. The solution presented is based on two Deep Convolutional Neural Networks (DCNN) stacked in two levels. The first DCNN is used to extract a low-dimensional feature vector using the normalized raw data as input. The second DCNN ingests a list of vectors taken from the former DCNN and estimates the RUL. Model selection was carried out by means of Bayesian optimization using a repeated random subsampling validation approach. The proposed methodology was ranked in the third place of the 2021 PHM Conference Data Challenge. ",
    "url": "https://arxiv.org/abs/2111.12689",
    "authors": [
      "David Solis-Martin",
      "Juan Galan-Paez",
      "Joaquin Borrego-Diaz"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.12451",
    "title": "Geometrically reduced modelling of pulsatile flow in perivascular  networks",
    "abstract": "Flow of cerebrospinal fluid in perivascular spaces is a key mechanism underlying brain transport and clearance. In this paper, we present a mathematical and numerical formalism for reduced models of pulsatile viscous fluid flow in networks of generalized annular cylinders. We apply this framework to study cerebrospinal fluid flow in perivascular spaces induced by pressure differences, cardiac pulse wave-induced vascular wall motion and vasomotion. The reduced models provide approximations of the cross-section average pressure and cross-section flux, both defined over the topologically one-dimensional centerlines of the network geometry. Comparing the full and reduced model predictions, we find that the reduced models capture pulsatile flow characteristics and provide accurate pressure and flux predictions across the range of idealized and image-based scenarios investigated at a fraction of the computational cost of the corresponding full models. The framework presented thus provides a robust and effective computational approach for large scale in-silico studies of pulsatile perivascular fluid flow and transport. ",
    "url": "https://arxiv.org/abs/2111.12451",
    "authors": [
      "C\u00e9cile Daversin-Catty",
      "Ingeborg G. Gjerde",
      "Marie E. Rognes"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2111.12521",
    "title": "Probabilistic Behavioral Distance and Tuning - Reducing and aggregating  complex systems",
    "abstract": "Given a complex system with a given interface to the rest of the world, what does it mean for a the system to behave close to a simpler specification describing the behavior at the interface? We give several definitions for useful notions of distances between a complex system and a specification by combining a behavioral and probabilistic perspective. These distances can be used to tune a complex system to a specification. We show that our approach can successfully tune non-linear networked systems to behave like much smaller networks, allowing us to aggregate large sub-networks into one or two effective nodes. Finally, we discuss similarities and differences between our approach and $H_\\infty$ model reduction. ",
    "url": "https://arxiv.org/abs/2111.12521",
    "authors": [
      "Frank Hellmann",
      "Ekaterina Zolotarevskaia",
      "J\u00fcrgen Kurths",
      "J\u00f6rg Raisch"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ]
  },
  {
    "id": "arXiv:2010.15764",
    "title": "Domain adaptation under structural causal models",
    "abstract": " Comments: 80 pages, 22 figures, accepted in JMLR ",
    "url": "https://arxiv.org/abs/2010.15764",
    "authors": [
      "Yuansi Chen",
      "Peter B\u00fchlmann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2108.02040",
    "title": "Convergence of gradient descent for learning linear neural networks",
    "abstract": " Comments: Minor changes ",
    "url": "https://arxiv.org/abs/2108.02040",
    "authors": [
      "Gabin Maxime Nguegnang",
      "Holger Rauhut",
      "Ulrich Terstiege"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2111.04398",
    "title": "Sub-realtime simulation of a neuronal network of natural density",
    "abstract": " Title: Sub-realtime simulation of a neuronal network of natural density ",
    "url": "https://arxiv.org/abs/2111.04398",
    "authors": [
      "Anno C. Kurth",
      "Johanna Senk",
      "Dennis Terhorst",
      "Justin Finnerty",
      "Markus Diesmann"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2111.06240",
    "title": "Improvements to short-term weather prediction with  recurrent-convolutional networks",
    "abstract": " Comments: 6 pages, 4 figures. Accepted to the session \"Bigdata Cup Challenges: IARAI's Weather4cast Competition\" at IEEE Big Data Conference 2021 ",
    "url": "https://arxiv.org/abs/2111.06240",
    "authors": [
      "Jussi Leinonen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  }
]