[
  {
    "id": "arXiv:2506.15685",
    "title": "Ignition Phase : Standard Training for Fast Adversarial Robustness",
    "abstract": "           Adversarial Training (AT) is a cornerstone defense, but many variants overlook foundational feature representations by primarily focusing on stronger attack generation. We introduce Adversarial Evolution Training (AET), a simple yet powerful framework that strategically prepends an Empirical Risk Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM phase cultivates a favorable feature manifold, enabling more efficient and effective robustness acquisition. Empirically, AET achieves comparable or superior robustness more rapidly, improves clean accuracy, and cuts training costs by 8-25\\%. Its effectiveness is shown across multiple datasets, architectures, and when augmenting established AT methods. Our findings underscore the impact of feature pre-conditioning via standard training for developing more efficient, principled robust defenses. Code is available in the supplementary material.         ",
    "url": "https://arxiv.org/abs/2506.15685",
    "authors": [
      "Wang Yu-Hang",
      "Liu ying",
      "Fang liang",
      "Wang Xuelin",
      "Junkang Guo",
      "Shiwei Li",
      "Lei Gao",
      "Jian Liu",
      "Wenfei Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15688",
    "title": "Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism",
    "abstract": "           Cellular traffic prediction is of great importance for operators to manage network resources and make decisions. Traffic is highly dynamic and influenced by many exogenous factors, which would lead to the degradation of traffic prediction accuracy. This paper proposes an end-to-end framework with two variants to explicitly characterize the spatiotemporal patterns of cellular traffic among neighboring cells. It uses convolutional neural networks with an attention mechanism to capture the spatial dynamics and Kalman filter for temporal modelling. Besides, we can fully exploit the auxiliary information such as social activities to improve prediction performance. We conduct extensive experiments on three real-world datasets. The results show that our proposed models outperform the state-of-the-art machine learning techniques in terms of prediction accuracy.         ",
    "url": "https://arxiv.org/abs/2506.15688",
    "authors": [
      "Hui Ma",
      "Kai Yang",
      "Man-On Pun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15690",
    "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs",
    "abstract": "           The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.         ",
    "url": "https://arxiv.org/abs/2506.15690",
    "authors": [
      "Tianyu Wang",
      "Lingyou Pang",
      "Akira Horiguchi",
      "Carey E. Priebe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2506.15694",
    "title": "Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction",
    "abstract": "           This study introduces a framework that integrates nonlinear feature extraction, classification, and efficient optimization. First, kernel principal component analysis with a radial basis function kernel reduces dimensionality while preserving 95% of the variance. Second, a multilayer perceptron (MLP) learns to predict disease status. Finally, a modified multiprocessing genetic algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten generations. We evaluated this approach on three datasets: the Wisconsin Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100% for chronic kidney disease. These results outperform those of other methods, such as grid search, random search, and Bayesian optimization. Compared with a standard genetic algorithm, kernel PCA revealed nonlinear relationships that improved classification, and the MIGA's parallel fitness evaluations reduced the tuning time by approximately 60%. The genetic algorithm incurs high computational cost from sequential fitness evaluations, but our multiprocessing interface GA (MIGA) parallelizes this step, slashing the tuning time and steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for breast cancer, Parkinson's disease, and CKD, respectively.         ",
    "url": "https://arxiv.org/abs/2506.15694",
    "authors": [
      "Iliyas Ibrahim Iliyas",
      "Souley Boukari",
      "Abdulsalam Yau Gital"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15696",
    "title": "CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction",
    "abstract": "           Survival prediction aims to evaluate the risk level of cancer patients. Existing methods primarily rely on pathology and genomics data, either individually or in combination. From the perspective of cancer pathogenesis, epigenetic changes, such as methylation data, could also be crucial for this task. Furthermore, no previous endeavors have utilized textual descriptions to guide the prediction. To this end, we are the first to explore the use of four modalities, including three clinical modalities and language, for conducting survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT) to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and inter-learning. We encode the clinical data as the raw features, which remain domain-specific knowledge for intra-learning. In terms of inter-learning, we use language to prompt the raw features and introduce an Autoregressive Mutual Traction module for synergistic representation. This tailored framework facilitates joint learning among multiple modalities. Our approach is evaluated across five public cancer datasets, and extensive experiments validate the effectiveness of our methods and proposed designs, leading to producing \\sota results. Codes will be released.         ",
    "url": "https://arxiv.org/abs/2506.15696",
    "authors": [
      "Haipeng Zhou",
      "Sicheng Yang",
      "Sihan Yang",
      "Jing Qin",
      "Lei Chen",
      "Lei Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15698",
    "title": "Global Context-aware Representation Learning for Spatially Resolved Transcriptomics",
    "abstract": "           Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that captures the spatial context of cells within tissues, enabling the study of complex biological networks. Recent graph-based methods leverage both gene expression and spatial information to identify relevant spatial domains. However, these approaches fall short in obtaining meaningful spot representations, especially for spots near spatial domain boundaries, as they heavily emphasize adjacent spots that have minimal feature differences from an anchor node. To address this, we propose Spotscape, a novel framework that introduces the Similarity Telescope module to capture global relationships between multiple spots. Additionally, we propose a similarity scaling strategy to regulate the distances between intra- and inter-slice spots, facilitating effective multi-slice integration. Extensive experiments demonstrate the superiority of Spotscape in various downstream tasks, including single-slice and multi-slice scenarios. Our code is available at the following link: https: //github.com/yunhak0/Spotscape.         ",
    "url": "https://arxiv.org/abs/2506.15698",
    "authors": [
      "Yunhak Oh",
      "Junseok Lee",
      "Yeongmin Kim",
      "Sangwoo Seo",
      "Namkyeong Lee",
      "Chanyoung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15699",
    "title": "BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap",
    "abstract": "           Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present $\\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. $\\texttt{BLUR}$ significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on $\\texttt{BLUR}$, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: this https URL ",
    "url": "https://arxiv.org/abs/2506.15699",
    "authors": [
      "Shengyuan Hu",
      "Neil Kale",
      "Pratiksha Thaker",
      "Yiwei Fu",
      "Steven Wu",
      "Virginia Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15700",
    "title": "Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking",
    "abstract": "           Control contraction metrics (CCMs) provide a framework to co-synthesize a controller and a corresponding contraction metric -- a positive-definite Riemannian metric under which a closed-loop system is guaranteed to be incrementally exponentially stable. However, the synthesized controller only ensures that all the trajectories of the system converge to one single trajectory and, as such, does not impose any notion of optimality across an entire trajectory. Furthermore, constructing CCMs requires a known dynamics model and non-trivial effort in solving an infinite-dimensional convex feasibility problem, which limits its scalability to complex systems featuring high dimensionality with uncertainty. To address these issues, we propose to integrate CCMs into reinforcement learning (RL), where CCMs provide dynamics-informed feedback for learning control policies that minimize cumulative tracking error under unknown dynamics. We show that our algorithm, called contraction actor-critic (CAC), formally enhances the capability of CCMs to provide a set of contracting policies with the long-term optimality of RL in a fully automated setting. Given a pre-trained dynamics model, CAC simultaneously learns a contraction metric generator (CMG) -- which generates a contraction metric -- and uses an actor-critic algorithm to learn an optimal tracking policy guided by that metric. We demonstrate the effectiveness of our algorithm relative to established baselines through extensive empirical studies, including simulated and real-world robot experiments, and provide a theoretical rationale for incorporating contraction theory into RL.         ",
    "url": "https://arxiv.org/abs/2506.15700",
    "authors": [
      "Minjae Cho",
      "Hiroyasu Tsukamoto",
      "Huy Trong Tran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15703",
    "title": "Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance",
    "abstract": "           Federated multi-view clustering has been proposed to mine the valuable information within multi-view data distributed across different devices and has achieved impressive results while preserving the privacy. Despite great progress, most federated multi-view clustering methods only used global pseudo-labels to guide the downstream clustering process and failed to exploit the global information when extracting features. In addition, missing data problem in federated multi-view clustering task is less explored. To address these problems, we propose a novel Federated Incomplete Multi-view Clustering method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a dual-head graph convolutional encoder at each client to extract two kinds of underlying features containing global and view-specific information. Subsequently, under the guidance of the fused graph, the two underlying features are fused into high-level features, based on which clustering is conducted under the supervision of pseudo-labeling. Finally, the high-level features are uploaded to the server to refine the graph fusion and pseudo-labeling computation. Extensive experimental results demonstrate the effectiveness and superiority of FIMCFG. Our code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.15703",
    "authors": [
      "Guoqing Chao",
      "Zhenghao Zhang",
      "Lei Meng",
      "Jie Wen",
      "Dianhui Chu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15708",
    "title": "Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification",
    "abstract": "           Graph neural networks (GNNs) have been developed to model the relationship between regions of interest (ROIs) in brains and have shown significant improvement in detecting brain diseases. However, most of these frameworks do not consider the intrinsic relationship of causality factor between brain ROIs, which is arguably more essential to observe cause and effect interaction between signals rather than typical correlation values. We propose a novel framework called CGB (Causal Graphs for Brains) for brain disease classification/detection, which models refined brain networks based on the causal discovery method, transfer entropy, and geometric curvature strategy. CGB unveils causal relationships between ROIs that bring vital information to enhance brain disease classification performance. Furthermore, CGB also performs a graph rewiring through a geometric curvature strategy to refine the generated causal graph to become more expressive and reduce potential information bottlenecks when GNNs model it. Our extensive experiments show that CGB outperforms state-of-the-art methods in classification tasks on brain disease datasets, as measured by average F1 scores.         ",
    "url": "https://arxiv.org/abs/2506.15708",
    "authors": [
      "Falih Gozi Febrinanto",
      "Adonia Simango",
      "Chengpei Xu",
      "Jingjing Zhou",
      "Jiangang Ma",
      "Sonika Tyagi",
      "Feng Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15709",
    "title": "Studying and Improving Graph Neural Network-based Motif Estimation",
    "abstract": "           Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.         ",
    "url": "https://arxiv.org/abs/2506.15709",
    "authors": [
      "Pedro C. Vieira",
      "Miguel E. P. Silva",
      "Pedro Manuel Pinto Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15711",
    "title": "Shadow defense against gradient inversion attack in federated learning",
    "abstract": "           Federated learning (FL) has emerged as a transformative framework for privacy-preserving distributed training, allowing clients to collaboratively train a global model without sharing their local data. This is especially crucial in sensitive fields like healthcare, where protecting patient data is paramount. However, privacy leakage remains a critical challenge, as the communication of model updates can be exploited by potential adversaries. Gradient inversion attacks (GIAs), for instance, allow adversaries to approximate the gradients used for training and reconstruct training images, thus stealing patient privacy. Existing defense mechanisms obscure gradients, yet lack a nuanced understanding of which gradients or types of image information are most vulnerable to such attacks. These indiscriminate calibrated perturbations result in either excessive privacy protection degrading model accuracy, or insufficient one failing to safeguard sensitive information. Therefore, we introduce a framework that addresses these challenges by leveraging a shadow model with interpretability for identifying sensitive areas. This enables a more targeted and sample-specific noise injection. Specially, our defensive strategy achieves discrepancies of 3.73 in PSNR and 0.2 in SSIM compared to the circumstance without defense on the ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover, it minimizes adverse effects on model performance, with less than 1\\% F1 reduction compared to SOTA methods. Our extensive experiments, conducted across diverse types of medical images, validate the generalization of the proposed framework. The stable defense improvements for FedAvg are consistently over 1.5\\% times in LPIPS and SSIM. It also offers a universal defense against various GIA types, especially for these sensitive areas in images.         ",
    "url": "https://arxiv.org/abs/2506.15711",
    "authors": [
      "Le Jiang",
      "Liyan Ma",
      "Guang Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15712",
    "title": "BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling",
    "abstract": "           Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.         ",
    "url": "https://arxiv.org/abs/2506.15712",
    "authors": [
      "Songqi Zhou",
      "Ruixue Liu",
      "Yixing Wang",
      "Jia Lu",
      "Benben Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15713",
    "title": "An application of machine learning to the motion response prediction of floating assets",
    "abstract": "           The real-time prediction of floating offshore asset behavior under stochastic metocean conditions remains a significant challenge in offshore engineering. While traditional empirical and frequency-domain methods work well in benign conditions, they struggle with both extreme sea states and nonlinear responses. This study presents a supervised machine learning approach using multivariate regression to predict the nonlinear motion response of a turret-moored vessel in 400 m water depth. We developed a machine learning workflow combining a gradient-boosted ensemble method with a custom passive weathervaning solver, trained on approximately $10^6$ samples spanning 100 features. The model achieved mean prediction errors of less than 5% for critical mooring parameters and vessel heading accuracy to within 2.5 degrees across diverse metocean conditions, significantly outperforming traditional frequency-domain methods. The framework has been successfully deployed on an operational facility, demonstrating its efficacy for real-time vessel monitoring and operational decision-making in offshore environments.         ",
    "url": "https://arxiv.org/abs/2506.15713",
    "authors": [
      "Michael T.M.B. Morris-Thomas",
      "Marius Martens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2506.15719",
    "title": "Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems",
    "abstract": "           Heat pumps (HPs) have emerged as a cost-effective and clean technology for sustainable energy systems, but their efficiency in producing hot water remains restricted by conventional threshold-based control methods. Although machine learning (ML) has been successfully implemented for various HP applications, optimization of household hot water demand forecasting remains understudied. This paper addresses this problem by introducing a novel approach that combines predictive ML with anomaly detection to create adaptive hot water production strategies based on household-specific consumption patterns. Our key contributions include: (1) a composite approach combining ML and isolation forest (iForest) to forecast household demand for hot water and steer responsive HP operations; (2) multi-step feature selection with advanced time-series analysis to capture complex usage patterns; (3) application and tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention mechanism on data from different types of real HP installations; and (4) experimental validation on six real household installations. Our experiments show that the best-performing model LightGBM achieves superior performance, with RMSE improvements of up to 9.37\\% compared to LSTM variants with $R^2$ values between 0.748-0.983. For anomaly detection, our iForest implementation achieved an F1-score of 0.87 with a false alarm rate of only 5.2\\%, demonstrating strong generalization capabilities across different household types and consumption patterns, making it suitable for real-world HP deployments.         ",
    "url": "https://arxiv.org/abs/2506.15719",
    "authors": [
      "Manal Rahal",
      "Bestoun S. Ahmed",
      "Roger Renstrom",
      "Robert Stener",
      "Albrecht Wurtz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15721",
    "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration",
    "abstract": "           Heterogeneous Large Language Model (LLM) fusion integrates the strengths of multiple source LLMs with different architectures into a target LLM with low computational overhead. While promising, existing methods suffer from two major limitations: 1) reliance on real data from limited domain for knowledge fusion, preventing the target LLM from fully acquiring knowledge across diverse domains, and 2) fixed data allocation proportions across domains, failing to dynamically adjust according to the target LLM's varying capabilities across domains, leading to a capability imbalance. To overcome these limitations, we propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework. Through the organization of knowledge domains into a hierarchical tree structure, Bohdi enables automatic domain exploration and multi-domain data generation through multi-model collaboration, thereby comprehensively extracting knowledge from source LLMs. By formalizing domain expansion and data sampling proportion allocation on the knowledge tree as a Hierarchical Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism to adaptively adjust sampling proportions based on the target LLM's performance feedback across domains. Integrated with our proposed Introspection-Rebirth (IR) mechanism, DynaBranches dynamically tracks capability shifts during target LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT), further enhancing its online adaptation capability. Comparative experimental results on a comprehensive suite of benchmarks demonstrate that Bohdi significantly outperforms existing baselines on multiple target LLMs, exhibits higher data efficiency, and virtually eliminates the imbalance in the target LLM's capabilities. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.15721",
    "authors": [
      "Junqi Gao",
      "Zhichang Guo",
      "Dazhi Zhang",
      "Dong Li",
      "Runze Liu",
      "Pengfei Li",
      "Kai Tian",
      "Biqing Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15725",
    "title": "Graph Diffusion that can Insert and Delete",
    "abstract": "           Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.         ",
    "url": "https://arxiv.org/abs/2506.15725",
    "authors": [
      "Matteo Ninniri",
      "Marco Podda",
      "Davide Bacciu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15737",
    "title": "A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture",
    "abstract": "           Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent (SGD) frequently encounters difficulties, including substantial computing expense and the risk of converging to local optima, attributable to its dependence on partial weight gradients. Therefore, this work investigates Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve local search efficiency. The findings indicate that the hybrid PSO-SGD technique decreases the median training MSE by 90 to 95 percent relative to conventional GA and PSO across various network sizes (e.g., from around 0.02 to approximately 0.001 in the Sphere function). RMHC attains substantial enhancements, reducing MSE by roughly 85 to 90 percent compared to GA. Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying subpar performance. These findings underscore that hybrid and evolutionary procedures significantly improve training efficiency and accuracy compared to conventional optimization methods and imply that the Building Block Hypothesis (BBH) may still be valid, indicating that advantageous weight structures are retained during evolutionary search.         ",
    "url": "https://arxiv.org/abs/2506.15737",
    "authors": [
      "Gautam Siddharth Kashyap",
      "Md Tabrez Nafis",
      "Samar Wazir"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15746",
    "title": "Neural Cellular Automata for ARC-AGI",
    "abstract": "           Cellular automata and their differentiable counterparts, Neural Cellular Automata (NCA), are highly expressive and capable of surprisingly complex behaviors. This paper explores how NCAs perform when applied to tasks requiring precise transformations and few-shot generalization, using the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that challenges their capabilities in ways not previously explored. Specifically, this paper uses gradient-based training to learn iterative update rules that transform input grids into their outputs from the training examples and apply them to the test inputs. Results suggest that gradient-trained NCA models are a promising and efficient approach to a range of abstract grid-based tasks from ARC. Along with discussing the impacts of various design modifications and training constraints, this work examines the behavior and properties of NCAs applied to ARC to give insights for broader applications of self-organizing systems.         ",
    "url": "https://arxiv.org/abs/2506.15746",
    "authors": [
      "Kevin Xu",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2506.15755",
    "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service",
    "abstract": "           Vision-Language Models (VLMs) have demonstrated great potential in real-world applications. While existing research primarily focuses on improving their accuracy, the efficiency remains underexplored. Given the real-time demands of many applications and the high inference overhead of VLMs, efficiency robustness is a critical issue. However, previous studies evaluate efficiency robustness under unrealistic assumptions, requiring access to the model architecture and parameters -- an impractical scenario in ML-as-a-service settings, where VLMs are deployed via inference APIs. To address this gap, we propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness in a realistic black-box setting. VLMInferSlow incorporates fine-grained efficiency modeling tailored to VLM inference and leverages zero-order optimization to search for adversarial examples. Experimental results show that VLMInferSlow generates adversarial images with imperceptible perturbations, increasing the computational cost by up to 128.47%. We hope this research raises the community's awareness about the efficiency robustness of VLMs.         ",
    "url": "https://arxiv.org/abs/2506.15755",
    "authors": [
      "Xiasi Wang",
      "Tianliang Yao",
      "Simin Chen",
      "Runqi Wang",
      "Lei YE",
      "Kuofeng Gao",
      "Yi Huang",
      "Yuan Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15758",
    "title": "Linear-Time Primitives for Algorithm Development in Graphical Causal Inference",
    "abstract": "           We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.         ",
    "url": "https://arxiv.org/abs/2506.15758",
    "authors": [
      "Marcel Wien\u00f6bst",
      "Sebastian Weichwald",
      "Leonard Henckel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.15788",
    "title": "Robust control for multi-legged elongate robots in noisy environments",
    "abstract": "           Modern two and four legged robots exhibit impressive mobility on complex terrain, largely attributed to advancement in learning algorithms. However, these systems often rely on high-bandwidth sensing and onboard computation to perceive/respond to terrain uncertainties. Further, current locomotion strategies typically require extensive robot-specific training, limiting their generalizability across platforms. Building on our prior research connecting robot-environment interaction and communication theory, we develop a new paradigm to construct robust and simply controlled multi-legged elongate robots (MERs) capable of operating effectively in cluttered, unstructured environments. In this framework, each leg-ground contact is thought of as a basic active contact (bac), akin to bits in signal transmission. Reliable locomotion can be achieved in open-loop on \"noisy\" landscapes via sufficient redundancy in bacs. In such situations, robustness is achieved through passive mechanical responses. We term such processes as those displaying mechanical intelligence (MI) and analogize these processes to forward error correction (FEC) in signal transmission. To augment MI, we develop feedback control schemes, which we refer to as computational intelligence (CI) and such processes analogize automatic repeat request (ARQ) in signal transmission. Integration of these analogies between locomotion and communication theory allow analysis, design, and prediction of embodied intelligence control schemes (integrating MI and CI) in MERs, showing effective and reliable performance (approximately half body lengths per cycle) on complex landscapes with terrain \"noise\" over twice the robot's height. Our work provides a foundation for systematic development of MER control, paving the way for terrain-agnostic, agile, and resilient robotic systems capable of operating in extreme environments.         ",
    "url": "https://arxiv.org/abs/2506.15788",
    "authors": [
      "Baxi Chong",
      "Juntao He",
      "Daniel Irvine",
      "Tianyu Wang",
      "Esteban Flores",
      "Daniel Soto",
      "Jianfeng Lin",
      "Zhaochen Xu",
      "Vincent R Nienhusser",
      "Grigoriy Blekherman",
      "Daniel I. Goldman"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.15790",
    "title": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis",
    "abstract": "           With the advance application of blockchain technology in various fields, ensuring the security and stability of smart contracts has emerged as a critical challenge. Current security analysis methodologies in vulnerability detection can be categorized into static analysis and dynamic analysis this http URL, these existing traditional vulnerability detection methods predominantly rely on analyzing original contract code, not all smart contracts provide accessible this http URL present ETrace, a novel event-driven vulnerability detection framework for smart contracts, which uniquely identifies potential vulnerabilities through LLM-powered trace analysis without requiring source code access. By extracting fine-grained event sequences from transaction logs, the framework leverages Large Language Models (LLMs) as adaptive semantic interpreters to reconstruct event analysis through chain-of-thought reasoning. ETrace implements pattern-matching to establish causal links between transaction behavior patterns and known attack behaviors. Furthermore, we validate the effectiveness of ETrace through preliminary experimental results.         ",
    "url": "https://arxiv.org/abs/2506.15790",
    "authors": [
      "Chenyang Peng",
      "Haijun Wang",
      "Yin Wu",
      "Hao Wu",
      "Ming Fan",
      "Yitao Zhao",
      "Ting Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.15792",
    "title": "Descriptor-based Foundation Models for Molecular Property Prediction",
    "abstract": "           Fast and accurate prediction of molecular properties with machine learning is pivotal to scientific advancements across myriad domains. Foundation models in particular have proven especially effective, enabling accurate training on small, real-world datasets. This study introduces CheMeleon, a novel molecular foundation model pre-trained on deterministic molecular descriptors from the Mordred package, leveraging a Directed Message-Passing Neural Network to predict these descriptors in a noise-free setting. Unlike conventional approaches relying on noisy experimental data or biased quantum mechanical simulations, CheMeleon uses low-noise molecular descriptors to learn rich molecular representations. Evaluated on 58 benchmark datasets from Polaris and MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks, outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop (36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%) and other foundation models. However, it struggles to distinguish activity cliffs like many of the tested models. The t-SNE projection of CheMeleon's learned representations demonstrates effective separation of chemical series, highlighting its ability to capture structural nuances. These results underscore the potential of descriptor-based pre-training for scalable and effective molecular property prediction, opening avenues for further exploration of descriptor sets and unlabeled datasets.         ",
    "url": "https://arxiv.org/abs/2506.15792",
    "authors": [
      "Jackson Burns",
      "Akshat Zalte",
      "William Green"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ]
  },
  {
    "id": "arXiv:2506.15809",
    "title": "DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling",
    "abstract": "           In recent years, graph learning has gained significant interest for modeling complex interactions among medical events in structured Electronic Health Record (EHR) data. However, existing graph-based approaches often work in a static manner, either restricting interactions within individual encounters or collapsing all historical encounters into a single snapshot. As a result, when it is necessary to identify meaningful groups of medical events spanning longitudinal encounters, existing methods are inadequate in modeling interactions cross encounters while accounting for temporal dependencies. To address this limitation, we introduce Deep Patient Journey (DeepJ), a novel graph convolutional transformer model with differentiable graph pooling to effectively capture intra-encounter and inter-encounter medical event interactions. DeepJ can identify groups of temporally and functionally related medical events, offering valuable insights into key event clusters pertinent to patient outcome prediction. DeepJ significantly outperformed five state-of-the-art baseline models while enhancing interpretability, demonstrating its potential for improved patient risk stratification.         ",
    "url": "https://arxiv.org/abs/2506.15809",
    "authors": [
      "Deyi Li",
      "Zijun Yao",
      "Muxuan Liang",
      "Mei Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15815",
    "title": "GratNet: A Photorealistic Neural Shader for Diffractive Surfaces",
    "abstract": "           Structural coloration is commonly modeled using wave optics for reliable and photorealistic rendering of natural, quasi-periodic and complex nanostructures. Such models often rely on dense, preliminary or preprocessed data to accurately capture the nuanced variations in diffractive surface reflectances. This heavy data dependency warrants implicit neural representation which has not been addressed comprehensively in the current literature. In this paper, we present a multi-layer perceptron (MLP) based method for data-driven rendering of diffractive surfaces with high accuracy and efficiency. We primarily approach this problem from a data compression perspective to devise a nuanced training and modeling method which is attuned to the domain and range characteristics of diffractive reflectance datasets. Importantly, our approach avoids over-fitting and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR), Structural Similarity Index Measure (SSIM) and a flipping difference evaluator (FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of the ground-truth. In comparison to a recent state-of-the-art offline, wave-optical, forward modeling approach, our method reproduces subjectively similar results with significant performance gains. We reduce the memory footprint of the raw datasets by two orders of magnitude in general. Lastly, we depict the working of our method with actual surface renderings.         ",
    "url": "https://arxiv.org/abs/2506.15815",
    "authors": [
      "Narayan Kandel",
      "Daljit Singh J.S. Dhillon"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15825",
    "title": "Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters",
    "abstract": "           In this paper, we first propose a novel algorithm for model fusion that leverages Wasserstein barycenters in training a global Deep Neural Network (DNN) in a distributed architecture. To this end, we divide the dataset into equal parts that are fed to \"agents\" who have identical deep neural networks and train only over the dataset fed to them (known as the local dataset). After some training iterations, we perform an aggregation step where we combine the weight parameters of all neural networks using Wasserstein barycenters. These steps form the proposed algorithm referred to as FedWB. Moreover, we leverage the processes created in the first part of the paper to develop an algorithm to tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test experiment is the CartPole toy problem, where we vary the lengths of the poles to create heterogeneous environments. We train a deep Q-Network (DQN) in each environment to learn to control each cart, while occasionally performing a global aggregation step to generalize the local models; the end outcome is a global DQN that functions across all environments.         ",
    "url": "https://arxiv.org/abs/2506.15825",
    "authors": [
      "Luiz Pereira",
      "M. Hadi Amini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15831",
    "title": "Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report",
    "abstract": "           Data changes to reflect evolving user behaviour, preferences, and changes in the environment. Such changes may occur due to expected shifts in the data distribution, i.e., concept drift, or unexpected anomalous changes. The presence of concept drift poses challenges for anomaly detection in time series. While anomalies are caused by undesirable changes in the data, differentiating abnormal changes from varying normal behaviours is difficult due to differing frequencies of occurrence, varying time intervals when normal patterns occur. Differentiating between concept drift and anomalies is critical for accurate analysis as studies have shown that the compounding effects of error propagation in downstream data analysis tasks lead to lower detection accuracy and increased overhead due to unnecessary model updates. Unfortunately, existing work has largely explored anomaly detection and concept drift detection in isolation. We develop AnDri, a system for Anomaly detection in the presence of Drift, which adjusts the normal patterns temporally, and distinguish abnormal subsequences and new concepts. Moreover, it introduces a new clustering method, Adjacent Hierarchical Clustering (AHC), which groups similar subsequences while respecting their temporal locality.         ",
    "url": "https://arxiv.org/abs/2506.15831",
    "authors": [
      "Jongjun Park",
      "Fei Chiang",
      "Mostafa Milani"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2506.15836",
    "title": "Code Rate Optimization via Neural Polar Decoders",
    "abstract": "           This paper proposes a method to optimize communication code rates via the application of neural polar decoders (NPDs). Employing this approach enables simultaneous optimization of code rates over input distributions while providing a practical coding scheme within the framework of polar codes. The proposed approach is designed for scenarios where the channel model is unknown, treating the channel as a black box that produces output samples from input samples. We employ polar codes to achieve our objectives, using NPDs to estimate mutual information (MI) between the channel inputs and outputs, and optimize a parametric model of the input distribution. The methodology involves a two-phase process: a training phase and an inference phase. In the training phase, two steps are repeated interchangeably. First, the estimation step estimates the MI of the channel inputs and outputs via NPDs. Second, the improvement step optimizes the input distribution parameters to maximize the MI estimate obtained by the NPDs. In the inference phase, the optimized model is used to construct polar codes. This involves incorporating the Honda-Yamamoto (HY) scheme to accommodate the optimized input distributions and list decoding to enhance decoding performance. Experimental results on memoryless and finite-state channels (FSCs) demonstrate the effectiveness of our approach, particularly in cases where the channel's capacity-achieving input distribution is non-uniform. For these cases, we show significant improvements in MI and bit error rates (BERs) over those achieved by uniform and independent and identically distributed (i.i.d.) input distributions, validating our method for block lengths up to 1024. This scalable approach has potential applications in real-world communication systems, bridging theoretical capacity estimation and practical coding performance.         ",
    "url": "https://arxiv.org/abs/2506.15836",
    "authors": [
      "Ziv Aharoni",
      "Bashar Huleihel",
      "Henry D Pfister",
      "Haim H Permuter"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15837",
    "title": "ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions",
    "abstract": "           Adverse weather conditions, particularly fog, pose a significant challenge to autonomous vehicles, surveillance systems, and other safety-critical applications by severely degrading visual information. We introduce ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly optimizes image restoration and object detection under varying fog intensities. A lightweight Haze Density Estimation Network (HDEN) classifies each input as light, medium, or heavy fog. Based on this score, the system dynamically routes the image through one of three CORUN branches: Light, Medium, or Complex, each tailored to its haze regime. A novel adaptive loss balances physical-model coherence and perceptual fidelity, ensuring both accurate defogging and preservation of fine details. On Cityscapes and the real-world RTTS benchmark, ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and increases object detection mAP by up to 13 points, while cutting inference time by 20 percent. These results highlight the importance of intensity-specific processing and seamless integration with downstream vision tasks. Code available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2506.15837",
    "authors": [
      "Fatmah AlHindaassi",
      "Mohammed Talha Alam",
      "Fakhri Karray"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15839",
    "title": "Link Priority Buffer-Aided Relay Selection with Energy Storage from Energy Harvest",
    "abstract": "           This paper proposes a novel relay selection scheme for buffer-aided wireless networks with relays equipped with both data buffers and energy storage. While buffer-aided relay networks have demonstrated significantly improved performance, energy harvesting has become an attractive solution in many wireless systems, garnering considerable attention when applied to buffer-aided relay networks. It is known that state-dependent selection rules must be used to achieve full diversity order in buffer-aided relay networks, requiring link priorities for data transmission to be set based on system states. This task becomes challenging when both data buffers and energy storage are involved. In this paper, we introduce a novel method for setting link priorities, which forms the basis for a new selection rule. The outage probability of the proposed selection scheme is derived. The simulation results demonstrate the superiority of our proposed algorithm which achieves full diversity in buffer-aided relay selection with energy storage, and consistently outperforms baseline approaches across various metrics.         ",
    "url": "https://arxiv.org/abs/2506.15839",
    "authors": [
      "Mohammad Alkhawatrah",
      "Yu Gong",
      "Chong Huang",
      "Gaojie Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2506.15850",
    "title": "Uncertainty Estimation by Human Perception versus Neural Models",
    "abstract": "           Modern neural networks (NNs) often achieve high predictive accuracy but remain poorly calibrated, producing overconfident predictions even when wrong. This miscalibration poses serious challenges in applications where reliable uncertainty estimates are critical. In this work, we investigate how human perceptual uncertainty compares to uncertainty estimated by NNs. Using three vision benchmarks annotated with both human disagreement and crowdsourced confidence, we assess the correlation between model-predicted uncertainty and human-perceived uncertainty. Our results show that current methods only weakly align with human intuition, with correlations varying significantly across tasks and uncertainty metrics. Notably, we find that incorporating human-derived soft labels into the training process can improve calibration without compromising accuracy. These findings reveal a persistent gap between model and human uncertainty and highlight the potential of leveraging human insights to guide the development of more trustworthy AI systems.         ",
    "url": "https://arxiv.org/abs/2506.15850",
    "authors": [
      "Pedro Mendes",
      "Paolo Romano",
      "David Garlan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15860",
    "title": "User-Guided Force-Directed Graph Layout",
    "abstract": "           Visual analysis of relational data is essential for many real-world analytics tasks, with layout quality being key to interpretability. However, existing layout algorithms often require users to navigate complex parameters to express their intent. We present a user-guided force-directed layout approach that enables intuitive control through freehand sketching. Our method uses classical image analysis techniques to extract structural information from sketches, which is then used to generate positional constraints that guide the layout process. We evaluate the approach on various real and synthetic graphs ranging from small to medium scale, demonstrating its ability to produce layouts aligned with user expectations. An implementation of our method along with documentation and a demo page is freely available on GitHub at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.15860",
    "authors": [
      "Hasan Balci",
      "Augustin Luna"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2506.15866",
    "title": "Understanding Online Polarization Through Human-Agent Interaction in a Synthetic LLM-Based Social Network",
    "abstract": "           The rise of social media has fundamentally transformed how people engage in public discourse and form opinions. While these platforms offer unprecedented opportunities for democratic engagement, they have been implicated in increasing social polarization and the formation of ideological echo chambers. Previous research has primarily relied on observational studies of social media data or theoretical modeling approaches, leaving a significant gap in our understanding of how individuals respond to and are influenced by polarized online environments. Here we present a novel experimental framework for investigating polarization dynamics that allows human users to interact with LLM-based artificial agents in a controlled social network simulation. Through a user study with 122 participants, we demonstrate that this approach can successfully reproduce key characteristics of polarized online discourse while enabling precise manipulation of environmental factors. Our results provide empirical validation of theoretical predictions about online polarization, showing that polarized environments significantly increase perceived emotionality and group identity salience while reducing expressed uncertainty. These findings extend previous observational and theoretical work by providing causal evidence for how specific features of online environments influence user perceptions and behaviors. More broadly, this research introduces a powerful new methodology for studying social media dynamics, offering researchers unprecedented control over experimental conditions while maintaining ecological validity.         ",
    "url": "https://arxiv.org/abs/2506.15866",
    "authors": [
      "Tim Donkers",
      "J\u00fcrgen Ziegler"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2506.15868",
    "title": "CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction",
    "abstract": "           Risk quantification is a critical component of safe autonomous driving, however, constrained by the limited perception range and occlusion of single-vehicle systems in complex and dense scenarios. Vehicle-to-everything (V2X) paradigm has been a promising solution to sharing complementary perception information, nevertheless, how to ensure the risk interpretability while understanding multi-agent interaction with V2X remains an open question. In this paper, we introduce the first V2X-enabled risk quantification pipeline, CooperRisk, to fuse perception information from multiple agents and quantify the scenario driving risk in future multiple timestamps. The risk is represented as a scenario risk map to ensure interpretability based on risk severity and exposure, and the multi-agent interaction is captured by the learning-based cooperative prediction model. We carefully design a risk-oriented transformer-based prediction model with multi-modality and multi-agent considerations. It aims to ensure scene-consistent future behaviors of multiple agents and avoid conflicting predictions that could lead to overly conservative risk quantification and cause the ego vehicle to become overly hesitant to drive. Then, the temporal risk maps could serve to guide a model predictive control planner. We evaluate the CooperRisk pipeline in a real-world V2X dataset V2XPnP, and the experiments demonstrate its superior performance in risk quantification, showing a 44.35% decrease in conflict rate between the ego vehicle and background traffic participants.         ",
    "url": "https://arxiv.org/abs/2506.15868",
    "authors": [
      "Mingyue Lei",
      "Zewei Zhou",
      "Hongchen Li",
      "Jia Hu",
      "Jiaqi Ma"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.15879",
    "title": "Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings",
    "abstract": "           This paper presents a machine learning methodology prototype using a large synthetic dataset of job listings to identify trends, predict salaries, and group similar job roles. Employing techniques such as regression, classification, clustering, and natural language processing (NLP) for text-based feature extraction and representation, this study aims to uncover the key features influencing job market dynamics and provide valuable insights for job seekers, employers, and researchers. Exploratory data analysis was conducted to understand the dataset's characteristics. Subsequently, regression models were developed to predict salaries, classification models to predict job titles, and clustering techniques were applied to group similar jobs. The analyses revealed significant factors influencing salary and job roles, and identified distinct job clusters based on the provided data. While the results are based on synthetic data and not intended for real-world deployment, the methodology demonstrates a transferable framework for job market analysis.         ",
    "url": "https://arxiv.org/abs/2506.15879",
    "authors": [
      "Abdel Rahman Alsheyab",
      "Mohammad Alkhasawneh",
      "Nidal Shahin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15884",
    "title": "How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?",
    "abstract": "           Community smells reflect poor organizational practices that often lead to socio-technical issues and the accumulation of Self-Admitted Technical Debt (SATD). While prior studies have explored these problems in general software systems, their interplay in machine learning (ML)-based projects remains largely underexamined. In this study, we investigated the prevalence of community smells and their relationship with SATD in open-source ML projects, analyzing data at the release level. First, we examined the prevalence of ten community smell types across the releases of 155 ML-based systems and found that community smells are widespread, exhibiting distinct distribution patterns across small, medium, and large projects. Second, we detected SATD at the release level and applied statistical analysis to examine its correlation with community smells. Our results showed that certain smells, such as Radio Silence and Organizational Silos, are strongly correlated with higher SATD occurrences. Third, we considered the six identified types of SATD to determine which community smells are most associated with each debt category. Our analysis revealed authority- and communication-related smells often co-occur with persistent code and design debt. Finally, we analyzed how the community smells and SATD evolve over the releases, uncovering project size-dependent trends and shared trajectories. Our findings emphasize the importance of early detection and mitigation of socio-technical issues to maintain the long-term quality and sustainability of ML-based systems.         ",
    "url": "https://arxiv.org/abs/2506.15884",
    "authors": [
      "Shamse Tasnim Cynthia",
      "Nuri Almarimi",
      "Banani Roy"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.15887",
    "title": "Fair Contracts in Principal-Agent Games with Heterogeneous Types",
    "abstract": "           Fairness is desirable yet challenging to achieve within multi-agent systems, especially when agents differ in latent traits that affect their abilities. This hidden heterogeneity often leads to unequal distributions of wealth, even when agents operate under the same rules. Motivated by real-world examples, we propose a framework based on repeated principal-agent games, where a principal, who also can be seen as a player of the game, learns to offer adaptive contracts to agents. By leveraging a simple yet powerful contract structure, we show that a fairness-aware principal can learn homogeneous linear contracts that equalize outcomes across agents in a sequential social dilemma. Importantly, this fairness does not come at the cost of efficiency: our results demonstrate that it is possible to promote equity and stability in the system while preserving overall performance.         ",
    "url": "https://arxiv.org/abs/2506.15887",
    "authors": [
      "Jakub T\u0142uczek",
      "Victor Villin",
      "Christos Dimitrakakis"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2506.15896",
    "title": "KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction",
    "abstract": "           Precision soil greenhouse gas (GHG) flux prediction is essential in agricultural systems for assessing environmental impacts, developing emission mitigation strategies and promoting sustainable agriculture. Due to the lack of advanced sensor and network technologies on majority of farms, there are challenges in obtaining comprehensive and diverse agricultural data. As a result, the scarcity of agricultural data seriously obstructs the application of machine learning approaches in precision soil GHG flux prediction. This research proposes a knowledge-guided graph neural network framework that addresses the above challenges by integrating knowledge embedded in an agricultural process-based model and graph neural network techniques. Specifically, we utilise the agricultural process-based model to simulate and generate multi-dimensional agricultural datasets for 47 countries that cover a wide range of agricultural variables. To extract key agricultural features and integrate correlations among agricultural features in the prediction process, we propose a machine learning framework that integrates the autoencoder and multi-target multi-graph based graph neural networks, which utilises the autoencoder to selectively extract significant agricultural features from the agricultural process-based model simulation data and the graph neural network to integrate correlations among agricultural features for accurately predict fertilisation-oriented soil GHG fluxes. Comprehensive experiments were conducted with both the agricultural simulation dataset and real-world agricultural dataset to evaluate the proposed approach in comparison with well-known baseline and state-of-the-art regression methods. The results demonstrate that our proposed approach provides superior accuracy and stability in fertilisation-oriented soil GHG prediction.         ",
    "url": "https://arxiv.org/abs/2506.15896",
    "authors": [
      "Yu Zhang",
      "Gaoshan Bi",
      "Simon Jeffery",
      "Max Davis",
      "Yang Li",
      "Qing Xue",
      "Po Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15898",
    "title": "TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation",
    "abstract": "           With the proliferation of location-tracking technologies, massive volumes of trajectory data are continuously being collected. As a fundamental task in trajectory data mining, trajectory similarity computation plays a critical role in a wide range of real-world applications. However, existing learning-based methods face three challenges: First, they ignore the semantic gap between GPS and grid features in trajectories, making it difficult to obtain meaningful trajectory embeddings. Second, the noise inherent in the trajectories, as well as the noise introduced during grid discretization, obscures the true motion patterns of the trajectories. Third, existing methods focus solely on point-wise and pair-wise losses, without utilizing the global ranking information obtained by sorting all trajectories according to their similarity to a given trajectory. To address the aforementioned challenges, we propose a novel trajectory similarity computation framework, named TrajDiff. Specifically, the semantic alignment module relies on cross-attention and an attention score mask mechanism with adaptive fusion, effectively eliminating semantic discrepancies between data at two scales and generating a unified representation. Additionally, the DDBM-based Noise-robust Pre-Training introduces the transfer patterns between any two trajectories into the model training process, enhancing the model's noise robustness. Finally, the overall ranking-aware regularization shifts the model's focus from a local to a global perspective, enabling it to capture the holistic ordering information among trajectories. Extensive experiments on three publicly available datasets show that TrajDiff consistently outperforms state-of-the-art baselines. In particular, it achieves an average HR@1 gain of 33.38% across all three evaluation metrics and datasets.         ",
    "url": "https://arxiv.org/abs/2506.15898",
    "authors": [
      "Xiao Zhang",
      "Xingyu Zhao",
      "Hong Xia",
      "Yuan Cao",
      "Guiyuan Jiang",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15901",
    "title": "Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach",
    "abstract": "           Background: Patients with both diabetes mellitus (DM) and atrial fibrillation (AF) face elevated mortality in intensive care units (ICUs), yet models targeting this high-risk group remain limited. Objective: To develop an interpretable machine learning (ML) model predicting 28-day mortality in ICU patients with concurrent DM and AF using early-phase clinical data. Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF was extracted from the MIMIC-IV database. Data preprocessing involved median/mode imputation, z-score normalization, and early temporal feature engineering. A two-step feature selection pipeline-univariate filtering (ANOVA F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable features. Seven ML models were trained with stratified 5-fold cross-validation and SMOTE oversampling. Interpretability was assessed via ablation and Accumulated Local Effects (ALE) analysis. Results: Logistic regression achieved the best performance (AUROC: 0.825; 95% CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS, age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects such as age-related risk acceleration and bilirubin thresholds. Conclusion: This interpretable ML model offers accurate risk prediction and clinical insights for early ICU triage in patients with DM and AF.         ",
    "url": "https://arxiv.org/abs/2506.15901",
    "authors": [
      "Li Sun",
      "Shuheng Chen",
      "Yong Si",
      "Junyi Fan",
      "Maryam Pishgar",
      "Elham Pishgar",
      "Kamiar Alaei",
      "Greg Placencia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15907",
    "title": "Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI",
    "abstract": "           Accurate graph similarity is critical for knowledge transfer in VLSI design, enabling the reuse of prior solutions to reduce engineering effort and turnaround time. We propose Pieceformer, a scalable, self-supervised similarity assessment framework, equipped with a hybrid message-passing and graph transformer encoder. To address transformer scalability, we incorporate a linear transformer backbone and introduce a partitioned training pipeline for efficient memory and parallelism management. Evaluations on synthetic and real-world CircuitNet datasets show that Pieceformer reduces mean absolute error (MAE) by 24.9% over the baseline and is the only method to correctly cluster all real-world design groups. We further demonstrate the practical usage of our model through a case study on a partitioning task, achieving up to 89% runtime reduction. These results validate the framework's effectiveness for scalable, unbiased design reuse in modern VLSI systems.         ",
    "url": "https://arxiv.org/abs/2506.15907",
    "authors": [
      "Hang Yang",
      "Yusheng Hu",
      "Yong Liu",
      "Cong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.15912",
    "title": "Early Attentive Sparsification Accelerates Neural Speech Transcription",
    "abstract": "           Transformer-based neural speech processing has achieved state-of-the-art performance. Since speech audio signals are known to be highly compressible, here we seek to accelerate neural speech transcription by time-domain signal sparsification early in the neural encoding stage, taking advantage of the interpretability of the self-attention mechanism in transformer audio encoders. With the Whisper family of models, we perform a systematic architecture search over the joint space of sparsification stage (a certain encoder layer) and compression ratio (sparsity). We found that the best resulting solutions under 1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration in English speech transcription tasks on Nvidia GPUs without any fine-tuning.         ",
    "url": "https://arxiv.org/abs/2506.15912",
    "authors": [
      "Zifei Xu",
      "Sayeh Sharify",
      "Hesham Mostafa",
      "Tristan Webb",
      "Wanzin Yazar",
      "Xin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.15924",
    "title": "FARFETCH'D: A Side-Channel Analysis Framework for Privacy Applications on Confidential Virtual Machines",
    "abstract": "           Confidential virtual machines (CVMs) based on trusted execution environments (TEEs) enable new privacy-preserving solutions. Yet, they leave side-channel leakage outside their threat model, shifting the responsibility of mitigating such attacks to developers. However, mitigations are either not generic or too slow for practical use, and developers currently lack a systematic, efficient way to measure and compare leakage across real-world deployments. In this paper, we present FARFETCH'D, an open-source toolkit that offers configurable side-channel tracing primitives on production AMD SEV-SNP hardware and couples them with statistical and machine-learning-based analysis pipelines for automated leakage estimation. We apply FARFETCH'D to three representative workloads that are deployed on CVMs to enhance user privacy - private information retrieval, private heavy hitters, and Wasm user-defined functions - and uncover previously unnoticed leaks, including a covert channel that exfiltrated data at 497 kbit/s. The results show that FARFETCH'D pinpoints vulnerabilities and guides low-overhead mitigations based on oblivious memory and differential privacy, giving practitioners a practical path to deploy CVMs with meaningful confidentiality guarantees.         ",
    "url": "https://arxiv.org/abs/2506.15924",
    "authors": [
      "Ruiyi Zhang",
      "Albert Cheu",
      "Adria Gascon",
      "Daniel Moghimi",
      "Phillipp Schoppmann",
      "Michael Schwarz",
      "Octavian Suciu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.15947",
    "title": "HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks",
    "abstract": "           Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm to support various low-altitude services through integrated air-ground infrastructure. To satisfy low-latency and high-computation demands, the integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC) systems plays a vital role, which offloads computing tasks from terminal devices to nearby UAVs, enabling flexible and resilient service provisions for ground users. To promote the development of LAENets, it is significant to achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges hinder this implementation, including the complexity of multi-dimensional UAV modeling and the difficulty of multi-objective coupled optimization. To this end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based Large Language Model (LLM) agent framework for model formulation. Specifically, we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG, empowering LLM agents to efficiently retrieve structural information from expert databases and generate more accurate optimization problems compared with traditional RAG-based LLM agents. After customizing carbon emission optimization problems for multi-UAV-assisted MEC networks, we propose a Double Regularization Diffusion-enhanced Soft Actor-Critic (R\\textsuperscript{2}DSAC) algorithm to solve the formulated multi-objective optimization problem. The R\\textsuperscript{2}DSAC algorithm incorporates diffusion entropy regularization and action entropy regularization to improve the performance of the diffusion policy. Furthermore, we dynamically mask unimportant neurons in the actor network to reduce the carbon emissions associated with model training. Simulation results demonstrate the effectiveness and reliability of the proposed HybridRAG-based LLM agent framework and the R\\textsuperscript{2}DSAC algorithm.         ",
    "url": "https://arxiv.org/abs/2506.15947",
    "authors": [
      "Jinbo Wen",
      "Cheng Su",
      "Jiawen Kang",
      "Jiangtian Nie",
      "Yang Zhang",
      "Jianhang Tang",
      "Dusit Niyato",
      "Chau Yuen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2506.15953",
    "title": "ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation",
    "abstract": "           Dexterous manipulation is a cornerstone capability for robotic systems aiming to interact with the physical world in a human-like manner. Although vision-based methods have advanced rapidly, tactile sensing remains crucial for fine-grained control, particularly in unstructured or visually occluded settings. We present ViTacFormer, a representation-learning approach that couples a cross-attention encoder to fuse high-resolution vision and touch with an autoregressive tactile prediction head that anticipates future contact signals. Building on this architecture, we devise an easy-to-challenging curriculum that steadily refines the visual-tactile latent space, boosting both accuracy and robustness. The learned cross-modal representation drives imitation learning for multi-fingered hands, enabling precise and adaptive manipulation. Across a suite of challenging real-world benchmarks, our method achieves approximately 50% higher success rates than prior state-of-the-art systems. To our knowledge, it is also the first to autonomously complete long-horizon dexterous manipulation tasks that demand highly precise control with an anthropomorphic hand, successfully executing up to 11 sequential stages and sustaining continuous operation for 2.5 minutes.         ",
    "url": "https://arxiv.org/abs/2506.15953",
    "authors": [
      "Liang Heng",
      "Haoran Geng",
      "Kaifeng Zhang",
      "Pieter Abbeel",
      "Jitendra Malik"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.15954",
    "title": "One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks",
    "abstract": "           Critical Learning Periods comprehend an important phenomenon involving deep learning, where early epochs play a decisive role in the success of many training recipes, such as data augmentation. Existing works confirm the existence of this phenomenon and provide useful insights. However, the literature lacks efforts to precisely identify when critical periods occur. In this work, we fill this gap by introducing a systematic approach for identifying critical periods during the training of deep neural networks, focusing on eliminating computationally intensive regularization techniques and effectively applying mechanisms for reducing computational costs, such as data pruning. Our method leverages generalization prediction mechanisms to pinpoint critical phases where training recipes yield maximum benefits to the predictive ability of models. By halting resource-intensive recipes beyond these periods, we significantly accelerate the learning phase and achieve reductions in training time, energy consumption, and CO$_2$ emissions. Experiments on standard architectures and benchmarks confirm the effectiveness of our method. Specifically, we achieve significant milestones by reducing the training time of popular architectures by up to 59.67%, leading to a 59.47% decrease in CO$_2$ emissions and a 60% reduction in financial costs, without compromising performance. Our work enhances understanding of training dynamics and paves the way for more sustainable and efficient deep learning practices, particularly in resource-constrained environments. In the era of the race for foundation models, we believe our method emerges as a valuable framework. The repository is available at this https URL ",
    "url": "https://arxiv.org/abs/2506.15954",
    "authors": [
      "Vinicius Yuiti Fukase",
      "Heitor Gama",
      "Barbara Bueno",
      "Lucas Libanio",
      "Anna Helena Reali Costa",
      "Artur Jordao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15975",
    "title": "Multi-use LLM Watermarking and the False Detection Problem",
    "abstract": "           Digital watermarking is a promising solution for mitigating some of the risks arising from the misuse of automatically generated text. These approaches either embed non-specific watermarks to allow for the detection of any text generated by a particular sampler, or embed specific keys that allow the identification of the LLM user. However, simultaneously using the same embedding for both detection and user identification leads to a false detection problem, whereby, as user capacity grows, unwatermarked text is increasingly likely to be falsely detected as watermarked. Through theoretical analysis, we identify the underlying causes of this phenomenon. Building on these insights, we propose Dual Watermarking which jointly encodes detection and identification watermarks into generated text, significantly reducing false positives while maintaining high detection accuracy. Our experimental results validate our theoretical findings and demonstrate the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2506.15975",
    "authors": [
      "Zihao Fu",
      "Chris Russell"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.15981",
    "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion",
    "abstract": "           The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.15981",
    "authors": [
      "Markus Frohmann",
      "Gabriel Meseguer-Brocal",
      "Markus Schedl",
      "Elena V. Epure"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.15988",
    "title": "Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation",
    "abstract": "           Stand-alone Visual Place Recognition (VPR) systems have little defence against a well-designed adversarial attack, which can lead to disastrous consequences when deployed for robot navigation. This paper extensively analyzes the effect of four adversarial attacks common in other perception tasks and four novel VPR-specific attacks on VPR localization performance. We then propose how to close the loop between VPR, an Adversarial Attack Detector (AAD), and active navigation decisions by demonstrating the performance benefit of simulated AADs in a novel experiment paradigm -- which we detail for the robotics community to use as a system framework. In the proposed experiment paradigm, we see the addition of AADs across a range of detection accuracies can improve performance over baseline; demonstrating a significant improvement -- such as a ~50% reduction in the mean along-track localization error -- can be achieved with True Positive and False Positive detection rates of only 75% and up to 25% respectively. We examine a variety of metrics including: Along-Track Error, Percentage of Time Attacked, Percentage of Time in an `Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on these results, we provide the first investigation into the efficacy of the Fast Gradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this work highlights the need for AADs in real-world systems for trustworthy navigation, and informs quantitative requirements for system design.         ",
    "url": "https://arxiv.org/abs/2506.15988",
    "authors": [
      "Connor Malone",
      "Owen Claxton",
      "Iman Shames",
      "Michael Milford"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.16001",
    "title": "AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction",
    "abstract": "           Time series forecasting requires architectures that simultaneously achieve three competing objectives: (1) strict temporal causality for reliable predictions, (2) sub-quadratic complexity for practical scalability, and (3) multi-scale pattern recognition for accurate long-horizon forecasting. We introduce AutoHFormer, a hierarchical autoregressive transformer that addresses these challenges through three key innovations: 1) Hierarchical Temporal Modeling: Our architecture decomposes predictions into segment-level blocks processed in parallel, followed by intra-segment sequential refinement. This dual-scale approach maintains temporal coherence while enabling efficient computation. 2) Dynamic Windowed Attention: The attention mechanism employs learnable causal windows with exponential decay, reducing complexity while preserving precise temporal relationships. This design avoids both the anti-causal violations of standard transformers and the sequential bottlenecks of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system is adopted to capture time patterns at multiple scales. It combines fixed oscillating patterns for short-term variations with learnable decay rates for long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X faster training and 6.06X memory reduction compared to PatchTST on PEMS08, while maintaining consistent accuracy across 96-720 step horizons in most of cases. These breakthroughs establish new benchmarks for efficient and precise time series modeling. Implementations of our method and all baselines in hierarchical autoregressive mechanism are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16001",
    "authors": [
      "Qianru Zhang",
      "Honggang Wen",
      "Ming Li",
      "Dong Huang",
      "Siu-Ming Yiu",
      "Christian S. Jensen",
      "Pietro Li\u00f2"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16009",
    "title": "Bridging Brain with Foundation Models through Self-Supervised Learning",
    "abstract": "           Foundation models (FMs), powered by self-supervised learning (SSL), have redefined the capabilities of artificial intelligence, demonstrating exceptional performance in domains like natural language processing and computer vision. These advances present a transformative opportunity for brain signal analysis. Unlike traditional supervised learning, which is limited by the scarcity of labeled neural data, SSL offers a promising solution by enabling models to learn meaningful representations from unlabeled data. This is particularly valuable in addressing the unique challenges of brain signals, including high noise levels, inter-subject variability, and low signal-to-noise ratios. This survey systematically reviews the emerging field of bridging brain signals with foundation models through the innovative application of SSL. It explores key SSL techniques, the development of brain-specific foundation models, their adaptation to downstream tasks, and the integration of brain signals with other modalities in multimodal SSL frameworks. The review also covers commonly used evaluation metrics and benchmark datasets that support comparative analysis. Finally, it highlights key challenges and outlines future research directions. This work aims to provide researchers with a structured understanding of this rapidly evolving field and a roadmap for developing generalizable brain foundation models powered by self-supervision.         ",
    "url": "https://arxiv.org/abs/2506.16009",
    "authors": [
      "Hamdi Altaheri",
      "Fakhri Karray",
      "Md. Milon Islam",
      "S M Taslim Uddin Raju",
      "Amir-Hossein Karimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16013",
    "title": "A Fast Iterative Robust Principal Component Analysis Method",
    "abstract": "           Principal Component Analysis (PCA) is widely used for dimensionality reduction and data analysis. However, PCA results are adversely affected by outliers often observed in real-world data. Existing robust PCA methods are often computationally expensive or exhibit limited robustness. In this work, we introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating the inliers center location and covariance. Our approach leverages Incremental PCA (IPCA) to iteratively construct a subset of data points that ensures improved location and covariance estimation that effectively mitigates the influence of outliers on PCA projection. We demonstrate that our method achieves competitive accuracy and performance compared to existing robust location and covariance methods while offering improved robustness to outlier contamination. We utilize simulated and real-world datasets to evaluate and demonstrate the efficacy of our approach in identifying and preserving underlying data structures in the presence of contamination.         ",
    "url": "https://arxiv.org/abs/2506.16013",
    "authors": [
      "Timbwaoga Aime Judicael Ouermi",
      "Jixian Li",
      "Chris R. Johnson"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2506.16017",
    "title": "EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training",
    "abstract": "           Monocular depth estimation and ego-motion estimation are significant tasks for scene perception and navigation in stable, accurate and efficient robot-assisted endoscopy. To tackle lighting variations and sparse textures in endoscopic scenes, multiple techniques including optical flow, appearance flow and intrinsic image decomposition have been introduced into the existing methods. However, the effective training strategy for multiple modules are still critical to deal with both illumination issues and information interference for self-supervised depth estimation in endoscopy. Therefore, a novel framework with multistep efficient finetuning is proposed in this work. In each epoch of end-to-end training, the process is divided into three steps, including optical flow registration, multiscale image decomposition and multiple transformation alignments. At each step, only the related networks are trained without interference of irrelevant information. Based on parameter-efficient finetuning on the foundation model, the proposed method achieves state-of-the-art performance on self-supervised depth estimation on SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with 4\\%$\\sim$10\\% lower error. The evaluation code of this work has been published on this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16017",
    "authors": [
      "Liangjing Shao",
      "Linxin Bai",
      "Chenkang Du",
      "Xinrong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.16023",
    "title": "Efficient Blockchain-based Steganography via Backcalculating Generative Adversarial Network",
    "abstract": "           Blockchain-based steganography enables data hiding via encoding the covert data into a specific blockchain transaction field. However, previous works focus on the specific field-embedding methods while lacking a consideration on required field-generation embedding. In this paper, we propose a generic blockchain-based steganography framework (GBSF). The sender generates the required fields such as amount and fees, where the additional covert data is embedded to enhance the channel capacity. Based on GBSF, we design a reversible generative adversarial network (R-GAN) that utilizes the generative adversarial network with a reversible generator to generate the required fields and encode additional covert data into the input noise of the reversible generator. We then explore the performance flaw of R-GAN. To further improve the performance, we propose R-GAN with Counter-intuitive data preprocessing and Custom activation functions, namely CCR-GAN. The counter-intuitive data preprocessing (CIDP) mechanism is used to reduce decoding errors in covert data, while it incurs gradient explosion for model convergence. The custom activation function named ClipSigmoid is devised to overcome the problem. Theoretical justification for CIDP and ClipSigmoid is also provided. We also develop a mechanism named T2C, which balances capacity and concealment. We conduct experiments using the transaction amount of the Bitcoin mainnet as the required field to verify the feasibility. We then apply the proposed schemes to other transaction fields and blockchains to demonstrate the scalability. Finally, we evaluate capacity and concealment for various blockchains and transaction fields and explore the trade-off between capacity and concealment. The results demonstrate that R-GAN and CCR-GAN are able to enhance the channel capacity effectively and outperform state-of-the-art works.         ",
    "url": "https://arxiv.org/abs/2506.16023",
    "authors": [
      "Zhuo Chen",
      "Jialing He",
      "Jiacheng Wang",
      "Zehui Xiong",
      "Tao Xiang",
      "Liehuang Zhu",
      "Dusit Niyato"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16050",
    "title": "Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments",
    "abstract": "           Anomaly detection and localization in automated industrial manufacturing can significantly enhance production efficiency and product quality. Existing methods are capable of detecting surface defects in pre-defined or controlled imaging environments. However, accurately detecting workpiece defects in complex and unstructured industrial environments with varying views, poses and illumination remains challenging. We propose a novel anomaly detection and localization method specifically designed to handle inputs with perturbative patterns. Our approach introduces a new framework based on a collaborative distillation heterogeneous teacher network (HetNet), an adaptive local-global feature fusion module, and a local multivariate Gaussian noise generation module. HetNet can learn to model the complex feature distribution of normal patterns using limited information about local disruptive changes. We conducted extensive experiments on mainstream benchmarks. HetNet demonstrates superior performance with approximately 10% improvement across all evaluation metrics on MSC-AD under industrial conditions, while achieving state-of-the-art results on other datasets, validating its resilience to environmental fluctuations and its capability to enhance the reliability of industrial anomaly detection systems across diverse scenarios. Tests in real-world environments further confirm that HetNet can be effectively integrated into production lines to achieve robust and real-time anomaly detection. Codes, images and videos are published on the project website at: this https URL ",
    "url": "https://arxiv.org/abs/2506.16050",
    "authors": [
      "Jiawen Yu",
      "Jieji Ren",
      "Yang Chang",
      "Qiaojun Yu",
      "Xuan Tong",
      "Boyang Wang",
      "Yan Song",
      "You Li",
      "Xinji Mai",
      "Wenqiang Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16052",
    "title": "A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text",
    "abstract": "           The proliferation of online communication platforms has created unprecedented opportunities for global connectivity while simultaneously enabling harmful behaviors such as cyberbullying, which affects approximately 54.4\\% of teenagers according to recent research. This paper presents a hybrid architecture that combines the contextual understanding capabilities of transformer-based models with the pattern recognition strengths of broad learning systems for effective cyberbullying detection. This approach integrates a modified DeBERTa model augmented with Squeeze-and-Excitation blocks and sentiment analysis capabilities with a Gated Broad Learning System (GBLS) classifier, creating a synergistic framework that outperforms existing approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa + GBLS model achieved good performance on four English datasets: 79.3\\% accuracy on HateXplain, 95.41\\% accuracy on SOSNet, 91.37\\% accuracy on Mendeley-I, and 94.67\\% accuracy on Mendeley-II. Beyond performance gains, the framework incorporates comprehensive explainability mechanisms including token-level attribution analysis, LIME-based local interpretations, and confidence calibration, addressing critical transparency requirements in automated content moderation. Ablation studies confirm the meaningful contribution of each architectural component, while failure case analysis reveals specific challenges in detecting implicit bias and sarcastic content, providing valuable insights for future improvements in cyberbullying detection systems.         ",
    "url": "https://arxiv.org/abs/2506.16052",
    "authors": [
      "Devesh Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16065",
    "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators",
    "abstract": "           The classical universal approximation (UA) theorem for neural networks establishes mild conditions under which a feedforward neural network can approximate a continuous function $f$ with arbitrary accuracy. A recent result shows that neural networks also enjoy a more general interval universal approximation (IUA) theorem, in the sense that the abstract interpretation semantics of the network using the interval domain can approximate the direct image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with arbitrary accuracy. These theorems, however, rest on the unrealistic assumption that the neural network computes over infinitely precise real numbers, whereas their software implementations in practice compute over finite-precision floating-point numbers. An open question is whether the IUA theorem still holds in the floating-point setting. This paper introduces the first IUA theorem for floating-point neural networks that proves their remarkable ability to perfectly capture the direct image map of any rounded target function $f$, showing no limits exist on their expressiveness. Our IUA theorem in the floating-point setting exhibits material differences from the real-valued setting, which reflects the fundamental distinctions between these two computational models. This theorem also implies surprising corollaries, which include (i) the existence of provably robust floating-point neural networks; and (ii) the computational completeness of the class of straight-line programs that use only floating-point additions and multiplications for the class of all floating-point programs that halt.         ",
    "url": "https://arxiv.org/abs/2506.16065",
    "authors": [
      "Geonho Hwang",
      "Wonyeol Lee",
      "Yeachan Park",
      "Sejun Park",
      "Feras Saad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2506.16066",
    "title": "Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI",
    "abstract": "           The growth of digital communication platforms has led to increased cyberbullying incidents worldwide, creating a need for automated detection systems to protect users. The rise of code-mixed Hindi-English (Hinglish) communication on digital platforms poses challenges for existing cyberbullying detection systems, which were designed primarily for monolingual text. This paper presents a framework for cyberbullying detection in Hinglish text using the Multilingual Representations for Indian Languages (MURIL) architecture to address limitations in current approaches. Evaluation across six benchmark datasets -- Bohra \\textit{et al.}, BullyExplain, BullySentemo, Kumar \\textit{et al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based approach outperforms existing multilingual models including RoBERTa and IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies of 86.97\\% on Bohra, 84.62\\% on BullyExplain, 86.03\\% on BullySentemo, 75.41\\% on Kumar datasets, 83.92\\% on HASOC 2021, and 94.63\\% on Mendeley dataset. The framework includes explainability features through attribution analysis and cross-linguistic pattern recognition. Ablation studies show that selective layer freezing, appropriate classification head design, and specialized preprocessing for code-mixed content improve detection performance, while failure analysis identifies challenges including context-dependent interpretation, cultural understanding, and cross-linguistic sarcasm detection, providing directions for future research in multilingual cyberbullying detection.         ",
    "url": "https://arxiv.org/abs/2506.16066",
    "authors": [
      "Devesh Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.16072",
    "title": "A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems",
    "abstract": "           Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for its near-optimal weighted sum rate performance. However, its practical deployment in massive multi-user (MU) multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) systems is hindered by the assumption of perfect channel state information (CSI) and high computational complexity. To address these issues, we first develop a wideband stochastic WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net), where each SWMMSE iteration is mapped to a network layer. Specifically, its DU module integrates approximation techniques and leverages beam-domain sparsity as well as frequency-domain subcarrier correlation, significantly accelerating convergence and reducing computational overhead. Furthermore, the RL module adaptively adjusts the network depth and generates compensation matrices to mitigate approximation errors. Simulation results under imperfect CSI demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance while offering superior computational and convergence efficiency.         ",
    "url": "https://arxiv.org/abs/2506.16072",
    "authors": [
      "Kexuan Wang",
      "An Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16073",
    "title": "TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network for Lipreading",
    "abstract": "           The word-level lipreading approach typically employs a two-stage framework with separate frontend and backend architectures to model dynamic lip movements. Each component has been extensively studied, and in the backend architecture, temporal convolutional networks (TCNs) have been widely adopted in state-of-the-art methods. Recently, dense skip connections have been introduced in TCNs to mitigate the limited density of the receptive field, thereby improving the modeling of complex temporal representations. However, their performance remains constrained owing to potential information loss regarding the continuous nature of lip movements, caused by blind spots in the receptive field. To address this limitation, we propose TD3Net, a temporal densely connected multi-dilated convolutional network that combines dense skip connections and multi-dilated temporal convolutions as the backend architecture. TD3Net covers a wide and dense receptive field without blind spots by applying different dilation factors to skip-connected features. Experimental results on a word-level lipreading task using two large publicly available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that the proposed method achieves performance comparable to state-of-the-art methods. It achieved higher accuracy with fewer parameters and lower floating-point operations compared to existing TCN-based backend architectures. Moreover, visualization results suggest that our approach effectively utilizes diverse temporal features while preserving temporal continuity, presenting notable advantages in lipreading systems. The code is available at our GitHub repository: this https URL ",
    "url": "https://arxiv.org/abs/2506.16073",
    "authors": [
      "Byung Hoon Lee",
      "Wooseok Shin",
      "Sung Won Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16078",
    "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations",
    "abstract": "           Safety alignment is a key requirement for building reliable Artificial General Intelligence. Despite significant advances in safety alignment, we observe that minor latent shifts can still trigger unsafe responses in aligned models. We argue that this stems from the shallow nature of existing alignment methods, which focus on surface-level refusal behaviors without sufficiently altering internal representations. Consequently, small shifts in hidden activations can re-trigger harmful behaviors embedded in the latent space. To explore the robustness of safety alignment to latent perturbations, we introduce a probing method that measures the Negative Log-Likelihood of the original response generated by the model. This probe quantifies local sensitivity in the latent space, serving as a diagnostic tool for identifying vulnerable directions. Based on this signal, we construct effective jailbreak trajectories, giving rise to the Activation Steering Attack (ASA). More importantly, these insights offer a principled foundation for improving alignment robustness. To this end, we introduce Layer-wise Adversarial Patch Training~(LAPT), a fine-tuning strategy that inject controlled perturbations into hidden representations during training. Experimental results highlight that LAPT strengthen alignment robustness without compromising general capabilities. Our findings reveal fundamental flaws in current alignment paradigms and call for representation-level training strategies that move beyond surface-level behavior supervision. Codes and results are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16078",
    "authors": [
      "Tianle Gu",
      "Kexin Huang",
      "Zongqi Wang",
      "Yixu Wang",
      "Jie Li",
      "Yuanqi Yao",
      "Yang Yao",
      "Yujiu Yang",
      "Yan Teng",
      "Yingchun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16079",
    "title": "Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion",
    "abstract": "           Lagrangian Neural Networks (LNNs) present a principled and interpretable framework for learning the system dynamics by utilizing inductive biases. While traditional dynamics models struggle with compounding errors over long horizons, LNNs intrinsically preserve the physical laws governing any system, enabling accurate and stable predictions essential for sustainable locomotion. This work evaluates LNNs for infinite horizon planning in quadrupedal robots through four dynamics models: (1) full-order forward dynamics (FD) training and inference, (2) diagonalized representation of Mass Matrix in full order FD, (3) full-order inverse dynamics (ID) training with FD inference, (4) reduced-order modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that LNNs bring improvements in sample efficiency (10x) and superior prediction accuracy (up to 2-10x) compared to baseline methods. Notably, the diagonalization approach of LNNs reduces computational complexity while retaining some interpretability, enabling real-time receding horizon control. These findings highlight the advantages of LNNs in capturing the underlying structure of system dynamics in quadrupeds, leading to improved performance and efficiency in locomotion planning and control. Additionally, our approach achieves a higher control frequency than previous LNN methods, demonstrating its potential for real-world deployment on quadrupeds.         ",
    "url": "https://arxiv.org/abs/2506.16079",
    "authors": [
      "Prakrut Kotecha",
      "Aditya Shirwatkar",
      "Shishir Kolathaya"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16095",
    "title": "Intelligent Operation and Maintenance and Prediction Model Optimization for Improving Wind Power Generation Efficiency",
    "abstract": "           This study explores the effectiveness of predictive maintenance models and the optimization of intelligent Operation and Maintenance (O&M) systems in improving wind power generation efficiency. Through qualitative research, structured interviews were conducted with five wind farm engineers and maintenance managers, each with extensive experience in turbine operations. Using thematic analysis, the study revealed that while predictive maintenance models effectively reduce downtime by identifying major faults, they often struggle with detecting smaller, gradual failures. Key challenges identified include false positives, sensor malfunctions, and difficulties in integrating new models with older turbine systems. Advanced technologies such as digital twins, SCADA systems, and condition monitoring have significantly enhanced turbine maintenance practices. However, these technologies still require improvements, particularly in AI refinement and real-time data integration. The findings emphasize the need for continuous development to fully optimize wind turbine performance and support the broader adoption of renewable energy.         ",
    "url": "https://arxiv.org/abs/2506.16095",
    "authors": [
      "Xun Liu",
      "Xiaobin Wu",
      "Jiaqi He",
      "Rajan Das Gupta"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2506.16096",
    "title": "A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders",
    "abstract": "           Recent developed graph-based methods for diagnosing brain disorders using functional connectivity highly rely on predefined brain atlases, but overlook the rich information embedded within atlases and the confounding effects of site and phenotype variability. To address these challenges, we propose a two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates the semantic similarity of brain regions and condition-based population graph modeling. In the first stage, termed brain representation learning, we leverage brain atlas knowledge from GPT-4 to enrich the graph representation and refine the brain graph through an adaptive node reassignment graph attention network. In the second stage, termed population disorder diagnosis, phenotypic data is incorporated into population graph construction and feature fusion to mitigate confounding effects and enhance diagnosis performance. Experiments on the ABIDE I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms state-of-the-art methods in prediction accuracy while enhancing interpretability. Overall, our proposed framework offers a reliable and personalized approach to brain disorder diagnosis, advancing clinical applicability.         ",
    "url": "https://arxiv.org/abs/2506.16096",
    "authors": [
      "Qianqian Liao",
      "Wuque Cai",
      "Hongze Sun",
      "Dongze Liu",
      "Duo Chen",
      "Dezhong Yao",
      "Daqing Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16110",
    "title": "Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification",
    "abstract": "           The message-passing paradigm of Graph Neural Networks often struggles with exchanging information across distant nodes typically due to structural bottlenecks in certain graph regions, a limitation known as \\textit{over-squashing}. To reduce such bottlenecks, \\textit{graph rewiring}, which modifies graph topology, has been widely used. However, existing graph rewiring techniques often overlook the need to preserve critical properties of the original graph, e.g., \\textit{spectral properties}. Moreover, many approaches rely on increasing edge count to improve connectivity, which introduces significant computational overhead and exacerbates the risk of over-smoothing. In this paper, we propose a novel graph rewiring method that leverages \\textit{spectrum-preserving} graph \\textit{sparsification}, for mitigating over-squashing. Our method generates graphs with enhanced connectivity while maintaining sparsity and largely preserving the original graph spectrum, effectively balancing structural bottleneck reduction and graph property preservation. Experimental results validate the effectiveness of our approach, demonstrating its superiority over strong baseline methods in classification accuracy and retention of the Laplacian spectrum.         ",
    "url": "https://arxiv.org/abs/2506.16110",
    "authors": [
      "Langzhang Liang",
      "Fanchen Bu",
      "Zixing Song",
      "Zenglin Xu",
      "Shirui Pan",
      "Kijung Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16114",
    "title": "GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks",
    "abstract": "           Generative recommendations (GR), which usually include item tokenizers and generative Large Language Models (LLMs), have demonstrated remarkable success across a wide range of scenarios. The majority of existing research efforts primarily concentrate on developing powerful item tokenizers or advancing LLM decoding strategies to attain superior performance. However, the critical fine-tuning step in GR frameworks, which is essential for adapting LLMs to recommendation data, remains largely unexplored. Current approaches predominantly rely on either the next-token prediction loss of supervised fine-tuning (SFT) or recommendationspecific direct preference optimization (DPO) strategies. Both methods ignore the exploration of possible positive unobserved samples, which is commonly referred to as the exposure bias problem. To mitigate this problem, this paper treats the GR as a multi-step generation task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The proposed framework integrates collaborative knowledge from traditional recommender systems to create an adaptive trajectory sampler and a comprehensive reward model. Leveraging the diverse generation property of GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR emerges as a promising approach to mitigate the exposure bias problem. Extensive empirical results on two real-world datasets and with two different GR backbones highlight the effectiveness and robustness of GFlowGR.         ",
    "url": "https://arxiv.org/abs/2506.16114",
    "authors": [
      "Yejing Wang",
      "Shengyu Zhou",
      "Jinyu Lu",
      "Qidong Liu",
      "Xinhang Li",
      "Wenlin Zhang",
      "Feng Li",
      "Pengjie Wang",
      "Jian Xu",
      "Bo Zheng",
      "Xiangyu Zhao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16144",
    "title": "Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction",
    "abstract": "           Automated algorithm performance prediction in numerical blackbox optimization often relies on problem characterizations, such as exploratory landscape analysis features. These features are typically used as inputs to machine learning models and are represented in a tabular format. However, such approaches often overlook algorithm configurations, a key factor influencing performance. The relationships between algorithm operators, parameters, problem characteristics, and performance outcomes form a complex structure best represented as a graph. This work explores the use of heterogeneous graph data structures and graph neural networks to predict the performance of optimization algorithms by capturing the complex dependencies between problems, algorithm configurations, and performance outcomes. We focus on two modular frameworks, modCMA-ES and modDE, which decompose two widely used derivative-free optimization algorithms: the covariance matrix adaptation evolution strategy (CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576 modDE variants on 24 BBOB problems across six runtime budgets and two problem dimensions. Achieving up to 36.6% improvement in MSE over traditional tabular-based methods, this work highlights the potential of geometric learning in black-box optimization.         ",
    "url": "https://arxiv.org/abs/2506.16144",
    "authors": [
      "Ana Kostovska",
      "Carola Doerr",
      "Sa\u0161o D\u017eeroski",
      "Pan\u010de Panov",
      "Tome Eftimov"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16146",
    "title": "Neural Prioritisation for Web Crawling",
    "abstract": "           Given the vast scale of the Web, crawling prioritisation techniques based on link graph traversal, popularity, link analysis, and textual content are frequently applied to surface documents that are most likely to be valuable. While existing techniques are effective for keyword-based search, both retrieval methods and user search behaviours are shifting from keyword-based matching to natural language semantic matching. The remarkable success of applying semantic matching and quality signals during ranking leads us to hypothesize that crawling could be improved by prioritizing Web pages with high semantic quality. To investigate this, we propose a semantic quality-driven prioritisation technique to enhance the effectiveness of crawling and align the crawler behaviour with recent shift towards natural language search. We embed semantic understanding directly into the crawling process -- leveraging recent neural semantic quality estimators to prioritise the crawling frontier -- with the goal of surfacing content that is semantically rich and valuable for modern search needs. Our experiments on the English subset of ClueWeb22-B and the Researchy Questions query set show that, compared to existing crawling techniques, neural crawling policies significantly improve harvest rate, maxNDCG, and search effectiveness during the early stages of crawling. Meanwhile, crawlers based on our proposed neural policies maintain comparable search performance on keyword queries from the MS MARCO Web Search query set. While this work does not propose a definitive and complete solution, it presents a forward-looking perspective on Web crawling and opens the door to a new line of research on leveraging semantic analysis to effectively align crawlers with the ongoing shift toward natural language search.         ",
    "url": "https://arxiv.org/abs/2506.16146",
    "authors": [
      "Francesza Pezzuti",
      "Sean MacAvaney",
      "Nicola Tonellotto"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2506.16157",
    "title": "MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models",
    "abstract": "           Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tasks. Despite its impressive performance, the robustness of RES models against adversarial examples remains largely unexplored. While prior adversarial attack methods have explored adversarial robustness on conventional segmentation models, they perform poorly when directly applied to RES, failing to expose vulnerabilities in its multimodal structure. Moreover, in practical open-world scenarios, users typically issue multiple, diverse referring expressions to interact with the same image, highlighting the need for adversarial examples that generalize across varied textual inputs. To address these multimodal challenges, we propose a novel adversarial attack strategy termed \\textbf{Multimodal Bidirectional Attack}, tailored for RES models. Our method introduces learnable proxy textual embedding perturbation and jointly performs visual-aligned optimization on the image modality and textual-adversarial optimization on the textual modality during attack generation. This dual optimization framework encourages adversarial images to actively adapt to more challenging text embedding during optimization, thereby enhancing their cross-text transferability, which refers to the ability of adversarial examples to remain effective under a variety of unseen or semantically diverse textual inputs. Extensive experiments conducted on multiple RES models and benchmark datasets demonstrate the superior effectiveness of our method compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2506.16157",
    "authors": [
      "Xingbai Chen",
      "Tingchao Fu",
      "Renyang Liu",
      "Wei Zhou",
      "Chao Yi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16171",
    "title": "Maximum Reachability Orientation of Mixed Graphs",
    "abstract": "           We aim to find orientations of mixed graphs optimizing the total reachability, a problem that has applications in causality and biology. For given a digraph $D$, we use $P(D)$ for the set of ordered pairs of distinct vertices in $V(D)$ and we define $\\kappa_D:P(D)\\rightarrow \\{0,1\\}$ by $\\kappa_D(u,v)=1$ if $v$ is reachable from $u$ in $D$, and $\\kappa_D(u,v)=0$, otherwise. We use $R(D)=\\sum_{(u,v)\\in P(D)}\\kappa_D(u,v)$. Now, given a mixed graph $G$, we aim to find an orientation $\\vec{G}$ of $G$ that maximizes $R(\\vec{G})$. Hakimi, Schmeichel, and Young proved that the problem can be solved in polynomial time when restricted to undirected inputs. They inquired about the complexity in mixed graphs. We answer this question by showing that this problem is NP-hard, and, moreover, APX-hard. We then develop a finer understanding of how quickly the problem becomes difficult when going from undirected to mixed graphs. To this end, we consider the parameterized complexity of the problem with respect to the number $k$ of preoriented arcs of $G$, a new form of parameterization. We show that the problem can be solved in $n^{O(k)}$ and that a $(1-\\epsilon)$-approximation can be computed in $f(k,\\epsilon)n^{O(1)}$ for any $\\epsilon > 0$.         ",
    "url": "https://arxiv.org/abs/2506.16171",
    "authors": [
      "Florian H\u00f6rsch"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2506.16186",
    "title": "Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis",
    "abstract": "           Accident detection using Closed Circuit Television (CCTV) footage is one of the most imperative features for enhancing transport safety and efficient traffic control. To this end, this research addresses the issues of supervised monitoring and data deficiency in accident detection systems by adapting excellent deep learning technologies. The motivation arises from rising statistics in the number of car accidents worldwide; this calls for innovation and the establishment of a smart, efficient and automated way of identifying accidents and calling for help to save lives. Addressing the problem of the scarcity of data, the presented framework joins Generative Adversarial Networks (GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model training. Video frames for accidents and non-accidents are collected from YouTube videos, and we perform resizing, image enhancement and image normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%, while the CNN model obtained 88%. Such results show that the proposed framework suits traffic safety applications due to its high real-time accident detection capabilities and broad-scale applicability. This work lays the foundation for intelligent surveillance systems in the future for real-time traffic monitoring, smart city framework, and integration of intelligent surveillance systems into emergency management systems.         ",
    "url": "https://arxiv.org/abs/2506.16186",
    "authors": [
      "Zhenghao Xi",
      "Xiang Liu",
      "Yaqi Liu",
      "Yitong Cai",
      "Yangyu Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16218",
    "title": "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models",
    "abstract": "           Federated prompt learning (FPL) for vision-language models is a powerful approach to collaboratively adapt models across distributed clients while preserving data privacy. However, existing FPL approaches suffer from a trade-off between performance and robustness, particularly in out-of-distribution (OOD) shifts, limiting their reliability in real-world scenarios. The inherent in-distribution (ID) data heterogeneity among different clients makes it more challenging to maintain this trade-off. To fill this gap, we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework, which captures diverse distributions among clients using ID global prompts, local prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of prompts to create both class-level and distribution-level separations, which adapt to OOD shifts through bi-level distributionally robust optimization. Additionally, FOCoOp improves the discrimination consistency among clients, i.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by semi-unbalanced optimal transport. The extensive experiments on real-world datasets demonstrate that FOCoOp effectively captures decentralized heterogeneous distributions and enhances robustness of different OOD shifts. The project is available at GitHub.         ",
    "url": "https://arxiv.org/abs/2506.16218",
    "authors": [
      "Xinting Liao",
      "Weiming Liu",
      "Jiaming Qian",
      "Pengyang Zhou",
      "Jiahe Xu",
      "Wenjie Wang",
      "Chaochao Chen",
      "Xiaolin Zheng",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16234",
    "title": "Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data",
    "abstract": "           Causal discovery from observational data typically assumes full access to data and availability of domain experts. In practice, data often arrive in batches, and expert knowledge is scarce. Language Models (LMs) offer a surrogate but come with their own issues-hallucinations, inconsistencies, and bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid Bayesian framework that bridges these gaps by adaptively integrating sequential batch data with LM-derived noisy, expert knowledge while accounting for both data-induced and LM-induced biases. Our proposed representation shift from Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates ambiguities within a coherent Bayesian framework, allowing grounding the global LM knowledge in local observational data. To guide LM interaction, we use a sequential optimization scheme that adaptively queries the most informative edges. Across varied datasets, BLANCE outperforms prior work in structural accuracy and extends to Bayesian parameter estimation, showing robustness to LM noise.         ",
    "url": "https://arxiv.org/abs/2506.16234",
    "authors": [
      "Prakhar Verma",
      "David Arbour",
      "Sunav Choudhary",
      "Harshita Chopra",
      "Arno Solin",
      "Atanu R. Sinha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16243",
    "title": "Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping",
    "abstract": "           Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and high-quality EEG data from ALS patients are scarce. This data scarcity, coupled with severe class imbalance between ALS and healthy control recordings, poses a challenge for training reliable machine learning classifiers. In this work, we address these issues by generating synthetic EEG signals for ALS patients using a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of ALS EEG signals and produce realistic synthetic samples. We preprocess and normalize EEG recordings, and train a CWGAN model to generate synthetic ALS signals. The CWGAN architecture and training routine are detailed, with key hyperparameters chosen for stable training. Qualitative evaluation of generated signals shows that they closely mimic real ALS EEG patterns. The CWGAN training converged with generator and discriminator loss curves stabilizing, indicating successful learning. The synthetic EEG signals appear realistic and have potential use as augmented data for training classifiers, helping to mitigate class imbalance and improve ALS detection accuracy. We discuss how this approach can facilitate data sharing and enhance diagnostic models.         ",
    "url": "https://arxiv.org/abs/2506.16243",
    "authors": [
      "Abdulvahap Mutlu",
      "\u015eeng\u00fcl Do\u011fan",
      "T\u00fcrker Tuncer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16254",
    "title": "Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks",
    "abstract": "           Enhancing the sustainability and efficiency of wireless sensor networks (WSN) in dynamic and unpredictable environments requires adaptive communication and energy harvesting strategies. We propose a novel adaptive control strategy for WSNs that optimizes data transmission and EH to minimize overall energy consumption while ensuring queue stability and energy storing constraints under dynamic environmental conditions. The notion of adaptability therein is achieved by transferring the known environment-specific knowledge to new conditions resorting to the lifelong reinforcement learning concepts. We evaluate our proposed method against two baseline frameworks: Lyapunov-based optimization, and policy-gradient reinforcement learning (RL). Simulation results demonstrate that our approach rapidly adapts to changing environmental conditions by leveraging transferable knowledge, achieving near-optimal performance approximately $30\\%$ faster than the RL method and $60\\%$ faster than the Lyapunov-based approach.         ",
    "url": "https://arxiv.org/abs/2506.16254",
    "authors": [
      "Hossein Mohammadi Firouzjaei",
      "Rafaela Scaciota",
      "Sumudu Samarakoon"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.16257",
    "title": "Detailed Small-Signal Stability Analysis of the Cigr\u00e9 High-Voltage Network Penetrated by Grid-Following Inverter-Based Resources",
    "abstract": "           This paper presents a detailed small-signal stability analysis of a modified version of the Cigr\u00e9 European high-voltage network, where one of the synchronous generators is replaced by a grid-following inverter-based resource (IBR). The analysis focuses on the influence of the parameters defining the grid-following IBR control scheme on the stability of the system. Given a set of potential grid configurations and the value of the IBR control parameters, stability is verified by the direct eigenvalue analysis of a high-detailed linearized model of the overall Cigr\u00e9 network. Starting from this procedure, we propose an adaptive sampling method for training a support vector machine classifier able to estimate the probability of stability of the power system over a domain defined by candidate intervals of the considered parameters. The training of the classifier is refined to identify with more accuracy the boundaries of the parameters' stability regions. The obtained results are then compared with those obtained by representing the grid with the classical Th\u00e9venin equivalent. Results suggest that, when the Th\u00e9venin equivalent is accurate, the predicted stability region is conservative yet contained within that of the full network.         ",
    "url": "https://arxiv.org/abs/2506.16257",
    "authors": [
      "Francesco Conte",
      "Fernando Mancilla-David",
      "Amritansh Sagar",
      "Chendan Li",
      "Federico Silvestro",
      "Samuele Grillo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.16262",
    "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision",
    "abstract": "           Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.         ",
    "url": "https://arxiv.org/abs/2506.16262",
    "authors": [
      "Weeyoung Kwon",
      "Jeahun Sung",
      "Minkyu Jeon",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16288",
    "title": "Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective",
    "abstract": "           The rapid adaptation ability of auto-regressive foundation models is often attributed to the diversity of their pre-training data. This is because, from a Bayesian standpoint, minimizing prediction error in such settings requires integrating over all plausible latent hypotheses consistent with observations. While this behavior is desirable in principle, it often proves too ambitious in practice: under high ambiguity, the number of plausible latent alternatives makes Bayes-optimal prediction computationally intractable. Cognitive science has long recognized this limitation, suggesting that under such conditions, heuristics or information-seeking strategies are preferable to exhaustive inference. Translating this insight to next-token prediction, we hypothesize that low- and high-ambiguity predictions pose different computational demands, making ambiguity-agnostic next-token prediction a detrimental inductive bias. To test this, we introduce MetaHMM, a synthetic sequence meta-learning benchmark with rich compositional structure and a tractable Bayesian oracle. We show that Transformers indeed struggle with high-ambiguity predictions across model sizes. Motivated by cognitive theories, we propose a method to convert pre-trained models into Monte Carlo predictors that decouple task inference from token prediction. Preliminary results show substantial gains in ambiguous contexts through improved capacity allocation and test-time scalable inference, though challenges remain.         ",
    "url": "https://arxiv.org/abs/2506.16288",
    "authors": [
      "Leo Gagnon",
      "Eric Elmoznino",
      "Sarthak Mittal",
      "Tom Marty",
      "Tejas Kasetty",
      "Dhanya Sridhar",
      "Guillaume Lajoie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16297",
    "title": "SycnMapV2: Robust and Adaptive Unsupervised Segmentation",
    "abstract": "           Human vision excels at segmenting visual cues without the need for explicit training, and it remains remarkably robust even as noise severity increases. In contrast, existing AI algorithms struggle to maintain accuracy under similar conditions. Here, we present SyncMapV2, the first to solve unsupervised segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop observed in SOTA this http URL superior performance extends across various types of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training, supervision, or loss functions. It is based on a learning paradigm that uses self-organizing dynamical equations combined with concepts from random networks. Moreover,unlike conventional methods that require re-initialization for each new input, SyncMapV2 adapts online, mimicking the continuous adaptability of human vision. Thus, we go beyond the accurate and robust results, and present the first algorithm that can do all the above online, adapting to input rather than re-initializing. In adaptability tests, SyncMapV2 demonstrates near-zero performance degradation, which motivates and fosters a new generation of robust and adaptive intelligence in the near future.         ",
    "url": "https://arxiv.org/abs/2506.16297",
    "authors": [
      "Heng Zhang",
      "Zikang Wan",
      "Danilo Vasconcellos Vargas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16302",
    "title": "Cascade-driven opinion dynamics on social networks",
    "abstract": "           Online social networks (OSNs) have transformed the way individuals fulfill their social needs and consume information. As OSNs become increasingly prominent sources for news dissemination, individuals often encounter content that influences their opinions through both direct interactions and broader network dynamics. In this paper, we propose the Friedkin-Johnsen on Cascade (FJC) model, which is, to the best of our knowledge, is the first attempt to integrate information cascades and opinion dynamics, specifically using the very popular Friedkin-Johnsen model. Our model, validated over real social cascades, highlights how the convergence of socialization and sharing news on these platforms can disrupt opinion evolution dynamics typically observed in offline settings. Our findings demonstrate that these cascades can amplify the influence of central opinion leaders, making them more resistant to divergent viewpoints, even when challenged by a critical mass of dissenting opinions. This research underscores the importance of understanding the interplay between social dynamics and information flow in shaping public discourse in the digital age.         ",
    "url": "https://arxiv.org/abs/2506.16302",
    "authors": [
      "Elisabetta Biondi",
      "Chiara Boldrini",
      "Andrea Passarella",
      "Marco Conti"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2506.16313",
    "title": "Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks",
    "abstract": "           Efficiently identifying the right trajectories for training remains an open problem in GFlowNets. To address this, it is essential to prioritize exploration in regions of the state space where the reward distribution has not been sufficiently learned. This calls for uncertainty-driven exploration, in other words, the agent should be aware of what it does not know. This attribute can be measured by joint predictions, which are particularly important for combinatorial and sequential decision problems. In this research, we integrate epistemic neural networks (ENN) with the conventional architecture of GFlowNets to enable more efficient joint predictions and better uncertainty quantification, thereby improving exploration and the identification of optimal trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the baseline method in GFlownets and evaluated in grid environments and structured sequence generation in various settings, demonstrating both its efficacy and efficiency.         ",
    "url": "https://arxiv.org/abs/2506.16313",
    "authors": [
      "Sajan Muhammad",
      "Salem Lahlou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16331",
    "title": "Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification",
    "abstract": "           Neural Networks are the state of the art for many tasks in the computer vision domain, including Writer Identification (WI) and Writer Verification (WV). The transparency of these \"black box\" systems is important for improvements of performance and reliability. For this work, two transparency techniques are applied to neural networks trained on WI and WV for the first time in this domain. The first technique provides pixel-level saliency maps, while the point-specific saliency maps of the second technique provide information on similarities between two images. The transparency techniques are evaluated using deletion and insertion score metrics. The goal is to support forensic experts with information on similarities in handwritten text and to explore the characteristics selected by a neural network for the identification process. For the qualitative evaluation, the highlights of the maps are compared to the areas forensic experts consider during the identification process. The evaluation results show that the pixel-wise saliency maps outperform the point-specific saliency maps and are suitable for the support of forensic experts.         ",
    "url": "https://arxiv.org/abs/2506.16331",
    "authors": [
      "Viktoria Pundy",
      "Marco Peer",
      "Florian Kleber"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16343",
    "title": "Analyzing the Influence of Knowledge Graph Information on Relation Extraction",
    "abstract": "           We examine the impact of incorporating knowledge graph information on the performance of relation extraction models across a range of datasets. Our hypothesis is that the positions of entities within a knowledge graph provide important insights for relation extraction tasks. We conduct experiments on multiple datasets, each varying in the number of relations, training examples, and underlying knowledge graphs. Our results demonstrate that integrating knowledge graph information significantly enhances performance, especially when dealing with an imbalance in the number of training examples for each relation. We evaluate the contribution of knowledge graph-based features by combining established relation extraction methods with graph-aware Neural Bellman-Ford networks. These features are tested in both supervised and zero-shot settings, demonstrating consistent performance improvements across various datasets.         ",
    "url": "https://arxiv.org/abs/2506.16343",
    "authors": [
      "Cedric M\u00f6ller",
      "Ricardo Usbeck"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2506.16380",
    "title": "Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data",
    "abstract": "           This paper presents a novel system for monitoring cattle behavior and detecting estrus (heat) periods using sensor data and machine learning. We designed and deployed a low-cost Bluetooth-based neck collar equipped with accelerometer and gyroscope sensors to capture real-time behavioral data from real cows, which was synced to the cloud. A labeled dataset was created using synchronized CCTV footage to annotate behaviors such as feeding, rumination, lying, and others. We evaluated multiple machine learning models -- Support Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks (CNN) -- for behavior classification. Additionally, we implemented a Long Short-Term Memory (LSTM) model for estrus detection using behavioral patterns and anomaly detection. Our system achieved over 93% behavior classification accuracy and 96% estrus detection accuracy on a limited test set. The approach offers a scalable and accessible solution for precision livestock monitoring, especially in resource-constrained environments.         ",
    "url": "https://arxiv.org/abs/2506.16380",
    "authors": [
      "Druva Dhakshinamoorthy",
      "Avikshit Jha",
      "Sabyasachi Majumdar",
      "Devdulal Ghosh",
      "Ranjita Chakraborty",
      "Hena Ray"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16381",
    "title": "InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems",
    "abstract": "           In modern speech synthesis, paralinguistic information--such as a speaker's vocal timbre, emotional state, and dynamic prosody--plays a critical role in conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS) systems rely on fixed style labels or inserting a speech prompt to control these cues, which severely limits flexibility. Recent attempts seek to employ natural-language instructions to modulate paralinguistic features, substantially improving the generalization of instruction-driven TTS models. Although many TTS systems now support customized synthesis via textual description, their actual ability to interpret and execute complex instructions remains largely unexplored. In addition, there is still a shortage of high-quality benchmarks and automated evaluation metrics specifically designed for instruction-based TTS, which hinders accurate assessment and iterative optimization of these models. To address these limitations, we introduce InstructTTSEval, a benchmark for measuring the capability of complex natural-language style control. We introduce three tasks, namely Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play, including English and Chinese subsets, each with 1k test cases (6k in total) paired with reference audio. We leverage Gemini as an automatic judge to assess their instruction-following abilities. Our evaluation of accessible instruction-following TTS systems highlights substantial room for further improvement. We anticipate that InstructTTSEval will drive progress toward more powerful, flexible, and accurate instruction-following TTS.         ",
    "url": "https://arxiv.org/abs/2506.16381",
    "authors": [
      "Kexin Huang",
      "Qian Tu",
      "Liwei Fan",
      "Chenchen Yang",
      "Dong Zhang",
      "Shimin Li",
      "Zhaoye Fei",
      "Qinyuan Cheng",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.16388",
    "title": "HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection",
    "abstract": "           This paper presents our approach to multi-label emotion detection in Hausa, a low-resource African language, as part of SemEval Track A. We fine-tuned AfriBERTa, a transformer-based model pre-trained on African languages, to classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and surprise. Our methodology involved data preprocessing, tokenization, and model fine-tuning using the Hugging Face Trainer API. The system achieved a validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the effectiveness of transformer-based models for emotion detection in low-resource languages.         ",
    "url": "https://arxiv.org/abs/2506.16388",
    "authors": [
      "Sani Abdullahi Sani",
      "Salim Abubakar",
      "Falalu Ibrahim Lawan",
      "Abdulhamid Abubakar",
      "Maryam Bala"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.16392",
    "title": "State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification",
    "abstract": "           While accurate, black-box system identification models lack interpretability of the underlying system dynamics. This paper proposes State-Space Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating Kolmogorov-Arnold Networks within a state-space framework. The proposed model is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein benchmarks. Results show that SS-KAN provides enhanced interpretability due to sparsity-promoting regularization and the direct visualization of its learned univariate functions, which reveal system nonlinearities at the cost of accuracy when compared to state-of-the-art black-box models, highlighting SS-KAN as a promising approach for interpretable nonlinear system identification, balancing accuracy and interpretability of nonlinear system dynamics.         ",
    "url": "https://arxiv.org/abs/2506.16392",
    "authors": [
      "Gon\u00e7alo Granjal Cruz",
      "Balazs Renczes",
      "Mark C Runacres",
      "Jan Decuyper"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.16395",
    "title": "OJBench: A Competition Level Code Benchmark For Large Language Models",
    "abstract": "           Recent advancements in large language models (LLMs) have demonstrated significant progress in math and code reasoning capabilities. However, existing code benchmark are limited in their ability to evaluate the full spectrum of these capabilities, particularly at the competitive level. To bridge this gap, we introduce OJBench, a novel and challenging benchmark designed to assess the competitive-level code reasoning abilities of LLMs. OJBench comprises 232 programming competition problems from NOI and ICPC, providing a more rigorous test of models' reasoning skills. We conducted a comprehensive evaluation using OJBench on 37 models, including both closed-source and open-source models, reasoning-oriented and non-reasoning-oriented models. Our results indicate that even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems. This highlights the significant challenges that models face in competitive-level code reasoning.         ",
    "url": "https://arxiv.org/abs/2506.16395",
    "authors": [
      "Zhexu Wang",
      "Yiping Liu",
      "Yejie Wang",
      "Wenyang He",
      "Bofei Gao",
      "Muxi Diao",
      "Yanxu Chen",
      "Kelin Fu",
      "Flood Sung",
      "Zhilin Yang",
      "Tianyu Liu",
      "Weiran Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.16400",
    "title": "Physical-Layer Signal Injection Attacks on EV Charging Ports: Bypassing Authentication via Electrical-Level Exploits",
    "abstract": "           The proliferation of electric vehicles in recent years has significantly expanded the charging infrastructure while introducing new security risks to both vehicles and chargers. In this paper, we investigate the security of major charging protocols such as SAE J1772, CCS, IEC 61851, GB/T 20234, and NACS, uncovering new physical signal spoofing attacks in their authentication mechanisms. By inserting a compact malicious device into the charger connector, attackers can inject fraudulent signals to sabotage the charging process, leading to denial of service, vehicle-induced charger lockout, and damage to the chargers or the vehicle's charge management system. To demonstrate the feasibility of our attacks, we propose PORTulator, a proof-of-concept (PoC) attack hardware, including a charger gun plugin device for injecting physical signals and a wireless controller for remote manipulation. By evaluating PORTulator on multiple real-world chargers, we identify 7 charging standards used by 20 charger piles that are vulnerable to our attacks. The root cause is that chargers use simple physical signals for authentication and control, making them easily spoofed by attackers. To address this issue, we propose enhancing authentication circuits by integrating non-resistive memory components and utilizing dynamic high-frequency Pulse Width Modulation (PWM) signals to counter such physical signal spoofing attacks.         ",
    "url": "https://arxiv.org/abs/2506.16400",
    "authors": [
      "Hetian Shi",
      "Yi He",
      "Shangru Song",
      "Jianwei Zhuge",
      "Jian Mao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16404",
    "title": "Generating Directed Graphs with Dual Attention and Asymmetric Encoding",
    "abstract": "           Directed graphs naturally model systems with asymmetric, ordered relationships, essential to applications in biology, transportation, social networks, and visual understanding. Generating such graphs enables tasks such as simulation, data augmentation and novel instance discovery; however, directed graph generation remains underexplored. We identify two key factors limiting progress in this direction: first, modeling edge directionality introduces a substantially larger dependency space, making the underlying distribution harder to learn; second, the absence of standardized benchmarks hinders rigorous evaluation. Addressing the former requires more expressive models that are sensitive to directional topologies. We propose Directo, the first generative model for directed graphs built upon the discrete flow matching framework. Our approach combines: (i) principled positional encodings tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism capturing both incoming and outgoing dependencies, and (iii) a robust, discrete generative framework. To support evaluation, we introduce a benchmark suite covering synthetic and real-world datasets. It shows that our method performs strongly across diverse settings and even competes with specialized models for particular classes, such as directed acyclic graphs. Our results highlight the effectiveness and generality of our approach, establishing a solid foundation for future research in directed graph generation.         ",
    "url": "https://arxiv.org/abs/2506.16404",
    "authors": [
      "Alba Carballo-Castro",
      "Manuel Madeira",
      "Yiming Qin",
      "Dorina Thanou",
      "Pascal Frossard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16407",
    "title": "Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks",
    "abstract": "           Visual Document Understanding (VDU) systems have achieved strong performance in information extraction by integrating textual, layout, and visual signals. However, their robustness under realistic adversarial perturbations remains insufficiently explored. We introduce the first unified framework for generating and evaluating multi-modal adversarial attacks on OCR-based VDU models. Our method covers six gradient-based layout attack scenarios, incorporating manipulations of OCR bounding boxes, pixels, and texts across both word and line granularities, with constraints on layout perturbation budget (e.g., IoU >= 0.6) to preserve plausibility. Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and six model families demonstrate that line-level attacks and compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation. Projected Gradient Descent (PGD)-based BBox perturbations outperform random-shift baselines in all investigated models. Ablation studies further validate the impact of layout budget, text modification, and adversarial transferability.         ",
    "url": "https://arxiv.org/abs/2506.16407",
    "authors": [
      "Dong Nguyen Tien",
      "Dung D. Le"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16409",
    "title": "LoRaIN: A Constructive Interference-Assisted Reliable and Energy-Efficient LoRa Indoor Network",
    "abstract": "           LoRa is a promising communication technology for enabling the next-generation indoor Internet of Things applications. Very few studies, however, have analyzed its performance indoors. Besides, these indoor studies investigate mostly the RSSI and SNR of the received packets at the gateway, which, as we show, may not unfold the poor performance of LoRa and its MAC protocol, LoRaWAN, indoors in terms of reliability and energy-efficiency. In this paper, we extensively evaluate the performance of LoRaWAN indoors and then use the key insights to boost its reliability and energy-efficiency by proposing LoRaIN, LoRa Indoor Network, a new link-layer protocol that can be effectively used for indoor deployments. The approach to boosting the reliability and energy efficiency in LoRaIN is underpinned by enabling constructive interference with specific timing requirements analyzed both empirically and mathematically for different pairs of channel bandwidth and spreading factor and relaying precious acknowledgments to the end-devices with the assistance of several booster nodes. The booster nodes do not need any special capability and can be a subset of the LoRa end-devices. To our knowledge, LoRaIN is the first protocol for boosting reliability and energy-efficiency in indoor LoRa networks. We evaluate its performance in an indoor testbed consisting of one LoRaWAN gateway and 20 end-devices. Our extensive evaluation shows that when 15% of the end-devices operate as booster nodes, the reliability at the gateway increases from 62% to 95%, and the end-devices are approximately 2.5x energy-efficient.         ",
    "url": "https://arxiv.org/abs/2506.16409",
    "authors": [
      "Mahbubur Rahman",
      "Abusayeed Saifullah"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.16412",
    "title": "Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse",
    "abstract": "           Generative AI (GAI) technologies are quickly reshaping the educational landscape. As adoption accelerates, understanding how students and educators perceive these tools is essential. This study presents one of the most comprehensive analyses to date of stakeholder discourse dynamics on GAI in education using social media data. Our dataset includes 1,199 Reddit posts and 13,959 corresponding top-level comments. We apply sentiment analysis, topic modeling, and author classification. To support this, we propose and validate a modular framework that leverages prompt-based large language models (LLMs) for analysis of online social discourse, and we evaluate this framework against classical natural language processing (NLP) models. Our GPT-4o pipeline consistently outperforms prior approaches across all tasks. For example, it achieved 90.6% accuracy in sentiment analysis against gold-standard human annotations. Topic extraction uncovered 12 latent topics in the public discourse with varying sentiment and author distributions. Teachers and students convey optimism about GAI's potential for personalized learning and productivity in higher education. However, key differences emerged: students often voice distress over false accusations of cheating by AI detectors, while teachers generally express concern about job security, academic integrity, and institutional pressures to adopt GAI tools. These contrasting perspectives highlight the tension between innovation and oversight in GAI-enabled learning environments. Our findings suggest a need for clearer institutional policies, more transparent GAI integration practices, and support mechanisms for both educators and students. More broadly, this study demonstrates the potential of LLM-based frameworks for modeling stakeholder discourse within online communities.         ",
    "url": "https://arxiv.org/abs/2506.16412",
    "authors": [
      "Paulina DeVito",
      "Akhil Vallala",
      "Sean Mcmahon",
      "Yaroslav Hinda",
      "Benjamin Thaw",
      "Hanqi Zhuang",
      "Hari Kalva"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2506.16418",
    "title": "Efficient Transformations in Deep Learning Convolutional Neural Networks",
    "abstract": "           This study investigates the integration of signal processing transformations -- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete Cosine Transform (DCT) -- within the ResNet50 convolutional neural network (CNN) model for image classification. The primary objective is to assess the trade-offs between computational efficiency, energy consumption, and classification accuracy during training and inference. Using the CIFAR-100 dataset (100 classes, 60,000 images), experiments demonstrated that incorporating WHT significantly reduced energy consumption while improving accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified ResNet50 incorporating WHT in the early convolutional layers achieved 74% accuracy, and an enhanced version with WHT applied to both early and late layers achieved 79% accuracy, with an average energy consumption of only 39 kJ per model. These results demonstrate the potential of WHT as a highly efficient and effective approach for energy-constrained CNN applications.         ",
    "url": "https://arxiv.org/abs/2506.16418",
    "authors": [
      "Berk Yilmaz",
      "Daniel Fidel Harvey",
      "Prajit Dhuri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2506.16427",
    "title": "Full-Pose Tracking via Robust Control for Over-Actuated Multirotors",
    "abstract": "           This paper presents a robust cascaded control architecture for over-actuated multirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI) control combined with structured H_inf control, initially proposed for under-actuated multirotors, to a broader range of multirotor configurations. To achieve precise and robust attitude and position tracking, we employ a weighted least-squares geometric guidance control allocation method, formulated as a quadratic optimization problem, enabling full-pose tracking. The proposed approach effectively addresses key challenges, such as preventing infeasible pose references and enhancing robustness against disturbances, as well as considering multirotor's actual physical limitations. Numerical simulations with an over-actuated hexacopter validate the method's effectiveness, demonstrating its adaptability to diverse mission scenarios and its potential for real-world aerial applications.         ",
    "url": "https://arxiv.org/abs/2506.16427",
    "authors": [
      "Mohamad Hachem",
      "Cl\u00e9ment Roos",
      "Thierry Miquel",
      "Murat Bronz"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.16440",
    "title": "Evaluating the Use of LLMs for Documentation to Code Traceability",
    "abstract": "           Large Language Models (LLMs) offer new potential for automating documentation-to-code traceability, yet their capabilities remain underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5 Sonnet, GPT-4o, and o3-mini) in establishing trace links between various software documentation (including API references and user guides) and source code. We create two novel datasets from two open-source projects (Unity Catalog and Crawl4AI). Through systematic experiments, we assess three key capabilities: (1) trace link identification accuracy, (2) relationship explanation quality, and (3) multi-step chain reconstruction. Results show that the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two datasets, substantially outperforming our baselines (TF-IDF, BM25, and CodeBERT). While fully correct relationship explanations range from 42.9% to 71.1%, partial accuracy exceeds 97%, indicating that fundamental connections are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy but vary in capturing precise intermediate links. Error analysis reveals that many false positives stem from naming-based assumptions, phantom links, or overgeneralization of architectural patterns. We demonstrate that task-framing, such as a one-to-many matching strategy, is critical for performance. These findings position LLMs as powerful assistants for trace discovery, but their limitations could necessitate human-in-the-loop tool design and highlight specific error patterns for future research.         ",
    "url": "https://arxiv.org/abs/2506.16440",
    "authors": [
      "Ebube Alor",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.16443",
    "title": "Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks",
    "abstract": "           Physics-informed neural networks (PINNs) offer a powerful approach to solving partial differential equations (PDEs), which are ubiquitous in the quantitative sciences. Applied to both forward and inverse problems across various scientific domains, PINNs have recently emerged as a valuable tool in the field of scientific machine learning. A key aspect of their training is that the data -- spatio-temporal points sampled from the PDE's input domain -- are readily available. Influence functions, a tool from the field of explainable AI (XAI), approximate the effect of individual training points on the model, enhancing interpretability. In the present work, we explore the application of influence function-based sampling approaches for the training data. Our results indicate that such targeted resampling based on data attribution methods has the potential to enhance prediction accuracy in physics-informed neural networks, demonstrating a practical application of an XAI method in PINN training.         ",
    "url": "https://arxiv.org/abs/2506.16443",
    "authors": [
      "Jonas R. Naujoks",
      "Aleksander Krasowski",
      "Moritz Weckbecker",
      "Galip \u00dcmit Yolcu",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Ren\u00e9 P. Klausen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2506.16447",
    "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models",
    "abstract": "           Backdoor unalignment attacks against Large Language Models (LLMs) enable the stealthy compromise of safety alignment using a hidden trigger while evading normal safety auditing. These attacks pose significant threats to the applications of LLMs in the real-world Large Language Model as a Service (LLMaaS) setting, where the deployed model is a fully black-box system that can only interact through text. Furthermore, the sample-dependent nature of the attack target exacerbates the threat. Instead of outputting a fixed label, the backdoored LLM follows the semantics of any malicious command with the hidden trigger, significantly expanding the target space. In this paper, we introduce BEAT, a black-box defense that detects triggered samples during inference to deactivate the backdoor. It is motivated by an intriguing observation (dubbed the probe concatenate effect), where concatenated triggered samples significantly reduce the refusal rate of the backdoored LLM towards a malicious probe, while non-triggered samples have little effect. Specifically, BEAT identifies whether an input is triggered by measuring the degree of distortion in the output distribution of the probe before and after concatenation with the input. Our method addresses the challenges of sample-dependent targets from an opposite perspective. It captures the impact of the trigger on the refusal signal (which is sample-independent) instead of sample-specific successful attack behaviors. It overcomes black-box access limitations by using multiple sampling to approximate the output distribution. Extensive experiments are conducted on various backdoor attacks and LLMs (including the closed-source GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense. Besides, we also preliminarily verify that BEAT can effectively defend against popular jailbreak attacks, as they can be regarded as 'natural backdoors'.         ",
    "url": "https://arxiv.org/abs/2506.16447",
    "authors": [
      "Biao Yi",
      "Tiansheng Huang",
      "Sishuo Chen",
      "Tong Li",
      "Zheli Liu",
      "Zhixuan Chu",
      "Yiming Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.16448",
    "title": "Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach",
    "abstract": "           EEG is a non-invasive, safe, and low-risk method to record electrophysiological signals inside the brain. Especially with recent technology developments like dry electrodes, consumer-grade EEG devices, and rapid advances in machine learning, EEG is commonly used as a resource for automatic emotion recognition. With the aim to develop a deep learning model that can perform EEG-based emotion recognition in a real-life context, we propose a novel approach to utilize multi-scale convolutional neural networks to accomplish such tasks. By implementing feature extraction kernels with many ratio coefficients as well as a new type of kernel that learns key information from four separate areas of the brain, our model consistently outperforms the state-of-the-art TSception model in predicting valence, arousal, and dominance scores across many performance evaluation metrics.         ",
    "url": "https://arxiv.org/abs/2506.16448",
    "authors": [
      "Tri Duc Ly",
      "Gia H. Ngo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16449",
    "title": "Unveiling Political Influence Through Social Media: Network and Causal Dynamics in the 2022 French Presidential Election",
    "abstract": "           During the 2022 French presidential election, we collected daily Twitter messages on key topics posted by political candidates and their close networks. Using a data-driven approach, we analyze interactions among political parties, identifying central topics that shape the landscape of political debate. Moving beyond traditional correlation analyses, we apply a causal inference technique: Convergent Cross Mapping, to uncover directional influences among political communities, revealing how some parties are more likely to initiate changes in activity while others tend to respond. This approach allows us to distinguish true influence from mere correlation, highlighting asymmetric relationships and hidden dynamics within the social media political network. Our findings demonstrate how specific issues, such as health and foreign policy, act as catalysts for cross-party influence, particularly during critical election phases. These insights provide a novel framework for understanding political discourse dynamics and have practical implications for campaign strategists and media analysts seeking to monitor and respond to shifts in political influence in real time.         ",
    "url": "https://arxiv.org/abs/2506.16449",
    "authors": [
      "Ixandra Achitouv",
      "David Chavalarias"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2506.16460",
    "title": "Black-Box Privacy Attacks on Shared Representations in Multitask Learning",
    "abstract": "           Multitask learning (MTL) has emerged as a powerful paradigm that leverages similarities among multiple learning tasks, each with insufficient samples to train a standalone model, to solve them simultaneously while minimizing data sharing across users and organizations. MTL typically accomplishes this goal by learning a shared representation that captures common structure among the tasks by embedding data from all tasks into a common feature space. Despite being designed to be the smallest unit of shared information necessary to effectively learn patterns across multiple tasks, these shared representations can inadvertently leak sensitive information about the particular tasks they were trained on. In this work, we investigate what information is revealed by the shared representations through the lens of inference attacks. Towards this, we propose a novel, black-box task-inference threat model where the adversary, given the embedding vectors produced by querying the shared representation on samples from a particular task, aims to determine whether that task was present when training the shared representation. We develop efficient, purely black-box attacks on machine learning models that exploit the dependencies between embeddings from the same task without requiring shadow models or labeled reference data. We evaluate our attacks across vision and language domains for multiple use cases of MTL and demonstrate that even with access only to fresh task samples rather than training data, a black-box adversary can successfully infer a task's inclusion in training. To complement our experiments, we provide theoretical analysis of a simplified learning setting and show a strict separation between adversaries with training samples and fresh samples from the target task's distribution.         ",
    "url": "https://arxiv.org/abs/2506.16460",
    "authors": [
      "John Abascal",
      "Nicol\u00e1s Berrios",
      "Alina Oprea",
      "Jonathan Ullman",
      "Adam Smith",
      "Matthew Jagielski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16476",
    "title": "Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection",
    "abstract": "           Implicit hate speech has recently emerged as a critical challenge for social media platforms. While much of the research has traditionally focused on harmful speech in general, the need for generalizable techniques to detect veiled and subtle forms of hate has become increasingly pressing. Based on lexicon analysis, we hypothesize that implicit hate speech is already present in publicly available harmful speech datasets but may not have been explicitly recognized or labeled by annotators. Additionally, crowdsourced datasets are prone to mislabeling due to the complexity of the task and often influenced by annotators' subjective interpretations. In this paper, we propose an approach to address the detection of implicit hate speech and enhance generalizability across diverse datasets by leveraging existing harmful speech datasets. Our method comprises three key components: influential sample identification, reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental results demonstrate the effectiveness of our approach in improving implicit hate detection, achieving a +12.9-point F1 score improvement compared to the baseline.         ",
    "url": "https://arxiv.org/abs/2506.16476",
    "authors": [
      "Saad Almohaimeed",
      "Saleh Almohaimeed",
      "Damla Turgut",
      "Ladislau B\u00f6l\u00f6ni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16492",
    "title": "Teaching Complex Systems based on Microservices",
    "abstract": "           Developing complex systems using microservices is a current challenge. In this paper, we present our experience with teaching this subject to more than 80 students at the University of S\u00e3o Paulo (USP), fostering team work and simulating the industry's environment. We show it is possible to teach such advanced concepts for senior undergraduate students of Computer Science and related fields.         ",
    "url": "https://arxiv.org/abs/2506.16492",
    "authors": [
      "Renato Cordeiro Ferreira",
      "Thatiane de Oliveira Rosa",
      "Alfredo Goldman",
      "Eduardo Guerra"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.16494",
    "title": "Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias",
    "abstract": "           Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart activity and are well-established tools for detecting and monitoring cardiovascular disease. However, manual ECG analysis can be time-consuming and prone to errors. Machine learning has emerged as a promising approach for automated heartbeat recognition and classification, but substantial variations in ECG signals make it challenging to develop generalizable models. ECG signals can vary widely across individuals and leads, while datasets often follow different labeling standards and may be biased, all of which greatly hinder supervised methods. Conventional unsupervised methods, e.g. principal component analysis, prioritize large (and often obvious) variances in the data and typically overlook subtle yet clinically relevant patterns. If labels are missing and/or variations are significant but small, both approaches fail. Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate these issues and identify medically relevant features in ECG signals, with no need for training or prior information. Using the MLII and V1 leads of the MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor embedding and uniform manifold approximation and projection can discriminate individual recordings in mixed populations with >= 90% accuracy and distinguish different arrhythmias in individual patients with a median accuracy of 98.96% and a median F1-score of 91.02%. The results show that NLDR holds much promise for cardiac monitoring, including the limiting cases of single-lead ECG and the current 12-lead standard of care, and for personalized health care beyond cardiology.         ",
    "url": "https://arxiv.org/abs/2506.16494",
    "authors": [
      "Amir Reza Vazifeh",
      "Jason W. Fleischer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2506.16507",
    "title": "Robust Reward Modeling via Causal Rubrics",
    "abstract": "           Reward models (RMs) are fundamental to aligning Large Language Models (LLMs) via human feedback, yet they often suffer from reward hacking. They tend to latch on to superficial or spurious attributes, such as response length or formatting, mistaking these cues learned from correlations in training data for the true causal drivers of quality (e.g., factuality, relevance). This occurs because standard training objectives struggle to disentangle these factors, leading to brittle RMs and misaligned policies. We introduce Crome (Causally Robust Reward Modeling), a novel framework grounded in an explicit causal model designed to mitigate reward hacking. Crome employs the following synthetic targeted augmentations during training: (1) Causal Augmentations, which are pairs that differ along specific causal attributes, to enforce sensitivity along each causal attribute individually, and (2) Neutral Augmentations, which are tie-label pairs varying primarily in spurious attributes, to enforce invariance along spurious attributes. Notably, our augmentations are produced without any knowledge of spurious factors, via answer interventions only along causal rubrics, that are identified by querying an oracle LLM. Empirically, Crome significantly outperforms standard baselines on RewardBench, improving average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in specific categories. The robustness of Crome is further testified by the consistent gains obtained in a Best-of-N inference setting across increasing N, across various benchmarks, including the popular RewardBench (covering chat, chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and the reasoning-specific GSM8k.         ",
    "url": "https://arxiv.org/abs/2506.16507",
    "authors": [
      "Pragya Srivastava",
      "Harman Singh",
      "Rahul Madhavan",
      "Gandharv Patil",
      "Sravanti Addepalli",
      "Arun Suggala",
      "Rengarajan Aravamudhan",
      "Soumya Sharma",
      "Anirban Laha",
      "Aravindan Raghuveer",
      "Karthikeyan Shanmugam",
      "Doina Precup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16553",
    "title": "One Sample is Enough to Make Conformal Prediction Robust",
    "abstract": "           Given any model, conformal prediction (CP) returns prediction sets guaranteed to include the true label with high adjustable probability. Robust CP (RCP) extends this to inputs with worst-case noise. A well-established approach is to use randomized smoothing for RCP since it is applicable to any black-box model and provides smaller sets compared to deterministic methods. However, current smoothing-based RCP requires many model forward passes per each input which is computationally expensive. We show that conformal prediction attains some robustness even with a forward pass on a single randomly perturbed input. Using any binary certificate we propose a single sample robust CP (RCP1). Our approach returns robust sets with smaller average set size compared to SOTA methods which use many (e.g. around 100) passes per input. Our key insight is to certify the conformal prediction procedure itself rather than individual scores. Our approach is agnostic to the setup (classification and regression). We further extend our approach to smoothing-based robust conformal risk control.         ",
    "url": "https://arxiv.org/abs/2506.16553",
    "authors": [
      "Soroush H. Zargarbashi",
      "Mohammad Sadegh Akhondzadeh",
      "Aleksandar Bojchevski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16575",
    "title": "Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System",
    "abstract": "           Large language models (LLMs) offer promising opportunities for organizational research. However, their built-in moderation systems can create problems when researchers try to analyze harmful content, often refusing to follow certain instructions or producing overly cautious responses that undermine validity of the results. This is particularly problematic when analyzing organizational conflicts such as microaggressions or hate speech. This paper introduces an Elo rating-based method that significantly improves LLM performance for harmful content analysis In two datasets, one focused on microaggression detection and the other on hate speech, we find that our method outperforms traditional LLM prompting techniques and conventional machine learning models on key measures such as accuracy, precision, and F1 scores. Advantages include better reliability when analyzing harmful content, fewer false positives, and greater scalability for large-scale datasets. This approach supports organizational applications, including detecting workplace harassment, assessing toxic communication, and fostering safer and more inclusive work environments.         ",
    "url": "https://arxiv.org/abs/2506.16575",
    "authors": [
      "Mustafa Akben",
      "Aaron Satko"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.16591",
    "title": "SparseDPD: A Sparse Neural Network-based Digital Predistortion FPGA Accelerator for RF Power Amplifier Linearization",
    "abstract": "           Digital predistortion (DPD) is crucial for linearizing radio frequency (RF) power amplifiers (PAs), improving signal integrity and efficiency in wireless systems. Neural network (NN)-based DPD methods surpass traditional polynomial models but face computational challenges limiting their practical deployment. This paper introduces SparseDPD, an FPGA accelerator employing a spatially sparse phase-normalized time-delay neural network (PNTDNN), optimized through unstructured pruning to reduce computational load without accuracy loss. Implemented on a Xilinx Zynq-7Z010 FPGA, SparseDPD operates at 170 MHz, achieving exceptional linearization performance (ACPR: -59.4 dBc, EVM: -54.0 dBc, NMSE: -48.2 dB) with only 241 mW dynamic power, using 64 parameters with 74% sparsity. This work demonstrates FPGA-based acceleration, making NN-based DPD practical and efficient for real-time wireless communication applications. Code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16591",
    "authors": [
      "Manno Versluis",
      "Yizhuo Wu",
      "Chang Gao"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2506.16602",
    "title": "SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics",
    "abstract": "           Graph neural networks have been useful in machine learning on graph-structured data, particularly for node classification and some types of graph classification tasks. However, they have had limited use in representing patterning of signals over graphs. Patterning of signals over graphs and in subgraphs carries important information in many domains including neuroscience. Neural signals are spatiotemporally patterned, high dimensional and difficult to decode. Graph signal processing and associated GCN models utilize the graph Fourier transform and are unable to efficiently represent spatially or spectrally localized signal patterning on graphs. Wavelet transforms have shown promise here, but offer non-canonical representations and cannot be tightly confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian harmonics optimally concentrate signal energy on specifically relevant subgraphs that are automatically learned with a mask. Thus, they can produce canonical and highly resolved representations of neural activity, focusing energy of harmonics on areas of the brain which are activated. We evaluated SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and two traffic dynamics datasets, comparing its performance against conventional GNNs and graph signal processing constructs. SlepNet outperforms the baselines in all datasets. Moreover, the extracted representations of signal patterns from SlepNet offers more resolution in distinguishing between similar patterns, and thus represent brain signaling transients as informative trajectories. Here we have shown that these extracted trajectory representations can be used for other downstream untrained tasks. Thus we establish that SlepNet is useful both for prediction and representation learning in spatiotemporal data.         ",
    "url": "https://arxiv.org/abs/2506.16602",
    "authors": [
      "Siddharth Viswanath",
      "Rahul Singh",
      "Yanlei Zhang",
      "J. Adam Noah",
      "Joy Hirsch",
      "Smita Krishnaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16611",
    "title": "Enabling Blockchain Interoperability Through Network Discovery Services",
    "abstract": "           Web3 technologies have experienced unprecedented growth in the last decade, achieving widespread adoption. As various blockchain networks continue to evolve, we are on the cusp of a paradigm shift in which they could provide services traditionally offered by the Internet, but in a decentralized manner, marking the emergence of the Internet of Blockchains. While significant progress has been achieved in enabling interoperability between blockchain networks, existing solutions often assume that networks are already mutually aware. This reveals a critical gap: the initial discovery of blockchain networks remains largely unaddressed. This paper proposes a decentralized architecture for blockchain network discovery that operates independently of any centralized authority. We also introduce a mechanism for discovering assets and services within a blockchain from external networks. Given the decentralized nature of the proposed discovery architecture, we design an incentive mechanism to encourage nodes to actively participate in maintaining the discovery network. The proposed architecture implemented and evaluated, using the Substrate framework, demonstrates its resilience and scalability, effectively handling up to 130,000 concurrent requests under the tested network configurations, with a median response time of 5.5 milliseconds, demonstrating the ability to scale its processing capacity further by increasing its network size.         ",
    "url": "https://arxiv.org/abs/2506.16611",
    "authors": [
      "Khalid Hassan",
      "Amirreza Sokhankhosh",
      "Sara Rouhani"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.16615",
    "title": "Centre driven Controlled Evolution of Wireless Virtual Networks based on Broadcast Tokens",
    "abstract": "           In a wireless sensor network, the virtual connectivity between nodes is a function of the keys shared between various nodes. Pre-embedding these key configurations in the nodes would make the network inflexible. On the other hand, permitting subsets of nodes to engage in a common key synthesis phase to create secure distributed connections amongst themselves, would decouple and conceal the information flow from the controlling centre. An intermediate solution is the notion of a centre driven key generation process through broadcast tokens, designed to extract different keys in different nodes based on some prior information stored at the nodes. As more tokens arrive, the virtual connectivity of the nodes are altered and the network evolves. This evolution can be distributed and can be controlled to converge to a certain specific connectivity profile. In this paper we present a framework and an algorithm which controls the simultaneous and distributed key release in different nodes, resulting in the creation of parallel virtual multicast groups. The design of the node shares and the supporting broadcast tokens have been discussed in conjunction with the process of balancing the spans of individual groups with spans of several coexistent multicast groups.         ",
    "url": "https://arxiv.org/abs/2506.16615",
    "authors": [
      "Vignesh Babu",
      "Atishay Jain",
      "Kannan Karthik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16618",
    "title": "Data marketplaces can increase the willingness to share social media data at low prices",
    "abstract": "           Living in the Post API age, researchers face unprecedented challenges in obtaining social media data, while users are concerned about how big tech companies use their data. Data donation offers a promising alternative, however, its scalability is limited by low participation and high dropout rates. Research suggests that data marketplaces could be a solution, but its realization remains challenging due to theoretical gaps in treating data as an asset. This paper examines whether data marketplaces can increase individuals willingness to sell their X (Twitter) data package and the minimum price they would accept. It also explores how privacy protections and the type of data buyer may affect these decisions. Results from two preregistered online survey experiments show that a data marketplace increases participants' willingness to sell their X data by 12 to 25 percentage points compared to data donation (depending on treatments), and by 6.8 points compared to onetime purchase offers. Although difference in minimum acceptable prices are not statistically significant, over 64 percentage of participants set their price within the marketplace's suggested range (0.25 to 2), substantially lower than the amounts offered in prior onetime purchase studies. Finally, in the marketplace setting, neither the type of buyer nor the inclusion of a privacy safeguard significantly influenced participants willingness to sell.         ",
    "url": "https://arxiv.org/abs/2506.16618",
    "authors": [
      "Meysam Alizadeh",
      "Fabrizio Gilardi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2506.16626",
    "title": "Few-Shot Learning-Based Cyber Incident Detection with Augmented Context Intelligence",
    "abstract": "           In recent years, the adoption of cloud services has been expanding at an unprecedented rate. As more and more organizations migrate or deploy their businesses to the cloud, a multitude of related cybersecurity incidents such as data breaches are on the rise. Many inherent attributes of cloud environments, for example, data sharing, remote access, dynamicity and scalability, pose significant challenges for the protection of cloud security. Even worse, cyber threats are becoming increasingly sophisticated and covert. Attack methods, such as Advanced Persistent Threats (APTs), are continually developed to bypass traditional security measures. Among the emerging technologies for robust threat detection, system provenance analysis is being considered as a promising mechanism, thus attracting widespread attention in the field of incident response. This paper proposes a new few-shot learning-based attack detection with improved data context intelligence. We collect operating system behavior data of cloud systems during realistic attacks and leverage an innovative semiotics extraction method to describe system events. Inspired by the advances in semantic analysis, which is a fruitful area focused on understanding natural languages in computational linguistics, we further convert the anomaly detection problem into a similarity comparison problem. Comprehensive experiments show that the proposed approach is able to generalize over unseen attacks and make accurate predictions, even if the incident detection models are trained with very limited samples.         ",
    "url": "https://arxiv.org/abs/2506.16626",
    "authors": [
      "Fei Zuo",
      "Junghwan Rhee",
      "Yung Ryn Choe",
      "Chenglong Fu",
      "Xianshan Qu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16627",
    "title": "FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models",
    "abstract": "           Neural signed-distance fields (SDFs) have become a versatile backbone for geometric learning, yet enforcing developable, CAD-style behavior still hinges on Gaussian curvature penalties that require full Hessian evaluation and second-order automatic differentiation, both of which are costly in memory and runtime. We present a curvature proxy that regularizes only the mixed second-order term (Weingarten term), allowing the two principal curvatures to adapt freely to data while suppressing unwanted warp. Two complementary instantiations realize this idea: (i) a finite-difference proxy that replaces each Hessian entry with four forward SDF evaluations and a single first-order gradient, and (ii) an autodiff proxy that computes the same mixed derivative via one Hessian-vector product, sidestepping explicit full Hessian assembly and remaining faster in practice. Both variants converge to the exact mixed second derivative, thus preserving the intended geometric bias without incurring full second-order graphs. On the ABC benchmarks, the proxies match or exceed the reconstruction fidelity of Hessian-based baselines while reducing GPU memory use and wall-clock time by a factor of two. Because the method is drop-in and framework-agnostic, it opens a practical path toward scalable, curvature-aware SDF learning for engineering-grade shape reconstruction.         ",
    "url": "https://arxiv.org/abs/2506.16627",
    "authors": [
      "Haotian Yin",
      "Aleksander Plocharski",
      "Michal Jan Wlodarczyk",
      "Mikolaj Kida",
      "Przemyslaw Musialski"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16644",
    "title": "Semantic Outlier Removal with Embedding Models and LLMs",
    "abstract": "           Modern text processing pipelines demand robust methods to remove extraneous content while preserving a document's core message. Traditional approaches such as HTML boilerplate extraction or keyword filters often fail in multilingual settings and struggle with context-sensitive nuances, whereas Large Language Models (LLMs) offer improved quality at high computational cost. We introduce SORE (Semantic Outlier Removal), a cost-effective, transparent method that leverages multilingual sentence embeddings and approximate nearest-neighbor search to identify and excise unwanted text segments. By first identifying core content via metadata embedding and then flagging segments that either closely match predefined outlier groups or deviate significantly from the core, SORE achieves near-LLM extraction precision at a fraction of the cost. Experiments on HTML datasets demonstrate that SORE outperforms structural methods and yield high precision in diverse scenarios. Our system is currently deployed in production, processing millions of documents daily across multiple languages while maintaining both efficiency and accuracy. To facilitate reproducibility and further research, we release our implementation and evaluation datasets.         ",
    "url": "https://arxiv.org/abs/2506.16644",
    "authors": [
      "Eren Akbiyik",
      "Jo\u00e3o Almeida",
      "Rik Melis",
      "Ritu Sriram",
      "Viviana Petrescu",
      "Vilhj\u00e1lmur Vilhj\u00e1lmsson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2506.16652",
    "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity",
    "abstract": "           Natural language instructions for robotic manipulation tasks often exhibit ambiguity and vagueness. For instance, the instruction \"Hang a mug on the mug tree\" may involve multiple valid actions if there are several mugs and branches to choose from. Existing language-conditioned policies typically rely on end-to-end models that jointly handle high-level semantic understanding and low-level action generation, which can result in suboptimal performance due to their lack of modularity and interpretability. To address these challenges, we introduce a novel robotic manipulation framework that can accomplish tasks specified by potentially ambiguous natural language. This framework employs a Vision-Language Model (VLM) to interpret abstract concepts in natural language instructions and generates task-specific code - an interpretable and executable intermediate representation. The generated code interfaces with the perception module to produce 3D attention maps that highlight task-relevant regions by integrating spatial and semantic information, effectively resolving ambiguities in instructions. Through extensive experiments, we identify key limitations of current imitation learning methods, such as poor adaptation to language and environmental variations. We show that our approach excels across challenging manipulation tasks involving language ambiguity, contact-rich manipulation, and multi-object interactions.         ",
    "url": "https://arxiv.org/abs/2506.16652",
    "authors": [
      "Guang Yin",
      "Yitong Li",
      "Yixuan Wang",
      "Dale McConachie",
      "Paarth Shah",
      "Kunimatsu Hashimoto",
      "Huan Zhang",
      "Katherine Liu",
      "Yunzhu Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.16656",
    "title": "Mesh-Informed Neural Operator : A Transformer Generative Approach",
    "abstract": "           Generative models in function spaces, situated at the intersection of generative modeling and operator learning, are attracting increasing attention due to their immense potential in diverse scientific and engineering applications. While functional generative models are theoretically domain- and discretization-agnostic, current implementations heavily rely on the Fourier Neural Operator (FNO), limiting their applicability to regular grids and rectangular domains. To overcome these critical limitations, we introduce the Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and cross-attention mechanisms, MINO offers a principled, domain- and discretization-agnostic backbone for generative modeling in function spaces. This advancement significantly expands the scope of such models to more diverse applications in generative, inverse, and regression tasks. Furthermore, MINO provides a unified perspective on integrating neural operators with general advanced deep learning architectures. Finally, we introduce a suite of standardized evaluation metrics that enable objective comparison of functional generative models, addressing another critical gap in the field.         ",
    "url": "https://arxiv.org/abs/2506.16656",
    "authors": [
      "Yaozhong Shi",
      "Zachary E. Ross",
      "Domniki Asimaki",
      "Kamyar Azizzadenesheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16677",
    "title": "PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration",
    "abstract": "           Trust prediction is a key issue in human-robot collaboration, especially in construction scenarios where maintaining appropriate trust calibration is critical for safety and efficiency. This paper introduces the Performance-guided Physiological signal-based Trust Prediction (PPTP), a novel framework designed to improve trust assessment. We designed a human-robot construction scenario with three difficulty levels to induce different trust states. Our approach integrates synchronized multimodal physiological signals (ECG, GSR, and EMG) with collaboration performance evaluation to predict human trust levels. Individual physiological signals are processed using collaboration performance information as guiding cues, leveraging the standardized nature of collaboration performance to compensate for individual variations in physiological responses. Extensive experiments demonstrate the efficacy of our cross-modality fusion method in significantly improving trust classification performance. Our model achieves over 81% accuracy in three-level trust classification, outperforming the best baseline method by 6.7%, and notably reaches 74.3% accuracy in high-resolution seven-level classification, which is a first in trust prediction research. Ablation experiments further validate the superiority of physiological signal processing guided by collaboration performance assessment.         ",
    "url": "https://arxiv.org/abs/2506.16677",
    "authors": [
      "Hao Guo",
      "Wei Fan",
      "Shaohui Liu",
      "Feng Jiang",
      "Chunzhi Yi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.16690",
    "title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches",
    "abstract": "           Stereo Depth estimation is a critical task in autonomous driving and robotics, where inaccuracies (such as misidentifying nearby objects as distant) can lead to dangerous situations. Adversarial attacks against stereo depth estimation can help reveal vulnerabilities before deployment. Previous work has shown that repeating optimized textures can effectively mislead stereo depth estimation in digital settings. However, our research reveals that these naively repeated texture structures perform poorly in physical-world implementations, i.e., when deployed as patches, limiting their practical utility for testing stereo depth estimation systems. In this work, for the first time, we discover that introducing regular intervals between repeated textures, creating a striped structure, significantly enhances the patch attack effectiveness. Through extensive experimentation, we analyze how variations of this novel structure influence the performance. Based on these insights, we develop a novel stereo depth attack that jointly optimizes both the striped structure and texture elements. Our generated adversarial patches can be inserted into any scenes and successfully attack state-of-the-art stereo depth estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can also attack commercial RGB-D cameras (Intel RealSense) in real-world conditions, demonstrating their practical relevance for security assessment of stereo systems.         ",
    "url": "https://arxiv.org/abs/2506.16690",
    "authors": [
      "Yun Xing",
      "Yue Cao",
      "Nhat Chung",
      "Jie Zhang",
      "Ivor Tsang",
      "Ming-Ming Cheng",
      "Yang Liu",
      "Lei Ma",
      "Qing Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16698",
    "title": "SIDE: Semantic ID Embedding for effective learning from sequences",
    "abstract": "           Sequence-based recommendations models are driving the state-of-the-art for industrial ad-recommendation systems. Such systems typically deal with user histories or sequence lengths ranging in the order of O(10^3) to O(10^4) events. While adding embeddings at this scale is manageable in pre-trained models, incorporating them into real-time prediction models is challenging due to both storage and inference costs. To address this scaling challenge, we propose a novel approach that leverages vector quantization (VQ) to inject a compact Semantic ID (SID) as input to the recommendation models instead of a collection of embeddings. Our method builds on recent works of SIDs by introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ fusion that fuses multiple content embeddings and categorical predictions into a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding conversion technique, called SIDE, that is validated with two content embedding collections, thereby eliminating the need for a large parameterized lookup table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which generalizes and enhances residual quantization techniques. The proposed enhancements when applied to a large-scale industrial ads-recommendation system achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in data footprint compared to traditional SID methods.         ",
    "url": "https://arxiv.org/abs/2506.16698",
    "authors": [
      "Dinesh Ramasamy",
      "Shakti Kumar",
      "Chris Cadonic",
      "Jiaxin Yang",
      "Sohini Roychowdhury",
      "Esam Abdel Rhman",
      "Srihari Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16723",
    "title": "TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data",
    "abstract": "           Serial pipeline training is an efficient paradigm for handling data heterogeneity in cross-silo federated learning with low communication overhead. However, even without centralized aggregation, direct transfer of models between clients can violate privacy regulations and remain susceptible to gradient leakage and linkage attacks. Additionally, ensuring resilience against semi-honest or malicious clients who may manipulate or misuse received models remains a grand challenge, particularly in privacy-sensitive domains such as healthcare. To address these challenges, we propose TriCon-SF, a novel serial federated learning framework that integrates triple shuffling and contribution awareness. TriCon-SF introduces three levels of randomization by shuffling model layers, data segments, and training sequences to break deterministic learning patterns and disrupt potential attack vectors, thereby enhancing privacy and robustness. In parallel, it leverages Shapley value methods to dynamically evaluate client contributions during training, enabling the detection of dishonest behavior and enhancing system accountability. Extensive experiments on non-IID healthcare datasets demonstrate that TriCon-SF outperforms standard serial and parallel federated learning in both accuracy and communication efficiency. Security analysis further supports its resilience against client-side privacy attacks.         ",
    "url": "https://arxiv.org/abs/2506.16723",
    "authors": [
      "Yuping Yan",
      "Yizhi Wang",
      "Yuanshuai Li",
      "Yaochu Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16735",
    "title": "3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting",
    "abstract": "           Recent approaches based on transform-based tensor nuclear norm (TNN) have demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by leveraging low-rank structures in latent representations. Recent developments incorporate deep transforms to improve low-rank tensor representation; however, existing approaches typically restrict the transform to the spectral mode, neglecting low-rank properties along other tensor modes. In this paper, we propose a novel 3-directional deep low-rank tensor representation (3DeepRep) model, which performs deep nonlinear transforms along all three modes of the HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of mode-i frontal slices in the corresponding latent space for each direction (i=1,2,3), forming a 3-directional TNN regularization. The outputs from the three directional branches are subsequently fused via a learnable aggregation module to produce the final result. An efficient gradient-based optimization algorithm is developed to solve the model in a self-supervised manner. Extensive experiments on real-world HSI datasets demonstrate that the proposed method achieves superior inpainting performance compared to existing state-of-the-art techniques, both qualitatively and quantitatively.         ",
    "url": "https://arxiv.org/abs/2506.16735",
    "authors": [
      "Yunshan Li",
      "Wenwu Gong",
      "Qianqian Wang",
      "Chao Wang",
      "Lili Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2506.16737",
    "title": "Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection",
    "abstract": "           Unmanned aerial vehicle (UAV) object detection plays a vital role in applications such as environmental monitoring and urban security. To improve robustness, recent studies have explored multimodal detection by fusing visible (RGB) and infrared (IR) imagery. However, due to UAV platform motion and asynchronous imaging, spatial misalignment frequently occurs between modalities, leading to weak alignment. This introduces two major challenges: semantic inconsistency at corresponding spatial locations and modality conflict during feature fusion. Existing methods often address these issues in isolation, limiting their effectiveness. In this paper, we propose Cross-modal Offset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that jointly tackles both challenges in weakly aligned UAV-based object detection. CoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA), which estimates attention-based spatial offsets and uses deformable convolution guided by a shared semantic space to align features more precisely; and the Dynamic Attention-guided Fusion Module (DAFM), which adaptively balances modality contributions through gating and refines fused features via spatial-channel dual attention. By integrating alignment and fusion in a unified design, CoDAF enables robust UAV object detection. Experiments on standard benchmarks validate the effectiveness of our approach, with CoDAF achieving a mAP of 78.6% on the DroneVehicle dataset.         ",
    "url": "https://arxiv.org/abs/2506.16737",
    "authors": [
      "Liu Zongzhen",
      "Luo Hui",
      "Wang Zhixing",
      "Wei Yuxing",
      "Zuo Haorui",
      "Zhang Jianlin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16743",
    "title": "Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention",
    "abstract": "           With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective this http URL, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art this http URL code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16743",
    "authors": [
      "Weinan Guan",
      "Wei Wang",
      "Bo Peng",
      "Ziwen He",
      "Jing Dong",
      "Haonan Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16744",
    "title": "IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification",
    "abstract": "           Hand gestures are a primary output of the human motor system, yet the decoding of their neuromuscular signatures remains a bottleneck for basic neuroscience and assistive technologies such as prosthetics. Traditional human-machine interface pipelines rely on a single biosignal modality, but multimodal fusion can exploit complementary information from sensors. We systematically compare linear and attention-based fusion strategies across three architectures: a Multimodal MLP, a Multimodal Transformer, and a Hierarchical Transformer, evaluating performance on scenarios with unimodal and multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2 (sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force). Across both datasets, the Hierarchical Transformer with attention-based fusion consistently achieved the highest accuracy, surpassing the multimodal and best single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7% on HD-sEMG. To investigate how modalities interact, we introduce an Isolation Network that selectively silences unimodal or cross-modal attention pathways, quantifying each group of token interactions' contribution to downstream decisions. Ablations reveal that cross-modal interactions contribute approximately 30% of the decision signal across transformer layers, highlighting the importance of attention-driven fusion in harnessing complementary modality information. Together, these findings reveal when and how multimodal fusion would enhance biosignal classification and also provides mechanistic insights of human muscle activities. The study would be beneficial in the design of sensor arrays for neurorobotic systems.         ",
    "url": "https://arxiv.org/abs/2506.16744",
    "authors": [
      "Eion Tyacke",
      "Kunal Gupta",
      "Jay Patel",
      "Rui Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2506.16753",
    "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
    "abstract": "           Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16753",
    "authors": [
      "Kosuke Nakanishi",
      "Akihiro Kubo",
      "Yuji Yasui",
      "Shin Ishii"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.16754",
    "title": "Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding",
    "abstract": "           The hyperbolic space, characterized by a constant negative curvature and exponentially expanding space, aligns well with the structural properties of heterogeneous graphs. However, although heterogeneous graphs inherently possess diverse power-law structures, most hyperbolic heterogeneous graph embedding models rely on a single hyperbolic space. This approach may fail to effectively capture the diverse power-law structures within heterogeneous graphs. To address this limitation, we propose a Metapath-based Hyperbolic Contrastive Learning framework (MHCL), which uses multiple hyperbolic spaces to capture diverse complex structures within heterogeneous graphs. Specifically, by learning each hyperbolic space to describe the distribution of complex structures corresponding to each metapath, it is possible to capture semantic information effectively. Since metapath embeddings represent distinct semantic information, preserving their discriminability is important when aggregating them to obtain node representations. Therefore, we use a contrastive learning approach to optimize MHCL and improve the discriminability of metapath embeddings. In particular, our contrastive learning method minimizes the distance between embeddings of the same metapath and maximizes the distance between those of different metapaths in hyperbolic space, thereby improving the separability of metapath embeddings with distinct semantic information. We conduct comprehensive experiments to evaluate the effectiveness of MHCL. The experimental results demonstrate that MHCL outperforms state-of-the-art baselines in various graph machine learning tasks, effectively capturing the complex structures of heterogeneous graphs.         ",
    "url": "https://arxiv.org/abs/2506.16754",
    "authors": [
      "Jongmin Park",
      "Seunghoon Han",
      "Won-Yong Shin",
      "Sungsu Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2506.16760",
    "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models",
    "abstract": "           Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation. Existing black-box jailbreak methods primarily rely on adversarial textual prompts or image perturbations, yet these approaches are highly detectable by standard content filtering systems and exhibit low query and computational efficiency. In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments. By leveraging LVLMs' cross-modal reasoning abilities, CAMO covertly reconstructs harmful instructions through multi-step reasoning, evading conventional detection mechanisms. Our approach supports adjustable reasoning complexity and requires significantly fewer queries than prior attacks, enabling both stealth and efficiency. Comprehensive evaluations conducted on leading LVLMs validate CAMO's effectiveness, showcasing robust performance and strong cross-model transferability. These results underscore significant vulnerabilities in current built-in safety mechanisms, emphasizing an urgent need for advanced, alignment-aware security and safety solutions in vision-language systems.         ",
    "url": "https://arxiv.org/abs/2506.16760",
    "authors": [
      "Lei Jiang",
      "Zixun Zhang",
      "Zizhou Wang",
      "Xiaobing Sun",
      "Zhen Li",
      "Liangli Zhen",
      "Xiaohua Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16773",
    "title": "Infrared and Visible Image Fusion Based on Implicit Neural Representations",
    "abstract": "           Infrared and visible light image fusion aims to combine the strengths of both modalities to generate images that are rich in information and fulfill visual or computational requirements. This paper proposes an image fusion method based on Implicit Neural Representations (INR), referred to as INRFuse. This method parameterizes a continuous function through a neural network to implicitly represent the multimodal information of the image, breaking through the traditional reliance on discrete pixels or explicit features. The normalized spatial coordinates of the infrared and visible light images serve as inputs, and multi-layer perceptrons is utilized to adaptively fuse the features of both modalities, resulting in the output of the fused image. By designing multiple loss functions, the method jointly optimizes the similarity between the fused image and the original images, effectively preserving the thermal radiation information of the infrared image while maintaining the texture details of the visible light image. Furthermore, the resolution-independent characteristic of INR allows for the direct fusion of images with varying resolutions and achieves super-resolution reconstruction through high-density coordinate queries. Experimental results indicate that INRFuse outperforms existing methods in both subjective visual quality and objective evaluation metrics, producing fused images with clear structures, natural details, and rich information without the necessity for a training dataset.         ",
    "url": "https://arxiv.org/abs/2506.16773",
    "authors": [
      "Shuchen Sun",
      "Ligen Shi",
      "Chang Liu",
      "Lina Wu",
      "Jun Qiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16786",
    "title": "Dependability of UAV-Based Networks and Computing Systems: A Survey",
    "abstract": "           Uncrewed Aerial Vehicle (UAV) computing and networking are becoming a fundamental computation infrastructure for diverse cyber-physical application systems. UAVs can be empowered by AI on edge devices and can communicate with other UAVs and ground stations via wireless communication networks. Dynamic computation demands and heterogeneous computing resources are distributed in the system and need to be controlled to maintain the quality of services and to accomplish critical missions. With the evolution of UAV-based systems, dependability assurance of such systems emerges as a crucial challenge. UAV-based systems confront diverse sources of uncertainty that may threaten their dependability, such as software bugs, component failures, network disconnections, battery shortages, and disturbances from the real world. In this paper, we conduct systematic literature reviews on the dependability of UAV-based networks and computing systems. The survey report reveals emerging research trends in this field and summarizes the literature into comprehensive categories by threat types and adopted technologies. Based on our literature reviews, we identify eight research fields that require further exploration in the future to achieve dependable UAV-based systems.         ",
    "url": "https://arxiv.org/abs/2506.16786",
    "authors": [
      "Qingyang Zhang",
      "Mohammad Dwipa Furqan",
      "Tasfia Nutzhat",
      "Fumio Machida",
      "Ermeson Andrade"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2506.16790",
    "title": "Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective",
    "abstract": "           Graph Neural Networks (GNNs) often suffer from performance degradation as the network depth increases. This paper addresses this issue by introducing initialization methods that enhance signal propagation (SP) within GNNs. We propose three key metrics for effective SP in GNNs: forward propagation, backward propagation, and graph embedding variation (GEV). While the first two metrics derive from classical SP theory, the third is specifically designed for GNNs. We theoretically demonstrate that a broad range of commonly used initialization methods for GNNs, which exhibit performance degradation with increasing depth, fail to control these three metrics simultaneously. To deal with this limitation, a direct exploitation of the SP analysis--searching for weight initialization variances that optimize the three metrics--is shown to significantly enhance the SP in deep GCNs. This approach is called Signal Propagation on Graph-guided Initialization (SPoGInit). Our experiments demonstrate that SPoGInit outperforms commonly used initialization methods on various tasks and architectures. Notably, SPoGInit enables performance improvements as GNNs deepen, which represents a significant advancement in addressing depth-related challenges and highlights the validity and effectiveness of the SP analysis framework.         ",
    "url": "https://arxiv.org/abs/2506.16790",
    "authors": [
      "Senmiao Wang",
      "Yupeng Chen",
      "Yushun Zhang",
      "Ruoyu Sun",
      "Tian Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16795",
    "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning",
    "abstract": "           Dynamic material handling (DMH) involves the assignment of dynamically arriving material transporting tasks to suitable vehicles in real time for minimising makespan and tardiness. In real-world scenarios, historical task records are usually available, which enables the training of a decision policy on multiple instances consisting of historical records. Recently, reinforcement learning has been applied to solve DMH. Due to the occurrence of dynamic events such as new tasks, adaptability is highly required. Solving DMH is challenging since constraints including task delay should be satisfied. A feedback is received only when all tasks are served, which leads to sparse reward. Besides, making the best use of limited computational resources and historical records for training a robust policy is crucial. The time allocated to different problem instances would highly impact the learning process. To tackle those challenges, this paper proposes a novel adaptive constrained evolutionary reinforcement learning (ACERL) approach, which maintains a population of actors for diverse exploration. ACERL accesses each actor for tackling sparse rewards and constraint violation to restrict the behaviour of the policy. Moreover, ACERL adaptively selects the most beneficial training instances for improving the policy. Extensive experiments on eight training and eight unseen test instances demonstrate the outstanding performance of ACERL compared with several state-of-the-art algorithms. Policies trained by ACERL can schedule the vehicles while fully satisfying the constraints. Additional experiments on 40 unseen noised instances show the robust performance of ACERL. Cross-validation further presents the overall effectiveness of ACREL. Besides, a rigorous ablation study highlights the coordination and benefits of each ingredient of ACERL.         ",
    "url": "https://arxiv.org/abs/2506.16795",
    "authors": [
      "Chengpeng Hu",
      "Ziming Wang",
      "Bo Yuan",
      "Jialin Liu",
      "Chengqi Zhang",
      "Xin Yao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16802",
    "title": "Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation",
    "abstract": "           Synthetic video generation is progressing very rapidly. The latest models can produce very realistic high-resolution videos that are virtually indistinguishable from real ones. Although several video forensic detectors have been recently proposed, they often exhibit poor generalization, which limits their applicability in a real-world scenario. Our key insight to overcome this issue is to guide the detector towards seeing what really matters. In fact, a well-designed forensic classifier should focus on identifying intrinsic low-level artifacts introduced by a generative architecture rather than relying on high-level semantic flaws that characterize a specific model. In this work, first, we study different generative architectures, searching and identifying discriminative features that are unbiased, robust to impairments, and shared across models. Then, we introduce a novel forensic-oriented data augmentation strategy based on the wavelet decomposition and replace specific frequency-related bands to drive the model to exploit more relevant forensic cues. Our novel training paradigm improves the generalizability of AI-generated video detectors, without the need for complex algorithms and large datasets that include multiple synthetic generators. To evaluate our approach, we train the detector using data from a single generative model and test it against videos produced by a wide range of other models. Despite its simplicity, our method achieves a significant accuracy improvement over state-of-the-art detectors and obtains excellent results even on very recent generative models, such as NOVA and FLUX. Code and data will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2506.16802",
    "authors": [
      "Riccardo Corvi",
      "Davide Cozzolino",
      "Ekta Prashnani",
      "Shalini De Mello",
      "Koki Nagano",
      "Luisa Verdoliva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16808",
    "title": "Using SRv6 to access Edge Applications in 5G Networks",
    "abstract": "           With the emergence of Multi-Access Edge Computing in 5G and beyond, it has become essential for operators to optimize the data path for the end-user while ensuring resources are used according to their policy. In this paper, we review existing solutions to access edge resources, underline their limits, and propose the use of Segment Routing over IPv6 (SRv6) in a 5G/edge architecture.         ",
    "url": "https://arxiv.org/abs/2506.16808",
    "authors": [
      "Louis Royer",
      "Emmanuel Lavinal",
      "Emmanuel Chaput"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.16809",
    "title": "Do high-order Gauss-Legendre methods admit a composition representation and a conjugate-symplectic counterpart?",
    "abstract": "           One of the most classical pairs of symplectic and conjugate-symplectic schemes is given by the Midpoint method (the Gauss--Runge--Kutta method of order 2) and the Trapezoidal rule. These can be interpreted as compositions of the Implicit and Explicit Euler methods, taken in direct and reverse order, respectively. This naturally raises the question of whether a similar composition structure exists for higher-order Gauss--Legendre methods. In this paper, we provide a positive answer in the case of the fourth-order method. The technique we employ also enables the derivation of a high-order dense output.         ",
    "url": "https://arxiv.org/abs/2506.16809",
    "authors": [
      "Felice Iavernaro",
      "Francesca Mazzia"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2506.16815",
    "title": "Robust Group Anomaly Detection for Quasi-Periodic Network Time Series",
    "abstract": "           Many real-world multivariate time series are collected from a network of physical objects embedded with software, electronics, and sensors. The quasi-periodic signals generated by these objects often follow a similar repetitive and periodic pattern, but have variations in the period, and come in different lengths caused by timing (synchronization) errors. Given a multitude of such quasi-periodic time series, can we build machine learning models to identify those time series that behave differently from the majority of the observations? In addition, can the models help human experts to understand how the decision was made? We propose a sequence to Gaussian Mixture Model (seq2GMM) framework. The overarching goal of this framework is to identify unusual and interesting time series within a network time series database. We further develop a surrogate-based optimization algorithm that can efficiently train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a plurality of public benchmark datasets, outperforming state-of-the-art anomaly detection techniques by a significant margin. We also theoretically analyze the convergence property of the proposed training algorithm and provide numerical results to substantiate our theoretical claims.         ",
    "url": "https://arxiv.org/abs/2506.16815",
    "authors": [
      "Kai Yang",
      "Shaoyu Dou",
      "Pan Luo",
      "Xin Wang",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16819",
    "title": "Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection",
    "abstract": "           The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16819",
    "authors": [
      "Yuchu Jiang",
      "Jiaming Chu",
      "Jian Zhao",
      "Xin Zhang",
      "Xu Yang",
      "Lei Jin",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16821",
    "title": "Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots",
    "abstract": "           Robust and accurate ball detection is a critical component for autonomous humanoid soccer robots, particularly in dynamic and challenging environments such as RoboCup outdoor fields. However, traditional supervised approaches require extensive manual annotation, which is costly and time-intensive. To overcome this problem, we present a self-supervised learning framework for domain-adaptive feature extraction to enhance ball detection performance. The proposed approach leverages a general-purpose pretrained model to generate pseudo-labels, which are then used in a suite of self-supervised pretext tasks -- including colorization, edge detection, and triplet loss -- to learn robust visual features without relying on manual annotations. Additionally, a model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid adaptation to new deployment scenarios with minimal supervision. A new dataset comprising 10,000 labeled images from outdoor RoboCup SPL matches is introduced, used to validate the method, and made available to the community. Experimental results demonstrate that the proposed pipeline outperforms baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting faster convergence.         ",
    "url": "https://arxiv.org/abs/2506.16821",
    "authors": [
      "Can Lin",
      "Daniele Affinita",
      "Marco E. P. Zimmatore",
      "Daniele Nardi",
      "Domenico D. Bloisi",
      "Vincenzo Suriani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16824",
    "title": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs",
    "abstract": "           Due to an exponential increase in published research articles, it is impossible for individual scientists to read all publications, even within their own research field. In this work, we investigate the use of large language models (LLMs) for the purpose of extracting the main concepts and semantic information from scientific abstracts in the domain of materials science to find links that were not noticed by humans and thus to suggest inspiring near/mid-term future research directions. We show that LLMs can extract concepts more efficiently than automated keyword extraction methods to build a concept graph as an abstraction of the scientific literature. A machine learning model is trained to predict emerging combinations of concepts, i.e. new research ideas, based on historical data. We demonstrate that integrating semantic concept information leads to an increased prediction performance. The applicability of our model is demonstrated in qualitative interviews with domain experts based on individualized model suggestions. We show that the model can inspire materials scientists in their creative thinking process by predicting innovative combinations of topics that have not yet been investigated.         ",
    "url": "https://arxiv.org/abs/2506.16824",
    "authors": [
      "Thomas Marwitz",
      "Alexander Colsmann",
      "Ben Breitung",
      "Christoph Brabec",
      "Christoph Kirchlechner",
      "Eva Blasco",
      "Gabriel Cadilha Marques",
      "Horst Hahn",
      "Michael Hirtz",
      "Pavel A. Levkin",
      "Yolita M. Eggeler",
      "Tobias Schl\u00f6der",
      "Pascal Friederich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16831",
    "title": "Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap",
    "abstract": "           This vision paper presents initial research on assessing the robustness and reliability of AI-enabled systems, and key factors in ensuring their safety and effectiveness in practical applications, including a focus on accountability. By exploring evolving definitions of these concepts and reviewing current literature, the study highlights major challenges and approaches in the field. A case study is used to illustrate real-world applications, emphasizing the need for innovative testing solutions. The incorporation of accountability is crucial for building trust and ensuring responsible AI development. The paper outlines potential future research directions and identifies existing gaps, positioning robustness, reliability, and accountability as vital areas for the development of trustworthy AI systems of the future.         ",
    "url": "https://arxiv.org/abs/2506.16831",
    "authors": [
      "Filippo Scaramuzza",
      "Damian A. Tamburri",
      "Willem-Jan van den Heuvel"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.16833",
    "title": "Hybrid-Sep: Language-queried audio source separation via pre-trained Model Fusion and Adversarial Diffusion Training",
    "abstract": "           Language-queried Audio Separation (LASS) employs linguistic queries to isolate target sounds based on semantic descriptions. However, existing methods face challenges in aligning complex auditory features with linguistic context while preserving separation precision. Current research efforts focus primarily on text description augmentation and architectural innovations, yet the potential of integrating pre-trained self-supervised learning (SSL) audio models and Contrastive Language-Audio Pretraining (CLAP) frameworks, capable of extracting cross-modal audio-text relationships, remains underexplored. To address this, we present HybridSep, a two-stage LASS framework that synergizes SSL-based acoustic representations with CLAP-derived semantic embeddings. Our framework introduces Adversarial Consistent Training (ACT), a novel optimization strategy that treats diffusion as an auxiliary regularization loss while integrating adversarial training to enhance separation fidelity. Experiments demonstrate that HybridSep achieves significant performance improvements over state-of-the-art baselines (e.g., AudioSep, FlowSep) across multiple metrics, establishing new benchmarks for LASS tasks.         ",
    "url": "https://arxiv.org/abs/2506.16833",
    "authors": [
      "Jianyuan Feng",
      "Guangzheng Li",
      "Yangfei Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.16844",
    "title": "Bandwidth Selectors on Semiparametric Bayesian Networks",
    "abstract": "           Semiparametric Bayesian networks (SPBNs) integrate parametric and non-parametric probabilistic models, offering flexibility in learning complex data distributions from samples. In particular, kernel density estimators (KDEs) are employed for the non-parametric component. Under the assumption of data normality, the normal rule is used to learn the bandwidth matrix for the KDEs in SPBNs. This matrix is the key hyperparameter that controls the trade-off between bias and variance. However, real-world data often deviates from normality, potentially leading to suboptimal density estimation and reduced predictive performance. This paper first establishes the theoretical framework for the application of state-of-the-art bandwidth selectors and subsequently evaluates their impact on SPBN performance. We explore the approaches of cross-validation and plug-in selectors, assessing their effectiveness in enhancing the learning capability and applicability of SPBNs. To support this investigation, we have extended the open-source package PyBNesian for SPBNs with the additional bandwidth selection techniques and conducted extensive experimental analyses. Our results demonstrate that the proposed bandwidth selectors leverage increasing information more effectively than the normal rule, which, despite its robustness, stagnates with more data. In particular, unbiased cross-validation generally outperforms the normal rule, highlighting its advantage in high sample size scenarios.         ",
    "url": "https://arxiv.org/abs/2506.16844",
    "authors": [
      "Victor Alejandre",
      "Concha Bielza",
      "Pedro Larra\u00f1aga"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.16855",
    "title": "Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning",
    "abstract": "           Time series analysis has achieved great success in cyber security such as intrusion detection and device identification. Learning similarities among multiple time series is a crucial problem since it serves as the foundation for downstream analysis. Due to the complex temporal dynamics of the event-triggered time series, it often remains unclear which similarity metric is appropriate for security-related tasks, such as anomaly detection and clustering. The overarching goal of this paper is to develop an unsupervised learning framework that is capable of learning similarities among a set of event-triggered time series. From the machine learning vantage point, the proposed framework harnesses the power of both hierarchical multi-resolution sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively learn the low-dimensional representations from the time series. Finally, the obtained similarity measure can be easily visualized for the explanation. The proposed framework aspires to offer a stepping stone that gives rise to a systematic approach to model and learn similarities among a multitude of event-triggered time series. Through extensive qualitative and quantitative experiments, it is revealed that the proposed method outperforms state-of-the-art methods considerably.         ",
    "url": "https://arxiv.org/abs/2506.16855",
    "authors": [
      "Shaoyu Dou",
      "Kai Yang",
      "Yang Jiao",
      "Chengbo Qiu",
      "Kui Ren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16856",
    "title": "ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control",
    "abstract": "           Autonomous parking plays a vital role in intelligent vehicle systems, particularly in constrained urban environments where high-precision control is required. While traditional rule-based parking systems struggle with environmental uncertainties and lack adaptability in crowded or dynamic scenes, human drivers demonstrate the ability to park intuitively without explicit modeling. Inspired by this observation, we propose a Transformer-based end-to-end framework for autonomous parking that learns from expert demonstrations. The network takes as input surround-view camera images, goal-point representations, ego vehicle motion, and pedestrian trajectories. It outputs discrete control sequences including throttle, braking, steering, and gear selection. A novel cross-attention module integrates BEV features with target points, and a GRU-based pedestrian predictor enhances safety by modeling dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both vertical and parallel parking scenarios. Experiments show our model achieves a high success rate of 96.57\\%, with average positional and orientation errors of 0.21 meters and 0.41 degrees, respectively. The ablation studies further demonstrate the effectiveness of key modules such as pedestrian prediction and goal-point attention fusion. The code and dataset will be released at: this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16856",
    "authors": [
      "Jun Fu",
      "Bin Tian",
      "Haonan Chen",
      "Shi Meng",
      "Tingting Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.16862",
    "title": "Optimal Depth of Neural Networks",
    "abstract": "           Determining the optimal depth of a neural network is a fundamental yet challenging problem, typically resolved through resource-intensive experimentation. This paper introduces a formal theoretical framework to address this question by recasting the forward pass of a deep network, specifically a Residual Network (ResNet), as an optimal stopping problem. We model the layer-by-layer evolution of hidden representations as a sequential decision process where, at each layer, a choice is made between halting computation to make a prediction or continuing to a deeper layer for a potentially more refined representation. This formulation captures the intrinsic trade-off between accuracy and computational cost. Our primary theoretical contribution is a proof that, under a plausible condition of diminishing returns on the residual functions, the expected optimal stopping depth is provably finite, even in an infinite-horizon setting. We leverage this insight to propose a novel and practical regularization term, $\\mathcal{L}_{\\rm depth}$, that encourages the network to learn representations amenable to efficient, early exiting. We demonstrate the generality of our framework by extending it to the Transformer architecture and exploring its connection to continuous-depth models via free-boundary problems. Empirical validation on ImageNet confirms that our regularizer successfully induces the theoretically predicted behavior, leading to significant gains in computational efficiency without compromising, and in some cases improving, final model accuracy.         ",
    "url": "https://arxiv.org/abs/2506.16862",
    "authors": [
      "Qian Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2506.16890",
    "title": "From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images",
    "abstract": "           The detection and localization of quality-related problems in industrially mass-produced products has historically relied on manual inspection, which is costly and error-prone. Machine learning has the potential to replace manual handling. As such, the desire is to facilitate an unsupervised (or self-supervised) approach, as it is often impossible to specify all conceivable defects ahead of time. A plethora of prior works have demonstrated the aptitude of common reconstruction-, embedding-, and synthesis-based methods in laboratory settings. However, in practice, we observe that most methods do not handle low data quality well or exude low robustness in unfavorable, but typical real-world settings. For practitioners it may be very difficult to identify the actual underlying problem when such methods underperform. Worse, often-reported metrics (e.g., AUROC) are rarely suitable in practice and may give misleading results. In our setting, we attempt to identify subtle anomalies on the surface of blasted forged metal parts, using rather low-quality RGB imagery only, which is a common industrial setting. We specifically evaluate two types of state-of-the-art models that allow us to identify and improve quality issues in production data, without having to obtain new data. Our contribution is to provide guardrails for practitioners that allow them to identify problems related to, e.g., (lack of) robustness or invariance, in either the chosen model or the data reliably in similar scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of likelihood-based approaches and outline a framework for proper empirical risk estimation that is more suitable for real-world scenarios.         ",
    "url": "https://arxiv.org/abs/2506.16890",
    "authors": [
      "Sebastian H\u00f6nel",
      "Jonas Nordqvist"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2506.16917",
    "title": "Error analysis of BDF schemes for the evolutionary incompressible Navier--Stokes equations",
    "abstract": "           Error bounds for fully discrete schemes for the evolutionary incompressible Navier--Stokes equations are derived in this paper. For the time integration we apply BDF-$q$ methods, $q\\le 5$, for which error bounds for $q\\ge 3$ cannot be found in the literature. Inf-sup stable mixed finite elements are used as spatial approximation. First, we analyze the standard Galerkin method and second a grad-div stabilized method. The grad-div stabilization allows to prove error bounds with constants independent of inverse powers of the viscosity coefficient. We prove optimal bounds for the velocity and pressure with order $(\\Delta t)^q$ in time for the BDF-$q$ scheme and order $h^{k+1}$ for the $L^2(\\Omega)$ error of the velocity in the first case and $h^k$ in the second case, $k$ being the degree of the polynomials in finite element velocity space.         ",
    "url": "https://arxiv.org/abs/2506.16917",
    "authors": [
      "Bosco Garc\u00eda-Archilla",
      "V. John",
      "Julia Novo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2506.16968",
    "title": "MM-AttacKG: A Multimodal Approach to Attack Graph Construction with Large Language Models",
    "abstract": "           Cyber Threat Intelligence (CTI) parsing aims to extract key threat information from massive data, transform it into actionable intelligence, enhance threat detection and defense efficiency, including attack graph construction, intelligence fusion and indicator extraction. Among these research topics, Attack Graph Construction (AGC) is essential for visualizing and understanding the potential attack paths of threat events from CTI reports. Existing approaches primarily construct the attack graphs purely from the textual data to reveal the logical threat relationships between entities within the attack behavioral sequence. However, they typically overlook the specific threat information inherent in visual modalities, which preserves the key threat details from inherently-multimodal CTI report. Therefore, we enhance the effectiveness of attack graph construction by analyzing visual information through Multimodal Large Language Models (MLLMs). Specifically, we propose a novel framework, MM-AttacKG, which can effectively extract key information from threat images and integrate it into attack graph construction, thereby enhancing the comprehensiveness and accuracy of attack graphs. It first employs a threat image parsing module to extract critical threat information from images and generate descriptions using MLLMs. Subsequently, it builds an iterative question-answering pipeline tailored for image parsing to refine the understanding of threat images. Finally, it achieves content-level integration between attack graphs and image-based answers through MLLMs, completing threat information enhancement. The experimental results demonstrate that MM-AttacKG can accurately identify key information in threat images and significantly improve the quality of multimodal attack graph construction, effectively addressing the shortcomings of existing methods in utilizing image-based threat information.         ",
    "url": "https://arxiv.org/abs/2506.16968",
    "authors": [
      "Yongheng Zhang",
      "Xinyun Zhao",
      "Yunshan Ma",
      "Haokai Ma",
      "Yingxiao Guan",
      "Guozheng Yang",
      "Yuliang Lu",
      "Xiang Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2506.16981",
    "title": "SmartGuard: Leveraging Large Language Models for Network Attack Detection through Audit Log Analysis and Summarization",
    "abstract": "           End-point monitoring solutions are widely deployed in today's enterprise environments to support advanced attack detection and investigation. These monitors continuously record system-level activities as audit logs and provide deep visibility into security events. Unfortunately, existing methods of semantic analysis based on audit logs have low granularity, only reaching the system call level, making it difficult to effectively classify highly covert behaviors. Additionally, existing works mainly match audit log streams with rule knowledge bases describing behaviors, which heavily rely on expertise and lack the ability to detect unknown attacks and provide interpretive descriptions. In this paper, we propose SmartGuard, an automated method that combines abstracted behaviors from audit event semantics with large language models. SmartGuard extracts specific behaviors (function level) from incoming system logs and constructs a knowledge graph, divides events by threads, and combines event summaries with graph embeddings to achieve information diagnosis and provide explanatory narratives through large language models. Our evaluation shows that SmartGuard achieves an average F1 score of 96\\% in assessing malicious behaviors and demonstrates good scalability across multiple models and unknown attacks. It also possesses excellent fine-tuning capabilities, allowing experts to assist in timely system updates.         ",
    "url": "https://arxiv.org/abs/2506.16981",
    "authors": [
      "Hao Zhang",
      "Shuo Shao",
      "Song Li",
      "Zhenyu Zhong",
      "Yan Liu",
      "Zhan Qin",
      "Kui Ren"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.16990",
    "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs",
    "abstract": "           LaTeX's precision and flexibility in typesetting have made it the gold standard for the preparation of scientific documentation. Large Language Models (LLMs) present a promising opportunity for researchers to produce publication-ready material using LaTeX with natural language instructions, yet current benchmarks completely lack evaluation of this ability. By introducing TeXpert, our benchmark dataset with natural language prompts for generating LaTeX code focused on components of scientific documents across multiple difficulty levels, we conduct an in-depth analysis of LLM performance in this regard and identify frequent error types. Our evaluation across open and closed-source LLMs highlights multiple key findings: LLMs excelling on standard benchmarks perform poorly in LaTeX generation with a significant accuracy drop-off as the complexity of tasks increases; open-source models like DeepSeek v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks; and formatting and package errors are unexpectedly prevalent, suggesting a lack of diverse LaTeX examples in the training datasets of most LLMs. Our dataset, code, and model evaluations are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16990",
    "authors": [
      "Sahil Kale",
      "Vijaykant Nadadur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17001",
    "title": "PersonalAI: Towards digital twins in the graph form",
    "abstract": "           The challenge of personalizing language models, specifically the ability to account for a user's history during interactions, is of significant interest. Despite recent advancements in large language models (LLMs) and Retrieval Augmented Generation that have enhanced the factual base of LLMs, the task of retaining extensive personal information and using it to generate personalized responses remains pertinent. To address this, we propose utilizing external memory in the form of knowledge graphs, which are constructed and updated by the LLM itself. We have expanded upon ideas of AriGraph architecture and for the first time introduced a combined graph featuring both standard edges and two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and DiaASQ benchmarks indicates that this approach aids in making the process of graph construction and knowledge extraction unified and robust. Furthermore, we augmented the DiaASQ benchmark by incorporating parameters such as time into dialogues and introducing contradictory statements made by the same speaker at different times. Despite these modifications, the performance of the question-answering system remained robust, demonstrating the proposed architecture's ability to maintain and utilize temporal dependencies.         ",
    "url": "https://arxiv.org/abs/2506.17001",
    "authors": [
      "Mikhail Menschikov",
      "Dmitry Evseev",
      "Ruslan Kostoev",
      "Ilya Perepechkin",
      "Ilnaz Salimov",
      "Victoria Dochkina",
      "Petr Anokhin",
      "Evgeny Burnaev",
      "Nikita Semenov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2506.17004",
    "title": "A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving",
    "abstract": "           3D semantic occupancy prediction is an emerging perception paradigm in autonomous driving, providing a voxel-level representation of both geometric details and semantic categories. However, the perception capability of a single vehicle is inherently constrained by occlusion, restricted sensor range, and narrow viewpoints. To address these limitations, collaborative perception enables the exchange of complementary information, thereby enhancing the completeness and accuracy. In the absence of a dedicated dataset for collaborative 3D semantic occupancy prediction, we augment an existing collaborative perception dataset by replaying it in CARLA with a high-resolution semantic voxel sensor to provide dense and comprehensive occupancy annotations. In addition, we establish benchmarks with varying prediction ranges designed to systematically assess the impact of spatial extent on collaborative prediction. We further develop a baseline model that performs inter-agent feature fusion via spatial alignment and attention aggregation. Experimental results demonstrate that our baseline model consistently outperforms single-agent models, with increasing gains observed as the prediction range expands.         ",
    "url": "https://arxiv.org/abs/2506.17004",
    "authors": [
      "Hanlin Wu",
      "Pengfei Lin",
      "Ehsan Javanmardi",
      "Naren Bao",
      "Bo Qian",
      "Hao Si",
      "Manabu Tsukada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.17007",
    "title": "Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators",
    "abstract": "           A major bottleneck in scientific discovery involves narrowing a large combinatorial set of objects, such as proteins or molecules, to a small set of promising candidates. While this process largely relies on expert knowledge, recent methods leverage reinforcement learning (RL) to enhance this filtering. They achieve this by estimating proxy reward functions from available datasets and using regularization to generate more diverse candidates. These reward functions are inherently uncertain, raising a particularly salient challenge for scientific discovery. In this work, we show that existing methods, often framed as sampling proportional to a reward function, are inadequate and yield suboptimal candidates, especially in large search spaces. To remedy this issue, we take a robust RL approach and introduce a unified operator that seeks robustness to the uncertainty of the proxy reward function. This general operator targets peakier sampling distributions while encompassing known soft RL operators. It also leads us to a novel algorithm that identifies higher-quality, diverse candidates in both synthetic and real-world tasks. Ultimately, our work offers a new, flexible perspective on discrete compositional generation tasks. Code: this https URL.         ",
    "url": "https://arxiv.org/abs/2506.17007",
    "authors": [
      "Marco Jiralerspong",
      "Esther Derman",
      "Danilo Vucetic",
      "Nikolay Malkin",
      "Bilun Sun",
      "Tianyu Zhang",
      "Pierre-Luc Bacon",
      "Gauthier Gidel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17012",
    "title": "A Novel Approach to Differential Privacy with Alpha Divergence",
    "abstract": "           As data-driven technologies advance swiftly, maintaining strong privacy measures becomes progressively difficult. Conventional $(\\epsilon, \\delta)$-differential privacy, while prevalent, exhibits limited adaptability for many applications. To mitigate these constraints, we present alpha differential privacy (ADP), an innovative privacy framework grounded in alpha divergence, which provides a more flexible assessment of privacy consumption. This study delineates the theoretical underpinnings of ADP and contrasts its performance with competing privacy frameworks across many scenarios. Empirical assessments demonstrate that ADP offers enhanced privacy guarantees in small to moderate iteration contexts, particularly where severe privacy requirements are necessary. The suggested method markedly improves privacy-preserving methods, providing a flexible solution for contemporary data analysis issues in a data-centric environment.         ",
    "url": "https://arxiv.org/abs/2506.17012",
    "authors": [
      "Yifeng Liu",
      "Zehua Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.17041",
    "title": "MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection",
    "abstract": "           Benchmark datasets for network intrusion detection commonly rely on synthetically generated traffic, which fails to reflect the statistical variability and temporal drift encountered in operational environments. This paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset, designed to enable realistic and reproducible evaluation of anomaly detection methods. A reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforming to the CICFlowMeter format, while preserving MAWILab's original anomaly labels. The resulting datasets comprise temporally distinct samples from January 2011, 2016, and 2021, drawn from trans-Pacific backbone traffic. To establish reference baselines, traditional machine learning methods, including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical results demonstrate that tree-based classifiers perform well on temporally static data but experience significant performance degradation over time. In contrast, the CNN-BiLSTM model maintains better performance, thus showing improved generalization. These findings underscore the limitations of synthetic benchmarks and static models, and motivate the adoption of realistic datasets with explicit temporal structure. All datasets, pipeline code, and model implementations are made publicly available to foster transparency and reproducibility.         ",
    "url": "https://arxiv.org/abs/2506.17041",
    "authors": [
      "Joshua Schraven",
      "Alexander Windmann",
      "Oliver Niggemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17047",
    "title": "Navigating the Deep: Signature Extraction on Deep Neural Networks",
    "abstract": "           Neural network model extraction has emerged in recent years as an important security concern, as adversaries attempt to recover a network's parameters via black-box queries. A key step in this process is signature extraction, which aims to recover the absolute values of the network's weights layer by layer. Prior work, notably by Carlini et al. (2020), introduced a technique inspired by differential cryptanalysis to extract neural network parameters. However, their method suffers from several limitations that restrict its applicability to networks with a few layers only. Later works focused on improving sign extraction, but largely relied on the assumption that signature extraction itself was feasible. In this work, we revisit and refine the signature extraction process by systematically identifying and addressing for the first time critical limitations of Carlini et al.'s signature extraction method. These limitations include rank deficiency and noise propagation from deeper layers. To overcome these challenges, we propose efficient algorithmic solutions for each of the identified issues, greatly improving the efficiency of signature extraction. Our approach permits the extraction of much deeper networks than was previously possible. We validate our method through extensive experiments on ReLU-based neural networks, demonstrating significant improvements in extraction depth and accuracy. For instance, our extracted network matches the target network on at least 95% of the input space for each of the eight layers of a neural network trained on the CIFAR-10 dataset, while previous works could barely extract the first three layers. Our results represent a crucial step toward practical attacks on larger and more complex neural network architectures.         ",
    "url": "https://arxiv.org/abs/2506.17047",
    "authors": [
      "Haolin Liu",
      "Adrien Siproudhis",
      "Samuel Experton",
      "Peter Lorenz",
      "Christina Boura",
      "Thomas Peyrin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.17060",
    "title": "Robust black start of an offshore wind farm with DRU based HVDC link using power synchronization control",
    "abstract": "           This paper introduces a universal power synchronization controller for grid-side control of the wind turbine conversion systems in an offshore wind farm with a diode rectifier in the offshore substation of the HVDC link. The controller incorporates voltage-power droop controllers in the outer loop to enable the operation of this setup. To effectively handle the impact of large delays during black start and power ramp phases, virtual active and reactive power quantities are defined. These quantities are computed based on the current references prior to any modifications that might be needed to meet converter current and voltage limits or source constraints. Utilizing them in the outer loop ensures a balanced power sharing and a stable operation whenever the original (unmodified) current references are not realized. Case studies confirm the robustness of the proposed controller.         ",
    "url": "https://arxiv.org/abs/2506.17060",
    "authors": [
      "Orcun Karaca",
      "Ioannis Tsoumas",
      "Mario Schweizer",
      "Ognjen Stanojev",
      "Lennart Harnefors"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.17063",
    "title": "Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks",
    "abstract": "           The exponential growth of IoT devices presents critical challenges in bandwidth-constrained wireless networks, particularly regarding efficient data transmission and privacy preservation. This paper presents a novel federated semantic communication (SC) framework that enables collaborative training of bandwidth-efficient models for image reconstruction across heterogeneous IoT devices. By leveraging SC principles to transmit only semantic features, our approach dramatically reduces communication overhead while preserving reconstruction quality. We address the fundamental challenge of client selection in federated learning environments where devices exhibit significant disparities in dataset sizes and data distributions. Our framework implements three distinct client selection strategies that explore different trade-offs between system performance and fairness in resource allocation. The system employs an end-to-end SC architecture with semantic bottlenecks, coupled with a loss-based aggregation mechanism that naturally adapts to client heterogeneity. Experimental evaluation on image data demonstrates that while Utilitarian selection achieves the highest reconstruction quality, Proportional Fairness maintains competitive performance while significantly reducing participation inequality and improving computational efficiency. These results establish that federated SC can successfully balance reconstruction quality, resource efficiency, and fairness in heterogeneous IoT deployments, paving the way for sustainable and privacy-preserving edge intelligence applications.         ",
    "url": "https://arxiv.org/abs/2506.17063",
    "authors": [
      "Samer Lahoud",
      "Kinda Khawam"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17065",
    "title": "Flow-Based Non-stationary Temporal Regime Causal Structure Learning",
    "abstract": "           Understanding causal relationships in multivariate time series is crucial in many scenarios, such as those dealing with financial or neurological data. Many such time series exhibit multiple regimes, i.e., consecutive temporal segments with a priori unknown boundaries, with each regime having its own causal structure. Inferring causal dependencies and regime shifts is critical for analyzing the underlying processes. However, causal structure learning in this setting is challenging due to (1) non stationarity, i.e., each regime can have its own causal graph and mixing function, and (2) complex noise distributions, which may be non Gaussian or heteroscedastic. Existing causal discovery approaches cannot address these challenges, since generally assume stationarity or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified framework for causal discovery that handles non stationary processes along with non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the number of regimes and their corresponding indices and learns each regime's Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm that maximizes the evidence lower bound of the data log likelihood. On the theoretical side, we prove, under mild assumptions, that temporal heteroscedastic causal models, introduced in FANTOM's formulation, are identifiable in both stationary and non stationary settings. In addition, extensive experiments on synthetic and real data show that FANTOM outperforms existing methods.         ",
    "url": "https://arxiv.org/abs/2506.17065",
    "authors": [
      "Abdellah Rahmani",
      "Pascal Frossard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17076",
    "title": "Neural Polar Decoders for DNA Data Storage",
    "abstract": "           Synchronization errors, such as insertions and deletions, present a fundamental challenge in DNA-based data storage systems, arising from both synthesis and sequencing noise. These channels are often modeled as insertion-deletion-substitution (IDS) channels, for which designing maximum-likelihood decoders is computationally expensive. In this work, we propose a data-driven approach based on neural polar decoders (NPDs) to design low-complexity decoders for channels with synchronization errors. The proposed architecture enables decoding over IDS channels with reduced complexity $O(AN log N )$, where $A$ is a tunable parameter independent of the channel. NPDs require only sample access to the channel and can be trained without an explicit channel model. Additionally, NPDs provide mutual information (MI) estimates that can be used to optimize input distributions and code design. We demonstrate the effectiveness of NPDs on both synthetic deletion and IDS channels. For deletion channels, we show that NPDs achieve near-optimal decoding performance and accurate MI estimation, with significantly lower complexity than trellis-based decoders. We also provide numerical estimates of the channel capacity for the deletion channel. We extend our evaluation to realistic DNA storage settings, including channels with multiple noisy reads and real-world Nanopore sequencing data. Our results show that NPDs match or surpass the performance of existing methods while using significantly fewer parameters than the state-of-the-art. These findings highlight the promise of NPDs for robust and efficient decoding in DNA data storage systems.         ",
    "url": "https://arxiv.org/abs/2506.17076",
    "authors": [
      "Ziv Aharoni",
      "Henry D. Pfister"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17093",
    "title": "Identifiability of Deep Polynomial Neural Networks",
    "abstract": "           Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability -- a key property for ensuring interpretability -- remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. This yields both generic conditions determined by the architecture, and effective conditions that depend on the network's parameters. We also settle an open conjecture on the expected dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach its maximum.         ",
    "url": "https://arxiv.org/abs/2506.17093",
    "authors": [
      "Konstantin Usevich",
      "Clara D\u00e9rand",
      "Ricardo Borsoi",
      "Marianne Clausel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Algebraic Geometry (math.AG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.17119",
    "title": "RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking",
    "abstract": "           We introduce a robust framework, RGBTrack, for real-time 6D pose estimation and tracking that operates solely on RGB data, thereby eliminating the need for depth input for such dynamic and precise object pose tracking tasks. Building on the FoundationPose architecture, we devise a novel binary search strategy combined with a render-and-compare mechanism to efficiently infer depth and generate robust pose hypotheses from true-scale CAD models. To maintain stable tracking in dynamic scenarios, including rapid movements and occlusions, RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman filter and a state machine for proactive object pose recovery. In addition, RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale using an initial depth estimate, enabling seamless integration with modern generative reconstruction techniques. Extensive evaluations on benchmark datasets demonstrate that RGBTrack's novel depth-free approach achieves competitive accuracy and real-time performance, making it a promising practical solution candidate for application areas including robotics, augmented reality, and computer vision. The source code for our implementation will be made publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.17119",
    "authors": [
      "Teng Guo",
      "Jingjin Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2506.17120",
    "title": "Reassessing Code Authorship Attribution in the Era of Language Models",
    "abstract": "           The study of Code Stylometry, and in particular Code Authorship Attribution (CAA), aims to analyze coding styles to identify the authors of code samples. CAA is crucial in cybersecurity and software forensics for addressing, detecting plagiarism, and supporting criminal prosecutions. However, CAA is a complex and error prone task, due to the need for recognizing nuanced relationships between coding patterns. This challenge is compounded in large software systems with numerous authors due to the subtle variability of patterns that signify the coding style of one author among many. Given the challenges related to this task, researchers have proposed and studied automated approaches that rely upon classical Machine Learning and Deep Learning techniques. However, such techniques have historically relied upon hand-crafted features, and due to the often intricate interaction of different features (e.g., formatting, etc.), have key limitations in properly characterizing authorship, and are sensitive to adversarial code perturbations. Recently, transformer-based Language Models (LMs) have shown remarkable efficacy across a range of software engineering tasks, and in the authorship attribution on natural language in the NLP domain. However, their effectiveness in CAA is not well understood. As such, we conduct the first extensive empirical study applying two larger state-of-the-art code LMs, and five smaller code LMs to the task of CAA to 6 diverse datasets that encompass 12k code snippets written by 463 developers. Furthermore, we perform an in-depth analysis of our studied models' performance on CAA using established machine learning interpretability techniques. The results of our analysis illustrate important findings that illuminate the behavior of LMs in understanding stylometric code patterns during the task of CAA, and point towards important directions for future work.         ",
    "url": "https://arxiv.org/abs/2506.17120",
    "authors": [
      "Atish Kumar Dipongkor",
      "Ziyu Yao",
      "Kevin Moran"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.17125",
    "title": "Large Language Model Unlearning for Source Code",
    "abstract": "           LLM4SE has demonstrated significant success, but LLMs' potential memorization of sensitive or outdated training data introduces critical risks to legal compliance, software security, and code quality. LLM unlearning techniques, which can eliminate the influence of undesired data from LLMs in a post-training way, present a promising solution to address these concerns. While recent efforts in LLM unlearning show effectiveness in natural language, their applicability to source code remains underexplored. Our empirical study reveals that existing LLM unlearning approaches, when applied to source code, cause severe model utility degradation, rendering models practically unusable for code generation. In this paper, we propose PROD, a novel unlearning approach that enables LLMs to forget undesired code content while effectively preserving their code generation capabilities. PROD suppresses the probability of forget data in LLMs' output distribution while promoting candidate distributional components, enabling the model to jointly learn to forget specific content and retain its general capabilities. To facilitate this study, we establish a benchmark for code unlearning evaluation, which includes three critical downstream tasks: copyrighted code unlearning, insecure code unlearning, and deprecated API unlearning. Our evaluation demonstrates that PROD achieves superior balance between forget quality and model utility compared to existing unlearning approaches across three downstream tasks, while consistently exhibiting improvements when applied to LLMs of varying series. PROD also exhibits superior robustness against adversarial attacks without generating or exposing the data to be forgotten. The results underscore that our approach not only extends the application boundary of unlearning techniques to source code, but also holds significant implications for advancing reliable code generation.         ",
    "url": "https://arxiv.org/abs/2506.17125",
    "authors": [
      "Xue Jiang",
      "Yihong Dong",
      "Zheng Fang",
      "Yingwei Ma",
      "Tangxinyu Wang",
      "Rongyu Cao",
      "Binhua Li",
      "Zhi Jin",
      "Wenpin Jiao",
      "Yongbin Li",
      "Ge Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.17136",
    "title": "Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations",
    "abstract": "           Semi-supervised learning addresses the issue of limited annotations in medical images effectively, but its performance is often inadequate for complex backgrounds and challenging tasks. Multi-modal fusion methods can significantly improve the accuracy of medical image segmentation by providing complementary information. However, they face challenges in achieving significant improvements under semi-supervised conditions due to the challenge of effectively leveraging unlabeled data. There is a significant need to create an effective and reliable multi-modal learning strategy for leveraging unlabeled data in semi-supervised segmentation. To address these issues, we propose a novel semi-supervised multi-modal medical image segmentation approach, which leverages complementary multi-modal information to enhance performance with limited labeled data. Our approach employs a multi-stage multi-modal fusion and enhancement strategy to fully utilize complementary multi-modal information, while reducing feature discrepancies and enhancing feature sharing and alignment. Furthermore, we effectively introduce contrastive mutual learning to constrain prediction consistency across modalities, thereby facilitating the robustness of segmentation results in semi-supervised tasks. Experimental results on two multi-modal datasets demonstrate the superior performance and robustness of the proposed framework, establishing its valuable potential for solving medical image segmentation tasks in complex scenarios.         ",
    "url": "https://arxiv.org/abs/2506.17136",
    "authors": [
      "Dongdong Meng",
      "Sheng Li",
      "Hao Wu",
      "Guoping Wang",
      "Xueqing Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.17162",
    "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
    "abstract": "           Malicious PDF files have emerged as a persistent threat and become a popular attack vector in web-based attacks. While machine learning-based PDF malware classifiers have shown promise, these classifiers are often susceptible to adversarial attacks, undermining their reliability. To address this issue, recent studies have aimed to enhance the robustness of PDF classifiers. Despite these efforts, the feature engineering underlying these studies remains outdated. Consequently, even with the application of cutting-edge machine learning techniques, these approaches fail to fundamentally resolve the issue of feature instability. To tackle this, we propose a novel approach for PDF feature extraction and PDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate Representation), an assembly-like language framework for PDF objects, from which we extract semantic features using a pretrained language model. Additionally, we construct an Object Reference Graph to capture structural features, drawing inspiration from program analysis. This dual approach enables us to analyze and detect PDF malware based on both semantic and structural features. Experimental results demonstrate that our proposed classifier achieves strong adversarial robustness while maintaining an exceptionally low false positive rate of only 0.07% on baseline dataset compared to state-of-the-art PDF malware classifiers.         ",
    "url": "https://arxiv.org/abs/2506.17162",
    "authors": [
      "Side Liu",
      "Jiang Ming",
      "Guodong Zhou",
      "Xinyi Liu",
      "Jianming Fu",
      "Guojun Peng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2506.17169",
    "title": "Continual Learning with Columnar Spiking Neural Networks",
    "abstract": "           This study investigates columnar-organized spiking neural networks (SNNs) for continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered Network), we show that microcolumns adapt most efficiently to new tasks when they lack shared structure with prior learning. We demonstrate how CoLaNET hyperparameters govern the trade-off between retaining old knowledge (stability) and acquiring new information (plasticity). Our optimal configuration learns ten sequential MNIST tasks effectively, maintaining 92% accuracy on each. It shows low forgetting, with only 4% performance degradation on the first task after training on nine subsequent tasks.         ",
    "url": "https://arxiv.org/abs/2506.17169",
    "authors": [
      "Denis Larionov",
      "Nikolay Bazenkov",
      "Mikhail Kiselev"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17180",
    "title": "CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models",
    "abstract": "           We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions designed to evaluate whether language models can determine if one statement causally explains another. Each question present an assertion-reason pair and challenge language models to distinguish between semantic relatedness and genuine causal explanatory relationships. Through comprehensive evaluation of 21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we identify two fundamental findings. First, language models frequently confuse semantic similarity with causality, relying on lexical and semantic overlap instead of inferring actual causal explanatory relationships. Second, as parameter size increases, models tend to shift from being overly skeptical about causal relationships to being excessively permissive in accepting them. Despite this shift, performance measured by the Matthews Correlation Coefficient plateaus at just 0.55, even for the best-performing this http URL, CLEAR-3K provides a crucial benchmark for developing and evaluating genuine causal reasoning in language models, which is an essential capability for applications that require accurate assessment of causal relationships.         ",
    "url": "https://arxiv.org/abs/2506.17180",
    "authors": [
      "Naiming Liu",
      "Richard Baraniuk",
      "Shashank Sonkar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2506.17185",
    "title": "A Common Pool of Privacy Problems: Legal and Technical Lessons from a Large-Scale Web-Scraped Machine Learning Dataset",
    "abstract": "           We investigate the contents of web-scraped data for training AI systems, at sizes where human dataset curators and compilers no longer manually annotate every sample. Building off of prior privacy concerns in machine learning models, we ask: What are the legal privacy implications of web-scraped machine learning datasets? In an empirical study of a popular training dataset, we find significant presence of personally identifiable information despite sanitization efforts. Our audit provides concrete evidence to support the concern that any large-scale web-scraped dataset may contain personal data. We use these findings of a real-world dataset to inform our legal analysis with respect to existing privacy and data protection laws. We surface various privacy risks of current data curation practices that may propagate personal information to downstream models. From our findings, we argue for reorientation of current frameworks of \"publicly available\" information to meaningfully limit the development of AI built upon indiscriminate scraping of the internet.         ",
    "url": "https://arxiv.org/abs/2506.17185",
    "authors": [
      "Rachel Hong",
      "Jevan Hutson",
      "William Agnew",
      "Imaad Huda",
      "Tadayoshi Kohno",
      "Jamie Morgenstern"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2506.17191",
    "title": "Facial Landmark Visualization and Emotion Recognition Through Neural Networks",
    "abstract": "           Emotion recognition from facial images is a crucial task in human-computer interaction, enabling machines to learn human emotions through facial expressions. Previous studies have shown that facial images can be used to train deep learning models; however, most of these studies do not include a through dataset analysis. Visualizing facial landmarks can be challenging when extracting meaningful dataset insights; to address this issue, we propose facial landmark box plots, a visualization technique designed to identify outliers in facial datasets. Additionally, we compare two sets of facial landmark features: (i) the landmarks' absolute positions and (ii) their displacements from a neutral expression to the peak of an emotional expression. Our results indicate that a neural network achieves better performance than a random forest classifier.         ",
    "url": "https://arxiv.org/abs/2506.17191",
    "authors": [
      "Israel Ju\u00e1rez-Jim\u00e9nez",
      "Tiffany Guadalupe Mart\u00ednez Paredes",
      "Jes\u00fas Garc\u00eda-Ram\u00edrez",
      "Eric Ramos Aguilar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.17204",
    "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning",
    "abstract": "           Effectively scaling up deep reinforcement learning models has proven notoriously difficult due to network pathologies during training, motivating various targeted interventions such as periodic reset and architectural advances such as layer normalization. Instead of pursuing more complex modifications, we show that introducing static network sparsity alone can unlock further scaling potential beyond their dense counterparts with state-of-the-art architectures. This is achieved through simple one-shot random pruning, where a predetermined percentage of network weights are randomly removed once before training. Our analysis reveals that, in contrast to naively scaling up dense DRL networks, such sparse networks achieve both higher parameter efficiency for network expressivity and stronger resistance to optimization challenges like plasticity loss and gradient interference. We further extend our evaluation to visual and streaming RL scenarios, demonstrating the consistent benefits of network sparsity.         ",
    "url": "https://arxiv.org/abs/2506.17204",
    "authors": [
      "Guozheng Ma",
      "Lu Li",
      "Zilin Wang",
      "Li Shen",
      "Pierre-Luc Bacon",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.15728",
    "title": "Smartphone-integrated RPA-CRISPR-Cas12a Detection System with Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in Early Stage",
    "abstract": "           Potato late blight, caused by the oomycete pathogen Phytophthora infestans, is one of the most devastating diseases affecting potato crops in the history. Although conventional detection methods of plant diseases such as PCR and LAMP are highly sensitive and specific, they rely on bulky and expensive laboratory equipment and involve complex operations, making them impracticable for point-of care diagnosis in the field. Here in this study, we report a portable RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA) microneedle patch was employed for sample extraction on the plant leaves within one minute, the DNA extraction efficiency achieved 56 ug/mg, which is approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of RPA-CRISPR-Cas12a isothermal assay was established to specifically target P. infestans with no cross-reactivity observed against closely-related species (P. sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P. infestans genomic DNA, offering sensitivity comparable to that of benchtop laboratory equipment. The system demonstrates the early-stage diagnosis capability by achieving a approximately 80% and 100% detection rate on the third and fourth day post-inoculation respectively, before visible symptoms observed on the leaves. The smartphone-based \"sample-to-result\" system decouples the limitations of traditional methods that rely heavily on specialized equipment, offering a promising way for early-stage plant disease detection and control in the field.         ",
    "url": "https://arxiv.org/abs/2506.15728",
    "authors": [
      "Jiangnan Zhao",
      "Hanbo Xu",
      "Cifu Xu",
      "Wenlong Yin",
      "Laixin Luo",
      "Gang Liu",
      "Yan Wang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Biomolecules (q-bio.BM)"
    ]
  },
  {
    "id": "arXiv:2506.15748",
    "title": "Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading",
    "abstract": "           Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged by significant inter-observer variability and the limited robustness of deep learning models, particularly near critical decision boundaries. To address these limitations, this paper proposes a novel framework, Diffusion-based Counterfactual Augmentation (DCA), which enhances model robustness and interpretability by generating targeted counterfactual examples. The method navigates the latent space of a diffusion model using a Stochastic Differential Equation (SDE), governed by balancing a classifier-informed boundary drive with a manifold constraint. The resulting counterfactuals are then used within a self-corrective learning strategy to improve the classifier by focusing on its specific areas of uncertainty. Extensive experiments on the public Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST) datasets demonstrate that this approach significantly improves classification accuracy across multiple model architectures. Furthermore, the method provides interpretability by visualizing minimal pathological changes and revealing that the learned latent space topology aligns with clinical knowledge of KOA progression. The DCA framework effectively converts model uncertainty into a robust training signal, offering a promising pathway to developing more accurate and trustworthy automated diagnostic systems. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.15748",
    "authors": [
      "Zhe Wang",
      "Yuhua Ru",
      "Aladine Chetouani",
      "Tina Shiang",
      "Fang Chen",
      "Fabian Bauer",
      "Liping Zhang",
      "Didier Hans",
      "Rachid Jennane",
      "William Ewing Palmer",
      "Mohamed Jarraya",
      "Yung Hsin Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.15753",
    "title": "Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation",
    "abstract": "           In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a natural gradient-based algorithm for link adaptation that whitens policy updates using the full inverse quantum Fisher information with Tikhonov regularization. QPPG bridges classical and quantum geometry, achieving stable learning even under noise. Evaluated on classical and quantum environments, including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG converges 4 times faster than REINFORCE and sustains a 1 dB gain under uncertainty. It reaches a 90 percent return in one hundred episodes with high noise robustness, showcasing the advantages of full QFI-based preconditioning for scalable quantum reinforcement learning.         ",
    "url": "https://arxiv.org/abs/2506.15753",
    "authors": [
      "Oluwaseyi Giwa",
      "Muhammad Ahmed Mohsin",
      "Muhammad Ali Jamshed"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.15762",
    "title": "Implicit neural representations for accurate estimation of the standard model of white matter",
    "abstract": "           Diffusion magnetic resonance imaging (dMRI) enables non-invasive investigation of tissue microstructure. The Standard Model (SM) of white matter aims to disentangle dMRI signal contributions from intra- and extra-axonal water compartments. However, due to the model its high-dimensional nature, extensive acquisition protocols with multiple b-values and diffusion tensor shapes are typically required to mitigate parameter degeneracies. Even then, accurate estimation remains challenging due to noise. This work introduces a novel estimation framework based on implicit neural representations (INRs), which incorporate spatial regularization through the sinusoidal encoding of the input coordinates. The INR method is evaluated on both synthetic and in vivo datasets and compared to parameter estimates using cubic polynomials, supervised neural networks, and nonlinear least squares. Results demonstrate superior accuracy of the INR method in estimating SM parameters, particularly in low signal-to-noise conditions. Additionally, spatial upsampling of the INR can represent the underlying dataset anatomically plausibly in a continuous way, which is unattainable with linear or cubic interpolation. The INR is fully unsupervised, eliminating the need for labeled training data. It achieves fast inference ($\\sim$6 minutes), is robust to both Gaussian and Rician noise, supports joint estimation of SM kernel parameters and the fiber orientation distribution function with spherical harmonics orders up to at least 8 and non-negativity constraints, and accommodates spatially varying acquisition protocols caused by magnetic gradient non-uniformities. The combination of these properties along with the possibility to easily adapt the framework to other dMRI models, positions INRs as a potentially important tool for analyzing and interpreting diffusion MRI data.         ",
    "url": "https://arxiv.org/abs/2506.15762",
    "authors": [
      "Tom Hendriks",
      "Gerrit Arends",
      "Edwin Versteeg",
      "Anna Vilanova",
      "Maxime Chamberland",
      "Chantal M.W. Tax"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2506.15791",
    "title": "TRUST: Transparent, Robust and Ultra-Sparse Trees",
    "abstract": "           Piecewise-constant regression trees remain popular for their interpretability, yet often lag behind black-box models like Random Forest in predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and Ultra-Sparse Trees), a novel regression tree model that combines the accuracy of Random Forests with the interpretability of shallow decision trees and sparse linear models. TRUST further enhances transparency by leveraging Large Language Models to generate tailored, user-friendly explanations. Extensive validation on synthetic and real-world benchmark datasets demonstrates that TRUST consistently outperforms other interpretable models -- including CART, Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy of Random Forest and offering substantial gains in both accuracy and interpretability over M5', a well-established model that is conceptually related.         ",
    "url": "https://arxiv.org/abs/2506.15791",
    "authors": [
      "Albert Dorador"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.15835",
    "title": "MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction",
    "abstract": "           Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the spatial relationships of anatomical structures, playing a crucial role in clinical diagnosis. Recently, deep-learning-based freehand 3D US has made significant advancements. It reconstructs volumes by estimating transformations between images without external tracking. However, image-only reconstruction poses difficulties in reducing cumulative drift and further improving reconstruction accuracy, particularly in scenarios involving complex motion trajectories. In this context, we propose an enhanced motion network (MoNetV2) to enhance the accuracy and generalizability of reconstruction under diverse scanning velocities and tactics. First, we propose a sensor-based temporal and multi-branch structure that fuses image and motion information from a velocity perspective to improve image-only reconstruction accuracy. Second, we devise an online multi-level consistency constraint that exploits the inherent consistency of scans to handle various scanning velocities and tactics. This constraint exploits both scan-level velocity consistency, path-level appearance consistency, and patch-level motion consistency to supervise inter-frame transformation estimation. Third, we distill an online multi-modal self-supervised strategy that leverages the correlation between network estimation and motion information to further reduce cumulative errors. Extensive experiments clearly demonstrate that MoNetV2 surpasses existing methods in both reconstruction quality and generalizability performance across three large datasets.         ",
    "url": "https://arxiv.org/abs/2506.15835",
    "authors": [
      "Mingyuan Luo",
      "Xin Yang",
      "Zhongnuo Yan",
      "Yan Cao",
      "Yuanji Zhang",
      "Xindi Hu",
      "Jin Wang",
      "Haoxuan Ding",
      "Wei Han",
      "Litao Sun",
      "Dong Ni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16089",
    "title": "Diffusion-Based Hypothesis Testing and Change-Point Detection",
    "abstract": "           Score-based methods have recently seen increasing popularity in modeling and generation. Methods have been constructed to perform hypothesis testing and change-point detection with score functions, but these methods are in general not as powerful as their likelihood-based peers. Recent works consider generalizing the score-based Fisher divergence into a diffusion-divergence by transforming score functions via multiplication with a matrix-valued function or a weight matrix. In this paper, we extend the score-based hypothesis test and change-point detection stopping rule into their diffusion-based analogs. Additionally, we theoretically quantify the performance of these diffusion-based algorithms and study scenarios where optimal performance is achievable. We propose a method of numerically optimizing the weight matrix and present numerical simulations to illustrate the advantages of diffusion-based algorithms.         ",
    "url": "https://arxiv.org/abs/2506.16089",
    "authors": [
      "Sean Moushegian",
      "Taposh Banerjee",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16189",
    "title": "CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization",
    "abstract": "           We study the problem of conformal prediction (CP) under geometric data shifts, where data samples are susceptible to transformations such as rotations or flips. While CP endows prediction models with post-hoc uncertainty quantification and formal coverage guarantees, their practicality breaks under distribution shifts that deteriorate model performance. To address this issue, we propose integrating geometric information--such as geometric pose--into the conformal procedure to reinstate its guarantees and ensure robustness under geometric shifts. In particular, we explore recent advancements on pose canonicalization as a suitable information extractor for this purpose. Evaluating the combined approach across discrete and continuous shifts and against equivariant and augmentation-based baselines, we find that integrating geometric information with CP yields a principled way to address geometric shifts while maintaining broad applicability to black-box predictors.         ",
    "url": "https://arxiv.org/abs/2506.16189",
    "authors": [
      "Putri A. van der Linden",
      "Alexander Timans",
      "Erik J. Bekkers"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16210",
    "title": "From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction",
    "abstract": "           In motion-robust magnetic resonance imaging (MRI), slice-to-volume reconstruction is critical for recovering anatomically consistent 3D brain volumes from 2D slices, especially under accelerated acquisitions or patient motion. However, this task remains challenging due to hierarchical structural disruptions. It includes local detail loss from k-space undersampling, global structural aliasing caused by motion, and volumetric anisotropy. Therefore, we propose a progressive refinement implicit neural representation (PR-INR) framework. Our PR-INR unifies motion correction, structural refinement, and volumetric synthesis within a geometry-aware coordinate space. Specifically, a motion-aware diffusion module is first employed to generate coarse volumetric reconstructions that suppress motion artifacts and preserve global anatomical structures. Then, we introduce an implicit detail restoration module that performs residual refinement by aligning spatial coordinates with visual features. It corrects local structures and enhances boundary precision. Further, a voxel continuous-aware representation module represents the image as a continuous function over 3D coordinates. It enables accurate inter-slice completion and high-frequency detail recovery. We evaluate PR-INR on five public MRI datasets under various motion conditions (3% and 5% displacement), undersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental results demonstrate that PR-INR outperforms state-of-the-art methods in both quantitative reconstruction metrics and visual quality. It further shows generalization and robustness across diverse unseen domains.         ",
    "url": "https://arxiv.org/abs/2506.16210",
    "authors": [
      "Zhenxuan Zhang",
      "Lipei Zhang",
      "Yanqi Cheng",
      "Zi Wang",
      "Fanwen Wang",
      "Haosen Zhang",
      "Yue Yang",
      "Yinzhe Wu",
      "Jiahao Huang",
      "Angelica I Aviles-Rivero",
      "Zhifan Gao",
      "Guang Yang",
      "Peter J. Lally"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16233",
    "title": "Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation",
    "abstract": "           Observational astronomy relies on visual feature identification to detect critical astrophysical phenomena. While machine learning (ML) increasingly automates this process, models often struggle with generalization in large-scale surveys due to the limited representativeness of labeled datasets -- whether from simulations or human annotation -- a challenge pronounced for rare yet scientifically valuable objects. To address this, we propose a conditional diffusion model to synthesize realistic galaxy images for augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains visual feature -- galaxy image pairs from volunteer annotation, we demonstrate that our model generates diverse, high-fidelity galaxy images closely adhere to the specified morphological feature conditions. Moreover, this model enables generative extrapolation to project well-annotated data into unseen domains and advancing rare object detection. Integrating synthesized images into ML pipelines improves performance in standard morphology classification, boosting completeness and purity by up to 30\\% across key metrics. For rare object detection, using early-type galaxies with prominent dust lane features ( $\\sim$0.1\\% in GZ2 dataset) as a test case, our approach doubled the number of detected instances from 352 to 872, compared to previous studies based on visual inspection. This study highlights the power of generative models to bridge gaps between scarce labeled data and the vast, uncharted parameter space of observational astronomy and sheds insight for future astrophysical foundation model developments. Our project homepage is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.16233",
    "authors": [
      "Chenrui Ma",
      "Zechang Sun",
      "Tao Jing",
      "Zheng Cai",
      "Yuan-Sen Ting",
      "Song Huang",
      "Mingyu Li"
    ],
    "subjectives": [
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16250",
    "title": "Graph-Cover-based Characterization of the Bethe Partition Function of Double-Edge Factor Graphs",
    "abstract": "           For standard factor graphs (S-FGs) with non-negative real-valued local functions, Vontobel provided a combinatorial characterization of the Bethe approximation of the partition function, also known as the Bethe partition function, using finite graph covers. The proof of this characterization, i.e., the graph-cover theorem for S-FGs, heavily relied on the method of types. In this paper, we study double-edge factor graphs (DE-FGs), a class of factor graphs where each local function takes complex values and satisfies some positive semi-definiteness constraints. DE-FGs and their partition functions are particularly relevant for quantum information processing. Approximating the partition function of a DE-FG is more difficult than for an S-FG, as it involves summing complex values instead of non-negative real values. We develop the sum-product algorithm (SPA) fixed-point-based Bethe approximation of the partition function. However, one cannot directly apply the method of types to prove a similar combinatorial characterization as in the case of S-FGs. We provide a combinatorial characterization of the Bethe partition function in terms of finite graph covers for a class of DE-FGs that satisfy a specific, easily checkable condition. Towards proving this characterization, we apply a suitable loop-calculus transform (LCT) to these graphs. Originally, the LCT was introduced by Chertkov and Chernyak as a special linear transform for S-FGs and later extended by Mori. Our proposed LCT is applicable for both DE-FGs and S-FGs and generalizes prior versions by handling zero-valued SPA fixed-point message components, which are common in DE-FGs. Supported by numerical results, we conjecture that this combinatorial characterization of the Bethe partition function in terms of finite graph covers holds more broadly for DE-FGs.         ",
    "url": "https://arxiv.org/abs/2506.16250",
    "authors": [
      "Yuwen Huang",
      "Pascal O. Vontobel"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2506.16289",
    "title": "The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units",
    "abstract": "           This paper explores the relationship between the condition number of a neural network's weight tensor and the extent of information encoded by the associated processing unit, viewed through the lens of information theory. We argue that a high condition number, though not sufficient for effective knowledge encoding, may indicate that the unit has learned to selectively amplify and compress information. We formalize this intuition, particularly for linear units with Gaussian inputs, linking the condition number and the transformation's log-volume scaling factor to the characteristics of the output entropy and the geometric properties of the learned transformation. Our analysis demonstrates that for a fixed weight norm, a concentrated distribution of singular values (high condition number) corresponds to reduced overall information transfer, indicating a specialized and efficient encoding strategy. Furthermore, we present a practical case study where these principles are applied to guide selective fine-tuning of a multimodal Large Language Model, aiming to mitigate catastrophic forgetting during cross-modal adaptation. Unlike many existing catastrophic forgetting mitigation methods that rely on access to pre-training statistics, which are often unavailable, our selective fine-tuning approach offers a way to bypass this common requirement.         ",
    "url": "https://arxiv.org/abs/2506.16289",
    "authors": [
      "Oswaldo Ludwig"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16332",
    "title": "Feedback-driven recurrent quantum neural network universality",
    "abstract": "           Quantum reservoir computing uses the dynamics of quantum systems to process temporal data, making it particularly well-suited for learning with noisy intermediate-scale quantum devices. Early experimental proposals, such as the restarting and rewinding protocols, relied on repeating previous steps of the quantum map to avoid backaction. However, this approach compromises real-time processing and increases computational overhead. Recent developments have introduced alternative protocols that address these limitations. These include online, mid-circuit measurement, and feedback techniques, which enable real-time computation while preserving the input history. Among these, the feedback protocol stands out for its ability to process temporal information with comparatively fewer components. Despite this potential advantage, the theoretical foundations of feedback-based quantum reservoir computing remain underdeveloped, particularly with regard to the universality and the approximation capabilities of this approach. This paper addresses this issue by presenting a recurrent quantum neural network architecture that extends a class of existing feedforward models to a dynamic, feedback-driven reservoir setting. We provide theoretical guarantees for variational recurrent quantum neural networks, including approximation bounds and universality results. Notably, our analysis demonstrates that the model is universal with linear readouts, making it both powerful and experimentally accessible. These results pave the way for practical and theoretically grounded quantum reservoir computing with real-time processing capabilities.         ",
    "url": "https://arxiv.org/abs/2506.16332",
    "authors": [
      "Lukas Gonon",
      "Rodrigo Mart\u00ednez-Pe\u00f1a",
      "Juan-Pablo Ortega"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16522",
    "title": "Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection",
    "abstract": "           We present a method to improve the detection limit for radionuclides using spectroscopic radiation detectors and the arrival time of each detected radiation quantum. We enable this method using a neural network with an attention mechanism. We illustrate the method on the detection of Cesium release from a nuclear facility during an upset, and our method shows $2\\times$ improvement over the traditional spectroscopic method. We hypothesize that our method achieves this performance increase by modulating its detection probability by the overall rate of probable detections, specifically by adapting detection thresholds based on temporal event distributions and local spectral features, and show evidence to this effect. We believe this method is applicable broadly and may be more successful for radionuclides with more complicated decay chains than Cesium; we also note that our method can generalize beyond the addition of arrival time and could integrate other data about each detection event, such as pulse quality, location in detector, or even combining the energy and time from detections in different detectors.         ",
    "url": "https://arxiv.org/abs/2506.16522",
    "authors": [
      "Pedro Rodr\u00edguez Fern\u00e1ndez",
      "Christian Svinth",
      "Alex Hagen"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2506.16556",
    "title": "VesselSDF: Distance Field Priors for Vascular Network Reconstruction",
    "abstract": "           Accurate segmentation of vascular networks from sparse CT scan slices remains a significant challenge in medical imaging, particularly due to the thin, branching nature of vessels and the inherent sparsity between imaging planes. Existing deep learning approaches, based on binary voxel classification, often struggle with structural continuity and geometric fidelity. To address this challenge, we present VesselSDF, a novel framework that leverages signed distance fields (SDFs) for robust vessel reconstruction. Our method reformulates vessel segmentation as a continuous SDF regression problem, where each point in the volume is represented by its signed distance to the nearest vessel surface. This continuous representation inherently captures the smooth, tubular geometry of blood vessels and their branching patterns. We obtain accurate vessel reconstructions while eliminating common SDF artifacts such as floating segments, thanks to our adaptive Gaussian regularizer which ensures smoothness in regions far from vessel surfaces while producing precise geometry near the surface boundaries. Our experimental results demonstrate that VesselSDF significantly outperforms existing methods and preserves vessel geometry and connectivity, enabling more reliable vascular analysis in clinical settings.         ",
    "url": "https://arxiv.org/abs/2506.16556",
    "authors": [
      "Salvatore Esposito",
      "Daniel Rebain",
      "Arno Onken",
      "Changjian Li",
      "Oisin Mac Aodha"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16592",
    "title": "Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images",
    "abstract": "           Breast ultrasound imaging is a valuable tool for early breast cancer detection, but automated tumor segmentation is challenging due to inherent noise, variations in scale of lesions, and fuzzy boundaries. To address these challenges, we propose a novel hybrid attention-based network for lesion segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in the encoder part for robust feature extraction with a multi-branch attention-enhanced decoder tailored for breast ultrasound images. The bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE), and Scaled Dot-Product Attention (SDPA) to learn global context, spatial relationships, and relative positional features. The Spatial Feature Enhancement Block (SFEB) is embedded at skip connections to refine and enhance spatial features, enabling the network to focus more effectively on tumor regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap metrics, enhancing robustness to class imbalance and irregular tumor shapes. Experiments on public datasets demonstrate that our method outperforms existing approaches, highlighting its potential to assist radiologists in early and accurate breast cancer diagnosis.         ",
    "url": "https://arxiv.org/abs/2506.16592",
    "authors": [
      "Muhammad Azeem Aslam",
      "Asim Naveed",
      "Nisar Ahmed"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16803",
    "title": "Temperature calibration of surface emissivities with an improved thermal image enhancement network",
    "abstract": "           Infrared thermography faces persistent challenges in temperature accuracy due to material emissivity variations, where existing methods often neglect the joint optimization of radiometric calibration and image degradation. This study introduces a physically guided neural framework that unifies temperature correction and image enhancement through a symmetric skip-CNN architecture and an emissivity-aware attention module. The pre-processing stage segments the ROIs of the image and and initially corrected the firing rate. A novel dual-constrained loss function strengthens the statistical consistency between the target and reference regions through mean-variance alignment and histogram matching based on Kullback-Leibler dispersion. The method works by dynamically fusing thermal radiation features and spatial context, and the model suppresses emissivity artifacts while recovering structural details. After validating the industrial blower system under different conditions, the improved network realizes the dynamic fusion of thermal radiation characteristics and spatial background, with accurate calibration results in various industrial conditions.         ",
    "url": "https://arxiv.org/abs/2506.16803",
    "authors": [
      "Ning Chu",
      "Siya Zheng",
      "Shanqing Zhang",
      "Li Li",
      "Caifang Cai",
      "Ali Mohammad-Djafari",
      "Feng Zhao",
      "Yuanbo Song"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.16918",
    "title": "A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials",
    "abstract": "           The behavior of materials is influenced by a wide range of phenomena occurring across various time and length scales. To better understand the impact of microstructure on macroscopic response, multiscale modeling strategies are essential. Numerical methods, such as the $\\text{FE}^2$ approach, account for micro-macro interactions to predict the global response in a concurrent manner. However, these methods are computationally intensive due to the repeated evaluations of the microscale. This challenge has led to the integration of deep learning techniques into computational homogenization frameworks to accelerate multiscale simulations. In this work, we employ neural operators to predict the microscale physics, resulting in a hybrid model that combines data-driven and physics-based approaches. This allows for physics-guided learning and provides flexibility for different materials and spatial discretizations. We apply this method to time-dependent solid mechanics problems involving viscoelastic material behavior, where the state is represented by internal variables only at the microscale. The constitutive relations of the microscale are incorporated into the model architecture and the internal variables are computed based on established physical principles. The results for homogenized stresses ($<6\\%$ error) show that the approach is computationally efficient ($\\sim 100 \\times$ faster).         ",
    "url": "https://arxiv.org/abs/2506.16918",
    "authors": [
      "Dhananjeyan Jeyaraj",
      "Hamidreza Eivazi",
      "Jendrik-Alexander Tr\u00f6ger",
      "Stefan Wittek",
      "Stefan Hartmann",
      "Andreas Rausch"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.16938",
    "title": "Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test",
    "abstract": "           Parameterized quantum circuits represent promising architectures for machine learning applications, yet many lack clear connections to classical models, potentially limiting their ability to translate the wide success of classical neural networks to the quantum realm. We examine a specific type of quantum neural network (QNN) built exclusively from SWAP test circuits, and discuss its mathematical equivalence to a classical two-layer feedforward network with quadratic activation functions under amplitude encoding. Our analysis across classical real-world and synthetic datasets reveals that while this architecture can successfully learn many practical tasks, it exhibits fundamental expressivity limitations due to violating the universal approximation theorem, particularly failing on harder problems like the parity check function. To address this limitation, we introduce a circuit modification using generalized SWAP test circuits that effectively implements classical neural networks with product layers. This enhancement enables successful learning of parity check functions in arbitrary dimensions which we analytically argue to be impossible for the original architecture beyond two dimensions regardless of network size. Our results establish a framework for enhancing QNN expressivity through classical task analysis and demonstrate that our SWAP test-based architecture offers broad representational capacity, suggesting potential promise also for quantum learning tasks.         ",
    "url": "https://arxiv.org/abs/2506.16938",
    "authors": [
      "Sebastian Nagies",
      "Emiliano Tolotti",
      "Davide Pastorello",
      "Enrico Blanzieri"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17036",
    "title": "Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction",
    "abstract": "           Modern industrial systems are often subject to multiple failure modes, and their conditions are monitored by multiple sensors, generating multiple time-series signals. Additionally, time-to-failure data are commonly available. Accurately predicting a system's remaining useful life (RUL) requires effectively leveraging multi-sensor time-series data alongside multi-mode failure event data. In most existing models, failure modes and RUL prediction are performed independently, ignoring the inherent relationship between these two tasks. Some models integrate multiple failure modes and event prediction using black-box machine learning approaches, which lack statistical rigor and cannot characterize the inherent uncertainty in the model and data. This paper introduces a unified approach to jointly model the multi-sensor time-series data and failure time concerning multiple failure modes. This proposed model integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian Process, and multinomial failure mode distributions in a hierarchical Bayesian framework with corresponding priors, enabling accurate prediction with robust uncertainty quantification. Posterior distributions are effectively obtained by Variational Bayes, and prediction is performed with Monte Carlo sampling. The advantages of the proposed model is validated through extensive numerical and case studies with jet-engine dataset.         ",
    "url": "https://arxiv.org/abs/2506.17036",
    "authors": [
      "Sina Aghaee Dabaghan Fard",
      "Minhee Kim",
      "Akash Deep",
      "Jaesung Lee"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2506.17064",
    "title": "Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings",
    "abstract": "           Generating diverse, all-atom conformational ensembles of dynamic proteins such as G-protein-coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all-atom protein structures, including every side-chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low-dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue-based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a 2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor in a membrane environment, the sequential and residue-based pooling strategy reproduces the reference ensemble with high structural fidelity (all-atom lDDT of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route to system-specific, all-atom ensemble generation for large proteins, providing a promising tool for structure-based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research.         ",
    "url": "https://arxiv.org/abs/2506.17064",
    "authors": [
      "Aditya Sengar",
      "Ali Hariri",
      "Daniel Probst",
      "Patrick Barth",
      "Pierre Vandergheynst"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17133",
    "title": "Robust Training with Data Augmentation for Medical Imaging Classification",
    "abstract": "           Deep neural networks are increasingly being used to detect and diagnose medical conditions using medical imaging. Despite their utility, these models are highly vulnerable to adversarial attacks and distribution shifts, which can affect diagnostic reliability and undermine trust among healthcare professionals. In this study, we propose a robust training algorithm with data augmentation (RTDA) to mitigate these vulnerabilities in medical image classification. We benchmark classifier robustness against adversarial perturbations and natural variations of RTDA and six competing baseline techniques, including adversarial training and data augmentation approaches in isolation and combination, using experimental data sets with three different imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that RTDA achieves superior robustness against adversarial attacks and improved generalization performance in the presence of distribution shift in each image classification task while maintaining high clean accuracy.         ",
    "url": "https://arxiv.org/abs/2506.17133",
    "authors": [
      "Josu\u00e9 Mart\u00ednez-Mart\u00ednez",
      "Olivia Brown",
      "Mostafa Karami",
      "Sheida Nabavi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.17165",
    "title": "Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network",
    "abstract": "           Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.         ",
    "url": "https://arxiv.org/abs/2506.17165",
    "authors": [
      "Mahin Montasir Afif",
      "Abdullah Al Noman",
      "K. M. Tahsin Kabir",
      "Md. Mortuza Ahmmed",
      "Md. Mostafizur Rahman",
      "Mufti Mahmud",
      "Md. Ashraful Babu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:1805.10330",
    "title": "Size-varying reversible causal graph dynamics",
    "abstract": "           Consider a network that evolves according to a reversible, nearest neighbours dynamics. Is the dynamics allowed to vary the size of the network? On the one hand it seems that, being the principal carriers of information, nodes cannot be destroyed without jeopardising bijectivity. On the other hand, there are plenty of bijective functions from the set of graphs to the set of graphs that are non-vertex-preserving. The question has been settled negatively -- for three different reasons. Yet, in this paper we do obtain reversible local node creation/destruction -- in three relaxed settings, whose equivalence we prove for robustness. We motivate our work both by theoretical computer science considerations (reversible computing, cellular automata extensions) and theoretical physics concerns (basic formalisms towards discrete quantum gravity).         ",
    "url": "https://arxiv.org/abs/1805.10330",
    "authors": [
      "Pablo Arrighi",
      "Am\u00e9lia Durbec",
      "Aur\u00e9lien Emmanuel"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2105.10184",
    "title": "Balancing the Spread of Two Opinions in Sparse Social Networks",
    "abstract": "           Inspired by the famous Target Set Selection problem, we propose a new discrete model to simultaneously spread two opinions within a social network and perform an initial study of its complexity. Here, we are given a social network, a seed-set of agents for each opinion, two thresholds for each agent, a budget, and a number of rounds. The first threshold represents the willingness of an agent to adopt an opinion if the agent has no opinion at all, while the second threshold states the willingness to acquire a second opinion if the agent already has one. The goal is to add at most budget-many agents to the initial seed-sets such that the process started with these extended seed-sets stabilizes within the given number of rounds, with each agent having either both opinions or none. That is, our goal is to ensure that the spread of opinions is balanced. We show that the problem is NP-hard, and thus we study the problem from the perspective of parameterized complexity. In particular, we show that the problem is FPT when parameterized by the number of rounds, the maximum threshold, and the treewidth combined. This algorithm also applies to the combined parameter, the treedepth and the maximum threshold. Finally, we show that the problem is FPT when parameterized by the vertex cover number, the $3$-path vertex cover number, or the vertex integrity of the input network alone. To complement our tractability results, we show that the problem is W[1]-hard with respect to a) the sizes of the initial seed-sets and the feedback-vertex set number combined, even if all thresholds are bounded by a constant, and b) the budget, the 4-path vertex cover number, and the feedback-vertex set number combined, even if every activation process stabilizes in at most 4 rounds.         ",
    "url": "https://arxiv.org/abs/2105.10184",
    "authors": [
      "Du\u0161an Knop",
      "\u0160imon Schierreich",
      "Ond\u0159ej Such\u00fd"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2211.05781",
    "title": "Demystify Transformers & Convolutions in Modern Image Deep Networks",
    "abstract": "           Vision transformers have gained popularity recently, leading to the development of new vision backbones with improved features and consistent performance gains. However, these advancements are not solely attributable to novel feature transformation designs; certain benefits also arise from advanced network-level and block-level architectures. This paper aims to identify the real gains of popular convolution and attention operators through a detailed study. We find that the key difference among these feature transformation modules, such as attention or convolution, lies in their spatial feature aggregation approach, known as the \"spatial token mixer\" (STM). To facilitate an impartial comparison, we introduce a unified architecture to neutralize the impact of divergent network-level and block-level designs. Subsequently, various STMs are integrated into this unified framework for comprehensive comparative analysis. Our experiments on various tasks and an analysis of inductive bias show a significant performance boost due to advanced network-level and block-level designs, but performance differences persist among different STMs. Our detailed analysis also reveals various findings about different STMs, including effective receptive fields, invariance, and adversarial robustness tests.         ",
    "url": "https://arxiv.org/abs/2211.05781",
    "authors": [
      "Xiaowei Hu",
      "Min Shi",
      "Weiyun Wang",
      "Sitong Wu",
      "Linjie Xing",
      "Wenhai Wang",
      "Xizhou Zhu",
      "Lewei Lu",
      "Jie Zhou",
      "Xiaogang Wang",
      "Yu Qiao",
      "Jifeng Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2308.05636",
    "title": "A Homomorphic Encryption Framework for Privacy-Preserving Spiking Neural Networks",
    "abstract": "           Machine learning (ML) is widely used today, especially through deep neural networks (DNNs), however, increasing computational load and resource requirements have led to cloud-based solutions. To address this problem, a new generation of networks called Spiking Neural Networks (SNN) has emerged, which mimic the behavior of the human brain to improve efficiency and reduce energy consumption. These networks often process large amounts of sensitive information, such as confidential data, and thus privacy issues arise. Homomorphic encryption (HE) offers a solution, allowing calculations to be performed on encrypted data without decrypting it. This research compares traditional DNNs and SNNs using the Brakerski/Fan-Vercauteren (BFV) encryption scheme. The LeNet-5 model, a widely-used convolutional architecture, is used for both DNN and SNN models based on the LeNet-5 architecture, and the networks are trained and compared using the FashionMNIST dataset. The results show that SNNs using HE achieve up to 40% higher accuracy than DNNs for low values of the plaintext modulus t, although their execution time is longer due to their time-coding nature with multiple time-steps.         ",
    "url": "https://arxiv.org/abs/2308.05636",
    "authors": [
      "Farzad Nikfam",
      "Raffaele Casaburi",
      "Alberto Marchisio",
      "Maurizio Martina",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2308.12053",
    "title": "Efficient and Flexible Neural Network Training through Layer-wise Feedback Propagation",
    "abstract": "           Gradient-based optimization has been a cornerstone of machine learning that enabled the vast advances of Artificial Intelligence (AI) development over the past decades. However, this type of optimization requires differentiation, and with recent evidence of the benefits of non-differentiable (e.g. neuromorphic) architectures over classical models w.r.t. efficiency, such constraints can become limiting in the future. We present Layer-wise Feedback Propagation (LFP), a novel training principle for neural network-like predictors that utilizes methods from the domain of explainability to decompose a reward to individual neurons based on their respective contributions. Leveraging these neuron-wise rewards, our method then implements a greedy approach reinforcing helpful parts of the network and weakening harmful ones. While having comparable computational complexity to gradient descent, LFP does not require gradient computation and generates sparse and thereby memory- and energy-efficient parameter updates and models. We establish the convergence of LFP theoretically and empirically, demonstrating its effectiveness on various models and datasets. Via two applications - neural network pruning and the approximation-free training of Spiking Neural Networks (SNNs) - we demonstrate that LFP combines increased efficiency in terms of computation and representation with flexibility w.r.t. choice of model architecture and objective function. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2308.12053",
    "authors": [
      "Leander Weber",
      "Jim Berend",
      "Moritz Weckbecker",
      "Alexander Binder",
      "Thomas Wiegand",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2309.17116",
    "title": "Sheaf Hypergraph Networks",
    "abstract": "           Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higherorder connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification.         ",
    "url": "https://arxiv.org/abs/2309.17116",
    "authors": [
      "Iulia Duta",
      "Giulia Cassar\u00e0",
      "Fabrizio Silvestri",
      "Pietro Li\u00f2"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.08396",
    "title": "Uncertainty-Aware Planning for Heterogeneous Robot Teams using Dynamic Topological Graphs and Mixed-Integer Programming",
    "abstract": "           Multi-robot planning and coordination in uncertain environments is a fundamental computational challenge, since the belief space increases exponentially with the number of robots. In this paper, we address the problem of planning in uncertain environments with a heterogeneous robot team of fast scout vehicles for information gathering and more risk-averse carrier robots from which the scouts vehicles are deployed. To overcome the computational challenges, we represent the environment and operational scenario using a topological graph, where the parameters of the edge weight distributions vary with the state of the robot team on the graph, and we formulate a computationally efficient mixed-integer program which removes the dependence on the number of robots from its decision space. Our formulation results in the capability to generate optimal multi-robot, long-horizon plans in seconds that could otherwise be computationally intractable. Ultimately our approach enables real-time re-planning, since the computation time is significantly faster than the time to execute one step. We evaluate our algorithm in a scenario where the robot team must traverse an environment while minimizing detection by observers in positions that are uncertain to the robot team. We demonstrate that our method is computationally tractable, can improve performance in the presence of imperfect information, and can be adjusted for different risk profiles.         ",
    "url": "https://arxiv.org/abs/2310.08396",
    "authors": [
      "Cora A. Duggan",
      "Kevin C. Wolfe",
      "Bradley Woosley",
      "Marin Kobilarov",
      "Joseph Moore"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2311.06835",
    "title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation",
    "abstract": "           This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as seen anomalies) to detect both seen anomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the training anomalies). Those labelled training data provide crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current supervised GAD methods tend to over-emphasise fitting the seen anomalies, leading to many errors of detecting the unseen anomalies as normal nodes. Further, existing open-set AD models were introduced to handle Euclidean data, failing to effectively capture discriminative features from graph structure and node attributes for GAD. In this work, we propose a novel open-set GAD approach, namely normal structure regularisation (NSReg), to achieve generalised detection ability to unseen anomalies, while maintaining its effectiveness on detecting seen anomalies. The key idea in NSReg is to introduce a regularisation term that enforces the learning of compact, semantically-rich representations of normal nodes based on their structural relations to other nodes. When being optimised with supervised anomaly detection losses, the regularisation term helps incorporate strong normality into the modelling, and thus, it effectively avoids over-fitting the seen anomalies and learns a better normality decision boundary, largely reducing the false negatives of detecting unseen anomalies as normal. Extensive empirical results on seven real-world datasets show that NSReg significantly outperforms state-of-the-art competing methods by at least 14% AUC-ROC on the unseen anomaly classes and by 10% AUC-ROC on all anomaly classes.         ",
    "url": "https://arxiv.org/abs/2311.06835",
    "authors": [
      "Qizhou Wang",
      "Guansong Pang",
      "Mahsa Salehi",
      "Xiaokun Xia",
      "Christopher Leckie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2311.16487",
    "title": "On the Robustness of Decision-Focused Learning",
    "abstract": "           Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles the task of training a machine learning (ML) model to predict missing parameters of an incomplete optimization problem, where the missing parameters are predicted. DFL trains an ML model in an end-to-end system, by integrating the prediction and optimization tasks, providing better alignment of the training and testing objectives. DFL has shown a lot of promise and holds the capacity to revolutionize decision-making in many real-world applications. However, very little is known about the performance of these models under adversarial attacks. We adopt ten unique DFL methods and benchmark their performance under two distinctly focused attacks adapted towards the Predict-then-Optimize problem setting. Our study proposes the hypothesis that the robustness of a model is highly correlated with its ability to find predictions that lead to optimal decisions without deviating from the ground-truth label. Furthermore, we provide insight into how to target the models that violate this condition and show how these models respond differently depending on the achieved optimality at the end of their training cycles.         ",
    "url": "https://arxiv.org/abs/2311.16487",
    "authors": [
      "Yehya Farhat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2403.10173",
    "title": "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention",
    "abstract": "           Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for robust object detection. While Spiking Neural Networks (SNNs) on neuromorphic hardware are often considered for energy-efficient and low latency event-based data processing, they often fall short of Artificial Neural Networks (ANNs) in accuracy and flexibility. Here, we introduce Attention-based Hybrid SNN-ANN backbones for event-based object detection to leverage the strengths of both SNN and ANN architectures. A novel Attention-based SNN-ANN bridge module captures sparse spatial and temporal relations from the SNN layer and converts them into dense feature maps for the ANN part of the backbone. Additionally, we present a variant that integrates DWConvL-STMs to the ANN blocks to capture slower dynamics. This multi-timescale network combines fast SNN processing for short timesteps with long-term dense RNN processing, effectively capturing both fast and slow dynamics. Experimental results demonstrate that our proposed method surpasses SNN-based approaches by significant margins, with results comparable to existing ANN and RNN-based methods. Unlike ANN-only networks, the hybrid setup allows us to implement the SNN blocks on digital neuromorphic hardware to investigate the feasibility of our approach. Extensive ablation studies and implementation on neuromorphic hardware confirm the effectiveness of our proposed modules and architectural choices. Our hybrid SNN-ANN architectures pave the way for ANN-like performance at a drastically reduced parameter, latency, and power budget.         ",
    "url": "https://arxiv.org/abs/2403.10173",
    "authors": [
      "Soikat Hasan Ahmed",
      "Jan Finkbeiner",
      "Emre Neftci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.04549",
    "title": "Stable Learning Using Spiking Neural Networks Equipped With Affine Encoders and Decoders",
    "abstract": "           We study the learning problem associated with spiking neural networks. Specifically, we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights, equipped with an affine encoder and decoder; we refer to these as affine spiking neural networks. These neural networks are shown to depend continuously on their parameters, which facilitates classical covering number-based generalization statements and supports stable gradient-based training. We demonstrate that the positivity of the weights enables a wide range of expressivity results, including rate-optimal approximation of smooth functions and dimension-independent approximation of Barron regular functions. In particular, we show in theory and simulations that affine spiking neural networks are capable of approximating shallow ReLU neural networks. Furthermore, we apply these affine spiking neural networks to standard machine learning benchmarks and reach competitive results. Finally, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.         ",
    "url": "https://arxiv.org/abs/2404.04549",
    "authors": [
      "A. Martina Neuman",
      "Dominik Dold",
      "Philipp Christian Petersen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2404.12746",
    "title": "Near-Tight Runtime Guarantees for Many-Objective Evolutionary Algorithms",
    "abstract": "           Despite significant progress in the field of mathematical runtime analysis of multi-objective evolutionary algorithms (MOEAs), the performance of MOEAs on discrete many-objective problems is little understood. In particular, the few existing performance guarantees for classic MOEAs on classic benchmarks are all roughly quadratic in the size of the Pareto front. In this work, we consider a large class of MOEAs including the (global) SEMO, SMS-EMOA, balanced NSGA-II, NSGA-III, and SPEA2. For these, we prove near-tight runtime guarantees for the four most common benchmark problems OneMinMax, CountingOnesCountingZeros, LeadingOnesTrailingZeros, and OneJumpZeroJump, and this for arbitrary numbers of objectives. Most of our bounds depend only linearly on the size of the largest incomparable set, showing that MOEAs on these benchmarks cope much better with many objectives than what previous works suggested. Most of our bounds are tight apart from small polynomial factors in the number of objectives and length of bitstrings. This is the first time that such tight bounds are proven for many-objective uses of this http URL the runtime of the SEMO on the LOTZ benchmark in $m \\ge 6$ objectives, our runtime guarantees are even smaller than the size of the largest incomparable set. This is again the first time that such runtime guarantees are proven.         ",
    "url": "https://arxiv.org/abs/2404.12746",
    "authors": [
      "Simon Wietheger",
      "Benjamin Doerr"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2404.13318",
    "title": "Client-Centered Federated Learning for Heterogeneous EHRs: Use Fewer Participants to Achieve the Same Performance",
    "abstract": "           The increasing volume of electronic health records (EHRs) presents the opportunity to improve the accuracy and robustness of models in clinical prediction tasks. Unlike traditional centralized approaches, federated learning enables training on data from multiple institutions while preserving patient privacy and complying with regulatory constraints. In practice, healthcare institutions (i.e., hosts) often need to build predictive models tailored to their specific needs using federated learning. In this scenario, two key challenges arise: (1) ensuring compatibility across heterogeneous EHR systems, and (2) managing federated learning costs within budget constraints. To address these challenges, we propose EHRFL, a federated learning framework designed for building a cost-effective, host-specific predictive model using patient EHR data. EHRFL consists of two components: (1) text-based EHR modeling, which facilitates cross-institution compatibility without costly data standardization, and (2) a participant selection strategy based on averaged patient embedding similarity to reduce the number of participants without degrading performance. Experiments on multiple open-source EHR datasets demonstrate the effectiveness of both components. We believe our framework offers a practical solution for enabling healthcare institutions to build institution-specific predictive models under budgetary constraints.         ",
    "url": "https://arxiv.org/abs/2404.13318",
    "authors": [
      "Jiyoun Kim",
      "Junu Kim",
      "Kyunghoon Hur",
      "Edward Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.14165",
    "title": "Boosting Ordered Statistics Decoding of Short LDPC Codes with Simple Neural Network Models",
    "abstract": "           Ordered statistics decoding has been instrumental in addressing decoding failures that persist after normalized min-sum decoding in short low-density parity-check codes. Despite its benefits, the high computational complexity of effective ordered statistics decoding has limited its application in complexity-sensitive scenarios. To mitigate this issue, we propose a novel variant of the ordered statistics decoder. This approach begins with the design of a neural network model that refines the measurement of codeword bits, utilizing iterative information from normalized min-sum decoding failures. Subsequently, a fixed decoding path is established, comprising a sequence of blocks, each featuring a variety of test error patterns. The introduction of a sliding window-assisted neural model facilitates early termination of the ordered statistics decoding process along this path, aiming to balance performance and computational complexity. Comprehensive simulations and complexity analyses demonstrate that the proposed hybrid method matches state-of-the-art approaches across various metrics, particularly excelling in reducing latency.         ",
    "url": "https://arxiv.org/abs/2404.14165",
    "authors": [
      "Guangwen Li",
      "Xiao Yu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2405.01306",
    "title": "Graph is all you need? Lightweight data-agnostic neural architecture search without training",
    "abstract": "           Neural architecture search (NAS) enables the automatic design of neural network models. However, training the candidates generated by the search algorithm for performance evaluation incurs considerable computational overhead. Our method, dubbed nasgraph, remarkably reduces the computational costs by converting neural architectures to graphs and using the average degree, a graph measure, as the proxy in lieu of the evaluation metric. Our training-free NAS method is data-agnostic and light-weight. It can find the best architecture among 200 randomly sampled architectures from NAS-Bench201 in 217 CPU seconds. Besides, our method is able to achieve competitive performance on various datasets including NASBench-101, NASBench-201, and NDS search spaces. We also demonstrate that nasgraph generalizes to more challenging tasks on Micro TransNAS-Bench-101.         ",
    "url": "https://arxiv.org/abs/2405.01306",
    "authors": [
      "Zhenhan Huang",
      "Tejaswini Pedapati",
      "Pin-Yu Chen",
      "Chunheng Jiang",
      "Jianxi Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.19805",
    "title": "Complexity of Injectivity and Verification of ReLU Neural Networks",
    "abstract": "           Neural networks with ReLU activation play a key role in modern machine learning. Understanding the functions represented by ReLU networks is a major topic in current research as this enables a better interpretability of learning processes. Injectivity of a function computed by a ReLU network, that is, the question if different inputs to the network always lead to different outputs, plays a crucial role whenever invertibility of the function is required, such as, e.g., for inverse problems or generative models. The exact computational complexity of deciding injectivity was recently posed as an open problem (Puthawala et al. [JMLR 2022]). We answer this question by proving coNP-completeness. On the positive side, we show that the problem for a single ReLU-layer is still tractable for small input dimension; more precisely, we present a parameterized algorithm which yields fixed-parameter tractability with respect to the input dimension. In addition, we study the network verification problem which is to verify that certain inputs only yield specific outputs. This is of great importance since neural networks are increasingly used in safety-critical systems. We prove that network verification is coNP-hard for a general class of input domains. Our results also exclude constant-factor polynomial-time approximations for the maximum of a function computed by a ReLU network. In this context, we also characterize surjectivity of functions computed by ReLU networks with one-dimensional output which turns out to be the complement of a basic network verification task. We reveal interesting connections to computational convexity by formulating the surjectivity problem as a zonotope containment problem         ",
    "url": "https://arxiv.org/abs/2405.19805",
    "authors": [
      "Vincent Froese",
      "Moritz Grillo",
      "Martin Skutella"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.03458",
    "title": "Distributional Adversarial Loss",
    "abstract": "           We initiate the study of a new notion of adversarial loss which we call distributional adversarial loss. In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions, and the adversarial loss over each example is the maximum loss over all the associated distributions. The goal is to minimize the overall adversarial loss. We show sample complexity bounds in the PAC-learning setting for our notion of adversarial loss. Our notion of adversarial loss contrasts the prior work on robust learning that considers a set of points, not distributions, as the perturbation set of each clean example. As an application of our approach, we show how to unify the two lines of work on randomized smoothing and robust learning in the PAC-learning setting and derive sample complexity bounds for randomized smoothing methods. Furthermore, we investigate the role of randomness in achieving robustness against adversarial attacks. We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks and show its effectiveness empirically.         ",
    "url": "https://arxiv.org/abs/2406.03458",
    "authors": [
      "Saba Ahmadi",
      "Siddharth Bhandari",
      "Avrim Blum",
      "Chen Dan",
      "Prabhav Jain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.11245",
    "title": "Deep-Reinforcement-Learning-Based AoI-Aware Resource Allocation for RIS-Aided IoV Networks",
    "abstract": "           Reconfigurable Intelligent Surface (RIS) is a pivotal technology in communication, offering an alternative path that significantly enhances the link quality in wireless communication environments. In this paper, we propose a RIS-assisted internet of vehicles (IoV) network, considering the vehicle-to-everything (V2X) communication method. In addition, in order to improve the timeliness of vehicle-to-infrastructure (V2I) links and the stability of vehicle-to-vehicle (V2V) links, we introduce the age of information (AoI) model and the payload transmission probability model. Therefore, with the objective of minimizing the AoI of V2I links and prioritizing transmission of V2V links payload, we construct this optimization problem as an Markov decision process (MDP) problem in which the BS serves as an agent to allocate resources and control phase-shift for the vehicles using the soft actor-critic (SAC) algorithm, which gradually converges and maintains a high stability. A AoI-aware joint vehicular resource allocation and RIS phase-shift control scheme based on SAC algorithm is proposed and simulation results show that its convergence speed, cumulative reward, AoI performance, and payload transmission probability outperforms those of proximal policy optimization (PPO), deep deterministic policy gradient (DDPG), twin delayed deep deterministic policy gradient (TD3) and stochastic algorithms.         ",
    "url": "https://arxiv.org/abs/2406.11245",
    "authors": [
      "Kangwei Qi",
      "Qiong Wu",
      "Pingyi Fan",
      "Nan Cheng",
      "Wen Chen",
      "Jiangzhou Wang",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2407.03146",
    "title": "Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach",
    "abstract": "           Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed, it may have an unfair effect in multi-class classification. While data augmentation generally improves the overall performance (and therefore is beneficial for many classes), it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method. To derive it, we first formulate the training of a classifier as a non-linear optimization problem that aims at simultaneously maximizing the individual class performances and balancing them. By rewriting this optimization problem as an adversarial two-player game, we propose a novel multiplicative weight algorithm, for which we prove the convergence. Interestingly, our formulation also reveals that the class-dependent effects of data augmentation is not due to data augmentation only, but is in fact a general phenomenon. Our empirical results over six datasets demonstrate that the performance of learned classifiers is indeed more fairly distributed over classes, with only limited impact on the average accuracy.         ",
    "url": "https://arxiv.org/abs/2407.03146",
    "authors": [
      "Yunpeng Jiang",
      "Yutong Ban",
      "Paul Weng"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.07575",
    "title": "Resource Allocation for Twin Maintenance and Computing Task Processing in Digital Twin Vehicular Edge Computing Network",
    "abstract": "           As a promising technology, vehicular edge computing (VEC) can provide computing and caching services by deploying VEC servers near vehicles. However, VEC networks still face challenges such as high vehicle mobility. Digital twin (DT), an emerging technology, can predict, estimate, and analyze real-time states by digitally modeling objects in the physical world. By integrating DT with VEC, a virtual vehicle DT can be created in the VEC server to monitor the real-time operating status of vehicles. However, maintaining the vehicle DT model requires ongoing attention from the VEC server, which also needs to offer computing services for the vehicles. Therefore, effective allocation and scheduling of VEC server resources are crucial. This study focuses on a general VEC network with a single VEC service and multiple vehicles, examining the two types of delays caused by twin maintenance and computational processing within the network. By transforming the problem using satisfaction functions, we propose an optimization problem aimed at maximizing each vehicle's resource utility to determine the optimal resource allocation strategy. Given the non-convex nature of the issue, we employ multi-agent Markov decision processes to reformulate the problem. Subsequently, we propose the twin maintenance and computing task processing resource collaborative scheduling (MADRL-CSTC) algorithm, which leverages multi-agent deep reinforcement learning. Through experimental comparisons with alternative algorithms, it demonstrates that our proposed approach is effective in terms of resource allocation.         ",
    "url": "https://arxiv.org/abs/2407.07575",
    "authors": [
      "Yu Xie",
      "Qiong Wu",
      "Pingyi Fan",
      "Nan Cheng",
      "Wen Chen",
      "Jiangzhou Wang",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2407.14701",
    "title": "Contextual modulation of language comprehension in a dynamic neural model of lexical meaning",
    "abstract": "           We propose and computationally implement a dynamic neural model of lexical meaning, and experimentally test its behavioral predictions. We demonstrate the architecture and behavior of the model using as a test case the English lexical item 'have', focusing on its polysemous use. In the model, 'have' maps to a semantic space defined by two continuous conceptual dimensions, connectedness and control asymmetry, previously proposed to parameterize the conceptual system for language. The mapping is modeled as coupling between a neural node representing the lexical item and neural fields representing the conceptual dimensions. While lexical knowledge is modeled as a stable coupling pattern, real-time lexical meaning retrieval is modeled as the motion of neural activation patterns between metastable states corresponding to semantic interpretations or readings. Model simulations capture two previously reported empirical observations: (1) contextual modulation of lexical semantic interpretation, and (2) individual variation in the magnitude of this modulation. Simulations also generate a novel prediction that the by-trial relationship between sentence reading time and acceptability should be contextually modulated. An experiment combining self-paced reading and acceptability judgments replicates previous results and confirms the new model prediction. Altogether, results support a novel perspective on lexical polysemy: that the many related meanings of a word are metastable neural activation states that arise from the nonlinear dynamics of neural populations governing interpretation on continuous semantic dimensions.         ",
    "url": "https://arxiv.org/abs/2407.14701",
    "authors": [
      "Michael C. Stern",
      "Maria M. Pi\u00f1ango"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.00256",
    "title": "Mobility-Aware Federated Self-supervised Learning in Vehicular Network",
    "abstract": "           Federated Learning (FL) is an advanced distributed machine learning approach, that protects the privacy of each vehicle by allowing the model to be trained on multiple devices simultaneously without the need to upload all data to a road side unit (RSU). This enables FL to handle scenarios with sensitive or widely distributed data. However, in these fields, it is well known that the labeling costs can be a significant expense, and models relying on labels are not suitable for these rapidly evolving fields especially in vehicular networks, or mobile internet of things (MIoT), where new data emerges constantly. To handle this issue, the self-supervised learning paves the way for training without labels. Additionally, for vehicles with high velocity, owing to blurred images, simple aggregation not only impacts the accuracy of the aggregated model but also reduces the convergence speed of FL. This paper proposes a FL algorithm based on image blur level to aggregation, called FLSimCo, which does not require labels and serves as a pre-training stage for self-supervised learning in the vehicular environment. Simulation results demonstrate that the proposed algorithm exhibits fast and stable convergence.         ",
    "url": "https://arxiv.org/abs/2408.00256",
    "authors": [
      "Xueying Gu",
      "Qiong Wu",
      "Pingyi Fan",
      "Qiang Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2408.08095",
    "title": "Evaluating Time-Dependent Methods and Seasonal Effects in Code Technical Debt Prediction",
    "abstract": "           Background. Code Technical Debt (Code TD) prediction has gained significant attention in recent software engineering research. However, no standardized approach to Code TD prediction fully captures the factors influencing its evolution. Objective. Our study aims to assess the impact of time-dependent models and seasonal effects on Code TD prediction. It evaluates such models against widely used Machine Learning models, also considering the influence of seasonality on prediction performance. Methods. We trained 11 prediction models with 31 Java open-source projects. To assess their performance, we predicted future observations of the SQALE index. To evaluate the practical usability of our TD forecasting model and its impact on practitioners, we surveyed 23 software engineering professionals. Results. Our study confirms the benefits of time-dependent techniques, with the ARIMAX model outperforming the others. Seasonal effects improved predictive performance, though the impact remained modest. \\ReviewerA{ARIMAX/SARIMAX models demonstrated to provide well-balanced long-term forecasts. The survey highlighted strong industry interest in short- to medium-term TD forecasts. Conclusions. Our findings support using techniques that capture time dependence in historical software metric data, particularly for Code TD. Effectively addressing this evidence requires adopting methods that account for temporal patterns.         ",
    "url": "https://arxiv.org/abs/2408.08095",
    "authors": [
      "Mikel Robredo",
      "Nyyti Saarimaki",
      "Matteo Esposito",
      "Davide Taibi",
      "Rafael Penaloza",
      "Valentina Lenarduzzi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.14831",
    "title": "DRL-Based Federated Self-Supervised Learning for Task Offloading and Resource Allocation in ISAC-Enabled Vehicle Edge Computing",
    "abstract": "           Intelligent Transportation Systems (ITS) leverage Integrated Sensing and Communications (ISAC) to enhance data exchange between vehicles and infrastructure in the Internet of Vehicles (IoV). This integration inevitably increases computing demands, risking real-time system stability. Vehicle Edge Computing (VEC) addresses this by offloading tasks to Road Side Unit (RSU), ensuring timely services. Our previous work FLSimCo algorithm, which uses local resources for Federated Self-Supervised Learning (SSL), though vehicles often can't complete all iterations task. Our improved algorithm offloads partial task to RSU and optimizes energy consumption by adjusting transmission power, CPU frequency, and task assignment ratios, balancing local and RSU-based training. Meanwhile, setting an offloading threshold further prevents inefficiencies. Simulation results show that the enhanced algorithm reduces energy consumption, improves offloading efficiency and the accuracy of Federated SSL.         ",
    "url": "https://arxiv.org/abs/2408.14831",
    "authors": [
      "Xueying Gu",
      "Qiong Wu",
      "Pingyi Fan",
      "Nan Cheng",
      "Wen Chen",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2410.02601",
    "title": "Diffusion & Adversarial Schr\u00f6dinger Bridges via Iterative Proportional Markovian Fitting",
    "abstract": "           The Iterative Markovian Fitting (IMF) procedure, which iteratively projects onto the space of Markov processes and the reciprocal class, successfully solves the Schr\u00f6dinger Bridge (SB) problem. However, an efficient practical implementation requires a heuristic modification - alternating between fitting forward and backward time diffusion at each iteration. This modification is crucial for stabilizing training and achieving reliable results in applications such as unpaired domain translation. Our work reveals a close connection between the modified version of IMF and the Iterative Proportional Fitting (IPF) procedure - a foundational method for the SB problem, also known as Sinkhorn's algorithm. Specifically, we demonstrate that the heuristic modification of the IMF effectively integrates both IMF and IPF procedures. We refer to this combined approach as the Iterative Proportional Markovian Fitting (IPMF) procedure. Through theoretical and empirical analysis, we establish the convergence of IPMF procedure under various settings, contributing to developing a unified framework for solving SB problems. Moreover, from a practical standpoint, the IPMF procedure enables a flexible trade-off between image similarity and generation quality, offering a new mechanism for tailoring models to specific tasks.         ",
    "url": "https://arxiv.org/abs/2410.02601",
    "authors": [
      "Sergei Kholkin",
      "Grigoriy Ksenofontov",
      "David Li",
      "Nikita Kornilov",
      "Nikita Gushchin",
      "Alexandra Suvorikova",
      "Alexey Kroshnin",
      "Evgeny Burnaev",
      "Alexander Korotin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.11215",
    "title": "A CLIP-Powered Framework for Robust and Generalizable Data Selection",
    "abstract": "           Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets invariably incurs substantial storage and computational overhead. Meanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance. Data selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. Existing works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples. To address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. Specifically, our framework consists of three key modules-dataset adaptation, sample scoring, and selection optimization-that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. Extensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality.         ",
    "url": "https://arxiv.org/abs/2410.11215",
    "authors": [
      "Suorong Yang",
      "Peng Ye",
      "Wanli Ouyang",
      "Dongzhan Zhou",
      "Furao Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.20564",
    "title": "Using Confidence Scores to Improve Eyes-free Detection of Speech Recognition Errors",
    "abstract": "           Conversational systems rely heavily on speech recognition to interpret and respond to user commands and queries. Despite progress on speech recognition accuracy, errors may still sometimes occur and can significantly affect the end-user utility of such systems. While visual feedback can help detect errors, it may not always be practical, especially for people who are blind or low-vision. In this study, we investigate ways to improve error detection by manipulating the audio output of the transcribed text based on the recognizer's confidence level in its result. Our findings show that selectively slowing down the audio when the recognizer exhibited uncertainty led to a 12% relative increase in participants' ability to detect errors compared to uniformly slowing the audio. It also reduced the time it took participants to listen to the recognition result and decide if there was an error by 11%.         ",
    "url": "https://arxiv.org/abs/2410.20564",
    "authors": [
      "Sadia Nowrin",
      "Keith Vertanen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2410.23623",
    "title": "On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection",
    "abstract": "           Large numbers of synthesized videos from diffusion models pose threats to information security and authenticity, leading to an increasing demand for generated content detection. However, existing video-level detection algorithms primarily focus on detecting facial forgeries and often fail to identify diffusion-generated content with a diverse range of semantics. To advance the field of video forensics, we propose an innovative algorithm named Multi-Modal Detection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the profound perceptual and comprehensive abilities of Large Multi-modal Models (LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's multi-modal space, enhancing its ability to detect unseen forgery content. Besides, MM-Det leverages an In-and-Across Frame Attention (IAFA) mechanism for feature augmentation in the spatio-temporal domain. A dynamic fusion strategy helps refine forgery representations for the fusion. Moreover, we construct a comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF), across a wide range of forgery videos. MM-Det achieves state-of-the-art performance in DVF, demonstrating the effectiveness of our algorithm. Both source code and DVF are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.23623",
    "authors": [
      "Xiufeng Song",
      "Xiao Guo",
      "Jiache Zhang",
      "Qirui Li",
      "Lei Bai",
      "Xiaoming Liu",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.24117",
    "title": "AlphaTrans: A Neuro-Symbolic Compositional Approach for Repository-Level Code Translation and Validation",
    "abstract": "           Code translation transforms programs from one programming language (PL) to another. Several rule-based transpilers have been designed to automate code translation between different pairs of PLs. However, the rules can become obsolete as the PLs evolve and cannot generalize to other PLs. Recent studies have explored the automation of code translation using Large Language Models (LLMs). One key observation is that such techniques may work well for crafted benchmarks but fail to generalize to the scale and complexity of real-world projects with dependencies, custom types, PL-specific features, etc. We propose AlphaTrans, a neuro-symbolic approach to automate repository-level code translation. AlphaTrans translates both source and test code, and employs multiple levels of validation to ensure the translation preserves the functionality of the source program. To break down the problem for LLMs, AlphaTrans leverages program analysis to decompose the program into fragments and translates them in the reverse call order. We leveraged AlphaTrans to translate ten real-world open-source projects consisting of <836, 8575, 2719> classes, methods, and tests. AlphaTrans breaks down these projects into 17874 fragments and translates the entire repository. 96.40% of the translated fragments are syntactically correct, and AlphaTrans validates the translations' runtime behavior and functional correctness for 27.03% and 25.14% of fragments. On average, the integrated translation and validation take 34 hours to translate a project, showing its scalability in practice. For the incorrect translations, AlphaTrans generates a report including existing translation, stack trace, test errors, or assertion failures. We provided these artifacts to two developers to fix the translation bugs in four projects. They were able to fix the issues in 20.1 hours on average and achieve all passing tests.         ",
    "url": "https://arxiv.org/abs/2410.24117",
    "authors": [
      "Ali Reza Ibrahimzada",
      "Kaiyao Ke",
      "Mrigank Pawagi",
      "Muhammad Salman Abid",
      "Rangeet Pan",
      "Saurabh Sinha",
      "Reyhaneh Jabbarvand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.13057",
    "title": "Learning Multi-Branch Cooperation for Enhanced Click-Through Rate Prediction at Taobao",
    "abstract": "           Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type usually constrains the model's capability to capture the complex feature relationships, especially for industrial data with enormous input feature fields. Recent research shows that effective CTR models often combine an MLP network with a dedicated feature interaction network in a two-parallel structure. However, the interplay and cooperative dynamics between different streams or branches remain under-researched. In this work, we introduce a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. Specifically, MBCnet consists of three branches: the Extensible Feature Grouping and Crossing (EFGC) branch that promotes the model's memorization ability of specific feature fields, the low rank Cross Net branch and Deep branch to enhance explicit and implicit feature crossing for improved generalization. Among these branches, a novel cooperation scheme is proposed based on two principles: Branch co-teaching and moderate differentiation. Branch co-teaching encourages well-learned branches to support poorly-learned ones on specific training samples. Moderate differentiation advocates branches to maintain a reasonable level of difference in their feature representations on the same inputs. This cooperation strategy improves learning through mutual knowledge sharing and boosts the discovery of diverse feature interactions across branches. Experiments on large-scale industrial datasets and online A/B test at Taobao app demonstrate MBCnet's superior performance, delivering a 0.09 point increase in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes are available online.         ",
    "url": "https://arxiv.org/abs/2411.13057",
    "authors": [
      "Xu Chen",
      "Zida Cheng",
      "Yuangang Pan",
      "Shuai Xiao",
      "Xiaoming Liu",
      "Jinsong Lan",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Ivor W. Tsang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.13681",
    "title": "Elephant in the Room: Dissecting and Reflecting on the Evolution of Online Social Network Research",
    "abstract": "           Billions of individuals engage with Online Social Networks (OSN) daily. The owners of OSN try to meet the demands of their end-users while complying with business necessities. Such necessities may, however, lead to the adoption of restrictive data access policies that hinder research activities from \"external\" scientists -- who may, in turn, resort to other means (e.g., rely on static datasets) for their studies. Given the abundance of literature on OSN, we -- as academics -- should take a step back and reflect on what we have done so far, after having written thousands of papers on OSN. This is the first paper that provides a holistic outlook to the entire body of research that focused on OSN -- since the seminal work by Acquisti and Gross (2006). First, we search through over 1 million peer-reviewed publications, and derive 13,842 papers that focus on OSN: we organize the metadata of these works in the Minerva-OSN dataset, the first of its kind -- which we publicly release. Next, by analyzing Minerva-OSN, we provide factual evidence elucidating trends and aspects that deserve to be brought to light, such as the predominant focus on Twitter or the difficulty in obtaining OSN data. Finally, as a constructive step to guide future research, we carry out an expert survey (n=50) with established scientists in this field, and coalesce suggestions to improve the status quo such as an increased involvement of OSN owners. Our findings should inspire a reflection to \"rescue\" research on OSN. Doing so would improve the overall OSN ecosystem, benefiting both their owners and end-users and, hence, our society.         ",
    "url": "https://arxiv.org/abs/2411.13681",
    "authors": [
      "Luca Pajola",
      "Saskia Laura Schr\u00f6er",
      "Pier Paolo Tricomi",
      "Mauro Conti",
      "Giovanni Apruzzese"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2411.16122",
    "title": "From Collapse to Stability: A Knowledge-Driven Ensemble Framework for Scaling Up Click-Through Rate Prediction Models",
    "abstract": "           Click-through rate (CTR) prediction plays a crucial role in modern recommender systems. While many existing methods utilize ensemble networks to improve CTR model performance, they typically restrict the ensemble to only two or three sub-networks. Whether increasing the number of sub-networks consistently enhances CTR model performance to align with scaling laws remains unclear. In this paper, we investigate larger ensemble networks and find three inherent limitations in commonly used ensemble methods: (1) performance degradation as the number of sub-networks increases; (2) sharp declines and high variance in sub-network performance; and (3) significant discrepancies between sub-network and ensemble predictions. Meanwhile, we analyze the underlying causes of these limitations from the perspective of dimensional collapse: the collapse within sub-networks becomes increasingly severe as the number of sub-networks grows, leading to a lower knowledge abundance. In this paper, we employ knowledge transfer methods, such as Knowledge Distillation (KD) and Deep Mutual Learning (DML), to address the aforementioned limitations. We find that KD enables CTR models to better follow scaling laws, while DML reduces variance among sub-networks and minimizes discrepancies with ensemble predictions. Furthermore, by combining KD and DML, we propose a model-agnostic and hyperparameter-free Knowledge-Driven Ensemble Framework (KDEF) for CTR Prediction.         ",
    "url": "https://arxiv.org/abs/2411.16122",
    "authors": [
      "Honghao Li",
      "Lei Sang",
      "Yi Zhang",
      "Guangming Cui",
      "Yiwen Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2412.00119",
    "title": "Training Multi-Layer Binary Neural Networks With Local Binary Error Signals",
    "abstract": "           Binary Neural Networks (BNNs) significantly reduce computational complexity and memory usage in machine and deep learning by representing weights and activations with just one bit. However, most existing training algorithms for BNNs rely on quantization-aware floating-point Stochastic Gradient Descent (SGD), limiting the full exploitation of binary operations to the inference phase only. In this work, we propose, for the first time, a fully binary and gradient-free training algorithm for multi-layer BNNs, eliminating the need for back-propagated floating-point gradients. Specifically, the proposed algorithm relies on local binary error signals and binary weight updates, employing integer-valued hidden weights that serve as a synaptic metaplasticity mechanism, thereby enhancing its neurobiological plausibility. Our proposed solution enables the training of binary multi-layer perceptrons by using exclusively XNOR, Popcount, and increment/decrement operations. Experimental results on multi-class classification benchmarks show test accuracy improvements of up to +35.47% over the only existing fully binary single-layer state-of-the-art solution. Compared to full-precision SGD, our solution improves test accuracy by up to +35.30% under the same total memory demand, while also reducing computational cost by two to three orders of magnitude in terms of the total number of Boolean gates. The proposed algorithm is made available to the scientific community as a public repository.         ",
    "url": "https://arxiv.org/abs/2412.00119",
    "authors": [
      "Luca Colombo",
      "Fabrizio Pittorino",
      "Manuel Roveri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.00630",
    "title": "Collective Creation of Intimacy: Exploring the Cosplay Commission Practice within the Otome Game Community in China",
    "abstract": "           Cosplay commission (cos-commission) is a new form of commodified romantic companionship within the Otome game community in China. To explore the motivations, practices, experiences, and challenges, we conducted semi-structured interviews with 15 participants in different roles. Our findings reveal that cos-commission, as a hybrid activity, provides participants with a chance to collaboratively build meaningful connections. It also offers a pathway for personal exploration and emotional recovery. However, the vague boundary between performative roles and intimate interactions can give rise to unexpected negative outcomes, such as attachment-driven entanglements and post-commission \"withdrawal symptoms.\" While digital platforms facilitate communication in cos-commissions, they often lack sufficient safeguards. This preliminary work provides insights into the formation process of hybrid intimate relationship and its potential to foster personalized, long-term support for mental well-being, and reveals potential privacy and security challenges.         ",
    "url": "https://arxiv.org/abs/2412.00630",
    "authors": [
      "Yihao Zhou",
      "Haowei Xu",
      "Lili Zhang",
      "Shengdong Zhao"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2412.05366",
    "title": "ExploraCoder: Advancing code generation for multiple unseen APIs via planning and chained exploration",
    "abstract": "           Large language models face intrinsic limitations in coding with APIs that are unseen in their training corpora. As libraries continuously evolve, it becomes impractical to exhaustively retrain LLMs with new API knowledge. This limitation hampers LLMs from solving programming problems which require newly introduced or privately maintained libraries. Inspired by exploratory programming paradigm in human behavior, we propose ExploraCoder, a training-free framework that empowers LLMs to invoke multiple unseen APIs in code solution by (1) planning a complex problem into several API invocation subtasks, and (2) experimenting with correct API usage at intermediate steps through a novel chain-of-API-exploration. We conduct evaluation on program synthesizing tasks involving complex API interactions. Experimental results demonstrate that ExploraCoder significantly improves performance for models lacking prior API knowledge, achieving absolute increases of up to 11.99% over retrieval-based approaches and 17.28% over pretraining-based methods in pass@10.         ",
    "url": "https://arxiv.org/abs/2412.05366",
    "authors": [
      "Yunkun Wang",
      "Yue Zhang",
      "Zhen Qin",
      "Chen Zhi",
      "Binhua Li",
      "Fei Huang",
      "Yongbin Li",
      "Shuiguang Deng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2412.18091",
    "title": "AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning",
    "abstract": "           As deep neural networks (DNNs) are increasingly deployed on edge devices, optimizing models for constrained computational resources is critical. Existing auto-pruning methods face challenges due to the diversity of DNN models, various operators (e.g., filters), and the difficulty in balancing pruning granularity with model accuracy. To address these limitations, we introduce AutoSculpt, a pattern-based automated pruning framework designed to enhance efficiency and accuracy by leveraging graph learning and deep reinforcement learning (DRL). AutoSculpt automatically identifies and prunes regular patterns within DNN architectures that can be recognized by existing inference engines, enabling runtime acceleration. Three key steps in AutoSculpt include: (1) Constructing DNNs as graphs to encode their topology and parameter dependencies, (2) embedding computationally efficient pruning patterns, and (3) utilizing DRL to iteratively refine auto-pruning strategies until the optimal balance between compression and accuracy is achieved. Experimental results demonstrate the effectiveness of AutoSculpt across various architectures, including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming all baselines. The codes can be available at this https URL ",
    "url": "https://arxiv.org/abs/2412.18091",
    "authors": [
      "Lixian Jing",
      "Jianpeng Qi",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.07076",
    "title": "Representation Learning of Point Cloud Upsampling in Global and Local Inputs",
    "abstract": "           In recent years, point cloud upsampling has been widely applied in tasks such as 3D reconstruction and object recognition. This study proposed a novel framework, ReLPU, which enhances upsampling performance by explicitly learning from both global and local structural features of point clouds. Specifically, we extracted global features from uniformly segmented inputs (Average Segments) and local features from patch-based inputs of the same point cloud. These two types of features were processed through parallel autoencoders, fused, and then fed into a shared decoder for upsampling. This dual-input design improved feature completeness and cross-scale consistency, especially in sparse and noisy regions. Our framework was applied to several state-of-the-art autoencoder-based networks and validated on standard datasets. Experimental results demonstrated consistent improvements in geometric fidelity and robustness. In addition, saliency maps confirmed that parallel global-local learning significantly enhanced the interpretability and performance of point cloud upsampling.         ",
    "url": "https://arxiv.org/abs/2501.07076",
    "authors": [
      "Tongxu Zhang",
      "Bei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.14313",
    "title": "Between Close Enough to Reveal and Far Enough to Protect: a New Privacy Region for Correlated Data",
    "abstract": "           When users make personal privacy choices, correlation between their data can cause inadvertent leakage about users who do not want to share their data by other users sharing their data. As a solution, we consider local redaction mechanisms. As prior works proposed data-independent privatization mechanisms, we study the family of data-independent local redaction mechanisms and upper-bound their utility when data correlation is modeled by a stationary Markov process. In contrast, we derive a novel data-dependent mechanism, which improves the utility by leveraging a data-dependent leakage measure.         ",
    "url": "https://arxiv.org/abs/2501.14313",
    "authors": [
      "Luis Ma\u00dfny",
      "Rawad Bitar",
      "Fangwei Ye",
      "Salim El Rouayheb"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2501.18250",
    "title": "Online Neural Model Fine-Tuning in Massive MIMO CSI Feedback: Taming The Communication Cost of Model Updates",
    "abstract": "           Efficient channel state information (CSI) compression is essential in frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems due to the significant feedback overhead. Recently, deep learning-based compression techniques have demonstrated superior performance across various data types, including CSI. However, these methods often suffer from performance degradation when the data distribution shifts, primarily due to limited generalization capabilities. To address this challenge, we propose an online model fine-tuning approach for CSI feedback in massive MIMO systems. We consider full-model fine-tuning, where both the encoder and decoder are jointly updated using recent CSI samples. A key challenge in this setup is the transmission of updated decoder parameters, which introduces additional feedback overhead. To mitigate this bottleneck, we incorporate the bit-rate of model updates into the fine-tuning objective and entropy code the updates jointly with the compressed CSI. To reduce the bit-rate, we design an efficient prior distribution that encourages the network to update only the most significant weights, thereby minimizing the overall model update cost. Our results show that full-model fine-tuning significantly enhances the rate-distortion (RD) performance of neural CSI compression despite the additional communication cost of model updates. Moreover, we investigate the impact of update frequency in dynamic wireless environments and identify an optimal fine-tuning interval that achieves the best RD trade-off.         ",
    "url": "https://arxiv.org/abs/2501.18250",
    "authors": [
      "Mehdi Sattari",
      "Deniz G\u00fcnd\u00fcz",
      "Tommy Svensson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2501.18426",
    "title": "Guaranteed prediction sets for functional surrogate models",
    "abstract": "           We propose a method for obtaining statistically guaranteed prediction sets for functional machine learning methods: surrogate models which map between function spaces, motivated by the need to build reliable PDE emulators. The method constructs nested prediction sets on a low-dimensional representation (an SVD) of the surrogate model's error, and then maps these sets to the prediction space using set-propagation techniques. This results in prediction sets for functional surrogate models with conformal prediction coverage guarantees. We use zonotopes as basis of the set construction, which allow an exact linear propagation and are closed under Cartesian products, making them well-suited to this high-dimensional problem. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also introduce a technique to capture the truncation error of the SVD, preserving the guarantees of the method.         ",
    "url": "https://arxiv.org/abs/2501.18426",
    "authors": [
      "Ander Gray",
      "Vignesh Gopakumar",
      "Sylvain Rousseau",
      "S\u00e9bastien Destercke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.02970",
    "title": "Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models",
    "abstract": "           To detect unauthorized data usage in training large-scale generative models (e.g., ChatGPT or Midjourney), membership inference attacks (MIA) have proven effective in distinguishing a single training instance (a member) from a single non-training instance (a non-member). This success is mainly credited to a memorization effect: models tend to perform better on a member than a non-member. However, we find that standard MIAs fail against distilled generative models (i.e., student models) that are increasingly deployed in practice for efficiency (e.g., ChatGPT 4o-mini). Trained exclusively on data generated from a large-scale model (a teacher model), the student model lacks direct exposure to any members (teacher's training data), nullifying the memorization effect that standard MIAs rely on. This finding reveals a serious privacy loophole, where generation-service providers could deploy a student model whose teacher was potentially trained on unauthorized data, yet claim the deployed model is clean because it was not directly trained on such data. Hence, are distilled models inherently unauditable for upstream privacy violations, and should we discard them when we care about privacy? We contend no, as we uncover a memory chain connecting the student and teacher's member data: the distribution of student-generated data aligns more closely with the distribution of the teacher's members than with non-members, thus we can detect unauthorized data usage even when direct instance-level memorization is absent. This leads us to posit that MIAs on distilled generative models should shift from instance-level scores to distribution-level statistics. We further propose three principles of distribution-based MIAs for detecting unauthorized training data through distilled generative models, and validate our position through an exemplar framework. We lastly discuss the implications of our position.         ",
    "url": "https://arxiv.org/abs/2502.02970",
    "authors": [
      "Muxing Li",
      "Zesheng Ye",
      "Yixuan Li",
      "Andy Song",
      "Guangquan Zhang",
      "Feng Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.04227",
    "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach Penetration-Testing Active Directory Networks",
    "abstract": "           Penetration-testing, while critical for validating defenses and uncovering vulnerabilities, is often limited by high operational costs and the scarcity of human expertise. This paper investigates the feasibility and effectiveness of using Large Language Model (LLM)-driven autonomous systems to address these challenges in real-world Microsoft Active Directory (AD) enterprise networks. Our novel prototype, cochise, represents the first demonstration of a fully autonomous, LLM-driven framework capable of compromising accounts within a real-life Microsoft AD testbed (GOAD). The evaluation deliberately utilizes GOAD to capture the intricate interactions and sometimes nondeterministic outcomes of live network pen-testing, moving beyond the limitations of synthetic benchmarks. We perform our empirical evaluation using five LLMs, comparing reasoning to non-reasoning models as well as including open-weight models. Through comprehensive quantitative and qualitative analysis, incorporating insights from cybersecurity experts, we demonstrate that autonomous LLMs can effectively conduct Assumed Breach simulations. Key findings highlight their ability to dynamically adapt attack strategies, perform inter-context attacks, and generate scenario-specific attack parameters. Cochise also exhibits robust self-correction mechanisms, automatically installing missing tools and rectifying invalid command generations. Critically, we find that the associated costs are competitive with those incurred by professional pen-testers, suggesting a path toward democratizing access to essential security testing for organizations with budgetary constraints. However, our research also illuminates existing limitations, including instances of LLM ``going down rabbit holes'', challenges in comprehensive information transfer between planning and execution modules, and critical safety concerns that necessitate human oversight.         ",
    "url": "https://arxiv.org/abs/2502.04227",
    "authors": [
      "Andreas Happe",
      "J\u00fcrgen Cito"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.04818",
    "title": "Harnessing omnipresent oscillator networks as computational resource",
    "abstract": "           Nature is pervaded with oscillatory dynamics. In networks of coupled oscillators patterns can arise when the system synchronizes to an external input. Hence, these networks provide processing and memory of input. We present a universal framework for harnessing oscillator networks as computational resource. This computing framework is introduced by the ubiquitous model for phase-locking, the Kuramoto model. We force the Kuramoto model by a nonlinear target-system, then after substituting the target-system with a trained feedback-loop it emulates the target-system. Our results are two-fold. Firstly, the trained network inherits performance properties of the Kuramoto model, where all-to-all coupling is performed in linear time with respect to the number of nodes and parameters for synchronization are abundant. The latter implies that the network is generically successful since the system learns via sychronization. Secondly, the learning capabilities of the oscillator network, which describe a type of collective intelligence, can be explained using Kuramoto model's order parameter. In summary, this work provides the foundation for utilizing nature's oscillator networks as a new class of information processing systems.         ",
    "url": "https://arxiv.org/abs/2502.04818",
    "authors": [
      "Thomas Geert de Jong",
      "Hirofumi Notsu",
      "Kohei Nakajima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Chaotic Dynamics (nlin.CD)"
    ]
  },
  {
    "id": "arXiv:2502.05731",
    "title": "Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies",
    "abstract": "           Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventional text mining faces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactive text mining with prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactive mining of DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactive text mining in knowledge-intensive tasks for other application scenarios.         ",
    "url": "https://arxiv.org/abs/2502.05731",
    "authors": [
      "Sam Yu-Te Lee",
      "Cheng-Wei Hung",
      "Mei-Hua Yuan",
      "Kwan-Liu Ma"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2502.07230",
    "title": "Physics-Informed Recurrent Network for State-Space Modeling of Gas Pipeline Networks",
    "abstract": "           As a part of the integrated energy system (IES), gas pipeline networks can provide additional flexibility to power systems through coordinated optimal dispatch. An accurate pipeline network model is critical for the optimal operation and control of IESs. However, inaccuracies or unavailability of accurate pipeline parameters often introduce errors in the state-space models of such networks. This paper proposes a physics-informed recurrent network (PIRN) to identify the state-space model of gas pipelines. It fuses sparse measurement data with fluid-dynamic behavior expressed by partial differential equations. By embedding the physical state-space model within the recurrent network, parameter identification becomes an end-to-end PIRN training task. The model can be realized in PyTorch through modifications to a standard RNN backbone. Case studies demonstrate that our proposed PIRN can accurately estimate gas pipeline models from sparse terminal node measurements, providing robust performance and significantly higher parameter efficiency. Furthermore, the identified state-space model of the pipeline network can be seamlessly integrated into optimization frameworks.         ",
    "url": "https://arxiv.org/abs/2502.07230",
    "authors": [
      "Siyuan Wang",
      "Wenchuan Wu",
      "Chenhui Lin",
      "Qi Wang",
      "Shuwei Xu",
      "Binbin Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.14259",
    "title": "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records",
    "abstract": "           Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.         ",
    "url": "https://arxiv.org/abs/2502.14259",
    "authors": [
      "Sujeong Im",
      "Jungwoo Oh",
      "Edward Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.17371",
    "title": "Sustainable Greenhouse Microclimate Modeling: A Comparative Analysis of Recurrent and Graph Neural Networks",
    "abstract": "           The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both environmental dependencies and their directionality. We benchmark RNNs against directed STGNNs on two 15-min-resolution datasets from Volos (Greece): a six-variable 2020 installation and a more complex eight-variable greenhouse monitored in autumn 2024. In the simpler 2020 case the RNN attains near-perfect accuracy, outperforming the STGNN. When additional drivers are available in 2024, the STGNN overtakes the RNN ($R^{2}=0.905$ vs $0.740$), demonstrating that explicitly modelling directional dependencies becomes critical as interaction complexity grows. These findings indicate when graph-based models are warranted and provide a stepping-stone toward digital twins that jointly optimise crop yield and PV power in agrivoltaic greenhouses.         ",
    "url": "https://arxiv.org/abs/2502.17371",
    "authors": [
      "Emiliano Seri",
      "Marcello Petitta",
      "Chryssoula Papaioannou",
      "Nikolaos Katsoulas",
      "Cristina Cornaro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2502.20843",
    "title": "Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments",
    "abstract": "           For robots to operate in general environments like households, they must be able to perform non-prehensile manipulation actions such as toppling and rolling to manipulate ungraspable objects. However, prior works on non-prehensile manipulation cannot yet generalize across environments with diverse geometries. The main challenge lies in adapting to varying environmental constraints: within a cabinet, the robot must avoid walls and ceilings; to lift objects to the top of a step, the robot must account for the step's pose and extent. While deep reinforcement learning (RL) has demonstrated impressive success in non-prehensile manipulation, accounting for such variability presents a challenge for the generalist policy, as it must learn diverse strategies for each new combination of constraints. To address this, we propose a modular and reconfigurable architecture that adaptively reconfigures network modules based on task requirements. To capture the geometric variability in environments, we extend the contact-based object representation (CORN) to environment geometries, and propose a procedural algorithm for generating diverse environments to train our agent. Taken together, the resulting policy can zero-shot transfer to novel real-world environments and objects despite training entirely within a simulator. We additionally release a simulation-based benchmark featuring nine digital twins of real-world scenes with 353 objects to facilitate non-prehensile manipulation research in realistic domains.         ",
    "url": "https://arxiv.org/abs/2502.20843",
    "authors": [
      "Yoonyoung Cho",
      "Junhyek Han",
      "Jisu Han",
      "Beomjoon Kim"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01437",
    "title": "Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep Reinforcement Learning",
    "abstract": "           Recent works have successfully demonstrated that sparse deep reinforcement learning agents can be competitive against their dense counterparts. This opens up opportunities for reinforcement learning applications in fields where inference time and memory requirements are cost-sensitive or limited by hardware. Until now, dense-to-sparse methods have relied on hand-designed sparsity schedules that are not synchronized with the agent's learning pace. Crucially, the final sparsity level is chosen as a hyperparameter, which requires careful tuning as setting it too high might lead to poor performances. In this work, we address these shortcomings by crafting a dense-to-sparse algorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at the agent's learning pace, we consider multiple online networks with different sparsity levels, where each online network is trained from a shared target network. At each target update, the online network with the smallest loss is chosen as the next target network, while the other networks are replaced by a pruned version of the chosen network. We evaluate the proposed approach on the Atari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN reaches high sparsity levels while keeping performances high.         ",
    "url": "https://arxiv.org/abs/2503.01437",
    "authors": [
      "Th\u00e9o Vincent",
      "Tim Faust",
      "Yogesh Tripathi",
      "Jan Peters",
      "Carlo D'Eramo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.06680",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation",
    "abstract": "           Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.         ",
    "url": "https://arxiv.org/abs/2503.06680",
    "authors": [
      "Wei Li",
      "Xin Zhang",
      "Zhongxin Guo",
      "Shaoguang Mao",
      "Wen Luo",
      "Guangyue Peng",
      "Yangyu Huang",
      "Houfeng Wang",
      "Scarlett Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.08558",
    "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
    "abstract": "           Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.         ",
    "url": "https://arxiv.org/abs/2503.08558",
    "authors": [
      "Chen Xu",
      "Tony Khuong Nguyen",
      "Emma Dixon",
      "Christopher Rodriguez",
      "Patrick Miller",
      "Robert Lee",
      "Paarth Shah",
      "Rares Ambrus",
      "Haruki Nishimura",
      "Masha Itkina"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.09149",
    "title": "Memory-enhanced Retrieval Augmentation for Long Video Understanding",
    "abstract": "           Efficient long-video understanding~(LVU) remains a challenging task in computer vision. Current long-context vision-language models~(LVLMs) suffer from information loss due to compression and brute-force downsampling. While retrieval-augmented generation (RAG) methods mitigate this issue, their applicability is limited due to explicit query dependency. To overcome this challenge, we introduce a novel memory-enhanced RAG-based approach called MemVid, which is inspired by the cognitive memory of human beings. Our approach operates in four basic steps: 1) memorizing holistic video information, 2) reasoning about the task's information needs based on memory, 3) retrieving critical moments based on the information needs, and 4) focusing on the retrieved moments to produce the final answer. To enhance the system's memory-grounded reasoning capabilities while achieving optimal end-to-end performance, we propose a curriculum learning strategy. This approach begins with supervised learning on well-annotated reasoning results, then progressively explores and reinforces more plausible reasoning outcomes through reinforcement learning. We perform extensive evaluations on popular LVU benchmarks, including MLVU, VideoMME and LVBench. In our experiments, MemVid demonstrates superior efficiency and effectiveness compared to both LVLMs and RAG methods.         ",
    "url": "https://arxiv.org/abs/2503.09149",
    "authors": [
      "Huaying Yuan",
      "Zheng Liu",
      "Minghao Qin",
      "Hongjin Qian",
      "Yan Shu",
      "Zhicheng Dou",
      "Ji-Rong Wen",
      "Nicu Sebe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2503.15551",
    "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
    "abstract": "           Batch prompting, which combines a batch of multiple queries sharing the same context in one inference, has emerged as a promising solution to reduce inference costs. However, our study reveals a significant security vulnerability in batch prompting: malicious users can inject attack instructions into a batch, leading to unwanted interference across all queries, which can result in the inclusion of harmful content, such as phishing links, or the disruption of logical reasoning. In this paper, we construct BATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of two types and 8k batch instances, to study the batch prompting vulnerability systematically. Our evaluation of both closed-source and open-weight LLMs demonstrates that all LLMs are susceptible to batch-prompting attacks. We then explore multiple defending approaches. While the prompting-based defense shows limited effectiveness for smaller LLMs, the probing-based approach achieves about 95% accuracy in detecting attacks. Additionally, we perform a mechanistic analysis to understand the attack and identify attention heads that are responsible for it.         ",
    "url": "https://arxiv.org/abs/2503.15551",
    "authors": [
      "Murong Yue",
      "Ziyu Yao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.17193",
    "title": "MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection",
    "abstract": "           In complex environments, detecting tiny infrared targets has always been challenging because of the low contrast and high noise levels inherent in infrared images. These factors often lead to the loss of crucial details during feature extraction. Moreover, existing detection methods have limitations in adequately integrating global and local information, which constrains the efficiency and accuracy of infrared small target detection. To address these challenges, this paper proposes a network architecture named MSCA-Net, which integrates three key components: Multi-Scale Enhanced Dilated Attention mechanism (MSEDA), Positional Convolutional Block Attention Module (PCBAM), and Channel Aggregation Feature Fusion Block (CAB). Specifically, MSEDA employs a multi-scale feature fusion attention mechanism to adaptively aggregate information across different scales, enriching feature representation. PCBAM captures the correlation between global and local features through a correlation matrix-based strategy, enabling deep feature interaction. Moreover, CAB enhances the representation of critical features by assigning greater weights to them, integrating both low-level and high-level information, and thereby improving the models detection performance in complex backgrounds. The experimental results demonstrate that MSCA-Net achieves strong small target detection performance in complex backgrounds. Specifically, it attains mIoU scores of 78.43%, 94.56%, and 67.08% on the NUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its effectiveness and strong potential for real-world applications.         ",
    "url": "https://arxiv.org/abs/2503.17193",
    "authors": [
      "Xiaojin Lu",
      "Taoran yue",
      "Jiaxi cai",
      "Yuanping Chen",
      "Cuihong Lv",
      "Shibing Chu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.19549",
    "title": "Noise Resilient Over-The-Air Federated Learning In Heterogeneous Wireless Networks",
    "abstract": "           In 6G wireless networks, Artificial Intelligence (AI)-driven applications demand the adoption of Federated Learning (FL) to enable efficient and privacy-preserving model training across distributed devices. Over-The-Air Federated Learning (OTA-FL) exploits the superposition property of multiple access channels, allowing edge users in 6G networks to efficiently share spectral resources and perform low-latency global model aggregation. However, these advantages come with challenges, as traditional OTA-FL techniques suffer due to the joint effects of Additive White Gaussian Noise (AWGN) at the server, fading, and both data and system heterogeneity at the participating edge devices. In this work, we propose the novel Noise Resilient Over-the-Air Federated Learning (NoROTA-FL) framework to jointly tackle these challenges in federated wireless networks. In NoROTA-FL, the local optimization problems find controlled inexact solutions, which manifests as an additional proximal constraint at the clients. This approach provides robustness against straggler-induced partial work, heterogeneity, noise, and fading. From a theoretical perspective, we leverage the zeroth- and first-order inexactness and establish convergence guarantees for non-convex optimization problems in the presence of heterogeneous data and varying system capabilities. Experimentally, we validate NoROTA-FL on real-world datasets, including FEMNIST, CIFAR10, and CIFAR100, demonstrating its robustness in noisy and heterogeneous environments. Compared to state-of-the-art baselines such as COTAF and FedProx, NoROTA-FL achieves significantly more stable convergence and higher accuracy, particularly in the presence of stragglers.         ",
    "url": "https://arxiv.org/abs/2503.19549",
    "authors": [
      "Zubair Shaban",
      "Nazreen Shah",
      "Ranjitha Prasad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2504.07717",
    "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
    "abstract": "           Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2504.07717",
    "authors": [
      "Yang Jiao",
      "Xiaodong Wang",
      "Kai Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07863",
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "abstract": "           Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2504.07863",
    "authors": [
      "Mengjia Niu",
      "Hamed Haddadi",
      "Guansong Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.08584",
    "title": "Boosting multi-demographic federated learning for chest radiograph analysis using general-purpose self-supervised representations",
    "abstract": "           Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P<0.001) but degraded performance for larger datasets (P<0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P<0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.         ",
    "url": "https://arxiv.org/abs/2504.08584",
    "authors": [
      "Mahshad Lotfinia",
      "Arash Tayebiarasteh",
      "Samaneh Samiei",
      "Mehdi Joodaki",
      "Soroosh Tayebi Arasteh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.09440",
    "title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
    "abstract": "           Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.         ",
    "url": "https://arxiv.org/abs/2504.09440",
    "authors": [
      "MingShan Liu",
      "Jialing Fang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.10545",
    "title": "HSTU-BLaIR: Lightweight Contrastive Text Embedding for Generative Recommender",
    "abstract": "           Recent advances in recommender systems have underscored the complementary strengths of generative modeling and pretrained language models. We propose HSTU-BLaIR, a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU)-based generative recommender with BLaIR, a lightweight contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities. We evaluate HSTU-BLaIR on two e-commerce datasets: three subsets from the Amazon Reviews 2023 dataset and the Steam dataset. We compare its performance against both the original HSTU-based recommender and a variant augmented with embeddings from OpenAI's state-of-the-art \\texttt{text-embedding-3-large} model. Despite the latter being trained on a substantially larger corpus with significantly more parameters, our lightweight BLaIR-enhanced approach -- pretrained on domain-specific data -- achieves better performance in nearly all cases. Specifically, HSTU-BLaIR outperforms the OpenAI embedding-based variant on all but one metric, where it is marginally lower, and matches it on another. These findings highlight the effectiveness of contrastive text embeddings in compute-efficient recommendation settings.         ",
    "url": "https://arxiv.org/abs/2504.10545",
    "authors": [
      "Yijun Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.11264",
    "title": "DeepSelective: Interpretable Prognosis Prediction via Feature Selection and Compression in EHR Data",
    "abstract": "           The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at this http URL .         ",
    "url": "https://arxiv.org/abs/2504.11264",
    "authors": [
      "Ruochi Zhang",
      "Qian Yang",
      "Xiaoyang Wang",
      "Tian Wang",
      "Qiong Zhou",
      "Ziqi Deng",
      "Kewei Li",
      "Yueying Wang",
      "Yusi Fan",
      "Jiale Zhang",
      "Lan Huang",
      "Chang Liu",
      "Fengfeng Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.12345",
    "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
    "abstract": "           Urban causal research is essential for understanding the complex, dynamic processes that shape cities and for informing evidence-based policies. However, current practices are often constrained by inefficient and biased hypothesis formulation, challenges in integrating multimodal data, and fragile experimental methodologies. Imagine a system that automatically estimates the causal impact of congestion pricing on commute times by income group or measures how new green spaces affect asthma rates across neighborhoods using satellite imagery and health reports, and then generates comprehensive, policy-ready outputs, including causal estimates, subgroup analyses, and actionable recommendations. In this Perspective, we propose UrbanCIA, an LLM-driven conceptual framework composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy insights. We begin by examining the current landscape of urban causal research through a structured taxonomy of research topics, data sources, and methodological approaches, revealing systemic limitations across the workflow. Next, we introduce the design principles and technological roadmap for the four modules in the proposed framework. We also propose evaluation criteria to assess the rigor and transparency of these AI-augmented processes. Finally, we reflect on the broader implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces LLM-driven tools as catalysts for more scalable, reproducible, and inclusive urban research.         ",
    "url": "https://arxiv.org/abs/2504.12345",
    "authors": [
      "Yutong Xia",
      "Ao Qu",
      "Yunhan Zheng",
      "Yihong Tang",
      "Dingyi Zhuang",
      "Yuxuan Liang",
      "Shenhao Wang",
      "Cathy Wu",
      "Lijun Sun",
      "Roger Zimmermann",
      "Jinhua Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2504.15920",
    "title": "ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion",
    "abstract": "           Graph Neural Networks (GNNs) have demonstrated impressive performance across diverse graph-based tasks by leveraging message passing to capture complex node relationships. However, when applied to large-scale real-world graphs, GNNs face two major challenges: First, it becomes increasingly difficult to ensure both scalability and efficiency, as the repeated aggregation of large neighborhoods leads to significant computational overhead; Second, the over-smoothing problem arises, where excessive or deep propagation makes node representations indistinguishable, severely hindering model expressiveness. To tackle these issues, we propose ScaleGNN, a novel framework that adaptively fuses multi-level graph features for both scalable and effective graph learning. ScaleGNN first constructs per-order neighbor matrices that capture only the exclusive structural information at each hop, avoiding the redundancy of conventional aggregation. A learnable fusion module then selectively integrates these features, emphasizing the most informative high-order neighbors. To further reduce redundancy and over-smoothing, we introduce a Local Contribution Score (LCS)-based masking mechanism to filter out less relevant high-order neighbors, ensuring that only the most meaningful information is aggregated. In addition, a task-aware feature fusion strategy dynamically balances low- and high-order information, preserving both local detail and global context without incurring excessive complexity. Extensive experiments on real-world datasets demonstrate that ScaleGNN consistently outperforms state-of-the-art GNNs in both predictive accuracy and computational efficiency, highlighting its practical value for large-scale graph learning.         ",
    "url": "https://arxiv.org/abs/2504.15920",
    "authors": [
      "Xiang Li",
      "Jianpeng Qi",
      "Haobing Liu",
      "Yuan Cao",
      "Guoqing Chao",
      "Zhongying Zhao",
      "Junyu Dong",
      "Yanwei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.17924",
    "title": "Learning Attentive Neural Processes for Planning with Pushing Actions",
    "abstract": "           Our goal is to enable robots to plan sequences of tabletop actions to push a block with unknown physical properties to a desired goal pose. We approach this problem by learning the constituent models of a Partially-Observable Markov Decision Process (POMDP), where the robot can observe the outcome of a push, but the physical properties of the block that govern the dynamics remain unknown. A common solution approach is to train an observation model in a supervised fashion, and do inference with a general inference technique such as particle filters. However, supervised training requires knowledge of the relevant physical properties that determine the problem dynamics, which we do not assume to be known. Planning also requires simulating many belief updates, which becomes expensive when using particle filters to represent the belief. We propose to learn an Attentive Neural Process that computes the belief over a learned latent representation of the relevant physical properties given a history of actions. To address the pushing planning problem, we integrate a trained Neural Process with a double-progressive widening sampling strategy. Simulation results indicate that Neural Process Tree with Double Progressive Widening (NPT-DPW) generates better-performing plans faster than traditional particle-filter methods that use a supervised-trained observation model, even in complex pushing scenarios.         ",
    "url": "https://arxiv.org/abs/2504.17924",
    "authors": [
      "Atharv Jain",
      "Seiji Shaw",
      "Nicholas Roy"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.18990",
    "title": "Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System",
    "abstract": "           Drivers are becoming increasingly reliant on advanced driver assistance systems (ADAS) as autonomous driving technology becomes more popular and developed with advanced safety features to enhance road safety. However, the increasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed to attacks and accidental faults. In this paper, we evaluate the resilience of a widely used ADAS against safety-critical attacks that target perception inputs. Various safety mechanisms are simulated to assess their impact on mitigating attacks and enhancing ADAS resilience. Experimental results highlight the importance of timely intervention by human drivers and automated safety mechanisms in preventing accidents in both driving and lateral directions and the need to resolve conflicts among safety interventions to enhance system resilience and reliability.         ",
    "url": "https://arxiv.org/abs/2504.18990",
    "authors": [
      "Cheng Chen",
      "Grant Xiao",
      "Daehyun Lee",
      "Lishan Yang",
      "Evgenia Smirni",
      "Homa Alemzadeh",
      "Xugui Zhou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.02819",
    "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
    "abstract": "           We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02819",
    "authors": [
      "Dmitriy Shopkhoev",
      "Ammar Ali",
      "Magauiya Zhussip",
      "Valentin Malykh",
      "Stamatios Lefkimmiatis",
      "Nikos Komodakis",
      "Sergey Zagoruyko"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.03991",
    "title": "Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges",
    "abstract": "           Video event detection is central to modern sports analytics, enabling automated understanding of key moments for performance evaluation, content creation, and tactical feedback. While deep learning has significantly advanced tasks like Temporal Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting (PES), existing surveys often overlook the fine-grained temporal demands and domain-specific challenges posed by sports. This survey first provides a clear conceptual distinction between TAL, AS, and PES, then introduces a methods-based taxonomy covering recent deep learning approaches for AS and PES, including feature-based pipelines, end-to-end architectures, and multimodal strategies. We further review benchmark datasets and evaluation protocols, identifying critical limitations such as reliance on broadcast-quality footage and lenient multi-label metrics that hinder real-world deployment. Finally, we outline open challenges and future directions toward more temporally precise, generalizable, and practical event spotting in sports video analysis.         ",
    "url": "https://arxiv.org/abs/2505.03991",
    "authors": [
      "Hao Xu",
      "Arbind Agrahari Baniya",
      "Sam Well",
      "Mohamed Reda Bouadjenek",
      "Richard Dazeley",
      "Sunil Aryal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.05625",
    "title": "SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation",
    "abstract": "           Estimating rate coefficients from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate coefficient estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a latent neural ODE learns the continuous and differentiable trajectory between chemical concentrations and their time derivatives; second, an explicit Chemical Reaction Neural Network (CRNN) extracts the underlying rate coefficients based on the learned dynamics; and third, fine-tune CRNN using a neural ODE solver to further improve rate coefficient estimation. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work on stiff Neural ODEs for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry.         ",
    "url": "https://arxiv.org/abs/2505.05625",
    "authors": [
      "Wenqing Peng",
      "Zhi-Song Liu",
      "Michael Boy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.05677",
    "title": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence",
    "abstract": "           Estimates of heterogeneous treatment assignment effects can inform treatment decisions. Under the presence of non-adherence (e.g., patients do not adhere to their assigned treatment), both the standard backdoor adjustment (SBD) and the conditional front-door adjustment (CFD) can recover unbiased estimates of the treatment assignment effects. However, the estimation variance of these approaches may vary widely across settings, which remains underexplored in the literature. In this work, we demonstrate theoretically and empirically that CFD yields lower-variance estimates than SBD when the true effect of treatment assignment is small (i.e., assigning an intervention leads to small changes in patients' future outcome). Additionally, since CFD requires estimating multiple nuisance parameters, we introduce LobsterNet, a multi-task neural network that implements CFD with joint modeling of the nuisance parameters. Empirically, LobsterNet reduces estimation error across several semi-synthetic and real-world datasets compared to baselines. Our findings suggest CFD with shared nuisance parameter modeling can improve treatment assignment effect estimation under non-adherence.         ",
    "url": "https://arxiv.org/abs/2505.05677",
    "authors": [
      "Winston Chen",
      "Trenton Chang",
      "Jenna Wiens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.06331",
    "title": "Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks",
    "abstract": "           Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws directly into the loss function. However, effective training of PINNs remains challenging due to internal covariate shift, which destabilizes feature distributions and impairs model expressiveness. While normalization techniques like Batch Normalization and Layer Normalization are standard remedies in deep learning, they disrupt the pointwise input-output mappings critical to preserving the physical consistency in PINNs. In this work, we introduce Mask-PINNs, a novel architecture that regulates internal feature distributions through a smooth, learnable mask function applied pointwise across hidden layers. Unlike conventional normalization methods, the proposed mask function preserves the deterministic nature of input-output relationships while suppressing activation drift and saturation. Theoretically, we demonstrate that Mask-PINNs control feature spread near initialization by attenuating gradient variance growth through a tailored modulation mechanism. Empirically, we validate the method on multiple PDE benchmarks across diverse activation functions. Our results show consistent improvements in prediction accuracy, convergence stability, and robustness, with relative L2 errors reduced by up to two orders of magnitude over baseline models. Furthermore, we demonstrate that Mask-PINNs enable the effective use of wider networks, overcoming a key limitation in existing PINN frameworks.         ",
    "url": "https://arxiv.org/abs/2505.06331",
    "authors": [
      "Feilong Jiang",
      "Xiaonan Hou",
      "Jianqiao Ye",
      "Min Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.07574",
    "title": "Security through the Eyes of AI: How Visualization is Shaping Malware Detection",
    "abstract": "           Malware, a persistent cybersecurity threat, increasingly targets interconnected digital systems such as desktop, mobile, and IoT platforms through sophisticated attack vectors. By exploiting these vulnerabilities, attackers compromise the integrity and resilience of modern digital ecosystems. To address this risk, security experts actively employ Machine Learning or Deep Learning-based strategies, integrating static, dynamic, or hybrid approaches to categorize malware instances. Despite their advantages, these methods have inherent drawbacks and malware variants persistently evolve with increased sophistication, necessitating advancements in detection strategies. Visualization-based techniques are emerging as scalable and interpretable solutions for detecting and understanding malicious behaviors across diverse platforms including desktop, mobile, IoT, and distributed systems as well as through analysis of network packet capture files. In this comprehensive survey of more than 100 high-quality research articles, we evaluate existing visualization-based approaches applied to malware detection and classification. As a first contribution, we propose a new all-encompassing framework to study the landscape of visualization-based malware detection techniques. Within this framework, we systematically analyze state-of-the-art approaches across the critical stages of the malware detection pipeline. By analyzing not only the single techniques but also how they are combined to produce the final solution, we shed light on the main challenges in visualization-based approaches and provide insights into the advancements and potential future directions in this critical field.         ",
    "url": "https://arxiv.org/abs/2505.07574",
    "authors": [
      "Matteo Brosolo",
      "Asmitha K. A.",
      "Mauro Conti",
      "Rafidha Rehiman K. A.",
      "Muhammed Shafi K. P.",
      "Serena Nicolazzo",
      "Antonino Nocera",
      "Vinod P"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.07895",
    "title": "Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks",
    "abstract": "           Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.         ",
    "url": "https://arxiv.org/abs/2505.07895",
    "authors": [
      "Jiafan Li",
      "Jiaqi Zhu",
      "Liang Chang",
      "Yilin Li",
      "Miaomiao Li",
      "Yang Wang",
      "Hongan Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.08013",
    "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer",
    "abstract": "           As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.         ",
    "url": "https://arxiv.org/abs/2505.08013",
    "authors": [
      "Gonglin Chen",
      "Tianwen Fu",
      "Haiwei Chen",
      "Wenbin Teng",
      "Hanyuan Xiao",
      "Yajie Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.08429",
    "title": "Visual Image Reconstruction from Brain Activity via Latent Representation",
    "abstract": "           Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.         ",
    "url": "https://arxiv.org/abs/2505.08429",
    "authors": [
      "Yukiyasu Kamitani",
      "Misato Tanaka",
      "Ken Shirakawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2505.09518",
    "title": "Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs",
    "abstract": "           Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its this http URL compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP, and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs, and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.         ",
    "url": "https://arxiv.org/abs/2505.09518",
    "authors": [
      "Maris F. L. Galesloot",
      "Roman Andriushchenko",
      "Milan \u010ce\u0161ka",
      "Sebastian Junges",
      "Nils Jansen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.13008",
    "title": "Adversarial Reasoning for Repair Based on Inferred Program Intent",
    "abstract": "           Automated program repair (APR) has shown promising results, particularly with the use of neural networks. Currently, most APR tools focus on code transformations specified by test suites, rather than reasoning about the program intent and the high-level bug specification. Without a proper understanding of program intent, these tools tend to generate patches that overfit incomplete test suites and fail to reflect the developers intentions. However, reasoning about program intent is challenging. In our work, we propose an approach called AdverIntent-Agent, based on critique and adversarial reasoning. Our approach is novel to shift the focus from generating multiple APR patches to inferring multiple potential program intents. Ideally, we aim to infer intents that are, to some extent, adversarial to each other, maximizing the probability that at least one aligns closely with the developers original intent. AdverIntent-Agent is a multi-agent approach consisting of three agents: a reasoning agent, a test agent, and a repair agent. First, the reasoning agent generates adversarial program intents along with the corresponding faulty statements. Next, the test agent produces adversarial test cases that align with each inferred intent, constructing oracles that use the same inputs but have different expected outputs. Finally, the repair agent uses dynamic and precise LLM prompts to generate patches that satisfy both the inferred program intent and the generated tests. AdverIntent-Agent was evaluated on two benchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntent-Agent correctly repaired 77 and 105 bugs in both benchmarks, respectively.         ",
    "url": "https://arxiv.org/abs/2505.13008",
    "authors": [
      "He Ye",
      "Aidan Z.H. Yang",
      "Chang Hu",
      "Yanlin Wang",
      "Tao Zhang",
      "Claire Le Goues"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.16039",
    "title": "An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection",
    "abstract": "           The brain is a highly complex organ that manages many important tasks, including movement, memory and thinking. Brain-related conditions, like tumors and degenerative disorders, can be hard to diagnose and treat. Magnetic Resonance Imaging (MRI) serves as a key tool for identifying these conditions, offering high-resolution images of brain structures. Despite this, interpreting MRI scans can be complicated. This study tackles this challenge by conducting a comparative analysis of Vision Transformer (ViT) and Transfer Learning (TL) models such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain diseases using MRI data from Bangladesh based dataset. ViT, known for their ability to capture global relationships in images, are particularly effective for medical imaging tasks. Transfer learning helps to mitigate data constraints by fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods such as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are employed to interpret model predictions. The results demonstrate that ViT surpasses transfer learning models, achieving a classification accuracy of 94.39%. The integration of XAI methods enhances model transparency, offering crucial insights to aid medical professionals in diagnosing brain diseases with greater precision.         ",
    "url": "https://arxiv.org/abs/2505.16039",
    "authors": [
      "Shuvashis Sarker",
      "Shamim Rahim Refat",
      "Faika Fairuj Preotee",
      "Shifat Islam",
      "Tashreef Muhammad",
      "Mohammad Ashraful Hoque"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.16901",
    "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks",
    "abstract": "           Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.         ",
    "url": "https://arxiv.org/abs/2505.16901",
    "authors": [
      "Hongyuan Tao",
      "Ying Zhang",
      "Zhenhao Tang",
      "Hongen Peng",
      "Xukun Zhu",
      "Bingchang Liu",
      "Yingguang Yang",
      "Ziyin Zhang",
      "Zhaogui Xu",
      "Haipeng Zhang",
      "Linchao Zhu",
      "Rui Wang",
      "Hang Yu",
      "Jianguo Li",
      "Peng Di"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.19157",
    "title": "Efficient and robust solvers for a cell-by-cell dual-poroelasticity problem",
    "abstract": "           This paper presents a scalable and robust solver for a cell-by-cell dual-poroelasticity model, describing the mechanical interactions between brains cells embedded in extracellular space. Explicitly representing the complex cellular shapes, the proposed approach models both intracellular and extracellular spaces as distinct poroelastic media, separated by a permeable cell membrane which allows hydrostatic and osmotic pressure-driven fluid exchange. Based on a three-field (displacement, total pressure, and fluid pressure) formulation, the solver leverages the framework of norm equivalent preconditioning and appropriately fitted norms to ensure robustness across all material parameters of the model. Scalability for large and complex geometries is achieved through efficient Algebraic Multigrid (AMG) approximations of the preconditioners' individual blocks. Furthermore, we accommodate diverse boundary conditions, including full Dirichlet boundary conditions for displacement, which we handle efficiently using the Sherman-Morrison-Woodbury formula. Our theoretical analysis is complemented by numerical experiments demonstrating the preconditioners' robustness and performance across various parameters relevant to realistic scenarios, and a large scale example of cellular swelling on a dense reconstruction of the mouse visual cortex highlights the method's potential for investigating complex physiological processes like cellular volume regulation in detailed biological structures.         ",
    "url": "https://arxiv.org/abs/2505.19157",
    "authors": [
      "Marius Causemann",
      "Miroslav Kuchta"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.20981",
    "title": "RefAV: Towards Planning-Centric Scenario Mining",
    "abstract": "           Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal data localized to HD maps during normal fleet testing. However, identifying interesting and safety-critical scenarios from uncurated driving logs remains a significant challenge. Traditional scenario mining techniques are error-prone and prohibitively time-consuming, often relying on hand-crafted structured queries. In this work, we revisit spatio-temporal scenario mining through the lens of recent vision-language models (VLMs) to detect whether a described scenario occurs in a driving log and, if so, precisely localize it in both time and space. To address this problem, we introduce RefAV, a large-scale dataset of 10,000 diverse natural language queries that describe complex multi-agent interactions relevant to motion planning derived from 1000 driving logs in the Argoverse 2 Sensor dataset. We evaluate several referential multi-object trackers and present an empirical analysis of our baselines. Notably, we find that naively repurposing off-the-shelf VLMs yields poor performance, suggesting that scenario mining presents unique challenges. Lastly, we discuss our recent CVPR 2025 competition and share insights from the community. Our code and dataset are available at this https URL and this https URL ",
    "url": "https://arxiv.org/abs/2505.20981",
    "authors": [
      "Cainan Davidson",
      "Deva Ramanan",
      "Neehar Peri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.22214",
    "title": "Thermal Modeling and Optimal Allocation of Avionics Safety-critical Tasks on Heterogeneous MPSoCs",
    "abstract": "           Multi-Processor Systems-on-Chip (MPSoC) can deliver high performance needed in many industrial domains, including aerospace. However, their high power consumption, combined with avionics safety standards, brings new thermal management challenges. This paper investigates techniques for offline thermal-aware allocation of periodic tasks on heterogeneous MPSoCs running at a fixed clock frequency, as required in avionics. The goal is to find the assignment of tasks to (i) cores and (ii) temporal isolation windows while minimizing the MPSoC temperature. To achieve that, we propose and analyze three power models, and integrate them within several novel optimization approaches based on heuristics, a black-box optimizer, and Integer Linear Programming (ILP). We perform the experimental evaluation on three popular MPSoC platforms (NXP i.MX8QM MEK, NXP i.MX8QM Ixora, NVIDIA TX2) and observe a difference of up to 5.5\u00b0C among the tested methods (corresponding to a 22% reduction w.r.t. the ambient temperature). We also show that our method, integrating the empirical power model with the ILP, outperforms the other methods on all tested platforms.         ",
    "url": "https://arxiv.org/abs/2505.22214",
    "authors": [
      "Ond\u0159ej Benedikt",
      "Michal Sojka",
      "P\u0159emysl \u0160\u016fcha",
      "Pavel Zaykov",
      "Zden\u011bk Hanz\u00e1lek"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2506.00691",
    "title": "Optimizing Sensory Neurons: Nonlinear Attention Mechanisms for Accelerated Convergence in Permutation-Invariant Neural Networks for Reinforcement Learning",
    "abstract": "           Training reinforcement learning (RL) agents often requires significant computational resources and prolonged training durations. To address this challenge, we build upon prior work that introduced a neural architecture with permutation-invariant sensory processing. We propose a modified attention mechanism that applies a non-linear transformation to the key vectors (K), producing enriched representations (K') through a custom mapping function. This Nonlinear Attention (NLA) mechanism enhances the representational capacity of the attention layer, enabling the agent to learn more expressive feature interactions. As a result, our model achieves significantly faster convergence and improved training efficiency, while maintaining performance on par with the baseline. These results highlight the potential of nonlinear attention mechanisms to accelerate reinforcement learning without sacrificing effectiveness.         ",
    "url": "https://arxiv.org/abs/2506.00691",
    "authors": [
      "Junaid Muzaffar",
      "Khubaib Ahmed",
      "Ingo Frommholz",
      "Zeeshan Pervez",
      "Ahsan ul Haq"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.01231",
    "title": "Towards Efficient Few-shot Graph Neural Architecture Search via Partitioning Gradient Contribution",
    "abstract": "           To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods.         ",
    "url": "https://arxiv.org/abs/2506.01231",
    "authors": [
      "Wenhao Song",
      "Xuan Wu",
      "Bo Yang",
      "You Zhou",
      "Yubin Xiao",
      "Yanchun Liang",
      "Hongwei Ge",
      "Heow Pueh Lee",
      "Chunguo Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.01358",
    "title": "Prediction of the Conditional Probability Densities of Time Interval Extrema with Application to Risk-Sensitive Scheduling",
    "abstract": "           Planning and scheduling activities in the electrical power system, such as the commitment of reserve generation, often involve the statistical characterization of peak demand. Due to the stationarity assumption of classical extreme value analysis (EVA), existing approaches in the industry apply EVA on simulated annual peaks created by weather-dependent surrogate models using Monte-Carlo simulations on a per-scenario basis. In day-ahead scheduling, the daily peak demand changes upon various factors besides temperature, Monte-Carlo experiments become intractable, and state-of-the-art generalized additive model for location, scale and shape (GAMLSS)-based nonstationary EVA is often impractical due to convergence issues on high-dimensional covariates. This article explores uncharted territories and proposes a novel nonstationary EVA estimator that predicts the probable peaks of high-resolution time intervals and their corresponding conditional probability densities based on calendar information and weather conditions where historical peaks are observed. Compared to GAMLSS, our method automatically discovers and robustly models complex relationships between the covariate and the peak demand density. We present a case study on the determination of day-ahead scheduling capacity and demonstrate that compared to the industry approach, our approach results in a 38% reduction in the yearly total committed capacity while maintaining the given risk requirement.         ",
    "url": "https://arxiv.org/abs/2506.01358",
    "authors": [
      "Buyi Yu",
      "Wenyuan Tang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.01444",
    "title": "Variance-Based Defense Against Blended Backdoor Attacks",
    "abstract": "           Backdoor attacks represent a subtle yet effective class of cyberattacks targeting AI models, primarily due to their stealthy nature. The model behaves normally on clean data but exhibits malicious behavior only when the attacker embeds a specific trigger into the input. This attack is performed during the training phase, where the adversary corrupts a small subset of the training data by embedding a pattern and modifying the labels to a chosen target. The objective is to make the model associate the pattern with the target label while maintaining normal performance on unaltered data. Several defense mechanisms have been proposed to sanitize training data-sets. However, these methods often rely on the availability of a clean dataset to compute statistical anomalies, which may not always be feasible in real-world scenarios where datasets can be unavailable or compromised. To address this limitation, we propose a novel defense method that trains a model on the given dataset, detects poisoned classes, and extracts the critical part of the attack trigger before identifying the poisoned instances. This approach enhances explainability by explicitly revealing the harmful part of the trigger. The effectiveness of our method is demonstrated through experimental evaluations on well-known image datasets and comparative analysis against three state-of-the-art algorithms: SCAn, ABL, and AGPD.         ",
    "url": "https://arxiv.org/abs/2506.01444",
    "authors": [
      "Sujeevan Aseervatham",
      "Achraf Kerzazi",
      "Youn\u00e8s Bennani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.02404",
    "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation",
    "abstract": "           Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing recognition for its potential to enhance large language models (LLMs) by structurally organizing domain-specific corpora and facilitating complex reasoning. However, current evaluations of GraphRAG models predominantly rely on traditional question-answering datasets. Their limited scope in questions and evaluation metrics fails to comprehensively assess the reasoning capacity improvements enabled by GraphRAG models. To address this gap, we introduce GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously evaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\) Challenging question design. Featuring college-level, domain-specific questions that demand multi-hop reasoning, the benchmark ensures that simple content retrieval is insufficient for problem-solving. For example, some questions require mathematical reasoning or programming. \\((ii)\\) Diverse task coverage. The dataset includes a broad spectrum of reasoning tasks, multiple-choice, true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16 disciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework. GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG pipeline, including graph construction, knowledge retrieval, and answer generation. Beyond final-answer correctness, it evaluates the logical coherence of the reasoning process. By applying nine contemporary GraphRAG methods to GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based structuring improves model reasoning capabilities. Our analysis reveals critical insights about graph architectures, retrieval efficacy, and reasoning capabilities, offering actionable guidance for the research community.         ",
    "url": "https://arxiv.org/abs/2506.02404",
    "authors": [
      "Yilin Xiao",
      "Junnan Dong",
      "Chuang Zhou",
      "Su Dong",
      "Qian-wen Zhang",
      "Di Yin",
      "Xing Sun",
      "Xiao Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.05692",
    "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code",
    "abstract": "           The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce SafeGenBench, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon.         ",
    "url": "https://arxiv.org/abs/2506.05692",
    "authors": [
      "Xinghang Li",
      "Jingzhe Ding",
      "Chao Peng",
      "Bing Zhao",
      "Xiang Gao",
      "Hongwan Gao",
      "Xinchen Gu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.07081",
    "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
    "abstract": "           Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.         ",
    "url": "https://arxiv.org/abs/2506.07081",
    "authors": [
      "Sathvik Udupa",
      "Shinji Watanabe",
      "Petr Schwarz",
      "Jan Cernocky"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2506.09399",
    "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration",
    "abstract": "           Out-of-Distribution (OOD) detection is essential for the trustworthiness of AI systems. Methods using prior information (i.e., subspace-based methods) have shown effective performance by extracting information geometry to detect OOD data with a more appropriate distance metric. However, these methods fail to address the geometry distorted by ill-distributed samples, due to the limitation of statically extracting information geometry from the training distribution. In this paper, we argue that the influence of ill-distributed samples can be corrected by dynamically adjusting the prior geometry in response to new data. Based on this insight, we propose a novel approach that dynamically updates the prior covariance matrix using real-time input features, refining its information. Specifically, we reduce the covariance along the direction of real-time input features and constrain adjustments to the residual space, thus preserving essential data characteristics and avoiding effects on unintended directions in the principal space. We evaluate our method on two pre-trained models for the CIFAR dataset and five pre-trained models for ImageNet-1k, including the self-supervised DINO model. Extensive experiments demonstrate that our approach significantly enhances OOD detection across various models. The code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.09399",
    "authors": [
      "Kaiyu Guo",
      "Zijian Wang",
      "Tan Pan",
      "Brian C. Lovell",
      "Mahsa Baktashmotlagh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.09878",
    "title": "Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks",
    "abstract": "           Virtualising the Radio-Access Network (RAN) is increasingly viewed as an enabler of affordable 5G expansion and a stepping-stone toward AI-native 6G. Most discussions, however, still approach spectrum policy, cloud engineering and organisational practice as separate topics. This paper offers an integrated perspective spanning four pillars -- science, technology, business strategy and culture. A comparative U.S.\\ case study illustrates how mid-band contiguity, complemented by selective mmWave capacity layers, can improve both coverage and churn when orchestrated through software-defined carrier aggregation. We derive analytic capacity and latency bounds for Split 7.2 $\\times$ vRAN/O-RAN deployments, quantify the throughput penalty of end-to-end 256-bit encryption, and show how GPU/FPGA off-load plus digital-twin--driven automation keeps the hybrid-automatic-repeat request (HARQ) round-trip within a 0.5 ms budget. When these technical enablers are embedded in a physics-first delivery roadmap, average vRAN cycle time drops an order of magnitude -- even in the presence of cultural head-winds such as dual-ladder'' erosion. Three cybernetic templates -- the Clock-Hierarchy Law, Ashby's Requisite Variety and a delay-cost curve -- are then used to explain why silo-constrained automation can amplify, rather than absorb, integration debt. Looking forward, silicon-paced 6G evolution (9-12 month node shrinks, sub-THz joint communication-and-sensing, chiplet architectures and optical I/O) calls for a dual-resolution planning grid that couples five-year spectrum physics with six-month silicon sprints.'' The paper closes with balanced, action-oriented recommendations for operators, vendors and researchers on sub-THz fronthaul, AI-native security, energy-proportional accelerators and zero-touch assurance.         ",
    "url": "https://arxiv.org/abs/2506.09878",
    "authors": [
      "Ryan Barker"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2506.10730",
    "title": "IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain",
    "abstract": "           Recently, the rapid advancements of vision-language models, such as CLIP, leads to significant progress in zero-/few-shot anomaly detection (ZFSAD) tasks. However, most existing CLIP-based ZFSAD methods commonly assume prior knowledge of categories and rely on carefully crafted prompts tailored to specific scenarios. While such meticulously designed text prompts effectively capture semantic information in the textual space, they fall short of distinguishing normal and anomalous instances within the joint embedding space. Moreover, these ZFSAD methods are predominantly explored in industrial scenarios, with few efforts conducted to medical tasks. To this end, we propose an innovative framework for ZFSAD tasks in medical domain, denoted as IQE-CLIP. We reveal that query embeddings, which incorporate both textual and instance-aware visual information, are better indicators for abnormalities. Specifically, we first introduce class-based prompting tokens and learnable prompting tokens for better adaptation of CLIP to the medical domain. Then, we design an instance-aware query module (IQM) to extract region-level contextual information from both text prompts and visual features, enabling the generation of query embeddings that are more sensitive to anomalies. Extensive experiments conducted on six medical datasets demonstrate that IQE-CLIP achieves state-of-the-art performance on both zero-shot and few-shot tasks. We release our code and data at this https URL.         ",
    "url": "https://arxiv.org/abs/2506.10730",
    "authors": [
      "Hong Huang",
      "Weixiang Sun",
      "Zhijian Wu",
      "Jingwen Niu",
      "Donghuan Lu",
      "Xian Wu",
      "Yefeng Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.10824",
    "title": "A Robust Optimization Framework for Flexible Industrial Energy Scheduling: Application to a Cement Plant with Market Participation",
    "abstract": "           This paper presents a scenario based robust optimization framework for short term energy scheduling in electricity intensive industrial plants, explicitly addressing uncertainty in planning decisions. The model is formulated as a two-stage Mixed Integer Linear Program (MILP) and integrates a hybrid scenario generation method capable of representing uncertain inputs such as electricity prices, renewable generation, and internal demand. A convex objective function combining expected and worst case operational costs allows for tunable risk aversion, enabling planners to balance economic performance and robustness. The resulting schedule ensures feasibility across all scenarios and supports coordinated use of industrial flexibility assets, including battery energy storage and shiftable production. To isolate the effects of market volatility, the framework is applied to a real world cement manufacturing case study considering only day-ahead electricity price uncertainty, with all other inputs treated deterministically. Results show improved resilience to forecast deviations, reduced cost variability, and more consistent operations. The proposed method offers a scalable and risk-aware approach for industrial flexibility planning under uncertainty.         ",
    "url": "https://arxiv.org/abs/2506.10824",
    "authors": [
      "Sebasti\u00e1n Rojas-Innocenti",
      "Enrique Baeyens",
      "Alejandro Mart\u00edn-Crespo",
      "Sergio Saludes-Rodil",
      "Fernando Frechoso Escudero"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.11611",
    "title": "KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity",
    "abstract": "           Graph Neural Networks (GNNs) have achieved impressive success across a wide range of graph-based tasks, yet they remain highly vulnerable to small, imperceptible perturbations and adversarial attacks. Although numerous defense methods have been proposed to address these vulnerabilities, many rely on heuristic metrics, overfit to specific attack patterns, and suffer from high computational complexity. In this paper, we propose Kernel Complexity-Based Edge Sanitization (KCES), a training-free, model-agnostic defense framework. KCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the graph's Gram matrix that characterizes GNN generalization via its test error bound. Building on GKC, we define a KC score for each edge, measuring the change in GKC when the edge is removed. Edges with high KC scores, typically introduced by adversarial perturbations, are pruned to mitigate their harmful effects, thereby enhancing GNNs' robustness. KCES can also be seamlessly integrated with existing defense strategies as a plug-and-play module without requiring training. Theoretical analysis and extensive experiments demonstrate that KCES consistently enhances GNN robustness, outperforms state-of-the-art baselines, and amplifies the effectiveness of existing defenses, offering a principled and efficient solution for securing GNNs.         ",
    "url": "https://arxiv.org/abs/2506.11611",
    "authors": [
      "Yaning Jia",
      "Shenyang Deng",
      "Chiyu Ma",
      "Yaoqing Yang",
      "Soroush Vosoughi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.11996",
    "title": "Improving Surgical Risk Prediction Through Integrating Automated Body Composition Analysis: a Retrospective Trial on Colectomy Surgery",
    "abstract": "           Objective: To evaluate whether preoperative body composition metrics automatically extracted from CT scans can predict postoperative outcomes after colectomy, either alone or combined with clinical variables or existing risk predictors. Main outcomes and measures: The primary outcome was the predictive performance for 1-year all-cause mortality following colectomy. A Cox proportional hazards model with 1-year follow-up was used, and performance was evaluated using the concordance index (C-index) and Integrated Brier Score (IBS). Secondary outcomes included postoperative complications, unplanned readmission, blood transfusion, and severe infection, assessed using AUC and Brier Score from logistic regression. Odds ratios (OR) described associations between individual CT-derived body composition metrics and outcomes. Over 300 features were extracted from preoperative CTs across multiple vertebral levels, including skeletal muscle area, density, fat areas, and inter-tissue metrics. NSQIP scores were available for all surgeries after 2012.         ",
    "url": "https://arxiv.org/abs/2506.11996",
    "authors": [
      "Hanxue Gu",
      "Yaqian Chen",
      "Jisoo Lee",
      "Diego Schaps",
      "Regina Woody",
      "Roy Colglazier",
      "Maciej A. Mazurowski",
      "Christopher Mantyh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.12034",
    "title": "Human-like Forgetting Curves in Deep Neural Networks",
    "abstract": "           This study bridges cognitive science and neural network design by examining whether artificial models exhibit human-like forgetting curves. Drawing upon Ebbinghaus' seminal work on memory decay and principles of spaced repetition, we propose a quantitative framework to measure information retention in neural networks. Our approach computes the recall probability by evaluating the similarity between a network's current hidden state and previously stored prototype representations. This retention metric facilitates the scheduling of review sessions, thereby mitigating catastrophic forgetting during deployment and enhancing training efficiency by prompting targeted reviews. Our experiments with Multi-Layer Perceptrons reveal human-like forgetting curves, with knowledge becoming increasingly robust through scheduled reviews. This alignment between neural network forgetting curves and established human memory models identifies neural networks as an architecture that naturally emulates human memory decay and can inform state-of-the-art continual learning algorithms.         ",
    "url": "https://arxiv.org/abs/2506.12034",
    "authors": [
      "Dylan Kline"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.12190",
    "title": "BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction",
    "abstract": "           Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+/HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging.         ",
    "url": "https://arxiv.org/abs/2506.12190",
    "authors": [
      "Naomi Fridman",
      "Bubby Solway",
      "Tomer Fridman",
      "Itamar Barnea",
      "Anat Goldstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.12456",
    "title": "Demographics-Informed Neural Network for Multi-Modal Spatiotemporal forecasting of Urban Growth and Travel Patterns Using Satellite Imagery",
    "abstract": "           This study presents a novel demographics informed deep learning framework designed to forecast urban spatial transformations by jointly modeling geographic satellite imagery, socio-demographics, and travel behavior dynamics. The proposed model employs an encoder-decoder architecture with temporal gated residual connections, integrating satellite imagery and demographic data to accurately forecast future spatial transformations. The study also introduces a demographics prediction component which ensures that predicted satellite imagery are consistent with demographic features, significantly enhancing physiological realism and socioeconomic accuracy. The framework is enhanced by a proposed multi-objective loss function complemented by a semantic loss function that balances visual realism with temporal coherence. The experimental results from this study demonstrate the superior performance of the proposed model compared to state-of-the-art models, achieving higher structural similarity (SSIM: 0.8342) and significantly improved demographic consistency (Demo-loss: 0.14 versus 0.95 and 0.96 for baseline models). Additionally, the study validates co-evolutionary theories of urban development, demonstrating quantifiable bidirectional influences between built environment characteristics and population patterns. The study also contributes a comprehensive multimodal dataset pairing satellite imagery sequences (2012-2023) with corresponding demographic and travel behavior attributes, addressing existing gaps in urban and transportation planning resources by explicitly connecting physical landscape evolution with socio-demographic patterns.         ",
    "url": "https://arxiv.org/abs/2506.12456",
    "authors": [
      "Eugene Kofi Okrah Denteh",
      "Andrews Danyo",
      "Joshua Kofi Asamoah",
      "Blessing Agyei Kyem",
      "Armstrong Aboah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.12699",
    "title": "SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation",
    "abstract": "           Large language models (LLMs) are sophisticated artificial intelligence systems that enable machines to generate human-like text with remarkable precision. While LLMs offer significant technological progress, their development using vast amounts of user data scraped from the web and collected from extensive user interactions poses risks of sensitive information leakage. Most existing surveys focus on the privacy implications of the training data but tend to overlook privacy risks from user interactions and advanced LLM capabilities. This paper aims to fill that gap by providing a comprehensive analysis of privacy in LLMs, categorizing the challenges into four main areas: (i) privacy issues in LLM training data, (ii) privacy challenges associated with user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and (iv) privacy challenges involving LLM agents. We evaluate the effectiveness and limitations of existing mitigation mechanisms targeting these proposed privacy challenges and identify areas for further research.         ",
    "url": "https://arxiv.org/abs/2506.12699",
    "authors": [
      "Yashothara Shanmugarasa",
      "Ming Ding",
      "M.A.P Chamikara",
      "Thierry Rakotoarivelo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2506.13205",
    "title": "Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments",
    "abstract": "           With the growing integration of vision-language models (VLMs), mobile agents are now widely used for tasks like UI automation and camera-based user assistance. These agents are often fine-tuned on limited user-generated datasets, leaving them vulnerable to covert threats during the training process. In this work we present GHOST, the first clean-label backdoor attack specifically designed for mobile agents built upon VLMs. Our method manipulates only the visual inputs of a portion of the training samples - without altering their corresponding labels or instructions - thereby injecting malicious behaviors into the model. Once fine-tuned with this tampered data, the agent will exhibit attacker-controlled responses when a specific visual trigger is introduced at inference time. The core of our approach lies in aligning the gradients of poisoned samples with those of a chosen target instance, embedding backdoor-relevant features into the poisoned training data. To maintain stealth and enhance robustness, we develop three realistic visual triggers: static visual patches, dynamic motion cues, and subtle low-opacity overlays. We evaluate our method across six real-world Android apps and three VLM architectures adapted for mobile use. Results show that our attack achieves high attack success rates (up to 94.67 percent) while maintaining high clean-task performance (FSR up to 95.85 percent). Additionally, ablation studies shed light on how various design choices affect the efficacy and concealment of the attack. Overall, this work is the first to expose critical security flaws in VLM-based mobile agents, highlighting their susceptibility to clean-label backdoor attacks and the urgent need for effective defense mechanisms in their training pipelines.         ",
    "url": "https://arxiv.org/abs/2506.13205",
    "authors": [
      "Xuan Wang",
      "Siyuan Liang",
      "Zhe Liu",
      "Yi Yu",
      "Yuliang Lu",
      "Xiaochun Cao",
      "Ee-Chien Chang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2506.13987",
    "title": "Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems",
    "abstract": "           Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.         ",
    "url": "https://arxiv.org/abs/2506.13987",
    "authors": [
      "Md Abrar Jahin",
      "Adiba Abid",
      "M. F. Mridha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.14684",
    "title": "Refining music sample identification with a self-supervised graph neural network",
    "abstract": "           Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge. In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%. To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.         ",
    "url": "https://arxiv.org/abs/2506.14684",
    "authors": [
      "Aditya Bhattacharjee",
      "Ivan Meresman Higgs",
      "Mark Sandler",
      "Emmanouil Benetos"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2506.14854",
    "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis",
    "abstract": "           Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.         ",
    "url": "https://arxiv.org/abs/2506.14854",
    "authors": [
      "Varun Mannam",
      "Zhenyu Shi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2506.15106",
    "title": "Local Differential Privacy for Distributed Stochastic Aggregative Optimization with Guaranteed Optimality",
    "abstract": "           Distributed aggregative optimization underpins many cooperative optimization and multi-agent control systems, where each agent's objective function depends both on its local optimization variable and an aggregate of all agents' optimization variables. Existing distributed aggregative optimization approaches typically require access to accurate gradients of the objective functions, which, however, are often hard to obtain in real-world applications. For example, in machine learning, gradients are commonly contaminated by two main sources of noise: the randomness inherent in sampled data, and the additional variability introduced by mini-batch computations. In addition to the issue of relying on accurate gradients, existing distributed aggregative optimization approaches require agents to share explicit information, which could breach the privacy of participating agents. We propose an algorithm that can solve both problems with existing distributed aggregative optimization approaches: not only can the proposed algorithm guarantee mean-square convergence to an exact optimal solution when the gradients are subject to noise, it also simultaneously ensures rigorous differential privacy, with the cumulative privacy budget guaranteed to be finite even when the number of iterations tends to infinity. To the best of our knowledge, this is the first algorithm able to guarantee both accurate convergence and rigorous differential privacy in distributed aggregative optimization. Besides characterizing the convergence rates under nonconvex/convex/strongly convex conditions, we also rigorously quantify the cost of differential privacy in terms of convergence rates. Experimental results on personalized machine learning using benchmark datasets confirm the efficacy of the proposed algorithm.         ",
    "url": "https://arxiv.org/abs/2506.15106",
    "authors": [
      "Ziqin Chen",
      "Yongqiang Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.15337",
    "title": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations",
    "abstract": "           Neural network potentials (NNPs) offer a powerful alternative to traditional force fields for molecular dynamics (MD) simulations. Accurate and stable MD simulations, crucial for evaluating material properties, require training data encompassing both low-energy stable structures and high-energy structures. Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as a teacher model to generate training data for a student model. However, in material-specific models, this fine-tuning process increases energy barriers, making it difficult to create training data containing high-energy structures. To address this, we propose a novel KD framework that leverages a non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy landscape facilitates the exploration of a wider range of structures, including the high-energy structures crucial for stable MD simulations. Our framework employs a two-stage training process: first, the student NNP is trained with a dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate the effectiveness of our framework by applying it to both organic (polyethylene glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving comparable or superior accuracy in reproducing physical properties compared to existing methods. Importantly, our method reduces the number of expensive DFT calculations by 10x compared to existing NNP generation methods, without sacrificing accuracy. Furthermore, the resulting student NNP achieves up to 106x speedup in inference compared to the teacher NNP, enabling significantly faster and more efficient MD simulations.         ",
    "url": "https://arxiv.org/abs/2506.15337",
    "authors": [
      "Naoki Matsumura",
      "Yuta Yoshimoto",
      "Yuto Iwasaki",
      "Meguru Yamazaki",
      "Yasufumi Sakai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2506.15412",
    "title": "Golden Partition Zone: Rethinking Neural Network Partitioning Under Inversion Threats in Collaborative Inference",
    "abstract": "           In collaborative inference, intermediate features transmitted from edge devices can be exploited by adversaries to reconstruct original inputs via model inversion attacks (MIA). While existing defenses focus on shallow layer protection, they often incur significant utility loss. A key open question is how to partition the edge cloud model to maximize resistance to MIA while minimizing accuracy degradation. We firest overturn the common belief that increasing model depth can resist MIA. Through theoretical analysis, we show that representational transitions in neural networks cause sharp changes in conditional entropy $H(x\\mid z)$, intra class mean squared radius ($R_c^2$) and feature dimensionality being critical factors. Experiments on three representative deep vision models show that partitioning at the representational transition or decision level layers yields over 4 times higher mean square error compared to shallow splits, indicating significantly stronger resistance to MIA. Positive label smoothing further enhances robustness by compressing $R_c^2$ and improving generalization. We also validate the resilience of decision level features under feature and inversion model enhancements, and observe that auxiliary data types influence both transition boundaries and reconstruction behavior.         ",
    "url": "https://arxiv.org/abs/2506.15412",
    "authors": [
      "Rongke Liu",
      "Youwen Zhu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2506.15626",
    "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction",
    "abstract": "           $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes. $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome. $\\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery. $\\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.         ",
    "url": "https://arxiv.org/abs/2506.15626",
    "authors": [
      "Vincent Roca",
      "Marc Tommasi",
      "Paul Andrey",
      "Aur\u00e9lien Bellet",
      "Markus D. Schirmer",
      "Hilde Henon",
      "Laurent Puy",
      "Julien Ramon",
      "Gr\u00e9gory Kuchcinski",
      "Martin Bretzner",
      "Renaud Lopes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2307.13124",
    "title": "Conformal prediction for frequency-severity modeling",
    "abstract": "           We present a model-agnostic framework for the construction of prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The framework effectiveness is showcased with simulated and real datasets using classical parametric models and contemporary machine learning methods. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction algorithm, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set in the conformal procedure.         ",
    "url": "https://arxiv.org/abs/2307.13124",
    "authors": [
      "Helton Graziadei",
      "Paulo C. Marques F.",
      "Eduardo F. L. de Melo",
      "Rodrigo S. Targino"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.15592",
    "title": "Solving a class of stochastic optimal control problems by physics-informed neural networks",
    "abstract": "           The aim of this work is to develop a deep learning method for solving high-dimensional stochastic control problems based on the Hamilton--Jacobi--Bellman (HJB) equation and physics-informed learning. Our approach is to parameterize the feedback control and the value function using a decoupled neural network with multiple outputs. We train this network by using a loss function with penalty terms that enforce the HJB equation along the sampled trajectories generated by the controlled system. More significantly, numerical results on various applications are carried out to demonstrate that the proposed approach is efficient and applicable.         ",
    "url": "https://arxiv.org/abs/2402.15592",
    "authors": [
      "Zhe Jiao",
      "Wantao Jia",
      "Weiqiu Zhu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.14769",
    "title": "Medical Artificial Intelligence for Early Detection of Lung Cancer: A Survey",
    "abstract": "           Lung cancer remains one of the leading causes of morbidity and mortality worldwide, making early diagnosis critical for improving therapeutic outcomes and patient prognosis. Computer-aided diagnosis systems, which analyze computed tomography images, have proven effective in detecting and classifying pulmonary nodules, significantly enhancing the detection rate of early-stage lung cancer. Although traditional machine learning algorithms have been valuable, they exhibit limitations in handling complex sample data. The recent emergence of deep learning has revolutionized medical image analysis, driving substantial advancements in this field. This review focuses on recent progress in deep learning for pulmonary nodule detection, segmentation, and classification. Traditional machine learning methods, such as support vector machines and k-nearest neighbors, have shown limitations, paving the way for advanced approaches like Convolutional Neural Networks, Recurrent Neural Networks, and Generative Adversarial Networks. The integration of ensemble models and novel techniques is also discussed, emphasizing the latest developments in lung cancer diagnosis. Deep learning algorithms, combined with various analytical techniques, have markedly improved the accuracy and efficiency of pulmonary nodule analysis, surpassing traditional methods, particularly in nodule classification. Although challenges remain, continuous technological advancements are expected to further strengthen the role of deep learning in medical diagnostics, especially for early lung cancer detection and diagnosis. A comprehensive list of lung cancer detection models reviewed in this work is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.14769",
    "authors": [
      "Guohui Cai",
      "Ying Cai",
      "Zeyu Zhang",
      "Yuanzhouhan Cao",
      "Lin Wu",
      "Daji Ergu",
      "Zhinbin Liao",
      "Yang Zhao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.08961",
    "title": "Belted and Ensembled Neural Network for Linear and Nonlinear Sufficient Dimension Reduction",
    "abstract": "           We introduce a unified, flexible, and easy-to-implement framework of sufficient dimension reduction that can accommodate both linear and nonlinear dimension reduction, and both the conditional distribution and the conditional mean as the targets of estimation. This unified framework is achieved by a specially structured neural network -- the Belted and Ensembled Neural Network (BENN) -- that consists of a narrow latent layer, which we call the belt, and a family of transformations of the response, which we call the ensemble. By strategically placing the belt at different layers of the neural network, we can achieve linear or nonlinear sufficient dimension reduction, and by choosing the appropriate transformation families, we can achieve dimension reduction for the conditional distribution or the conditional mean. Moreover, thanks to the advantage of the neural network, the method is very fast to compute, overcoming a computation bottleneck of the traditional sufficient dimension reduction estimators, which involves the inversion of a matrix of dimension either p or n. We develop the algorithm and convergence rate of our method, compare it with existing sufficient dimension reduction methods, and apply it to two data examples.         ",
    "url": "https://arxiv.org/abs/2412.08961",
    "authors": [
      "Yin Tang",
      "Bing Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2502.11909",
    "title": "Neural Guided Diffusion Bridges",
    "abstract": "           We propose a novel method for simulating conditioned diffusion processes (diffusion bridges) in Euclidean spaces. By training a neural network to approximate bridge dynamics, our approach eliminates the need for computationally intensive Markov Chain Monte Carlo (MCMC) methods or score modeling. Compared to existing methods, it offers greater robustness across various diffusion specifications and conditioning scenarios. This applies in particular to rare events and multimodal distributions, which pose challenges for score-learning- and MCMC-based approaches. We introduce a flexible variational family, partially specified by a neural network, for approximating the diffusion bridge path measure. Once trained, it enables efficient sampling of independent bridges at a cost comparable to sampling the unconditioned (forward) process.         ",
    "url": "https://arxiv.org/abs/2502.11909",
    "authors": [
      "Gefan Yang",
      "Frank van der Meulen",
      "Stefan Sommer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.14253",
    "title": "CINNAMON: A hybrid approach to change point detection and parameter estimation in single-particle tracking data",
    "abstract": "           Change point detection has become an important part of the analysis of the single-particle tracking data, as it allows one to identify moments, in which the motion patterns of observed particles undergo significant changes. The segmentation of diffusive trajectories based on those moments may provide insight into various phenomena in soft condensed matter and biological physics. In this paper, we propose CINNAMON, a hybrid approach to classifying single-particle tracking trajectories, detecting change points within them, and estimating diffusion parameters in the segments between the change points. Our method is based on a combination of neural networks, feature-based machine learning, and statistical techniques. It has been benchmarked in the second Anomalous Diffusion Challenge. The method offers a high level of interpretability due to its analytical and feature-based components. A potential use of features from topological data analysis is also discussed.         ",
    "url": "https://arxiv.org/abs/2503.14253",
    "authors": [
      "Jakub Malinowski",
      "Marcin Kostrzewa",
      "Micha\u0142 Balcerek",
      "Weronika Tomczuk",
      "Janusz Szwabi\u0144ski"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.06316",
    "title": "DeepGDel: Deep Learning-based Gene Deletion Prediction Framework for Growth-Coupled Production in Genome-Scale Metabolic Models",
    "abstract": "           In genome-scale constraint-based metabolic models, gene deletion strategies are crucial for achieving growth-coupled production, where cell growth and target metabolite production are simultaneously achieved. While computational methods for calculating gene deletions have been widely explored and contribute to developing gene deletion strategy databases, current approaches are limited in leveraging new data-driven paradigms, such as machine learning, for more efficient strain design. Therefore, it is necessary to propose a fundamental framework for this objective. In this study, we first formulate the problem of gene deletion strategy prediction and then propose a framework for predicting gene deletion strategies for growth-coupled production in genome-scale metabolic models. The proposed framework leverages deep learning algorithms to learn and integrate sequential gene and metabolite data representation, enabling the automatic gene deletion strategy prediction. Computational experiment results demonstrate the feasibility of the proposed framework, showing substantial improvements over baseline methods. Specifically, the proposed framework achieves a 14.69%, 22.52%, and 13.03% increase in overall accuracy across three metabolic models of different scales under study, while maintaining balanced precision and recall in predicting gene deletion statuses. The source code and examples for the framework are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.06316",
    "authors": [
      "Ziwei Yang",
      "Takeyuki Tamura"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.00310",
    "title": "Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction",
    "abstract": "           Robust estimation of heterogeneous treatment effects is a fundamental challenge for optimal decision-making in domains ranging from personalized medicine to educational policy. In recent years, predictive machine learning has emerged as a valuable toolbox for causal estimation, enabling more flexible effect estimation. However, accurately estimating conditional average treatment effects (CATE) remains a major challenge, particularly in the presence of many covariates. In this article, we propose pretraining strategies that leverage a phenomenon in real-world applications: factors that are prognostic of the outcome are frequently also predictive of treatment effect heterogeneity. In medicine, for example, components of the same biological signaling pathways frequently influence both baseline risk and treatment response. Specifically, we demonstrate our approach within the R-learner framework, which estimates the CATE by solving individual prediction problems based on a residualized loss. We use this structure to incorporate side information and develop models that can exploit synergies between risk prediction and causal effect estimation. In settings where these synergies are present, this cross-task learning enables more accurate signal detection, yields lower estimation error, reduced false discovery rates, and higher power for detecting heterogeneity.         ",
    "url": "https://arxiv.org/abs/2505.00310",
    "authors": [
      "Maximilian Schuessler",
      "Erik Sverdrup",
      "Robert Tibshirani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2505.16028",
    "title": "Comprehensive Lung Disease Detection Using Deep Learning Models and Hybrid Chest X-ray Data with Explainable AI",
    "abstract": "           Advanced diagnostic instruments are crucial for the accurate detection and treatment of lung diseases, which affect millions of individuals globally. This study examines the effectiveness of deep learning and transfer learning models using a hybrid dataset, created by merging four individual datasets from Bangladesh and global sources. The hybrid dataset significantly enhances model accuracy and generalizability, particularly in detecting COVID-19, pneumonia, lung opacity, and normal lung conditions from chest X-ray images. A range of models, including CNN, VGG16, VGG19, InceptionV3, Xception, ResNet50V2, InceptionResNetV2, MobileNetV2, and DenseNet121, were applied to both individual and hybrid datasets. The results showed superior performance on the hybrid dataset, with VGG16, Xception, ResNet50V2, and DenseNet121 each achieving an accuracy of 99%. This consistent performance across the hybrid dataset highlights the robustness of these models in handling diverse data while maintaining high accuracy. To understand the models implicit behavior, explainable AI techniques were employed to illuminate their black-box nature. Specifically, LIME was used to enhance the interpretability of model predictions, especially in cases of misclassification, contributing to the development of reliable and interpretable AI-driven solutions for medical imaging.         ",
    "url": "https://arxiv.org/abs/2505.16028",
    "authors": [
      "Shuvashis Sarker",
      "Shamim Rahim Refat",
      "Faika Fairuj Preotee",
      "Tanvir Rouf Shawon",
      "Raihan Tanvir"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2506.04863",
    "title": "Observations on robust diffusive stability and common Lyapunov functions",
    "abstract": "           We consider the problem of robust diffusive stability (RDS) for a pair of coupled stable discrete-time positive linear-time invariant (LTI) systems. We first show that the existence of a common diagonal Lyapunov function is sufficient for RDS and highlight how this condition differs from recent results using linear copositive Lyapunov functions. We also present an extension of these results, showing that the weaker condition of \\emph{joint} linear copositive function existence is also sufficient for RDS. Finally, we present two results on RDS for extended Leslie matrices arising in population dynamics.         ",
    "url": "https://arxiv.org/abs/2506.04863",
    "authors": [
      "Blake McGrane-Corrigan",
      "Rafael de Andrade Moral",
      "Oliver Mason"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2506.15258",
    "title": "Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference",
    "abstract": "           Medical imaging data contain sensitive patient information requiring strong privacy protection. Many analytical setups require data to be sent to a server for inference purposes. Homomorphic encryption (HE) provides a solution by allowing computations to be performed on encrypted data without revealing the original information. However, HE inference is computationally expensive, particularly for large images (e.g., chest X-rays). In this study, we propose an HE inference framework for medical images that uses VQGAN to compress images into latent representations, thereby significantly reducing the computational burden while preserving image quality. We approximate the activation functions with lower-degree polynomials to balance the accuracy and efficiency in compliance with HE requirements. We observed that a downsampling factor of eight for compression achieved an optimal balance between performance and computational cost. We further adapted the squeeze and excitation module, which is known to improve traditional CNNs, to enhance the HE framework. Our method was tested on two chest X-ray datasets for multi-label classification tasks using vanilla CNN backbones. Although HE inference remains relatively slow and introduces minor performance differences compared with unencrypted inference, our approach shows strong potential for practical use in medical images         ",
    "url": "https://arxiv.org/abs/2506.15258",
    "authors": [
      "Jonghun Kim",
      "Gyeongdeok Jo",
      "Sinyoung Ra",
      "Hyunjin Park"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]