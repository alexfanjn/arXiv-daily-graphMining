[
  {
    "id": "arXiv:2504.07102",
    "title": "Behavior Importance-Aware Graph Neural Architecture Search for Cross-Domain Recommendation",
    "abstract": "           Cross-domain recommendation (CDR) mitigates data sparsity and cold-start issues in recommendation systems. While recent CDR approaches using graph neural networks (GNNs) capture complex user-item interactions, they rely on manually designed architectures that are often suboptimal and labor-intensive. Additionally, extracting valuable behavioral information from source domains to improve target domain recommendations remains challenging. To address these challenges, we propose Behavior importance-aware Graph Neural Architecture Search (BiGNAS), a framework that jointly optimizes GNN architecture and data importance for CDR. BiGNAS introduces two key components: a Cross-Domain Customized Supernetwork and a Graph-Based Behavior Importance Perceptron. The supernetwork, as a one-shot, retrain-free module, automatically searches the optimal GNN architecture for each domain without the need for retraining. The perceptron uses auxiliary learning to dynamically assess the importance of source domain behaviors, thereby improving target domain recommendations. Extensive experiments on benchmark CDR datasets and a large-scale industry advertising dataset demonstrate that BiGNAS consistently outperforms state-of-the-art baselines. To the best of our knowledge, this is the first work to jointly optimize GNN architecture and behavior data importance for cross-domain recommendation.         ",
    "url": "https://arxiv.org/abs/2504.07102",
    "authors": [
      "Chendi Ge",
      "Xin Wang",
      "Ziwei Zhang",
      "Yijian Qin",
      "Hong Chen",
      "Haiyang Wu",
      "Yang Zhang",
      "Yuekui Yang",
      "Wenwu Zhu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07103",
    "title": "FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG",
    "abstract": "           Retrieval-Augmented Generation (RAG) enables large language models to provide more precise and pertinent responses by incorporating external knowledge. In the Query-Focused Summarization (QFS) task, GraphRAG-based approaches have notably enhanced the comprehensiveness and diversity of generated responses. However, existing GraphRAG-based approaches predominantly focus on coarse-grained information summarization without being aware of the specific query, and the retrieved content lacks sufficient contextual information to generate comprehensive responses. To address the deficiencies of current RAG systems, we propose Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance the performance of the QFS task. FG-RAG employs Context-Aware Entity Expansion in graph retrieval to expand the coverage of retrieved entities in the graph, thus providing enough contextual information for the retrieved content. Furthermore, FG-RAG utilizes Query-Level Fine-Grained Summarization to incorporate fine-grained details during response generation, enhancing query awareness for the generated summarization. Our evaluation demonstrates that FG-RAG outperforms other RAG systems in multiple metrics of comprehensiveness, diversity, and empowerment when handling the QFS task. Our implementation is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.07103",
    "authors": [
      "Yubin Hong",
      "Chaofan Li",
      "Jingyi Zhang",
      "Yingxia Shao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07111",
    "title": "High-Performance Gradient Evaluation for Complex Soft Materials Using MPI-based DFS Algorithm",
    "abstract": "           This article presents a depth-first search (DFS)-based algorithm for evaluating sensitivity gradients in the topology optimization of soft materials exhibiting complex deformation behavior. The algorithm is formulated using a time-dependent adjoint sensitivity approach and is implemented within a PETSc-based C++ MPI framework for efficient parallel computing. It has been found that on a single processor, the sensitivity analysis for these complex materials can take approximately 45 minutes. This necessitates the use of high-performance computing (HPC) to achieve feasible optimization times. This work provides insights into the algorithmic framework and its application to large-scale generative design for physics integrated simulation of soft materials under complex loading conditions.         ",
    "url": "https://arxiv.org/abs/2504.07111",
    "authors": [
      "Anurag Bhattacharyya"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2504.07113",
    "title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities",
    "abstract": "           Large language model (LLM) routing has emerged as a crucial strategy for balancing computational costs with performance by dynamically assigning queries to the most appropriate model based on query complexity. Despite recent advances showing that preference-data-based routers can outperform traditional methods, current evaluation benchmarks remain limited. They largely focus on general model capabilities while overlooking task-specific behaviors and critical concerns such as privacy, safety, and potential backdoor vulnerabilities introduced through preference data. In response, we propose the DSC benchmark: Diverse, Simple, and Categorized, an evaluation framework that categorizes router performance across a broad spectrum of query types, including coding, translation, mathematics, human instructions, general knowledge, and LLM jailbreaking. Additionally, it integrates privacy and safety assessments to reveal hidden risks. Our experiments on three preference-based routers and two commercial counterparts demonstrate that while these systems improve efficiency, they often make suboptimal, category-driven decisions. For instance, a BERT-based router directs all coding and mathematics queries to the most powerful LLM even when simpler models would suffice, while routing jailbreaking attempts to weaker models, thereby elevating safety risks.         ",
    "url": "https://arxiv.org/abs/2504.07113",
    "authors": [
      "Aly M. Kassem",
      "Bernhard Sch\u00f6lkopf",
      "Zhijing Jin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2504.07127",
    "title": "An evolutionary approach to predict slope displacement of earth embankments under earthquake ground motions",
    "abstract": "           Accurate slope stability analysis of earth embankments under ground shaking is of great importance for practical use in earthquake geotechnics. This study aims to predict soil slope displacements of earth embankments subjected to earthquake loading using evolutionary algorithms. Comprehensive real case histories of slope displacement of earth embankments under past earthquakes in different areas of the world were gathered and analyzed. A robust model was then developed to predict earthquake induced soil slope displacements using gene expression programming (GEP). Characteristics of earthquake ground motion including earthquake magnitude, earthquake predominant period, maximum earthquake acceleration and also geotechnical specifications of earth embankment including yield acceleration and fundamental period of earth embankment were taken as most influential factors on the slope displacements of earth embankments under earthquakes. Subsequently, performance of developed GEP-based predictive model was assessed using a sensitivity analysis under various effective factors. Finally, the accuracy of the predictive model was evaluated through comparison with the available relationships for estimation of seismic soil slope displacements. The results clearly indicate favorable accuracy of developed GEP-based model to predict slope displacements of earth embankments subjected to earthquake ground motions.         ",
    "url": "https://arxiv.org/abs/2504.07127",
    "authors": [
      "Jin Zhenyang",
      "Sanglin Zhao",
      "Fan Siyu",
      "Javdanian Hamed"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2504.07131",
    "title": "Embedding Reliability Verification Constraints into Generation Expansion Planning",
    "abstract": "           Generation planning approaches face challenges in managing the incompatible mathematical structures between stochastic production simulations for reliability assessment and optimization models for generation planning, which hinders the integration of reliability constraints. This study proposes an approach to embedding reliability verification constraints into generation expansion planning by leveraging a weighted oblique decision tree (WODT) technique. For each planning year, a generation mix dataset, labeled with reliability assessment simulations, is generated. An WODT model is trained using this dataset. Reliability-feasible regions are extracted via depth-first search technique and formulated as disjunctive constraints. These constraints are then transformed into mixed-integer linear form using a convex hull modeling technique and embedded into a unit commitment-integrated generation expansion planning model. The proposed approach is validated through a long-term generation planning case study for the Electric Reliability Council of Texas (ERCOT) region, demonstrating its effectiveness in achieving reliable and optimal planning solutions.         ",
    "url": "https://arxiv.org/abs/2504.07131",
    "authors": [
      "Peng Liu",
      "Lian Cheng",
      "Benjamin P.Omell",
      "Anthony P.Burgard"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2504.07134",
    "title": "Boundary representation learning via Transformer",
    "abstract": "           The recent rise of generative artificial intelligence (AI), powered by Transformer networks, has achieved remarkable success in natural language processing, computer vision, and graphics. However, the application of Transformers in computer-aided design (CAD), particularly for processing boundary representation (B-rep) models, remains largely unexplored. To bridge this gap, this paper introduces Boundary Representation Transformer (BRT), a novel method adapting Transformer for B-rep learning. B-rep models pose unique challenges due to their irregular topology and continuous geometric definitions, which are fundamentally different from the structured and discrete data Transformers are designed for. To address this, BRT proposes a continuous geometric embedding method that encodes B-rep surfaces (trimmed and untrimmed) into B\u00e9zier triangles, preserving their shape and continuity without discretization. Additionally, BRT employs a topology-aware embedding method that organizes these geometric embeddings into a sequence of discrete tokens suitable for Transformers, capturing both geometric and topological characteristics within B-rep models. This enables the Transformer's attention mechanism to effectively learn shape patterns and contextual semantics of boundary elements in a B-rep model. Extensive experiments demonstrate that BRT achieves state-of-the-art performance in part classification and feature recognition tasks.         ",
    "url": "https://arxiv.org/abs/2504.07134",
    "authors": [
      "Qiang Zou",
      "Lizhen Zhu"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07135",
    "title": "SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection",
    "abstract": "           In the era of rapidly evolving large language models (LLMs), state-of-the-art rumor detection systems, particularly those based on Message Propagation Trees (MPTs), which represent a conversation tree with the post as its root and the replies as its descendants, are facing increasing threats from adversarial attacks that leverage LLMs to generate and inject malicious messages. Existing methods are based on the assumption that different nodes exhibit varying degrees of influence on predictions. They define nodes with high predictive influence as important nodes and target them for attacks. If the model treats nodes' predictive influence more uniformly, attackers will find it harder to target high predictive influence nodes. In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions. Extensive experiments on the Twitter and Weibo datasets demonstrate that SINCon not only preserves high classification accuracy on clean data but also significantly enhances resistance against LLM-driven message injection attacks.         ",
    "url": "https://arxiv.org/abs/2504.07135",
    "authors": [
      "Mingqing Zhang",
      "Qiang Liu",
      "Xiang Tao",
      "Shu Wu",
      "Liang Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07137",
    "title": "Large Language Model (LLM) for Software Security: Code Analysis, Malware Analysis, Reverse Engineering",
    "abstract": "           Large Language Models (LLMs) have recently emerged as powerful tools in cybersecurity, offering advanced capabilities in malware detection, generation, and real-time monitoring. Numerous studies have explored their application in cybersecurity, demonstrating their effectiveness in identifying novel malware variants, analyzing malicious code structures, and enhancing automated threat analysis. Several transformer-based architectures and LLM-driven models have been proposed to improve malware analysis, leveraging semantic and structural insights to recognize malicious intent more accurately. This study presents a comprehensive review of LLM-based approaches in malware code analysis, summarizing recent advancements, trends, and methodologies. We examine notable scholarly works to map the research landscape, identify key challenges, and highlight emerging innovations in LLM-driven cybersecurity. Additionally, we emphasize the role of static analysis in malware detection, introduce notable datasets and specialized LLM models, and discuss essential datasets supporting automated malware research. This study serves as a valuable resource for researchers and cybersecurity professionals, offering insights into LLM-powered malware detection and defence strategies while outlining future directions for strengthening cybersecurity resilience.         ",
    "url": "https://arxiv.org/abs/2504.07137",
    "authors": [
      "Hamed Jelodar",
      "Samita Bai",
      "Parisa Hamedi",
      "Hesamodin Mohammadian",
      "Roozbeh Razavi-Far",
      "Ali Ghorbani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07140",
    "title": "Secure Text Mail Encryption with Generative Adversarial Networks",
    "abstract": "           This work presents an encryption model based on Generative Adversarial Networks (GANs). Encryption of RTF-8 data is realized by dynamically generating decimal numbers that lead to the encryption and decryption of alphabetic strings in integer representation by simple addition rules, the modulus of the dimension of the considered alphabet. The binary numbers for the private dynamical keys correlate with the binary numbers of public reference keys from a mapping defined by the specific GAN configuration. For reversible encryption with bijective mapping between dynamic and reference keys as defined by the GAN encryptor with random combinations of NOT logical gates between bitwise subcomponents of the transmitted text signal, secure text encryption can be realized by transferring a GAN-encrypted public key with encrypted text from a sender to a receiver. Using the technique described above, secure text mail transfer can be realized from component-wise encryption of text mail strings with total key sizes of up to $10^{8}$ bits that define random decimal numbers obtained from the GAN. From the present model, we assert that encrypted texts can be transmitted more efficiently and securely than from RSA encryption, as long as users of the specific configuration of the GAN encryption model are unaware of the GAN encryptor circuit.         ",
    "url": "https://arxiv.org/abs/2504.07140",
    "authors": [
      "Alexej Schelle"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07152",
    "title": "Evolutionary Generation of Random Surreal Numbers for Benchmarking",
    "abstract": "           There are many areas of scientific endeavour where large, complex datasets are needed for benchmarking. Evolutionary computing provides a means towards creating such sets. As a case study, we consider Conway's Surreal numbers. They have largely been treated as a theoretical construct, with little effort towards empirical study, at least in part because of the difficulty of working with all but the smallest numbers. To advance this status, we need efficient algorithms, and in order to develop such we need benchmark data sets of surreal numbers. In this paper, we present a method for generating ensembles of random surreal numbers to benchmark algorithms. The approach uses an evolutionary algorithm to create the benchmark datasets where we can analyse and control features of the resulting test sets. Ultimately, the process is designed to generate networks with defined properties, and we expect this to be useful for other types of network data.         ",
    "url": "https://arxiv.org/abs/2504.07152",
    "authors": [
      "Matthew Roughan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2504.07155",
    "title": "Compound Fault Diagnosis for Train Transmission Systems Using Deep Learning with Fourier-enhanced Representation",
    "abstract": "           Fault diagnosis prevents train disruptions by ensuring the stability and reliability of their transmission systems. Data-driven fault diagnosis models have several advantages over traditional methods in terms of dealing with non-linearity, adaptability, scalability, and automation. However, existing data-driven models are trained on separate transmission components and only consider single faults due to the limitations of existing datasets. These models will perform worse in scenarios where components operate with each other at the same time, affecting each component's vibration signals. To address some of these challenges, we propose a frequency domain representation and a 1-dimensional convolutional neural network for compound fault diagnosis and applied it on the PHM Beijing 2024 dataset, which includes 21 sensor channels, 17 single faults, and 42 compound faults from 4 interacting components, that is, motor, gearbox, left axle box, and right axle box. Our proposed model achieved 97.67% and 93.93% accuracies on the test set with 17 single faults and on the test set with 42 compound faults, respectively.         ",
    "url": "https://arxiv.org/abs/2504.07155",
    "authors": [
      "Jonathan Adam Rico",
      "Nagarajan Raghavan",
      "Senthilnath Jayavelu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07163",
    "title": "Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks",
    "abstract": "           This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used.         ",
    "url": "https://arxiv.org/abs/2504.07163",
    "authors": [
      "Jordi Serra",
      "Anton Aguilar",
      "Ebrahim Abu-Helalah",
      "Ra\u00fal Parada",
      "Paolo Dini"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.07189",
    "title": "Multi-Agent Trustworthy Consensus under Random Dynamic Attacks",
    "abstract": "           In this work, we study the consensus problem in which legitimate agents send their values over an undirected communication network in the presence of an unknown subset of malicious or faulty agents. In contrast to former works, we generalize and characterize the properties of consensus dynamics with dependent sequences of malicious transmissions with dynamic (time-varying) rates, based on not necessarily independent trust observations. We consider a detection algorithm utilizing stochastic trust observations available to legitimate agents. Under these conditions, legitimate agents almost surely classify their neighbors and form their trusted neighborhoods correctly with decaying misclassification probabilities. We further prove that the consensus process converges almost surely despite the existence of malicious agents. For a given value of failure probability, we characterize the deviation from the nominal consensus value ideally occurring when there are no malicious agents in the system. We also examine the convergence rate of the process in finite time. Numerical simulations show the convergence among agents and indicate the deviation under different attack scenarios.         ",
    "url": "https://arxiv.org/abs/2504.07189",
    "authors": [
      "Orhan Eren Akg\u00fcn",
      "Sarper Ayd\u0131n",
      "Stephanie Gil",
      "Angelia Nedi\u0107"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.07213",
    "title": "Evolutionary algorithms meet self-supervised learning: a comprehensive survey",
    "abstract": "           The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.         ",
    "url": "https://arxiv.org/abs/2504.07213",
    "authors": [
      "Adriano Vinhas",
      "Jo\u00e3o Correia",
      "Penousal Machado"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07220",
    "title": "Leveraging Machine Learning Techniques in Intrusion Detection Systems for Internet of Things",
    "abstract": "           As the Internet of Things (IoT) continues to expand, ensuring the security of connected devices has become increasingly critical. Traditional Intrusion Detection Systems (IDS) often fall short in managing the dynamic and large-scale nature of IoT networks. This paper explores how Machine Learning (ML) and Deep Learning (DL) techniques can significantly enhance IDS performance in IoT environments. We provide a thorough overview of various IDS deployment strategies and categorize the types of intrusions common in IoT systems. A range of ML methods -- including Support Vector Machines, Naive Bayes, K-Nearest Neighbors, Decision Trees, and Random Forests -- are examined alongside advanced DL models such as LSTM, CNN, Autoencoders, RNNs, and Deep Belief Networks. Each technique is evaluated based on its accuracy, efficiency, and suitability for real-world IoT applications. We also address major challenges such as high false positive rates, data imbalance, encrypted traffic analysis, and the resource constraints of IoT devices. In addition, we highlight the emerging role of Generative AI and Large Language Models (LLMs) in improving threat detection, automating responses, and generating intelligent security policies. Finally, we discuss ethical and privacy concerns, underscoring the need for responsible and transparent implementation. This paper aims to provide a comprehensive framework for developing adaptive, intelligent, and secure IDS solutions tailored for the evolving landscape of IoT.         ",
    "url": "https://arxiv.org/abs/2504.07220",
    "authors": [
      "Saeid Jamshidi",
      "Amin Nikanjam",
      "Nafi Kawser Wazed",
      "Foutse Khomh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.07233",
    "title": "Skill Demand Forecasting Using Temporal Knowledge Graph Embeddings",
    "abstract": "           Rapid technological advancements pose a significant threat to a large portion of the global workforce, potentially leaving them behind. In today's economy, there is a stark contrast between the high demand for skilled labour and the limited employment opportunities available to those who are not adequately prepared for the digital economy. To address this critical juncture and gain a deeper and more rapid understanding of labour market dynamics, in this paper, we approach the problem of skill need forecasting as a knowledge graph (KG) completion task, specifically, temporal link prediction. We introduce our novel temporal KG constructed from online job advertisements. We then train and evaluate different temporal KG embeddings for temporal link prediction. Finally, we present predictions of demand for a selection of skills practiced by workers in the information technology industry. The code and the data are available on our GitHub repository this https URL.         ",
    "url": "https://arxiv.org/abs/2504.07233",
    "authors": [
      "Yousra Fettacha",
      "Adil Bahaj",
      "Mounir Ghogho"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2504.07257",
    "title": "Better Decisions through the Right Causal World Model",
    "abstract": "           Reinforcement learning (RL) agents have shown remarkable performances in various environments, where they can discover effective policies directly from sensory inputs. However, these agents often exploit spurious correlations in the training data, resulting in brittle behaviours that fail to generalize to new or slightly modified environments. To address this, we introduce the Causal Object-centric Model Extraction Tool (COMET), a novel algorithm designed to learn the exact interpretable causal world models (CWMs). COMET first extracts object-centric state descriptions from observations and identifies the environment's internal states related to the depicted objects' properties. Using symbolic regression, it models object-centric transitions and derives causal relationships governing object dynamics. COMET further incorporates large language models (LLMs) for semantic inference, annotating causal variables to enhance interpretability. By leveraging these capabilities, COMET constructs CWMs that align with the true causal structure of the environment, enabling agents to focus on task-relevant features. The extracted CWMs mitigate the danger of shortcuts, permitting the development of RL systems capable of better planning and decision-making across dynamic scenarios. Our results, validated in Atari environments such as Pong and Freeway, demonstrate the accuracy and robustness of COMET, highlighting its potential to bridge the gap between object-centric reasoning and causal inference in reinforcement learning.         ",
    "url": "https://arxiv.org/abs/2504.07257",
    "authors": [
      "Elisabeth Dillies",
      "Quentin Delfosse",
      "Jannis Bl\u00fcml",
      "Raban Emunds",
      "Florian Peter Busch",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07285",
    "title": "A Scalable Approach to Clustering Embedding Projections",
    "abstract": "           Interactive visualization of embedding projections is a useful technique for understanding data and evaluating machine learning models. Labeling data within these visualizations is critical for interpretation, as labels provide an overview of the projection and guide user navigation. However, most methods for producing labels require clustering the points, which can be computationally expensive as the number of points grows. In this paper, we describe an efficient clustering approach using kernel density estimation in the projected 2D space instead of points. This algorithm can produce high-quality cluster regions from a 2D density map in a few hundred milliseconds, orders of magnitude faster than current approaches. We contribute the design of the algorithm, benchmarks, and applications that demonstrate the utility of the algorithm, including labeling and summarization.         ",
    "url": "https://arxiv.org/abs/2504.07285",
    "authors": [
      "Donghao Ren",
      "Fred Hohman",
      "Dominik Moritz"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07297",
    "title": "Data Fusion of Deep Learned Molecular Embeddings for Property Prediction",
    "abstract": "           Data-driven approaches such as deep learning can result in predictive models for material properties with exceptional accuracy and efficiency. However, in many problems data is sparse, severely limiting their accuracy and applicability. To improve predictions, techniques such as transfer learning and multi-task learning have been used. The performance of multi-task learning models depends on the strength of the underlying correlations between tasks and the completeness of the dataset. We find that standard multi-task models tend to underperform when trained on sparse datasets with weakly correlated properties. To address this gap, we use data fusion techniques to combine the learned molecular embeddings of various single-task models and trained a multi-task model on this combined embedding. We apply this technique to a widely used benchmark dataset of quantum chemistry data for small molecules as well as a newly compiled sparse dataset of experimental data collected from literature and our own quantum chemistry and thermochemical calculations. The results show that the fused, multi-task models outperform standard multi-task models for sparse datasets and can provide enhanced prediction on data-limited properties compared to single-task models.         ",
    "url": "https://arxiv.org/abs/2504.07297",
    "authors": [
      "Robert J Appleton",
      "Brian C Barnes",
      "Alejandro Strachan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2504.07323",
    "title": "Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's Handshake Mechanism",
    "abstract": "           WhatsApp, the world's largest messaging application, uses a version of the Signal protocol to provide end-to-end encryption (E2EE) with strong security guarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from the start of a new conversation -- even when the recipient is offline -- a stash of ephemeral (one-time) prekeys must be stored on a server. While the critical role of these one-time prekeys in achieving PFS has been outlined in the Signal specification, we are the first to demonstrate a targeted depletion attack against them on individual WhatsApp user devices. Our findings not only reveal an attack that can degrade PFS for certain messages, but also expose inherent privacy risks and serious availability implications arising from the refilling and distribution procedure essential for this security mechanism.         ",
    "url": "https://arxiv.org/abs/2504.07323",
    "authors": [
      "Gabriel K. Gegenhuber",
      "Philipp \u00c9. Frenzel",
      "Maximilian G\u00fcnther",
      "Aljosha Judmayer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.07337",
    "title": "FLASH: Flexible Learning of Adaptive Sampling from History in Temporal Graph Neural Networks",
    "abstract": "           Aggregating temporal signals from historic interactions is a key step in future link prediction on dynamic graphs. However, incorporating long histories is resource-intensive. Hence, temporal graph neural networks (TGNNs) often rely on historical neighbors sampling heuristics such as uniform sampling or recent neighbors selection. These heuristics are static and fail to adapt to the underlying graph structure. We introduce FLASH, a learnable and graph-adaptive neighborhood selection mechanism that generalizes existing heuristics. FLASH integrates seamlessly into TGNNs and is trained end-to-end using a self-supervised ranking loss. We provide theoretical evidence that commonly used heuristics hinders TGNNs performance, motivating our design. Extensive experiments across multiple benchmarks demonstrate consistent and significant performance improvements for TGNNs equipped with FLASH.         ",
    "url": "https://arxiv.org/abs/2504.07337",
    "authors": [
      "Or Feldman",
      "Krishna Sri Ipsit Mantri",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Chaim Baskin",
      "Moshe Eliasof"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.07343",
    "title": "Code Generation with Small Language Models: A Deep Evaluation on Codeforces",
    "abstract": "           Large Language Models (LLMs) have demonstrated capabilities in code generation, potentially boosting developer productivity. However, their widespread adoption remains limited by high computational costs, significant energy demands, and security risks such as data leakage and adversarial attacks. As a lighter-weight alternative, Small Language Models (SLMs) offer faster inference, lower deployment overhead, and better adaptability to domain-specific tasks, making them an attractive option for real-world applications. While prior research has benchmarked LLMs on competitive programming tasks, such evaluations often focus narrowly on metrics like Elo scores or pass rates, overlooking deeper insights into model behavior, failure patterns, and problem diversity. Furthermore, the potential of SLMs to tackle complex tasks such as competitive programming remains underexplored. In this study, we benchmark five open SLMs - LLAMA 3.2 3B, GEMMA 2 9B, GEMMA 3 12B, DEEPSEEK-R1 14B, and PHI-4 14B - across 280 Codeforces problems spanning Elo ratings from 800 to 2100 and covering 36 distinct topics. All models were tasked with generating Python solutions. PHI-4 14B achieved the best performance among SLMs, with a pass@3 of 63.6%, approaching the proprietary O3-MINI-HIGH (86.8%). In addition, we evaluated PHI-4 14B on C++ and found that combining outputs from both Python and C++ increases its aggregated pass@3 to 73.6%. A qualitative analysis of PHI-4 14B's incorrect outputs revealed that some failures were due to minor implementation issues - such as handling edge cases or correcting variable initialization - rather than deeper reasoning flaws.         ",
    "url": "https://arxiv.org/abs/2504.07343",
    "authors": [
      "D\u00e9bora Souza",
      "Rohit Gheyi",
      "Lucas Albuquerque",
      "Gustavo Soares",
      "M\u00e1rcio Ribeiro"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.07345",
    "title": "Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics",
    "abstract": "           The cacophony of urban sounds presents a significant challenge for smart city applications that rely on accurate acoustic scene analysis. Effectively analyzing these complex soundscapes, often characterized by overlapping sound sources, diverse acoustic events, and unpredictable noise levels, requires precise source separation. This task becomes more complicated when only limited training data is available. This paper introduces a novel Quantum-Inspired Genetic Algorithm (p-QIGA) for source separation, drawing inspiration from quantum information theory to enhance acoustic scene analysis in smart cities. By leveraging quantum superposition for efficient solution space exploration and entanglement to handle correlated sources, p-QIGA achieves robust separation even with limited data. These quantum-inspired concepts are integrated into a genetic algorithm framework to optimize source separation parameters. The effectiveness of our approach is demonstrated on two datasets: the TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban soundscapes, and the Silent Cities dataset, capturing quieter urban environments during the COVID-19 pandemic. Experimental results show that the p-QIGA achieves accuracy comparable to state-of-the-art methods while exhibiting superior resilience to noise and limited training data, achieving up to 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and outperforming baseline methods by up to 2 dB with only 10% of the training data. This research highlights the potential of p-QIGA to advance acoustic signal processing in smart cities, particularly for noise pollution monitoring and acoustic surveillance.         ",
    "url": "https://arxiv.org/abs/2504.07345",
    "authors": [
      "Minh K. Quan",
      "Mayuri Wijayasundara",
      "Sujeeva Setunge",
      "Pubudu N. Pathirana"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2504.07362",
    "title": "Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation under Differential Privacy",
    "abstract": "           The shuffle model of DP (Differential Privacy) provides high utility by introducing a shuffler that randomly shuffles noisy data sent from users. However, recent studies show that existing shuffle protocols suffer from the following two major drawbacks. First, they are vulnerable to local data poisoning attacks, which manipulate the statistics about input data by sending crafted data, especially when the privacy budget epsilon is small. Second, the actual value of epsilon is increased by collusion attacks by the data collector and users. In this paper, we address these two issues by thoroughly exploring the potential of the augmented shuffle model, which allows the shuffler to perform additional operations, such as random sampling and dummy data addition. Specifically, we propose a generalized framework for local-noise-free protocols in which users send (encrypted) input data to the shuffler without adding noise. We show that this generalized protocol provides DP and is robust to the above two attacks if a simpler mechanism that performs the same process on binary input data provides DP. Based on this framework, we propose three concrete protocols providing DP and robustness against the two attacks. Our first protocol generates the number of dummy values for each item from a binomial distribution and provides higher utility than several state-of-the-art existing shuffle protocols. Our second protocol significantly improves the utility of our first protocol by introducing a novel dummy-count distribution: asymmetric two-sided geometric distribution. Our third protocol is a special case of our second protocol and provides pure epsilon-DP. We show the effectiveness of our protocols through theoretical analysis and comprehensive experiments.         ",
    "url": "https://arxiv.org/abs/2504.07362",
    "authors": [
      "Takao Murakami",
      "Yuichi Sei",
      "Reo Eriguchi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07375",
    "title": "Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction",
    "abstract": "           Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations. Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations. However, such models are only designed to accommodate 2D egocentric video inputs. There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance. In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames. To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.07375",
    "authors": [
      "Junyi Ma",
      "Wentao Bao",
      "Jingyi Xu",
      "Guanzhong Sun",
      "Xieyuanli Chen",
      "Hesheng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07382",
    "title": "Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction",
    "abstract": "           Advances in image generation enable hyper-realistic synthetic faces but also pose risks, thus making synthetic face detection crucial. Previous research focuses on the general differences between generated images and real images, often overlooking the discrepancies among various generative techniques. In this paper, we explore the intrinsic relationship between synthetic images and their corresponding generation technologies. We find that specific images exhibit significant reconstruction discrepancies across different generative methods and that matching generation techniques provide more accurate reconstructions. Based on this insight, we propose a Multi-Reconstruction-based detector. By reversing and reconstructing images using multiple generative models, we analyze the reconstruction differences among real, GAN-generated, and DM-generated images to facilitate effective differentiation. Additionally, we introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic Asian faces generated with various GANs and DMs. This dataset complements existing synthetic face datasets. Experimental results demonstrate that our detector achieves exceptional performance, with strong generalization and robustness.         ",
    "url": "https://arxiv.org/abs/2504.07382",
    "authors": [
      "Qingchao Jiang",
      "Zhishuo Xu",
      "Zhiying Zhu",
      "Ning Chen",
      "Haoyue Wang",
      "Zhongjie Ba"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07397",
    "title": "MicroNAS: An Automated Framework for Developing a Fall Detection System",
    "abstract": "           This work presents MicroNAS, an automated neural architecture search tool specifically designed to create models optimized for microcontrollers with small memory resources. The ESP32 microcontroller, with 320 KB of memory, is used as the target platform. The artificial intelligence contribution lies in a novel method for optimizing convolutional neural network and gated recurrent unit architectures by considering the memory size of the target microcontroller as a guide. A comparison is made between memory-driven model optimization and traditional two-stage methods, which use pruning, to show the effectiveness of the proposed framework. To demonstrate the engineering application of MicroNAS, a fall detection system (FDS) for lower-limb amputees is developed as a pilot study. A critical challenge in fall detection studies, class imbalance in the dataset, is addressed. The results show that MicroNAS models achieved higher F1-scores than alternative approaches, such as ensemble methods and H2O Automated Machine Learning, presenting a significant step forward in real-time FDS development. Biomechanists using body-worn sensors for activity detection can adopt the open-source code to design machine learning models tailored for microcontroller platforms with limited memory.         ",
    "url": "https://arxiv.org/abs/2504.07397",
    "authors": [
      "Seyed Mojtaba Mohasel",
      "John Sheppard",
      "Lindsey K. Molina",
      "Richard R. Neptune",
      "Shane R. Wurdeman",
      "Corey A. Pew"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07414",
    "title": "Decomposition-Based Optimal Bounds for Privacy Amplification via Shuffling",
    "abstract": "           Shuffling has been shown to amplify differential privacy guarantees, offering a stronger privacy-utility trade-off. To characterize and compute this amplification, two fundamental analytical frameworks have been proposed: the privacy blanket by Balle et al. (CRYPTO 2019) and the clone paradigm (including both the standard clone and stronger clone) by Feldman et al. (FOCS 2021, SODA 2023). All these methods rely on decomposing local randomizers. In this work, we introduce a unified analysis framework--the general clone paradigm--which encompasses all possible decompositions. We identify the optimal decomposition within the general clone paradigm. Moreover, we develop a simple and efficient algorithm to compute the exact value of the optimal privacy amplification bounds via Fast Fourier Transform. Experimental results demonstrate that the computed upper bounds for privacy amplification closely approximate the lower bounds, highlighting the tightness of our approach. Finally, using our algorithm, we conduct the first systematic analysis of the joint composition of LDP protocols in the shuffle model.         ",
    "url": "https://arxiv.org/abs/2504.07414",
    "authors": [
      "Pengcheng Su",
      "Haibo Cheng",
      "Ping Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07433",
    "title": "From Token to Line: Enhancing Code Generation with a Long-Term Perspective",
    "abstract": "           The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \\textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.         ",
    "url": "https://arxiv.org/abs/2504.07433",
    "authors": [
      "Tingwei Lu",
      "Yangning Li",
      "Liyuan Wang",
      "Binghuai Lin",
      "Jiwei Tang",
      "Wanshi Xu",
      "Hai-Tao Zheng",
      "Yinghui Li",
      "Bingxu An",
      "Zhao Wei",
      "Yong Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07441",
    "title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer",
    "abstract": "           Robust object detection for Unmanned Surface Vehicles (USVs) in complex water environments is essential for reliable navigation and operation. Specifically, water surface object detection faces challenges from blurred edges and diverse object scales. Although vision-radar fusion offers a feasible solution, existing approaches suffer from cross-modal feature conflicts, which negatively affect model robustness. To address this problem, we propose a robust vision-radar fusion model WS-DETR. In particular, we first introduce a Multi-Scale Edge Information Integration (MSEII) module to enhance edge perception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale object detection in the encoder. Then, we adopt self-moving point representations for continuous convolution and residual connection to efficiently extract irregular features under the scenarios of irregular point cloud data. To further mitigate cross-modal conflicts, an Adaptive Feature Interactive Fusion (AFIF) module is introduced to integrate visual and radar features through geometric alignment and semantic fusion. Extensive experiments on the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art (SOTA) performance, maintaining its superiority even under adverse weather and lighting conditions.         ",
    "url": "https://arxiv.org/abs/2504.07441",
    "authors": [
      "Huilin Yin",
      "Pengyu Wang",
      "Senmao Li",
      "Jun Yan",
      "Daniel Watzenig"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07457",
    "title": "CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber Defenders",
    "abstract": "           The increasing frequency and sophistication of cyberattacks demand innovative approaches to strengthen defense capabilities. Training on live infrastructure poses significant risks to organizations, making secure, isolated cyber ranges an essential tool for conducting Red vs. Blue Team training events. These events enable security teams to refine their skills without impacting operational environments. While such training provides a strong foundation, the ever-evolving nature of cyber threats necessitates additional support for effective defense. To address this challenge, we introduce CyberAlly, a knowledge graph-enhanced AI assistant designed to enhance the efficiency and effectiveness of Blue Teams during incident response. Integrated into our cyber range alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks Blue Team actions, and suggests tailored mitigation recommendations based on insights from prior Red vs. Blue Team exercises. This demonstration highlights the feasibility and impact of CyberAlly in augmenting incident response and equipping defenders to tackle evolving threats with greater precision and confidence.         ",
    "url": "https://arxiv.org/abs/2504.07457",
    "authors": [
      "Minjune Kim",
      "Jeff Wang",
      "Kristen Moore",
      "Diksha Goel",
      "Derui Wang",
      "Ahmad Mohsin",
      "Ahmed Ibrahim",
      "Robin Doss",
      "Seyit Camtepe",
      "Helge Janicke"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07459",
    "title": "Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts",
    "abstract": "           We propose a novel framework for generating causal graphs from narrative texts, bridging high-level causality and detailed event-specific relationships. Our method first extracts concise, agent-centered vertices using large language model (LLM)-based summarization. We introduce an \"Expert Index,\" comprising seven linguistically informed features, integrated into a Situation-Task-Action-Consequence (STAC) classification model. This hybrid system, combining RoBERTa embeddings with the Expert Index, achieves superior precision in causal link identification compared to pure LLM-based approaches. Finally, a structured five-iteration prompting process refines and constructs connected causal graphs. Experiments on 100 narrative chapters and short stories demonstrate that our approach consistently outperforms GPT-4o and Claude 3.5 in causal graph quality, while maintaining readability. The open-source tool provides an interpretable, efficient solution for capturing nuanced causal chains in narratives.         ",
    "url": "https://arxiv.org/abs/2504.07459",
    "authors": [
      "Zehan Li",
      "Ruhua Pan",
      "Xinyu Pi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07465",
    "title": "Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
    "abstract": "           Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies.         ",
    "url": "https://arxiv.org/abs/2504.07465",
    "authors": [
      "Shichen Li",
      "Chenhui Shao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07467",
    "title": "Defense against Prompt Injection Attacks via Mixture of Encodings",
    "abstract": "           Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics.         ",
    "url": "https://arxiv.org/abs/2504.07467",
    "authors": [
      "Ruiyi Zhang",
      "David Sullivan",
      "Kyle Jackson",
      "Pengtao Xie",
      "Mei Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07476",
    "title": "CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources",
    "abstract": "           Digital images of Chinas maps play a crucial role in map detection, particularly in ensuring national sovereignty, territorial integrity, and map compliance. However, there is currently no publicly available dataset specifically dedicated to problematic maps the CME dataset. Existing datasets primarily focus on general map data and are insufficient for effectively identifying complex issues such as national boundary misrepresentations, missing elements, and blurred boundaries. Therefore, this study creates a Problematic Map dataset that covers five key problem areas, aiming to provide diverse samples for problematic map detection technologies, support high-precision map compliance detection, and enhance map data quality and timeliness. This dataset not only provides essential resources for map compliance, national security monitoring, and map updates, but also fosters innovation and application of related technologies.         ",
    "url": "https://arxiv.org/abs/2504.07476",
    "authors": [
      "Yan Xu",
      "Zhenqiang Zhang",
      "Zhiwei Zhou",
      "Liting Geng",
      "Yue Li",
      "Jintao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07478",
    "title": "Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security",
    "abstract": "           Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity. This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection. Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition. The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic. These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains.         ",
    "url": "https://arxiv.org/abs/2504.07478",
    "authors": [
      "Caroline Panggabean",
      "Chandrasekar Venkatachalam",
      "Priyanka Shah",
      "Sincy John",
      "Renuka Devi P",
      "Shanmugavalli Venkatachalam"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07480",
    "title": "Echoes of Disagreement: Measuring Disparity in Social Consensus",
    "abstract": "           Public discourse and opinions stem from multiple social groups. Each group has beliefs about a topic (such as vaccination, abortion, gay marriage, etc.), and opinions are exchanged and blended to produce consensus. A particular measure of interest corresponds to measuring the influence of each group on the consensus and the disparity between groups on the extent to which they influence the consensus. In this paper, we study and give provable algorithms for optimizing the disparity under the DeGroot or the Friedkin-Johnsen models of opinion dynamics. Our findings provide simple poly-time algorithms to optimize disparity for most cases, fully characterize the instances that optimize disparity, and show how simple interventions such as contracting vertices or adding links affect disparity. Finally, we test our developed algorithms in a variety of real-world datasets.         ",
    "url": "https://arxiv.org/abs/2504.07480",
    "authors": [
      "Marios Papachristou",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2504.07500",
    "title": "Energy-Efficient UAV Replacement in Software-Defined UAV Networks",
    "abstract": "           Unmanned Aerial Vehicles (UAVs) in networked environments face significant challenges due to energy constraints and limited battery life, which necessitate periodic replacements to maintain continuous operation. Efficiently managing the handover of data flows during these replacements is crucial to avoid disruptions in communication and to optimize energy consumption. This paper addresses the complex issue of energy-efficient UAV replacement in software-defined UAV network. We introduce a novel approach based on establishing a strict total ordering relation for UAVs and data flows, allowing us to formulate the problem as an integer linear program. By utilizing the Gurobi solver, we obtain optimal handover schedules for the tested problem instances. Additionally, we propose a heuristic algorithm that significantly reduces computational complexity while maintaining near-optimal performance. Through comprehensive simulations, we demonstrate that our heuristic offers practical and scalable solution, ensuring energy-efficient UAV replacement while minimizing network disruptions. Our results suggest that the proposed approach can enhance UAV battery life and improve overall network reliability in real-world applications.         ",
    "url": "https://arxiv.org/abs/2504.07500",
    "authors": [
      "Mohammad Javad-Kalbasi",
      "Shahrokh Valaee"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2504.07522",
    "title": "Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data",
    "abstract": "           Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings.         ",
    "url": "https://arxiv.org/abs/2504.07522",
    "authors": [
      "Jose Cribeiro-Ramallo",
      "Federico Matteucci",
      "Paul Enciu",
      "Alexander Jenke",
      "Vadim Arzamasov",
      "Thorsten Strufe",
      "Klemens B\u00f6hm"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2504.07524",
    "title": "DGOcc: Depth-aware Global Query-based Network for Monocular 3D Occupancy Prediction",
    "abstract": "           Monocular 3D occupancy prediction, aiming to predict the occupancy and semantics within interesting regions of 3D scenes from only 2D images, has garnered increasing attention recently for its vital role in 3D scene understanding. Predicting the 3D occupancy of large-scale outdoor scenes from 2D images is ill-posed and resource-intensive. In this paper, we present \\textbf{DGOcc}, a \\textbf{D}epth-aware \\textbf{G}lobal query-based network for monocular 3D \\textbf{Occ}upancy prediction. We first explore prior depth maps to extract depth context features that provide explicit geometric information for the occupancy network. Then, in order to fully exploit the depth context features, we propose a Global Query-based (GQ) Module. The cooperation of attention mechanisms and scale-aware operations facilitates the feature interaction between images and 3D voxels. Moreover, a Hierarchical Supervision Strategy (HSS) is designed to avoid upsampling the high-dimension 3D voxel features to full resolution, which mitigates GPU memory utilization and time cost. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that the proposed method achieves the best performance on monocular semantic occupancy prediction while reducing GPU and time overhead.         ",
    "url": "https://arxiv.org/abs/2504.07524",
    "authors": [
      "Xu Zhao",
      "Pengju Zhang",
      "Bo Liu",
      "Yihong Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07528",
    "title": "Deep Learning Based Service Composition in Integrated Aerial-Terrestrial Networks",
    "abstract": "           The explosive growth of user devices and emerging applications is driving unprecedented traffic demands, accompanied by stringent Quality of Service (QoS) requirements. Addressing these challenges necessitates innovative service orchestration methods capable of seamless integration across the edge-cloud continuum. Terrestrial network-based service orchestration methods struggle to deliver timely responses to growing traffic demands or support users with poor or lack of access to terrestrial infrastructure. Exploiting both aerial and terrestrial resources in service composition increases coverage and facilitates the use of full computing and communication potentials. This paper proposes a service placement and composition mechanism for integrated aerial-terrestrial networks over the edge-cloud continuum while considering the dynamic nature of the network. The service function placement and service orchestration are modeled in an optimization framework. Considering the dynamicity, the Aerial Base Station (ABS) trajectory might not be deterministic, and their mobility pattern might not be known as assumed knowledge. Also, service requests can traverse through access nodes due to users' mobility. By incorporating predictive algorithms, including Deep Reinforcement Learning (DRL) approaches, the proposed method predicts ABS locations and service requests. Subsequently, a heuristic isomorphic graph matching approach is proposed to enable efficient, latency-aware service orchestration. Simulation results demonstrate the efficiency of the proposed prediction and service composition schemes in terms of accuracy, cost optimization, scalability, and responsiveness, ensuring timely and reliable service delivery under diverse network conditions.         ",
    "url": "https://arxiv.org/abs/2504.07528",
    "authors": [
      "Mohammad Farhoudi",
      "Masoud Shokrnezhad",
      "Somayeh Kianpisheh",
      "Tarik Taleb"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.07537",
    "title": "Formalizing Representation Theorems for a Logical Framework with Rewriting",
    "abstract": "           Representation theorems for formal systems often take the form of an inductive translation that satisfies certain invariants, which are proved inductively. Theory morphisms and logical relations are common patterns of such inductive constructions. They allow representing the translation and the proofs of the invariants as a set of translation rules, corresponding to the cases of the inductions. Importantly, establishing the invariants is reduced to checking a finite set of, typically decidable, statements. Therefore, in a framework supporting theory morphisms and logical relations, translations that fit one of these patterns become much easier to formalize and to verify. The $\\lambda\\Pi$-calculus modulo rewriting is a logical framework designed for representing and translating between formal systems that has previously not systematically supported such patterns. In this paper, we extend it with theory morphisms and logical relations. We apply these to define and verify invariants for a number of translations between formal systems. In doing so, we identify some best practices that enable us to obtain elegant novel formalizations of some challenging translations, in particular type erasure translations from typed to untyped languages.         ",
    "url": "https://arxiv.org/abs/2504.07537",
    "authors": [
      "Thomas Traversi\u00e9",
      "Florian Rabe"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2504.07551",
    "title": "Topology optimization of decoupling feeding networks for antenna arrays",
    "abstract": "           Near-field and radiation coupling between nearby radiating elements is unavoidable, and it is considered a limiting factor for applications in wireless communications and active sensing. This article proposes a density-based topology optimization approach to design decoupling networks for such systems. The decoupling networks are designed based on a multi-objective optimization problem with the radiating elements replaced by their time-domain impulse response for efficient computations and to enable the solution of the design problem using gradient-based optimization methods. We use the adjoint-field method to compute the gradients of the optimization objectives. Additionally, nonlinear filters are applied during the optimization procedure to impose minimum-size control on the optimized designs. We demonstrate the concept by designing the decoupling network for a two-element planar antenna array; the antenna is designed in a separate optimization problem. The optimized decoupling networks provide a signal path that destructively interferes with the coupling between the radiating elements while preserving their individual matching to the feeding ports. Compact decoupling networks capable of suppressing the mutual coupling by more than 10 dB between two closely separated planar antennas operating around 2.45 GHz are presented and validated experimentally.         ",
    "url": "https://arxiv.org/abs/2504.07551",
    "authors": [
      "Pan Lu",
      "Eddie Wadbro",
      "Jonas Starck",
      "Martin Berggren",
      "Emadeldeen Hassan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.07575",
    "title": "Explicit Uncertainty Modeling for Video Watch Time Prediction",
    "abstract": "           In video recommendation, a critical component that determines the system's recommendation accuracy is the watch-time prediction module, since how long a user watches a video directly reflects personalized preferences. One of the key challenges of this problem is the user's stochastic watch-time behavior. To improve the prediction accuracy for such an uncertain behavior, existing approaches show that one can either reduce the noise through duration bias modeling or formulate a distribution modeling task to capture the uncertainty. However, the uncontrolled uncertainty is not always equally distributed across users and videos, inducing a balancing paradox between the model accuracy and the ability to capture out-of-distribution samples. In practice, we find that the uncertainty of the watch-time prediction model also provides key information about user behavior, which, in turn, could benefit the prediction task itself. Following this notion, we derive an explicit uncertainty modeling strategy for the prediction model and propose an adversarial optimization framework that can better exploit the user watch-time behavior. This framework has been deployed online on an industrial video sharing platform that serves hundreds of millions of daily active users, which obtains a significant increase in users' video watch time by 0.31% through the online A/B test. Furthermore, extended offline experiments on two public datasets verify the effectiveness of the proposed framework across various watch-time prediction backbones.         ",
    "url": "https://arxiv.org/abs/2504.07575",
    "authors": [
      "Shanshan Wu",
      "Shuchang Liu",
      "Shuai Zhang",
      "Xiaoyu Yang",
      "Xiang Li",
      "Lantao Hu",
      "Han Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.07579",
    "title": "Controlling Complex Systems",
    "abstract": "           This chapter provides a comprehensive overview of controlling collective behavior in complex systems comprising large ensembles of interacting dynamical agents. Building upon traditional control theory's foundation in individual systems, we introduce tools designed to address the unique challenges of coordinating networks that exhibit emergent phenomena, including consensus, synchronization, and pattern formation. We analyze how local agent interactions generate macroscopic behaviors and investigate the fundamental role of network topology in determining system dynamics. Inspired by natural systems, we emphasize control strategies that achieve global coordination through localized interventions while considering practical implementation challenges. The chapter concludes by presenting novel frameworks for managing very large agent ensembles and leveraging interacting networks for control purposes.         ",
    "url": "https://arxiv.org/abs/2504.07579",
    "authors": [
      "Marco Coraggio",
      "Davide Salzano",
      "Mario di Bernardo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2504.07589",
    "title": "Copy-and-Paste? Identifying EVM-Inequivalent Code Smells in Multi-chain Reuse Contracts",
    "abstract": "           As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on different blockchains not being able to achieve consistent execution as on Ethereum. This inconsistency reveals design flaws in reused contracts, exposing code smells that hinder code reusability, and we define this inconsistency as EVM-Inequivalent Code Smells. In this paper, we conducted the first empirical study to reveal the causes and characteristics of EVM-Inequivalent Code Smells. To ensure the identified smells reflect real developer concerns, we collected and analyzed 1,379 security audit reports and 326 Stack Overflow posts related to reused contracts on EVM-compatible blockchains, such as Binance Smart Chain (BSC) and Polygon. Using the open card sorting method, we defined six types of EVM-Inequivalent Code Smells. For automated detection, we developed a tool named EquivGuard. It employs static taint analysis to identify key paths from different patterns and uses symbolic execution to verify path reachability. Our analysis of 905,948 contracts across six major blockchains shows that EVM-Inequivalent Code Smells are widespread, with an average prevalence of 17.70%. While contracts with code smells do not necessarily lead to financial loss and attacks, their high prevalence and significant asset management underscore the potential threats of reusing these smelly Ethereum contracts. Thus, developers are advised to abandon Copy-and-Paste programming practices and detect EVM-Inequivalent Code Smells before reusing Ethereum contracts.         ",
    "url": "https://arxiv.org/abs/2504.07589",
    "authors": [
      "Zexu Wang",
      "Jiachi Chen",
      "Tao Zhang",
      "Yu Zhang",
      "Weizhe Zhang",
      "Yuming Feng",
      "Zibin Zheng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.07590",
    "title": "DWFS-Obfuscation: Dynamic Weighted Feature Selection for Robust Malware Familial Classification under Obfuscation",
    "abstract": "           Due to its open-source nature, the Android operating system has consistently been a primary target for attackers. Learning-based methods have made significant progress in the field of Android malware detection. However, traditional detection methods based on static features struggle to identify obfuscated malicious code, while methods relying on dynamic analysis suffer from low efficiency. To address this, we propose a dynamic weighted feature selection method that analyzes the importance and stability of features, calculates scores to filter out the most robust features, and combines these selected features with the program's structural information. We then utilize graph neural networks for classification, thereby improving the robustness and accuracy of the detection system. We analyzed 8,664 malware samples from eight malware families and tested a total of 44,940 malware variants generated using seven obfuscation strategies. Experiments demonstrate that our proposed method achieves an F1-score of 95.56% on the unobfuscated dataset and 92.28% on the obfuscated dataset, indicating that the model can effectively detect obfuscated malware.         ",
    "url": "https://arxiv.org/abs/2504.07590",
    "authors": [
      "Xingyuan Wei",
      "Zijun Cheng",
      "Ning Li",
      "Qiujian Lv",
      "Ziyang Yu",
      "Degang Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07592",
    "title": "Hardness of 4-Colourings G-Colourable Graphs",
    "abstract": "           We study the complexity of a class of promise graph homomorphism problems. For a fixed graph H, the H-colouring problem is to decide whether a given graph has a homomorphism to H. By a result of Hell and Ne\u0161et\u0159il, this problem is NP-hard for any non-bipartite loop-less graph H. Brakensiek and Guruswami [SODA 2018] conjectured the hardness extends to promise graph homomorphism problems as follows: fix a pair of non-bipartite loop-less graphs G, H such that there is a homomorphism from G to H, it is NP-hard to distinguish between graphs that are G-colourable and those that are not H-colourable. We confirm this conjecture in the cases when both G and H are 4-colourable. This is a common generalisation of previous results of Khanna, Linial, and Safra [Comb. 20(3): 393-415 (2000)] and of Krokhin and Opr\u0161al [FOCS 2019]. The result is obtained by combining the algebraic approach to promise constraint satisfaction with methods of topological combinatorics and equivariant obstruction theory.         ",
    "url": "https://arxiv.org/abs/2504.07592",
    "authors": [
      "Sergey Avvakumov",
      "Marek Filakovsk\u00fd",
      "Jakub Opr\u0161al",
      "Gianluca Tasinato",
      "Uli Wagner"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Algebraic Topology (math.AT)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2504.07598",
    "title": "On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition",
    "abstract": "           Gait recognition from video streams is a challenging problem in computer vision biometrics due to the subtle differences between gaits and numerous confounding factors. Recent advancements in self-supervised pretraining have led to the development of robust gait recognition models that are invariant to walking covariates. While neural scaling laws have transformed model development in other domains by linking performance to data, model size, and compute, their applicability to gait remains unexplored. In this work, we conduct the first empirical study scaling on skeleton-based self-supervised gait recognition to quantify the effect of data quantity, model size and compute on downstream gait recognition performance. We pretrain multiple variants of GaitPT - a transformer-based architecture - on a dataset of 2.7 million walking sequences collected in the wild. We evaluate zero-shot performance across four benchmark datasets to derive scaling laws for data, model size, and compute. Our findings demonstrate predictable power-law improvements in performance with increased scale and confirm that data and compute scaling significantly influence downstream accuracy. We further isolate architectural contributions by comparing GaitPT with GaitFormer under controlled compute budgets. These results provide practical insights into resource allocation and performance estimation for real-world gait recognition systems.         ",
    "url": "https://arxiv.org/abs/2504.07598",
    "authors": [
      "Adrian Cosma",
      "Andy C\u01cetrun\u01ce",
      "Emilian R\u01cedoi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07603",
    "title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions",
    "abstract": "           Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast. Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks. Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR. However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving. To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions. In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation. Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle. We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems.         ",
    "url": "https://arxiv.org/abs/2504.07603",
    "authors": [
      "Youngwan Jin",
      "Michal Kovac",
      "Yagiz Nalcakan",
      "Hyeongjin Ju",
      "Hanbin Song",
      "Sanghyeop Yeo",
      "Shiho Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07612",
    "title": "SaRoHead: A Dataset for Satire Detection in Romanian Multi-Domain News Headlines",
    "abstract": "           The headline is an important part of a news article, influenced by expressiveness and connection to the exposed subject. Although most news outlets aim to present reality objectively, some publications prefer a humorous approach in which stylistic elements of satire, irony, and sarcasm blend to cover specific topics. Satire detection can be difficult because a headline aims to expose the main idea behind a news article. In this paper, we propose SaRoHead, the first corpus for satire detection in Romanian multi-domain news headlines. Our findings show that the clickbait used in some non-satirical headlines significantly influences the model.         ",
    "url": "https://arxiv.org/abs/2504.07612",
    "authors": [
      "Mihnea-Alexandru V\u00eerlan",
      "R\u0103zvan-Alexandru Sm\u0103du",
      "Dumitru-Clementin Cercel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07627",
    "title": "Robustness of Online Identification-based Policy Iteration to Noisy Data",
    "abstract": "           This article investigates the core mechanisms of indirect data-driven control for unknown systems, focusing on the application of policy iteration (PI) within the context of the linear quadratic regulator (LQR) optimal control problem. Specifically, we consider a setting where data is collected sequentially from a linear system subject to exogenous process noise, and is then used to refine estimates of the optimal control policy. We integrate recursive least squares (RLS) for online model estimation within a certainty-equivalent framework, and employ PI to iteratively update the control policy. In this work, we investigate first the convergence behavior of RLS under two different models of adversarial noise, namely point-wise and energy bounded noise, and then we provide a closed-loop analysis of the combined model identification and control design process. This iterative scheme is formulated as an algorithmic dynamical system consisting of the feedback interconnection between two algorithms expressed as discrete-time systems. This system theoretic viewpoint on indirect data-driven control allows us to establish convergence guarantees to the optimal controller in the face of uncertainty caused by noisy data. Simulations illustrate the theoretical results.         ",
    "url": "https://arxiv.org/abs/2504.07627",
    "authors": [
      "Bowen Song",
      "Andrea Iannelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.07633",
    "title": "Kernel Logistic Regression Learning for High-Capacity Hopfield Networks",
    "abstract": "           Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches.         ",
    "url": "https://arxiv.org/abs/2504.07633",
    "authors": [
      "Akira Tamamori"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.07634",
    "title": "Agent That Debugs: Dynamic State-Guided Vulnerability Repair",
    "abstract": "           In recent years, more vulnerabilities have been discovered every day, while manual vulnerability repair requires specialized knowledge and is time-consuming. As a result, many detected or even published vulnerabilities remain unpatched, thereby increasing the exposure of software systems to attacks. Recent advancements in agents based on Large Language Models have demonstrated their increasing capabilities in code understanding and generation, which can be promising to achieve automated vulnerability repair. However, the effectiveness of agents based on static information retrieval is still not sufficient for patch generation. To address the challenge, we propose a program repair agent called VulDebugger that fully utilizes both static and dynamic context, and it debugs programs in a manner akin to humans. The agent inspects the actual state of the program via the debugger and infers expected states via constraints that need to be satisfied. By continuously comparing the actual state with the expected state, it deeply understands the root causes of the vulnerabilities and ultimately accomplishes repairs. We experimentally evaluated VulDebugger on 50 real-life projects. With 60.00% successfully fixed, VulDebugger significantly outperforms state-of-the-art approaches for vulnerability repair.         ",
    "url": "https://arxiv.org/abs/2504.07634",
    "authors": [
      "Zhengyao Liu",
      "Yunlong Ma",
      "Jingxuan Xu",
      "Junchen Ai",
      "Xiang Gao",
      "Hailong Sun",
      "Abhik Roychoudhury"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.07645",
    "title": "Prediction of Usage Probabilities of Shopping-Mall Corridors Using Heterogeneous Graph Neural Networks",
    "abstract": "           We present a method based on graph neural network (GNN) for prediction of probabilities of usage of shopping-mall corridors. The heterogeneous graph network of shops and corridor paths are obtained from floorplans of the malls by creating vector layers for corridors, shops and entrances. These are subsequently assimilated into nodes and edges of graphs. The prediction of the usage probability is based on the shop features, namely, the area and usage categories they fall into, and on the graph connecting these shops, corridor junctions and entrances by corridor paths. Though the presented method is applicable for training on datasets obtained from a field survey or from pedestrian-detecting sensors, the target data of the supervised deep-learning work flow in this work are obtained from a probability method. We also include a context-specific representation learning of latent features. The usage-probability prediction is made on each edge, which is a connection by a section of corridor path between the adjacent nodes representing the shops or corridor points. To create a feature for each edge, the hidden-layer feature vectors acquired in the message-passing GNN layers at the nodes of each edge are averaged and concatenated with the vector obtained by their multiplication. These edge-features are then passed to multilayer perceptrons (MLP) to make the final prediction of usage probability on each edge. The samples of synthetic learning dataset for each shopping mall are obtained by changing the shops' usage and area categories, and by subsequently feeding the graph into the probability model. When including different shopping malls in a single dataset, we also propose to consider graph-level features to inform the model with specific identifying features of each mall.         ",
    "url": "https://arxiv.org/abs/2504.07645",
    "authors": [
      "Malik M Barakathullah",
      "Immanuel Koh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07660",
    "title": "End-to-End Facial Expression Detection in Long Videos",
    "abstract": "           Facial expression detection involves two interrelated tasks: spotting, which identifies the onset and offset of expressions, and recognition, which classifies them into emotional categories. Most existing methods treat these tasks separately using a two-step training pipelines. A spotting model first detects expression intervals. A recognition model then classifies the detected segments. However, this sequential approach leads to error propagation, inefficient feature learning, and suboptimal performance due to the lack of joint optimization of the two tasks. We propose FEDN, an end-to-end Facial Expression Detection Network that jointly optimizes spotting and recognition. Our model introduces a novel attention-based feature extraction module, incorporating segment attention and sliding window attention to improve facial feature learning. By unifying two tasks within a single network, we greatly reduce error propagation and enhance overall performance. Experiments on CASME}^2 and CASME^3 demonstrate state-of-the-art accuracy for both spotting and detection, underscoring the benefits of joint optimization for robust facial expression detection in long videos.         ",
    "url": "https://arxiv.org/abs/2504.07660",
    "authors": [
      "Yini Fang",
      "Alec Diallo",
      "Yiqi Shi",
      "Frederic Jumelle",
      "Bertram Shi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07668",
    "title": "Distributed Fault-Tolerant Control for Heterogeneous MAS with Prescribed Performance under Communication Failures",
    "abstract": "           This paper presents a novel approach employing prescribed performance control to address the distributed fault-tolerant formation control problem in a heterogeneous UAV-UGV cooperative system under a directed interaction topology and communication link failures. The proposed distributed fault-tolerant control scheme enables UAVs to accurately track a virtual leader's trajectory and achieve the desired formation, while ensuring UGVs converge within the convex hull formed by leader UAVs. By accounting for differences in system parameters and state dimensions between UAVs and UGVs, the method leverages performance functions to guarantee predefined transient and steady-state behavior. Additionally, a variable prescribed performance boundary control strategy with an adaptive learning rate is introduced to tackle actuator saturation, ensuring reliable formation tracking in real-world scenarios. Simulation results demonstrate the effectiveness and robustness of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2504.07668",
    "authors": [
      "Yongkang Zhang",
      "Bin Jiang",
      "Yajie Ma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.07687",
    "title": "FMNV: A Dataset of Media-Published News Videos for Fake News Detection",
    "abstract": "           News media, particularly video-based platforms, have become deeply embedded in daily life, concurrently amplifying risks of misinformation dissemination. Consequently, multimodal fake news detection has garnered significant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public engagement, whereas professionally crafted fake news videos disseminated by media outlets often politically or virally motivated pose substantially greater societal harm. To address this gap, we construct FMNV, a novel dataset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture integrating CLIP and Faster R-CNN for video feature extraction, enhanced by co-attention mechanisms for feature refinement and multimodal aggregation. Comparative experiments demonstrate both the generalization capability of FMNV across multiple baselines and the superior detection efficacy of FMNVD. This work establishes critical benchmarks for detecting high-impact fake news in media ecosystems while advancing methodologies for cross-modal inconsistency analysis.         ",
    "url": "https://arxiv.org/abs/2504.07687",
    "authors": [
      "Yihao Wang",
      "Zhong Qian",
      "Peifeng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2504.07691",
    "title": "Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation",
    "abstract": "           Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures. However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation. To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD. Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges. To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space. Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced. These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge. Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures.         ",
    "url": "https://arxiv.org/abs/2504.07691",
    "authors": [
      "Yanglin Huang",
      "Kai Hu",
      "Yuan Zhang",
      "Zhineng Chen",
      "Xieping Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07697",
    "title": "Transformer-Based Robust Underwater Inertial Navigation in Prolonged Doppler Velocity Log Outages",
    "abstract": "           Autonomous underwater vehicles (AUV) have a wide variety of applications in the marine domain, including exploration, surveying, and mapping. Their navigation systems rely heavily on fusing data from inertial sensors and a Doppler velocity log (DVL), typically via nonlinear filtering. The DVL estimates the AUV's velocity vector by transmitting acoustic beams to the seabed and analyzing the Doppler shift from the reflected signals. However, due to environmental challenges, DVL beams can deflect or fail in real-world settings, causing signal outages. In such cases, the AUV relies solely on inertial data, leading to accumulated navigation errors and mission terminations. To cope with these outages, we adopted ST-BeamsNet, a deep learning approach that uses inertial readings and prior DVL data to estimate AUV velocity during isolated outages. In this work, we extend ST-BeamsNet to address prolonged DVL outages and evaluate its impact within an extended Kalman filter framework. Experiments demonstrate that the proposed framework improves velocity RMSE by up to 63% and reduces final position error by up to 95% compared to pure inertial navigation. This is in scenarios involving up to 50 seconds of complete DVL outage.         ",
    "url": "https://arxiv.org/abs/2504.07697",
    "authors": [
      "Zeev Yampolsky",
      "Nadav Cohen",
      "Itzik Klein"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.07717",
    "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
    "abstract": "           Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.         ",
    "url": "https://arxiv.org/abs/2504.07717",
    "authors": [
      "Yang Jiao",
      "Xiaodong Wang",
      "Kai Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07738",
    "title": "Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information",
    "abstract": "           In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities.         ",
    "url": "https://arxiv.org/abs/2504.07738",
    "authors": [
      "A. Loreti",
      "K. Chen",
      "R. George",
      "R. Firth",
      "A. Agnello",
      "S. Tanaka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07740",
    "title": "Zero-Shot Cross-Domain Code Search without Fine-Tuning",
    "abstract": "           Code search aims to retrieve semantically relevant code snippets for natural language queries. While pre-trained language models (PLMs) have shown remarkable performance in this task, they struggle in cross-domain scenarios, often requiring costly fine-tuning or facing performance drops in zero-shot settings. RAPID, which generates synthetic data for model fine-tuning, is currently the only effective method for zero-shot cross-domain code search. Despite its effectiveness, RAPID demands substantial computational resources for fine-tuning and needs to maintain specialized models for each domain, underscoring the need for a zero-shot, fine-tuning-free approach for cross-domain code search. The key to tackling zero-shot cross-domain code search lies in bridging the gaps among domains. In this work, we propose to break the query-code matching process of code search into two simpler tasks: query-comment matching and code-code matching. Our empirical study reveals the strong complementarity among the three matching schemas in zero-shot cross-domain settings, i.e., query-code, query-comment, and code-code matching. Based on the findings, we propose CodeBridge, a zero-shot, fine-tuning-free approach for cross-domain code search. Specifically, CodeBridge uses Large Language Models (LLMs) to generate comments and pseudo-code, then combines query-code, query-comment, and code-code matching via PLM-based similarity scoring and sampling-based fusion. Experimental results show that our approach outperforms the state-of-the-art PLM-based code search approaches, i.e., CoCoSoDa and UniXcoder, by an average of 21.4% and 24.9% in MRR, respectively, across three datasets. Our approach also yields results that are better than or comparable to those of the zero-shot cross-domain code search approach RAPID, which requires costly fine-tuning.         ",
    "url": "https://arxiv.org/abs/2504.07740",
    "authors": [
      "Keyu Liang",
      "Zhongxin Liu",
      "Chao Liu",
      "Zhiyuan Wan",
      "David Lo",
      "Xiaohu Yang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07745",
    "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
    "abstract": "           Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall description of videos, they struggle with fine-grained understanding, particularly in aspects such as visual dynamics and video details inquiries. To tackle these shortcomings, we find that fine-tuning Video-LLMs on self-supervised fragment tasks, greatly improve their fine-grained video understanding abilities. Hence we propose two key contributions:(1) Self-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs. Moreover, it relieves researchers from labor-intensive annotations and smartly circumvents the limitations of natural language, which often fails to capture the complex spatiotemporal variations in videos; (2) A novel benchmark dataset, namely FineVidBench, for rigorously assessing Video-LLMs' performance at both the scene and fragment levels, offering a comprehensive evaluation of their capabilities. We assessed multiple models and validated the effectiveness of SF$^2$T on them. Experimental results reveal that our approach improves their ability to capture and interpret spatiotemporal details.         ",
    "url": "https://arxiv.org/abs/2504.07745",
    "authors": [
      "Yangliu Hu",
      "Zikai Song",
      "Na Feng",
      "Yawei Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07761",
    "title": "Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection",
    "abstract": "           In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc. This study focuses on the topic of fake ID detection, covering several limitations in the field. In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons. In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection. Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image. Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis. The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases. In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub.         ",
    "url": "https://arxiv.org/abs/2504.07761",
    "authors": [
      "Javier Mu\u00f1oz-Haro",
      "Ruben Tolosana",
      "Ruben Vera-Rodriguez",
      "Aythami Morales",
      "Julian Fierrez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07793",
    "title": "Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
    "abstract": "           Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{this https URL}{\\texttt{this https URL}}$.         ",
    "url": "https://arxiv.org/abs/2504.07793",
    "authors": [
      "Yifan Ding",
      "Arturas Aleksandrauskas",
      "Amirhossein Ahmadian",
      "Jonas Unger",
      "Fredrik Lindsten",
      "Gabriel Eilertsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07813",
    "title": "P2Object: Single Point Supervised Object Detection and Instance Segmentation",
    "abstract": "           Object recognition using single-point supervision has attracted increasing attention recently. However, the performance gap compared with fully-supervised algorithms remains large. Previous works generated class-agnostic \\textbf{\\textit{proposals in an image}} offline and then treated mixed candidates as a single bag, putting a huge burden on multiple instance learning (MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which constructs balanced \\textbf{\\textit{instance-level proposal bags}} by generating proposals in an anchor-like way and refining the proposals in a coarse-to-fine paradigm. Through further research, we find that the bag of proposals, either at the image level or the instance level, is established on discrete box sampling. This leads the pseudo box estimation into a sub-optimal solution, resulting in the truncation of object boundaries or the excessive inclusion of background. Hence, we conduct a series exploration of discrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask Network (P2MNet). P2BNet++ conducts an approximately continuous proposal sampling strategy by better utilizing spatial clues. P2MNet further introduces low-level image information to assist in pixel prediction, and a boundary self-prediction is designed to relieve the limitation of the estimated boxes. Benefiting from the continuous object-aware \\textbf{\\textit{pixel-level perception}}, P2MNet can generate more precise bounding boxes and generalize to segmentation tasks. Our method largely surpasses the previous methods in terms of the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating great potential to bridge the performance gap compared with fully supervised tasks.         ",
    "url": "https://arxiv.org/abs/2504.07813",
    "authors": [
      "Pengfei Chen",
      "Xuehui Yu",
      "Xumeng Han",
      "Kuiran Wang",
      "Guorong Li",
      "Lingxi Xie",
      "Zhenjun Han",
      "Jianbin Jiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07815",
    "title": "Siren Federate: Bridging document, relational, and graph models for exploratory graph analysis",
    "abstract": "           Investigative workflows require interactive exploratory analysis on large heterogeneous knowledge graphs. Current databases show limitations in enabling such task. This paper discusses the architecture of Siren Federate, a system that efficiently supports exploratory graph analysis by bridging document-oriented, relational and graph models. Technical contributions include distributed join algorithms, adaptive query planning, query plan folding, semantic caching, and semi-join decomposition for path query. Semi-join decomposition addresses the exponential growth of intermediate results in path-based queries. Experiments show that Siren Federate exhibits low latency and scales well with the amount of data, the number of users, and the number of computing nodes.         ",
    "url": "https://arxiv.org/abs/2504.07815",
    "authors": [
      "Georgeta Bordea",
      "Stephane Campinas",
      "Matteo Catena",
      "Renaud Delbru"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.07822",
    "title": "DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting",
    "abstract": "           Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.         ",
    "url": "https://arxiv.org/abs/2504.07822",
    "authors": [
      "Wanna Cui",
      "Peizheng Wang",
      "Faliang Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07830",
    "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations",
    "abstract": "           We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.         ",
    "url": "https://arxiv.org/abs/2504.07830",
    "authors": [
      "Genglin Liu",
      "Salman Rahman",
      "Elisa Kreiss",
      "Marzyeh Ghassemi",
      "Saadia Gabriel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.07835",
    "title": "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks",
    "abstract": "           Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility. In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.07835",
    "authors": [
      "Erin Carson",
      "Xinye Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.07837",
    "title": "A Review of HPC-Accelerated CFD in National Security and Defense",
    "abstract": "           Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.         ",
    "url": "https://arxiv.org/abs/2504.07837",
    "authors": [
      "James Afful"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2504.07839",
    "title": "Deep Learning-based Intrusion Detection Systems: A Survey",
    "abstract": "           Intrusion Detection Systems (IDS) have long been a hot topic in the cybersecurity community. In recent years, with the introduction of deep learning (DL) techniques, IDS have made great progress due to their increasing generalizability. The rationale behind this is that by learning the underlying patterns of known system behaviors, IDS detection can be generalized to intrusions that exploit zero-day vulnerabilities. In this survey, we refer to this type of IDS as DL-based IDS (DL-IDS). From the perspective of DL, this survey systematically reviews all the stages of DL-IDS, including data collection, log storage, log parsing, graph summarization, attack detection, and attack investigation. To accommodate current researchers, a section describing the publicly available benchmark datasets is included. This survey further discusses current challenges and potential future research directions, aiming to help researchers understand the basic ideas and visions of DL-IDS research, as well as to motivate their research interests.         ",
    "url": "https://arxiv.org/abs/2504.07839",
    "authors": [
      "Zhiwei Xu",
      "Yujuan Wu",
      "Shiheng Wang",
      "Jiabao Gao",
      "Tian Qiu",
      "Ziqi Wang",
      "Hai Wan",
      "Xibin Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07848",
    "title": "Opinion dynamics and the unpredictability of opinion trajectories in an adaptive social network model",
    "abstract": "           Understanding opinion dynamics in social networks is critical for predicting social behavior and detecting polarization. Traditional approaches often rely on static snapshots of network states, which can obscure the underlying dynamics of opinion evolution. In this study, we introduce a dynamic framework that quantifies the unpredictability of opinion trajectories using the normalized Lempel-Ziv (nLZ) complexity. Our approach leverages an adaptive social network model where each node is characterized by three behavioral parameters - homophily, neophily, and social conformity - and where opinions evolve continuously according to a system of ordinary differential equations. The results reveal distinct nLZ complexity signatures for each node type: homophilic nodes exhibit consistently rising complexity, reflecting increasingly unpredictable opinion shifts that are counterintuitive given their tendency for similarity; neophilic nodes maintain low and stable complexity, suggesting that openness to novelty can, surprisingly, lead to stable opinion dynamics; and conformic nodes display a U-shaped complexity trend, transitioning from early opinion stagnation to later unpredictability. In fully heterogeneous networks, modest interaction effects emerge, with slight shifts in the unpredictability of each faction's trajectories. These findings underscore the importance of temporal analysis in uncovering hidden dynamical patterns, offering novel insights into the mechanisms underlying social adaptation and polarization.         ",
    "url": "https://arxiv.org/abs/2504.07848",
    "authors": [
      "Akshay Gangadhar",
      "Hiroki Sayama"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2504.07863",
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "abstract": "           Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.         ",
    "url": "https://arxiv.org/abs/2504.07863",
    "authors": [
      "Mengjia Niu",
      "Hamed Haddadi",
      "Guansong Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07867",
    "title": "SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos",
    "abstract": "           Video Scene Graph Generation (VidSGG) is an important topic in understanding dynamic kitchen environments. Current models for VidSGG require extensive training to produce scene graphs. Recently, Vision Language Models (VLM) and Vision Foundation Models (VFM) have demonstrated impressive zero-shot capabilities in a variety of tasks. However, VLMs like Gemini struggle with the dynamics for VidSGG, failing to maintain stable object identities across frames. To overcome this limitation, we propose SAMJAM, a zero-shot pipeline that combines SAM2's temporal tracking with Gemini's semantic understanding. SAM2 also improves upon Gemini's object grounding by producing more accurate bounding boxes. In our method, we first prompt Gemini to generate a frame-level scene graph. Then, we employ a matching algorithm to map each object in the scene graph with a SAM2-generated or SAM2-propagated mask, producing a temporally-consistent scene graph in dynamic environments. Finally, we repeat this process again in each of the following frames. We empirically demonstrate that SAMJAM outperforms Gemini by 8.33% in mean recall on the EPIC-KITCHENS and EPIC-KITCHENS-100 datasets.         ",
    "url": "https://arxiv.org/abs/2504.07867",
    "authors": [
      "Joshua Li",
      "Fernando Jose Pena Cantu",
      "Emily Yu",
      "Alexander Wong",
      "Yuchen Cui",
      "Yuhao Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07870",
    "title": "Open Datasets for Grid Modeling and Visualization: An Alberta Power Network Case",
    "abstract": "           In the power and energy industry, multiple entities in grid operational logs are frequently recorded and updated. Thanks to recent advances in IT facilities and smart metering services, a variety of datasets such as system load, generation mix, and grid connection are often publicly available. While these resources are valuable in evaluating power grid's operational conditions and system resilience, the lack of fine-grained, accurate locational information constrain the usage of current data, which further hinders the development of smart grid and renewables integration. For instance, electricity end users are not aware of nodal generation mix or carbon emissions, while the general public have limited understanding about the effect of demand response or renewables integration if only the whole system's demands and generations are available. In this work, we focus on recovering power grid topology and line flow directions from open public dataset. Taking the Alberta grid as a working example, we start from mapping multi-modal power system datasets to the grid topology integrated with geographical information. By designing a novel optimization-based scheme to recover line flow directions, we are able to analyze and visualize the interactions between generations and demand vectors in an efficient manner. Proposed research is fully open-sourced and highly generalizable, which can help model and visualize grid information, create synthetic dataset, and facilitate analytics and decision-making framework for clean energy transition.         ",
    "url": "https://arxiv.org/abs/2504.07870",
    "authors": [
      "Ben Cheng",
      "Yize Chen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.07887",
    "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
    "abstract": "           Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.         ",
    "url": "https://arxiv.org/abs/2504.07887",
    "authors": [
      "Riccardo Cantini",
      "Alessio Orsino",
      "Massimo Ruggiero",
      "Domenico Talia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07901",
    "title": "Redefining Machine Translation on Social Network Services with Large Language Models",
    "abstract": "           The globalization of social interactions has heightened the need for machine translation (MT) on Social Network Services (SNS), yet traditional models struggle with culturally nuanced content like memes, slang, and pop culture references. While large language models (LLMs) have advanced general-purpose translation, their performance on SNS-specific content remains limited due to insufficient specialized training data and evaluation benchmarks. This paper introduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel dataset developed through three innovations: (1) Supervised Finetuning with Dual-LLM Back-Translation Sampling, an unsupervised sampling method using LLM-based back-translation to select diverse data for large-scale finetuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation, building reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation, evaluating phenomena like humor localization, emoji semantics, and meme adaptation. Experiments show RedTrans outperforms state-of-the-art LLMs. Besides, RedTrans has already been deployed in a real-world production environment, demonstrating that domain-specific adaptation, effectively bridges the gap between generic and culturally grounded translation systems.         ",
    "url": "https://arxiv.org/abs/2504.07901",
    "authors": [
      "Hongcheng Guo",
      "Fei Zhao",
      "Shaosheng Cao",
      "Xinze Lyu",
      "Ziyan Liu",
      "Yue Wang",
      "Boyang Wang",
      "Zhoujun Li",
      "Chonggang Lu",
      "Zhe Xu",
      "Yao Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.07156",
    "title": "PLM-eXplain: Divide and Conquer the Protein Embedding Space",
    "abstract": "           Protein language models (PLMs) have revolutionised computational biology through their ability to generate powerful sequence representations for diverse prediction tasks. However, their black-box nature limits biological interpretation and translation to actionable insights. We present an explainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by factoring PLM embeddings into two components: an interpretable subspace based on established biochemical features, and a residual subspace that preserves the model's predictive power. Using embeddings from ESM2, our adapter incorporates well-established properties, including secondary structure and hydropathy while maintaining high performance. We demonstrate the effectiveness of our approach across three protein-level classification tasks: prediction of extracellular vesicle association, identification of transmembrane helices, and prediction of aggregation propensity. PLM-X enables biological interpretation of model decisions without sacrificing accuracy, offering a generalisable solution for enhancing PLM interpretability across various downstream applications. This work addresses a critical need in computational biology by providing a bridge between powerful deep learning models and actionable biological insights.         ",
    "url": "https://arxiv.org/abs/2504.07156",
    "authors": [
      "Jan van Eck",
      "Dea Gogishvili",
      "Wilson Silva",
      "Sanne Abeln"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07186",
    "title": "Disjunctive domination in maximal outerplanar graphs",
    "abstract": "           A disjunctive dominating set of a graph $G$ is a set $D \\subseteq V(G)$ such that every vertex in $V(G)\\setminus D$ has a neighbor in $D$ or has at least two vertices in $D$ at distance $2$ from it. The disjunctive domination number of $G$, denoted by $\\gamma_2^d(G)$, is the minimum cardinality of a disjunctive dominating set of $G$. In this paper, we show that if $G$ is a maximal outerplanar graph of order $n \\ge 7$ with $k$ vertices of degree $2$, then $\\gamma_2^d(G)\\le \\lfloor\\frac{2}{9}(n+k)\\rfloor$, and this bound is sharp.         ",
    "url": "https://arxiv.org/abs/2504.07186",
    "authors": [
      "Michael A. Henning",
      "Paras Vinubhai Maniya",
      "Dinabandhu Pradhan"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2504.07273",
    "title": "Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits",
    "abstract": "           In recent years, neural networks (NNs) have driven significant advances in machine learning. However, as tasks grow more complex, NNs often require large numbers of trainable parameters, which increases computational and energy demands. Variational quantum circuits (VQCs) offer a promising alternative: they leverage quantum mechanics to capture intricate relationships and typically need fewer parameters. In this work, we evaluate NNs and VQCs on simple supervised and reinforcement learning tasks, examining models with different parameter sizes. We simulate VQCs and execute selected parts of the training process on real quantum hardware to approximate actual training times. Our results show that VQCs can match NNs in performance while using significantly fewer parameters, despite longer training durations. As quantum technology and algorithms advance, and VQC architectures improve, we posit that VQCs could become advantageous for certain machine learning tasks.         ",
    "url": "https://arxiv.org/abs/2504.07273",
    "authors": [
      "Michael K\u00f6lle",
      "Alexander Feist",
      "Jonas Stein",
      "Sebastian W\u00f6lckert",
      "Claudia Linnhoff-Popien"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07379",
    "title": "Representation Meets Optimization: Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology",
    "abstract": "           Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as an effective counterpart to the original multilayer perceptron-based Physics-Informed Neural Networks (PINNs). Both representation models can address inverse problems and facilitate gray-box system identification. However, a comprehensive understanding of their performance in terms of accuracy and speed remains underexplored. In particular, we introduce a modified PIKAN architecture, tanh-cPIKAN, which is based on Chebyshev polynomials for parametrization of the univariate functions with an extra nonlinearity for enhanced performance. We then present a systematic investigation of how choices of the optimizer, representation, and training configuration influence the performance of PINNs and PIKANs in the context of systems pharmacology modeling. We benchmark a wide range of first-order, second-order, and hybrid optimizers, including various learning rate schedulers. We use the new Optax library to identify the most effective combinations for learning gray-boxes under ill-posed, non-unique, and data-sparse conditions. We examine the influence of model architecture (MLP vs. KAN), numerical precision (single vs. double), the need for warm-up phases for second-order methods, and sensitivity to the initial learning rate. We also assess the optimizer scalability for larger models and analyze the trade-offs introduced by JAX in terms of computational efficiency and numerical accuracy. Using two representative systems pharmacology case studies - a pharmacokinetics model and a chemotherapy drug-response model - we offer practical guidance on selecting optimizers and representation models/architectures for robust and efficient gray-box discovery. Our findings provide actionable insights for improving the training of physics-informed networks in biomedical applications and beyond.         ",
    "url": "https://arxiv.org/abs/2504.07379",
    "authors": [
      "Nazanin Ahmadi Daryakenari",
      "Khemraj Shukla",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.07426",
    "title": "Conditional Data Synthesis Augmentation",
    "abstract": "           Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings.         ",
    "url": "https://arxiv.org/abs/2504.07426",
    "authors": [
      "Xinyu Tian",
      "Xiaotong Shen"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07468",
    "title": "Novel Pooling-based VGG-Lite for Pneumonia and Covid-19 Detection from Imbalanced Chest X-Ray Datasets",
    "abstract": "           This paper proposes a novel pooling-based VGG-Lite model in order to mitigate class imbalance issues in Chest X-Ray (CXR) datasets. Automatic Pneumonia detection from CXR images by deep learning model has emerged as a prominent and dynamic area of research, since the inception of the new Covid-19 variant in 2020. However, the standard Convolutional Neural Network (CNN) models encounter challenges associated with class imbalance, a prevalent issue found in many medical datasets. The innovations introduced in the proposed model architecture include: (I) A very lightweight CNN model, `VGG-Lite', is proposed as a base model, inspired by VGG-16 and MobileNet-V2 architecture. (II) On top of this base model, we leverage an ``Edge Enhanced Module (EEM)\" through a parallel branch, consisting of a ``negative image layer\", and a novel custom pooling layer ``2Max-Min Pooling\". This 2Max-Min Pooling layer is entirely novel in this investigation, providing more attention to edge components within pneumonia CXR images. Thus, it works as an efficient spatial attention module (SAM). We have implemented the proposed framework on two separate CXR datasets. The first dataset is obtained from a readily available source on the internet, and the second dataset is a more challenging CXR dataset, assembled by our research team from three different sources. Experimental results reveal that our proposed framework has outperformed pre-trained CNN models, and three recent trend existing models ``Vision Transformer\", ``Pooling-based Vision Transformer (PiT)'' and ``PneuNet\", by substantial margins on both datasets. The proposed framework VGG-Lite with EEM, has achieved a macro average of 95% accuracy, 97.1% precision, 96.1% recall, and 96.6% F1 score on the ``Pneumonia Imbalance CXR dataset\", without employing any pre-processing technique.         ",
    "url": "https://arxiv.org/abs/2504.07468",
    "authors": [
      "Santanu Roy",
      "Ashvath Suresh",
      "Palak Sahu",
      "Tulika Rudra Gupta"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07493",
    "title": "Quickest change detection for UAV-based sensing",
    "abstract": "           This paper addresses the problem of quickest change detection (QCD) at two spatially separated locations monitored by a single unmanned aerial vehicle (UAV) equipped with a sensor. At any location, the UAV observes i.i.d. data sequentially in discrete time instants. The distribution of the observation data changes at some unknown, arbitrary time and the UAV has to detect this change in the shortest possible time. Change can occur at most at one location over the entire infinite time horizon. The UAV switches between these two locations in order to quickly detect the change. To this end, we propose Location Switching and Change Detection (LS-CD) algorithm which uses a repeated one-sided sequential probability ratio test (SPRT) based mechanism for observation-driven location switching and change detection. The primary goal is to minimize the worst-case average detection delay (WADD) while meeting constraints on the average run length to false alarm (ARL2FA) and the UAV's time-averaged energy consumption. We provide a rigorous theoretical analysis of the algorithm's performance by using theory of random walk. Specifically, we derive tight upper and lower bounds to its ARL2FA and a tight upper bound to its WADD. In the special case of a symmetrical setting, our analysis leads to a new asymptotic upper bound to the ARL2FA of the standard CUSUM algorithm, a novel contribution not available in the literature, to our knowledge. Numerical simulations demonstrate the efficacy of LS-CD.         ",
    "url": "https://arxiv.org/abs/2504.07493",
    "authors": [
      "Saqib Abbas",
      "Anurag Kumar",
      "Arpan Chattopadhyay"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.07606",
    "title": "Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases",
    "abstract": "           Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid and effective prediction. In this work, an automatic system which analyses in real-time echocardiography video sequences is proposed for the challenging and more specific task of prediction of heart failure times. This system is based on a novel deep learning framework, and works in two stages. The first one transforms the data included in a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any kind of machine learning-based framework, including a deep learning one. This initial stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage is focused on building and training a Vision Transformer (ViT). Self-supervised learning (SSL) methods, which have been so far barely explored in the literature about heart failure prediction, are applied to effectively train the ViT from scratch, even with scarce databases of echocardiograms. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures.         ",
    "url": "https://arxiv.org/abs/2504.07606",
    "authors": [
      "Andr\u00e9s Bell-Navas",
      "Mar\u00eda Villalba-Orero",
      "Enrique Lara-Pezzi",
      "Jes\u00fas Garicano-Mena",
      "Soledad Le Clainche"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.07741",
    "title": "Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks",
    "abstract": "           This work proposes a novel methodology for turbulence modeling in Large Eddy Simulation (LES) based on Graph Neural Networks (GNNs), which embeds the discrete rotational, reflectional and translational symmetries of the Navier-Stokes equations into the model architecture. In addition, suitable invariant input and output spaces are derived that allow the GNN models to be embedded seamlessly into the LES framework to obtain a symmetry-preserving simulation setup. The suitability of the proposed approach is investigated for two canonical test cases: Homogeneous Isotropic Turbulence (HIT) and turbulent channel flow. For both cases, GNN models are trained successfully in actual simulations using Reinforcement Learning (RL) to ensure that the models are consistent with the underlying LES formulation and discretization. It is demonstrated for the HIT case that the resulting GNN-based LES scheme recovers rotational and reflectional equivariance up to machine precision in actual simulations. At the same time, the stability and accuracy remain on par with non-symmetry-preserving machine learning models that fail to obey these properties. The same modeling strategy translates well to turbulent channel flow, where the GNN model successfully learns the more complex flow physics and is able to recover the turbulent statistics and Reynolds stresses. It is shown that the GNN model learns a zonal modeling strategy with distinct behaviors in the near-wall and outer regions. The proposed approach thus demonstrates the potential of GNNs for turbulence modeling, especially in the context of LES and RL.         ",
    "url": "https://arxiv.org/abs/2504.07741",
    "authors": [
      "Marius Kurz",
      "Andrea Beck",
      "Benjamin Sanderse"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07770",
    "title": "Sublevels in arrangements and the spherical arc crossing number of complete graphs",
    "abstract": "           Levels and sublevels in arrangements -- and, dually, $k$-sets and $(\\leq k)$-sets -- are fundamental notions in discrete and computational geometry and natural generalizations of convex polytopes, which correspond to the $0$-level. A long-standing conjecture of Eckhoff, Linhart, and Welzl, which would generalize McMullen's Upper Bound Theorem for polytopes and provide an exact refinement of asymptotic bounds by Clarkson, asserts that for all $k\\leq \\lfloor \\frac{n-d-2}{2}\\rfloor$, the number of $(\\leq k)$-sets of a set $S$ of $n$ points in $\\mathbf{R}^d$ is maximized if $S$ is the vertex set of a neighborly polytope. As a new tool for studying this conjecture and related problems, we introduce the $g$-matrix, which generalizes both the $g$-vector of a simple polytope and a Gale dual version of the $g$-vector studied by Lee and Welzl. Our main result is that the $g$-matrix of every vector configuration in $\\mathbf{R}^3$ is non-negative, which implies the Eckhoff--Linhart--Welzl conjecture in the case where $d=n-4$. As a corollary, we obtain the following result about crossing numbers: Consider a configuration $V\\subset S^2 \\subset \\mathbf{R}^3$ of $n$ unit vectors, and connect every pair of vectors by the unique shortest geodesic arc between them in the unit sphere $S^2$. This yields a drawing of the complete graph $K_n$ in $S^2$, which we call a spherical arc drawing. Complementing previous results for rectilinear drawings, we show that the number of crossings in any spherical arc drawing of $K_n$ is at least $\\frac{1}{4}\\lfloor \\frac{n}{2}\\rfloor \\lfloor \\frac{n-1}{2}\\rfloor \\lfloor \\frac{n-2}{2}\\rfloor \\lfloor \\frac{n-3}{2}\\rfloor$, which equals the conjectured value of the crossing number of $K_n$. Moreover, the lower bound is attained if $V$ is coneighborly, i.e., if every open linear halfspace contains at least $\\lfloor (n-2)/2 \\rfloor$ of the vectors in $V$.         ",
    "url": "https://arxiv.org/abs/2504.07770",
    "authors": [
      "Elizaveta Streltsova",
      "Uli Wagner"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2504.07775",
    "title": "Focal Cortical Dysplasia Type II Detection Using Cross Modality Transfer Learning and Grad-CAM in 3D-CNNs for MRI Analysis",
    "abstract": "           Focal cortical dysplasia (FCD) type II is a major cause of drug-resistant epilepsy, often curable only by surgery. Despite its clinical importance, the diagnosis of FCD is very difficult in MRI because of subtle abnormalities, leading to misdiagnosis. This study investigates the use of 3D convolutional neural networks (3D-CNNs) for FCD detection, using a dataset of 170 subjects (85 FCD patients and 85 controls) composed of T1-weighted and FLAIR MRI scans. In particular, it investigates the benefits obtained from cross-modality transfer learning and explainable artificial intelligence (XAI) techniques, in particular Gradient-weighted Class Activation Mapping (Grad-CAM). ResNet architectures (ResNet-18, -34, and -50) were implemented, employing transfer learning strategies that used pre-trained weights from segmentation tasks. Results indicate that transfer learning significantly enhances classification accuracy (up to 80.3%) and interpretability, as measured by a novel Heat-Score metric, which evaluates the model's focus on clinically relevant regions. Improvements in the Heat-Score metric underscore the model's seizure zone localization capabilities, bringing AI predictions and clinical insights closer together. These results highlight the importance of transfer learning, including cross-modality, and XAI in advancing AI-based medical diagnostics, especially for difficult-to-diagnose pathologies such as FCD.         ",
    "url": "https://arxiv.org/abs/2504.07775",
    "authors": [
      "Lorenzo Lasagni",
      "Antonio Ciccarone",
      "Renzo Guerrini",
      "Matteo Lenge",
      "Ludovico D'incerti"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2504.07777",
    "title": "Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of Experts and Physical-Inspired Neural Network",
    "abstract": "           Fast moving celestial objects are characterized by velocities across the celestial sphere that significantly differ from the motions of background stars. In observational images, these objects exhibit distinct shapes, contrasting with the typical appearances of stars. Depending on the observational method employed, these celestial entities may be designated as near-Earth objects or asteroids. Historically, fast moving celestial objects have been observed using ground-based telescopes, where the relative stability of stars and Earth facilitated effective image differencing techniques alongside traditional fast moving celestial object detection and classification algorithms. However, the growing prevalence of space-based telescopes, along with their diverse observational modes, produces images with different properties, rendering conventional methods less effective. This paper presents a novel algorithm for detecting fast moving celestial objects within star fields. Our approach enhances state-of-the-art fast moving celestial object detection neural networks by transforming them into physical-inspired neural networks. These neural networks leverage the point spread function of the telescope and the specific observational mode as prior information; they can directly identify moving fast moving celestial objects within star fields without requiring additional training, thereby addressing the limitations of traditional techniques. Additionally, all neural networks are integrated using the mixture of experts technique, forming a comprehensive fast moving celestial object detection algorithm. We have evaluated our algorithm using simulated observational data that mimics various observations carried out by space based telescope scenarios and real observation images. Results demonstrate that our method effectively detects fast moving celestial objects across different observational modes.         ",
    "url": "https://arxiv.org/abs/2504.07777",
    "authors": [
      "Peng Jia",
      "Ge Li",
      "Bafeng Cheng",
      "Yushan Li",
      "Rongyu Sun"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2504.07875",
    "title": "QubitHammer Attacks: Qubit Flipping Attacks in Multi-tenant Superconducting Quantum Computers",
    "abstract": "           Quantum computing is rapidly evolving its capabilities, with a corresponding surge in its deployment within cloud-based environments. Various quantum computers are accessible today via pay-as-you-go cloud computing models, offering unprecedented convenience. Due to its rapidly growing demand, quantum computers are shifting from a single-tenant to a multi-tenant model to enhance resource utilization. However, this widespread accessibility to shared multi-tenant systems also introduces potential security vulnerabilities. In this work, we present for the first time a set of novel attacks, named together as the QubitHammer attacks, which target state-of-the-art superconducting quantum computers. We show that in a multi-tenant cloud-based quantum system, an adversary with the basic capability to deploy custom pulses, similar to any standard user today, can utilize the QubitHammer attacks to significantly degrade the fidelity of victim circuits located on the same quantum computer. Upon extensive evaluation, the QubitHammer attacks achieve a very high variational distance of up to 0.938 from the expected outcome, thus demonstrating their potential to degrade victim computation. Our findings exhibit the effectiveness of these attacks across various superconducting quantum computers from a leading vendor, suggesting that QubitHammer represents a new class of security attacks. Further, the attacks are demonstrated to bypass all existing defenses proposed so far for ensuring the reliability in multi-tenant superconducting quantum computers.         ",
    "url": "https://arxiv.org/abs/2504.07875",
    "authors": [
      "Yizhuo Tan",
      "Navnil Choudhury",
      "Kanad Basu",
      "Jakub Szefer"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2504.07904",
    "title": "The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound",
    "abstract": "           Data augmentation is a central component of joint embedding self-supervised learning (SSL). Approaches that work for natural images may not always be effective in medical imaging tasks. This study systematically investigated the impact of data augmentation and preprocessing strategies in SSL for lung ultrasound. Three data augmentation pipelines were assessed: (1) a baseline pipeline commonly used across imaging domains, (2) a novel semantic-preserving pipeline designed for ultrasound, and (3) a distilled set of the most effective transformations from both pipelines. Pretrained models were evaluated on multiple classification tasks: B-line detection, pleural effusion detection, and COVID-19 classification. Experiments revealed that semantics-preserving data augmentation resulted in the greatest performance for COVID-19 classification - a diagnostic task requiring global image context. Cropping-based methods yielded the greatest performance on the B-line and pleural effusion object classification tasks, which require strong local pattern recognition. Lastly, semantics-preserving ultrasound image preprocessing resulted in increased downstream performance for multiple tasks. Guidance regarding data augmentation and preprocessing strategies was synthesized for practitioners working with SSL in ultrasound.         ",
    "url": "https://arxiv.org/abs/2504.07904",
    "authors": [
      "Blake VanBerlo",
      "Alexander Wong",
      "Jesse Hoey",
      "Robert Arntfield"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.07923",
    "title": "Trading Graph Neural Network",
    "abstract": "           This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN) that can structurally estimate the impact of asset features, dealer features and relationship features on asset prices in trading networks. It combines the strength of the traditional simulated method of moments (SMM) and recent machine learning techniques -- Graph Neural Network (GNN). It outperforms existing reduced-form methods with network centrality measures in prediction accuracy. The method can be used on networks with any structure, allowing for heterogeneity among both traders and assets.         ",
    "url": "https://arxiv.org/abs/2504.07923",
    "authors": [
      "Xian Wu"
    ],
    "subjectives": [
      "Trading and Market Microstructure (q-fin.TR)",
      "Machine Learning (cs.LG)",
      "General Economics (econ.GN)",
      "Pricing of Securities (q-fin.PR)"
    ]
  },
  {
    "id": "arXiv:1904.03979",
    "title": "Collaborative Spectrum Sharing for Hybrid Satellite-Terrestrial Networks with Large-Scale CSI",
    "abstract": "           Satellites and terrestrial cellular networks can be integrated together for extended broadband coverage in e.g., maritime communication scenarios. The co-channel interference (CCI) is a challenging issue for spectrum sharing between satellites and terrestrial networks. Different from previous studies that adopt full channel state information (CSI) or CSI with Gaussian estimation errors for CCI mitigation, we consider a more practical case with only slowly-varying large-scale CSI to facilitate overhead reduction. A joint power and channel allocation scheme is proposed for the terrestrial system, under the constraint of leakage interference to satellite mobile terminals (MTs). The proposed scheme provides near-optimal performance according to both theoretical analysis and simulation results.         ",
    "url": "https://arxiv.org/abs/1904.03979",
    "authors": [
      "Yanmin Wang",
      "Wei Feng",
      "Ming Xiao",
      "Cheng-Xiang Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2204.07661",
    "title": "Finding Pareto Trade-offs in Fair and Accurate Detection of Toxic Speech",
    "abstract": "           Optimizing NLP models for fairness poses many challenges. Lack of differentiable fairness measures prevents gradient-based loss training or requires surrogate losses that diverge from the true metric of interest. In addition, competing objectives (e.g., accuracy vs. fairness) often require making trade-offs based on stakeholder preferences, but stakeholders may not know their preferences before seeing system performance under different trade-off settings. To address these challenges, we begin by formulating a differentiable version of a popular fairness measure, Accuracy Parity, to provide balanced accuracy across demographic groups. Next, we show how model-agnostic, HyperNetwork optimization can efficiently train arbitrary NLP model architectures to learn Pareto-optimal trade-offs between competing metrics. Focusing on the task of toxic language detection, we show the generality and efficacy of our methods across two datasets, three neural architectures, and three fairness losses.         ",
    "url": "https://arxiv.org/abs/2204.07661",
    "authors": [
      "Soumyajit Gupta",
      "Venelin Kovatchev",
      "Anubrata Das",
      "Maria De-Arteaga",
      "Matthew Lease"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2302.07425",
    "title": "Bandit Social Learning: Exploration under Myopic Behavior",
    "abstract": "           We study social learning dynamics motivated by reviews on online platforms. The agents collectively follow a simple multi-armed bandit protocol, but each agent acts myopically, without regards to exploration. We allow the greedy (exploitation-only) algorithm, as well as a wide range of behavioral biases. Specifically, we allow myopic behaviors that are consistent with (parameterized) confidence intervals for the arms' expected rewards. We derive stark learning failures for any such behavior, and provide matching positive results. The learning-failure results extend to Bayesian agents and Bayesian bandit environments. In particular, we obtain general, quantitatively strong results on failure of the greedy bandit algorithm, both for ``frequentist\" and ``Bayesian\" versions. Failure results known previously are quantitatively weak, and either trivial or very specialized. Thus, we provide a theoretical foundation for designing non-trivial bandit algorithms, \\ie algorithms that intentionally explore, which has been missing from the literature. Our general behavioral model can be interpreted as agents' optimism or pessimism. The matching positive results entail a maximal allowed amount of optimism. Moreover, we find that no amount of pessimism helps against the learning failures, whereas even a small-but-constant fraction of extreme optimists avoids the failures and leads to near-optimal regret rates.         ",
    "url": "https://arxiv.org/abs/2302.07425",
    "authors": [
      "Kiarash Banihashem",
      "MohammadTaghi Hajiaghayi",
      "Suho Shin",
      "Aleksandrs Slivkins"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.00508",
    "title": "Geometry and Local Recovery of Global Minima of Two-layer Neural Networks at Overparameterization",
    "abstract": "           Under mild assumptions, we investigate the geometry of the loss landscape for two-layer neural networks in the vicinity of global minima. Utilizing novel techniques, we demonstrate: (i) how global minima with zero generalization error become geometrically separated from other global minima as the sample size grows; and (ii) the local convergence properties and rate of gradient flow dynamics. Our results indicate that two-layer neural networks can be locally recovered in the regime of overparameterization.         ",
    "url": "https://arxiv.org/abs/2309.00508",
    "authors": [
      "Leyang Zhang",
      "Yaoyu Zhang",
      "Tao Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2310.07056",
    "title": "TextPSG: Panoptic Scene Graph Generation from Textual Descriptions",
    "abstract": "           Panoptic Scene Graph has recently been proposed for comprehensive scene understanding. However, previous works adopt a fully-supervised learning manner, requiring large amounts of pixel-wise densely-annotated data, which is always tedious and expensive to obtain. To address this limitation, we study a new problem of Panoptic Scene Graph Generation from Purely Textual Descriptions (Caption-to-PSG). The key idea is to leverage the large collection of free image-caption data on the Web alone to generate panoptic scene graphs. The problem is very challenging for three constraints: 1) no location priors; 2) no explicit links between visual regions and textual entities; and 3) no pre-defined concept sets. To tackle this problem, we propose a new framework TextPSG consisting of four modules, i.e., a region grouper, an entity grounder, a segment merger, and a label generator, with several novel techniques. The region grouper first groups image pixels into different segments and the entity grounder then aligns visual segments with language entities based on the textual description of the segment being referred to. The grounding results can thus serve as pseudo labels enabling the segment merger to learn the segment similarity as well as guiding the label generator to learn object semantics and relation predicates, resulting in a fine-grained structured scene understanding. Our framework is effective, significantly outperforming the baselines and achieving strong out-of-distribution robustness. We perform comprehensive ablation studies to corroborate the effectiveness of our design choices and provide an in-depth analysis to highlight future directions. Our code, data, and results are available on our project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2310.07056",
    "authors": [
      "Chengyang Zhao",
      "Yikang Shen",
      "Zhenfang Chen",
      "Mingyu Ding",
      "Chuang Gan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2402.06038",
    "title": "Understanding Contrastive Representation Learning from Positive Unlabeled (PU) Data",
    "abstract": "           Pretext Invariant Representation Learning (PIRL) followed by Supervised Fine-Tuning (SFT) has become a standard paradigm for learning with limited labels. We extend this approach to the Positive Unlabeled (PU) setting, where only a small set of labeled positives and a large unlabeled pool -- containing both positives and negatives are available. We study this problem under two regimes: (i) without access to the class prior, and (ii) when the prior is known or can be estimated. We introduce Positive Unlabeled Contrastive Learning (puCL), an unbiased and variance reducing contrastive objective that integrates weak supervision from labeled positives judiciously into the contrastive loss. When the class prior is known, we propose Positive Unlabeled InfoNCE (puNCE), a prior-aware extension that re-weights unlabeled samples as soft positive negative mixtures. For downstream classification, we develop a pseudo-labeling algorithm that leverages the structure of the learned embedding space via PU aware clustering. Our framework is supported by theory; offering bias-variance analysis, convergence insights, and generalization guarantees via augmentation concentration; and validated empirically across standard PU benchmarks, where it consistently outperforms existing methods, particularly in low-supervision regimes.         ",
    "url": "https://arxiv.org/abs/2402.06038",
    "authors": [
      "Anish Acharya",
      "Li Jing",
      "Bhargav Bhushanam",
      "Dhruv Choudhary",
      "Michael Rabbat",
      "Sujay Sanghavi",
      "Inderjit S Dhillon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.12949",
    "title": "A Novel Energy-Efficient Cross-Layer Design for Scheduling and Routing in 6TiSCH Networks",
    "abstract": "           The 6TiSCH protocol stack plays a vital role in enabling reliable and energy-efficient communications for the Industrial Internet of Things (IIoT). However, it faces challenges, including prolonged network formation, inefficient parent switching, high control packet overhead, and suboptimal resource utilization. To tackle these issues, we propose in this paper a novel cross-layer optimization framework aiming to enhance the coordination between the Scheduling Function (SF), the Routing Protocol for Low-Power and Lossy Networks (RPL), and queue management. Our solution introduces a slot-aware parent switching mechanism, early slot reservation to mitigate queue overflow, and a refined slot locking strategy to improve slot availability. To reduce control overhead, the proposed method merges 6P cell reservation information into RPL control packets (DIO/DAO), thus minimizing control exchanges during parent switching and node joining. Optimized slot selection further reduces latency and jitter. Through extensive simulations on the 6TiSCH simulator and under varying network densities and traffic loads, we demonstrate significant improvements over the standard 6TiSCH benchmark in terms of traffic load, joining time, latency, and energy efficiency. These enhancements make the proposed solution suitable for time-sensitive IIoT applications.         ",
    "url": "https://arxiv.org/abs/2403.12949",
    "authors": [
      "Ahlam Hannachi",
      "Wael Jaafar",
      "Salim Bitam",
      "Nabil Ouazene"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2404.05297",
    "title": "Automated Attack Synthesis for Constant Product Market Makers",
    "abstract": "           Decentralized Finance (DeFi) enables many novel applications that were impossible in traditional finances. However, it also introduces new types of vulnerabilities. An example of such vulnerabilities is a composability bug between token contracts and Decentralized Exchange (DEX) that follows the Constant Product Market Maker (CPMM) model. This type of bug, which we refer to as CPMM composability bug, originates from issues in token contracts that make them incompatible with CPMMs, thereby endangering other tokens within the CPMM ecosystem. Since 2022, 23 exploits of such kind have resulted in a total loss of 2.2M USD. BlockSec, a smart contract auditing company, reported that 138 exploits of such kind occurred just in February 2023. In this paper, we propose CPMMX , a tool that automatically detects CPMM composability bugs across entire blockchains. To achieve such scalability, we first formalized CPMM composability bugs and found that these bugs can be induced by breaking two safety invariants. Based on this finding, we designed CPMMX equipped with a two-step approach, called shallow-then-deep search. In more detail, it first uses shallow search to find transactions that break the invariants. Then, it uses deep search to refine these transactions, making them profitable for the attacker. We evaluated CPMMX against five baselines on two public datasets and one synthetic dataset. In our evaluation, CPMMX detected 2.5x to 1.5x more vulnerabilities compared to baseline methods. It also analyzed contracts significantly faster, achieving higher F1 scores than the baselines. Additionally, we applied CPMMX to all contracts on the latest blocks of the Ethereum and Binance networks and discovered 26 new exploits that can result in 15.7K USD profit in total.         ",
    "url": "https://arxiv.org/abs/2404.05297",
    "authors": [
      "Sujin Han",
      "Jinseo Kim",
      "Sung-Ju Lee",
      "Insu Yun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2404.10702",
    "title": "Retrieval Augmented Verification for Zero-Shot Detection of Multimodal Disinformation",
    "abstract": "           The rise of disinformation on social media, especially through the strategic manipulation or repurposing of images, paired with provocative text, presents a complex challenge for traditional fact-checking methods. In this paper, we introduce a novel zero-shot approach to identify and interpret such multimodal disinformation, leveraging real-time evidence from credible sources. Our framework goes beyond simple true-or-false classifications by analyzing both the textual and visual components of social media claims in a structured, interpretable manner. By constructing a graph-based representation of entities and relationships within the claim, combined with pretrained visual features, our system automatically retrieves and matches external evidence to identify inconsistencies. Unlike traditional models dependent on labeled datasets, our method empowers users with transparency, illuminating exactly which aspects of the claim hold up to scrutiny and which do not. Our framework achieves competitive performance with state-of-the-art methods while offering enhanced explainability.         ",
    "url": "https://arxiv.org/abs/2404.10702",
    "authors": [
      "Arka Ujjal Dey",
      "Artemis Llabr\u00e9s",
      "Ernest Valveny",
      "Dimosthenis Karatzas"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2405.01814",
    "title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
    "abstract": "           Transformer-based large language models (LLMs) exhibit impressive performance in generative tasks but also introduce significant challenges in real-world serving due to inefficient use of the expensive, computation-optimized accelerators. Although disaggregated serving architectures have been proposed to split different phases of LLM inference, the efficiency of decoding phase is still low. This is caused by the varying resource demands of different operators in the transformer-based LLMs. Specifically, the attention operator is memory-intensive, exhibiting a memory access pattern that clashes with the strengths of modern accelerators, especially for long context requests. To enhance the efficiency of LLM decoding, we introduce model-attention disaggregation. This approach leverages a collection of cheap, memory-optimized devices for the attention operator while still utilizing high-end accelerators for other parts of the model. This heterogeneous setup ensures that each component is tailored to its specific workload, maximizing overall performance and cost efficiency. Our comprehensive analysis and experiments confirm the viability of splitting the attention computation over multiple devices. Also, the communication bandwidth required between heterogeneous devices proves to be manageable with prevalent networking technologies. To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. Experimental results indicate that Lamina can provide 16.1 ~ 90.1% higher estimated throughput than existing solutions with similar costs.         ",
    "url": "https://arxiv.org/abs/2405.01814",
    "authors": [
      "Shaoyuan Chen",
      "Wencong Xiao",
      "Yutong Lin",
      "Mingxing Zhang",
      "Yingdi Shan",
      "Jinlei Jiang",
      "Kang Chen",
      "Yongwei Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2405.16924",
    "title": "Demystifying amortized causal discovery with transformers",
    "abstract": "           Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA (Ke et al., 2023), a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.         ",
    "url": "https://arxiv.org/abs/2405.16924",
    "authors": [
      "Francesco Montagna",
      "Max Cairney-Leeming",
      "Dhanya Sridhar",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.11835",
    "title": "OoDIS: Anomaly Instance Segmentation and Detection Benchmark",
    "abstract": "           Safe navigation of self-driving cars and robots requires a precise understanding of their environment. Training data for perception systems cannot cover the wide variety of objects that may appear during deployment. Thus, reliable identification of unknown objects, such as wild animals and untypical obstacles, is critical due to their potential to cause serious accidents. Significant progress in semantic segmentation of anomalies has been facilitated by the availability of out-of-distribution (OOD) benchmarks. However, a comprehensive understanding of scene dynamics requires the segmentation of individual objects, and thus the segmentation of instances is essential. Development in this area has been lagging, largely due to the lack of dedicated benchmarks. The situation is similar in object detection. While there is interest in detecting and potentially tracking every anomalous object, the availability of dedicated benchmarks is clearly limited. To address this gap, this work extends some commonly used anomaly segmentation benchmarks to include the instance segmentation and object detection tasks. Our evaluation of anomaly instance segmentation and object detection methods shows that both of these challenges remain unsolved problems. We provide a competition and benchmark website under this https URL ",
    "url": "https://arxiv.org/abs/2406.11835",
    "authors": [
      "Alexey Nekrasov",
      "Rui Zhou",
      "Miriam Ackermann",
      "Alexander Hermans",
      "Bastian Leibe",
      "Matthias Rottmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.12816",
    "title": "Neural Approximate Mirror Maps for Constrained Diffusion Models",
    "abstract": "           Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number of objects). When the training data all satisfy a certain constraint, enforcing this constraint on a diffusion model makes it more reliable for generating valid synthetic data and solving constrained inverse problems. However, existing methods for constrained diffusion models are restricted in the constraints they can handle. For instance, recent work proposed to learn mirror diffusion models (MDMs), but analytical mirror maps only exist for convex constraints and can be challenging to derive. We propose neural approximate mirror maps (NAMMs) for general, possibly non-convex constraints. Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that transforms data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set. A generative model, such as an MDM, can then be trained in the learned mirror space and its samples restored to the constraint set by the inverse map. We validate our approach on a variety of constraints, showing that compared to an unconstrained diffusion model, a NAMM-based MDM substantially improves constraint satisfaction. We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems.         ",
    "url": "https://arxiv.org/abs/2406.12816",
    "authors": [
      "Berthy T. Feng",
      "Ricardo Baptista",
      "Katherine L. Bouman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2407.04752",
    "title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking",
    "abstract": "           Recent advancements in large language models (LLMs) with billions of parameters have improved performance in various applications, but their inference processes demand significant energy and computational resources. In contrast, the human brain, with approximately 86 billion neurons, is much more energy-efficient than LLMs with similar parameters. Inspired by this, we redesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training efficiency: Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$ to $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to divide outlier channels and allocate different $T$ for GIF neurons, which further compresses spike length to approximate $log_2T$ bits. The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2 perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear layers, significantly exceeding PB-LLMs.         ",
    "url": "https://arxiv.org/abs/2407.04752",
    "authors": [
      "Xingrun Xing",
      "Boyan Gao",
      "Zheng Zhang",
      "David A. Clifton",
      "Shitao Xiao",
      "Li Du",
      "Guoqi Li",
      "Jiajun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2408.11002",
    "title": "On the Cop Number of String Graphs",
    "abstract": "           Cops and Robber is a well-studied two-player pursuit-evasion game played on a graph, where a group of cops tries to capture the robber. The \\emph{cop number} of a graph is the minimum number of cops required to capture the robber. Gaven\u010diak et al.~[Eur. J. of Comb. 72, 45--69 (2018)] studied the game on intersection graphs and established that the cop number for the class of string graphs is at most 15, and asked as an open question to improve this bound for string graphs and subclasses of string graphs. We address this question and establish that the cop number of a string graph is at most 13. To this end, we develop a novel \\textit{guarding} technique. We further establish that this technique can be useful for other Cops and Robber games on graphs admitting a representation. In particular, we show that four cops have a winning strategy for a variant of Cops and Robber, named Fully Active Cops and Robber, on planar graphs, addressing an open question of Gromovikov et al.~[Austr. J. Comb. 76(2), 248--265 (2020)]. In passing, we also improve the known bounds on the cop number of boxicity 2 graphs. Finally, as a corollary of our result on the cop number of string graphs, we establish that the chromatic number of string graphs with girth at least $5$ is at most $14$.         ",
    "url": "https://arxiv.org/abs/2408.11002",
    "authors": [
      "Sandip Das",
      "Harmender Gahlawat"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2408.16357",
    "title": "Law of Vision Representation in MLLMs",
    "abstract": "           We present the \"Law of Vision Representation\" in multimodal large language models (MLLMs). It reveals a strong correlation between the combination of cross-modal alignment, correspondence in vision representation, and MLLM performance. We quantify the two factors using the cross-modal Alignment and Correspondence score (AC score). Through extensive experiments involving thirteen different vision representation settings and evaluations across eight benchmarks, we find that the AC score is linearly correlated to model performance. By leveraging this relationship, we are able to identify and train the optimal vision representation only, which does not require finetuning the language model every time, resulting in a 99.7% reduction in computational cost.         ",
    "url": "https://arxiv.org/abs/2408.16357",
    "authors": [
      "Shijia Yang",
      "Bohan Zhai",
      "Quanzeng You",
      "Jianbo Yuan",
      "Hongxia Yang",
      "Chenfeng Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.10365",
    "title": "Robust image representations with counterfactual contrastive learning",
    "abstract": "           Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive pairs. Positive contrastive pairs should preserve semantic meaning while discarding unwanted variations related to the data acquisition domain. Traditional contrastive pipelines attempt to simulate domain shifts through pre-defined generic image transformations. However, these do not always mimic realistic and relevant domain variations for medical imaging, such as scanner differences. To tackle this issue, we herein introduce counterfactual contrastive learning, a novel framework leveraging recent advances in causal image synthesis to create contrastive positive pairs that faithfully capture relevant domain variations. Our method, evaluated across five datasets encompassing both chest radiography and mammography data, for two established contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive learning in terms of robustness to acquisition shift. Notably, counterfactual contrastive learning achieves superior downstream performance on both in-distribution and external datasets, especially for images acquired with scanners under-represented in the training set. Further experiments show that the proposed framework extends beyond acquisition shifts, with models trained with counterfactual contrastive learning reducing subgroup disparities across biological sex.         ",
    "url": "https://arxiv.org/abs/2409.10365",
    "authors": [
      "M\u00e9lanie Roschewitz",
      "Fabio De Sousa Ribeiro",
      "Tian Xia",
      "Galvin Khara",
      "Ben Glocker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.18025",
    "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
    "abstract": "           Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training.         ",
    "url": "https://arxiv.org/abs/2409.18025",
    "authors": [
      "Jakub \u0141ucki",
      "Boyi Wei",
      "Yangsibo Huang",
      "Peter Henderson",
      "Florian Tram\u00e8r",
      "Javier Rando"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.02113",
    "title": "Mamba Neural Operator: Who Wins? Transformers vs. State-Space Models for PDEs",
    "abstract": "           Partial differential equations (PDEs) are widely used to model complex physical systems, but solving them efficiently remains a significant challenge. Recently, Transformers have emerged as the preferred architecture for PDEs due to their ability to capture intricate dependencies. However, they struggle with representing continuous dynamics and long-range interactions. To overcome these limitations, we introduce the Mamba Neural Operator (MNO), a novel framework that enhances neural operator-based techniques for solving PDEs. MNO establishes a formal theoretical connection between structured state-space models (SSMs) and neural operators, offering a unified structure that can adapt to diverse architectures, including Transformer-based models. By leveraging the structured design of SSMs, MNO captures long-range dependencies and continuous dynamics more effectively than traditional Transformers. Through extensive analysis, we show that MNO significantly boosts the expressive power and accuracy of neural operators, making it not just a complement but a superior framework for PDE-related tasks, bridging the gap between efficient representation and accurate solution approximation.         ",
    "url": "https://arxiv.org/abs/2410.02113",
    "authors": [
      "Chun-Wun Cheng",
      "Jiahao Huang",
      "Yi Zhang",
      "Guang Yang",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Angelica I Aviles-Rivero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2410.04639",
    "title": "Radial Basis Operator Networks",
    "abstract": "           Operator networks are designed to approximate nonlinear operators, which provide mappings between infinite-dimensional spaces such as function spaces. These networks are playing an increasingly important role in machine learning, with their most notable contributions in the field of scientific computing. Their significance stems from their ability to handle the type of data often encountered in scientific applications. For instance, in climate modeling or fluid dynamics, input data typically consists of discretized continuous fields (like temperature distributions or velocity fields). We introduce the radial basis operator network (RBON), which represents a significant advancement as the first operator network capable of learning an operator in both the time domain and frequency domain when adjusted to accept complex-valued inputs. Despite the small, single hidden-layer structure, the RBON boasts small $L^2$ relative test error for both in- and out-of-distribution data (OOD) of less than $1\\times 10^{-7}$ in some benchmark cases. Moreover, the RBON maintains small error on OOD data from entirely different function classes from the training data.         ",
    "url": "https://arxiv.org/abs/2410.04639",
    "authors": [
      "Jason Kurz",
      "Sean Oughton",
      "Shitao Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.11741",
    "title": "POLO -- Point-based, multi-class animal detection",
    "abstract": "           Automated wildlife surveys based on drone imagery and object detection technology are a powerful and increasingly popular tool in conservation biology. Most detectors require training images with annotated bounding boxes, which are tedious, expensive, and not always unambiguous to create. To reduce the annotation load associated with this practice, we develop POLO, a multi-class object detection model that can be trained entirely on point labels. POLO is based on simple, yet effective modifications to the YOLOv8 architecture, including alterations to the prediction process, training losses, and post-processing. We test POLO on drone recordings of waterfowl containing up to multiple thousands of individual birds in one image and compare it to a regular YOLOv8. Our experiments show that at the same annotation cost, POLO achieves improved accuracy in counting animals in aerial imagery.         ",
    "url": "https://arxiv.org/abs/2410.11741",
    "authors": [
      "Giacomo May",
      "Emanuele Dalsasso",
      "Benjamin Kellenberger",
      "Devis Tuia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2410.13453",
    "title": "Adaptive Augmentation Policy Optimization with LLM Feedback",
    "abstract": "           Data augmentation is a critical component of deep learning pipelines, enhancing model generalization by increasing dataset diversity. Traditional augmentation strategies rely on manually designed transformations, stochastic sampling, or automated search-based approaches. Although automated methods improve performance, they often require extensive computational resources and are tailored to specific datasets. In this work, we propose a Large Language Model (LLM)-guided augmentation optimization strategy that refines augmentation policies based on model performance feedback. We introduce two approaches: (1) LLM-Guided Augmentation Policy Optimization, where augmentation policies are selected by an LLM prior to training and iteratively refined across multiple training cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization, where policies adapt in real-time based on performance metrics. This in-training approach eliminates the need for full model retraining before receiving LLM feedback, thereby reducing computational costs while improving performance. Our methodology employs an LLM to dynamically select augmentation transformations based on dataset characteristics, model architecture, and prior training outcomes. Unlike traditional search-based methods, our approach leverages the contextual knowledge of LLMs, particularly in specialized domains like medical imaging, to recommend augmentation strategies tailored to domain-specific data. We evaluate our approach on multiple domain-specific image classification datasets where augmentation is key to model robustness. Results show that LLM-guided augmentation optimization outperforms traditional methods, improving model accuracy. These findings highlight the potential of LLMs in automating and adapting deep learning training workflows.         ",
    "url": "https://arxiv.org/abs/2410.13453",
    "authors": [
      "Ant Duru",
      "Alptekin Temizel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.04548",
    "title": "Convergence and Robustness of Value and Policy Iteration for the Linear Quadratic Regulator",
    "abstract": "           This paper revisits and extends the convergence and robustness properties of value and policy iteration algorithms for discrete-time linear quadratic regulator problems. In the model-based case, we extend current results concerning the region of exponential convergence of both algorithms. In the case where there is uncertainty on the value of the system matrices, we provide input-to-state stability results capturing the effect of model parameter uncertainties. Our findings offer new insights into these algorithms at the heart of several approximate dynamic programming schemes, highlighting their convergence and robustness behaviors. Numerical examples illustrate the significance of some of the theoretical results.         ",
    "url": "https://arxiv.org/abs/2411.04548",
    "authors": [
      "Bowen Song",
      "Chenxuan Wu",
      "Andrea Iannelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2411.10034",
    "title": "EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations",
    "abstract": "           Vibrometry-based side channels pose a significant privacy risk, exploiting sensors like mmWave radars, light sensors, and accelerometers to detect vibrations from sound sources or proximate objects, enabling speech eavesdropping. Despite various proposed defenses, these involve costly hardware solutions with inherent physical limitations. This paper presents EveGuard, a software-driven defense framework that creates adversarial audio, protecting voice privacy from side channels without compromising human perception. We leverage the distinct sensing capabilities of side channels and traditional microphones, where side channels capture vibrations and microphones record changes in air pressure, resulting in different frequency responses. EveGuard first proposes a perturbation generator model (PGM) that effectively suppresses sensor-based eavesdropping while maintaining high audio quality. Second, to enable end-to-end training of PGM, we introduce a new domain translation task called Eve-GAN for inferring an eavesdropped signal from a given audio. We further apply few-shot learning to mitigate the data collection overhead for Eve-GAN training. Our extensive experiments show that EveGuard achieves a protection rate of more than 97 percent from audio classifiers and significantly hinders eavesdropped audio reconstruction. We further validate the performance of EveGuard across three adaptive attack mechanisms. We have conducted a user study to verify the perceptual quality of our perturbed audio.         ",
    "url": "https://arxiv.org/abs/2411.10034",
    "authors": [
      "Jung-Woo Chang",
      "Ke Sun",
      "David Xia",
      "Xinyu Zhang",
      "Farinaz Koushanfar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2411.10868",
    "title": "Destabilizing a Social Network Model via Intrinsic Feedback Vulnerabilities",
    "abstract": "           Social influence plays a significant role in shaping individual sentiments and actions, particularly in a world of ubiquitous digital interconnection. The rapid development of generative AI has engendered well-founded concerns regarding the potential scalable implementation of radicalization techniques in social media. Motivated by these developments, we present a case study investigating the effects of small but intentional perturbations on a simple social network. We employ Taylor's classic model of social influence and tools from robust control theory (most notably the Dynamical Structure Function (DSF)), to identify perturbations that qualitatively alter the system's behavior while remaining as unobtrusive as possible. We examine two such scenarios: perturbations to an existing link and perturbations that introduce a new link to the network. In each case, we identify destabilizing perturbations of minimal norm and simulate their effects. Remarkably, we find that small but targeted alterations to network structure may lead to the radicalization of all agents, exhibiting the potential for large-scale shifts in collective behavior to be triggered by comparatively minuscule adjustments in social influence. Given that this method of identifying perturbations that are innocuous yet destabilizing applies to any suitable dynamical system, our findings emphasize a need for similar analyses to be carried out on real systems (e.g., real social networks), to identify the places where such dynamics may already exist.         ",
    "url": "https://arxiv.org/abs/2411.10868",
    "authors": [
      "Lane H. Rogers",
      "Emma J. Reid",
      "Robert A. Bridges"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2412.04042",
    "title": "Recognizing 2-Layer and Outer $k$-Planar Graphs",
    "abstract": "           The crossing number of a graph is the least number of crossings over all drawings of the graph in the plane. Computing the crossing number of a given graph is NP-hard, but fixed-parameter tractable (FPT) with respect to the natural parameter. Two well-known variants of the problem are 2-layer crossing minimization and circular crossing minimization, where every vertex must lie on one of two layers, namely two parallel lines, or a circle, respectively. Both variants are NP-hard, but FPT with respect to the natural parameter. Recently, a local version of the crossing number has also received considerable attention. A graph is $k$-planar if it admits a drawing with at most $k$ crossings per edge. In contrast to the crossing number, recognizing $k$-planar graphs is NP-hard even if $k=1$. In this paper, we consider the two above variants in the local setting. The $k$-planar graphs that admit a straight-line drawing with vertices on two layers or on a circle are called 2-layer $k$-planar and outer $k$-planar graphs, respectively. We study the parameterized complexity of the two recognition problems with respect to $k$. For $k=0$, both problems can easily be solved in linear time. Two groups independently showed that outer 1-planar graphs can also be recognized in linear time [Hong et al., Algorithmica 2015; Auer et al., Algorithmica 2016]. One group asked whether outer 2-planar graphs can be recognized in polynomial time. Our main contribution consists of XP-algorithms for recognizing 2-layer $k$-planar graphs and outer $k$-planar graphs. We complement these results by showing that both recognition problems are XNLP-hard. This implies that both problems are W$[t]$-hard for every $t$ and that it is unlikely that they admit FPT-algorithms. On the other hand, we present an FPT-algorithm for recognizing 2-layer $k$-planar graphs where the order of the vertices on one layer is specified.         ",
    "url": "https://arxiv.org/abs/2412.04042",
    "authors": [
      "Yasuaki Kobayashi",
      "Yuto Okada",
      "Alexander Wolff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2412.14602",
    "title": "Towards Scalable and Deep Graph Neural Networks via Noise Masking",
    "abstract": "           In recent years, Graph Neural Networks (GNNs) have achieved remarkable success in many graph mining tasks. However, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation and non-linear transformation during training. One commonly employed approach to address this challenge is model-simplification, which only executes the Propagation (P) once in the pre-processing, and Combine (C) these receptive fields in different ways and then feed them into a simple model for better performance. Despite their high predictive performance and scalability, these methods still face two limitations. First, existing approaches mainly focus on exploring different C methods from the model perspective, neglecting the crucial problem of performance degradation with increasing P depth from the data-centric perspective, known as the over-smoothing problem. Second, pre-processing overhead takes up most of the end-to-end processing time, especially for large-scale graphs. To address these limitations, we present random walk with noise masking (RMask), a plug-and-play module compatible with the existing model-simplification works. This module enables the exploration of deeper GNNs while preserving their scalability. Unlike the previous model-simplification works, we focus on continuous P and found that the noise existing inside each P is the cause of the over-smoothing issue, and use the efficient masking mechanism to eliminate them. Experimental results on six real-world datasets demonstrate that model-simplification works equipped with RMask yield superior performance compared to their original version and can make a good trade-off between accuracy and efficiency.         ",
    "url": "https://arxiv.org/abs/2412.14602",
    "authors": [
      "Yuxuan Liang",
      "Wentao Zhang",
      "Zeang Sheng",
      "Ling Yang",
      "Quanqing Xu",
      "Jiawei Jiang",
      "Yunhai Tong",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.20439",
    "title": "Image Augmentation Agent for Weakly Supervised Semantic Segmentation",
    "abstract": "           Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets.         ",
    "url": "https://arxiv.org/abs/2412.20439",
    "authors": [
      "Wangyu Wu",
      "Xianglin Qiu",
      "Siqi Song",
      "Zhenhong Chen",
      "Xiaowei Huang",
      "Fei Ma",
      "Jimin Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.04507",
    "title": "Effective Two-Stage Double Auction for Dynamic Resource Provision over Edge Networks via Discovering The Power of Overbooking",
    "abstract": "           To facilitate responsive and cost-effective computing resource scheduling and service delivery over edge-assisted mobile networks, this paper investigates a novel two-stage double auction methodology via utilizing an interesting idea of resource overbooking to overcome dynamic and uncertain nature from edge servers (sellers) and demand from mobile devices (as buyers). The proposed auction integrates multiple essential factors such as social welfare maximization and decision-making latency (e.g., the time for determining winning seller-buyer pairs) reduction, by introducing a stagewise strategy: an overbooking-driven pre-double auction (OPDAuction) for determining long-term cooperations between sellers and buyers before practical resource transactions as Stage I, and a real-time backup double auction (RBDAuction) for handling residual resource demands during actual transactions. In particular, by applying a proper overbooking rate, OPDAuction helps with facilitating trading contracts between appropriate sellers and buyers as guidance for future transactions, by allowing the booked resources to exceed supply. Then, since pre-auctions may cause risks, our RBDAuction adjusts to real-time market changes, further enhancing the overall social welfare. More importantly, we offer an interesting view to show that our proposed two-stage auction can support significant design properties such as truthfulness, individual rationality, and budget balance. Through extensive experiments, we demonstrate good performance in social welfare, time efficiency, and computational scalability, outstripping conventional methods in dynamic edge computing settings.         ",
    "url": "https://arxiv.org/abs/2501.04507",
    "authors": [
      "Sicheng Wu",
      "Minghui Liwang",
      "Deqing Wang",
      "Xianbin Wang",
      "Chao Wu",
      "Junyi Tang",
      "Li Li",
      "Xiaoyu Xia"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2501.05550",
    "title": "Emergent weight morphologies in deep neural networks",
    "abstract": "           Whether deep neural networks can exhibit emergent behaviour is not only relevant for understanding how deep learning works, it is also pivotal for estimating potential security risks of increasingly capable artificial intelligence systems. Here, we show that training deep neural networks gives rise to emergent weight morphologies independent of the training data. Specifically, in analogy to condensed matter physics, we derive a theory that predict that the homogeneous state of deep neural networks is unstable in a way that leads to the emergence of periodic channel structures. We verified these structures by performing numerical experiments on a variety of data sets. Our work demonstrates emergence in the training of deep neural networks, which impacts the achievable performance of deep neural networks.         ",
    "url": "https://arxiv.org/abs/2501.05550",
    "authors": [
      "Pascal de Jong",
      "Felix Meigel",
      "Steffen Rulands"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)"
    ]
  },
  {
    "id": "arXiv:2501.06465",
    "title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in Healthcare",
    "abstract": "           We introduce the world's first clinical terminology for the Chinese healthcare community, namely MedCT, accompanied by a clinical foundation model MedBERT and an entity linking model MedLink. The MedCT system enables standardized and programmable representation of Chinese clinical data, successively stimulating the development of new medicines, treatment pathways, and better patient outcomes for the populous Chinese community. Moreover, the MedCT knowledge graph provides a principled mechanism to minimize the hallucination problem of large language models (LLMs), therefore achieving significant levels of accuracy and safety in LLM-based clinical applications. By leveraging the LLMs' emergent capabilities of generativeness and expressiveness, we were able to rapidly built a production-quality terminology system and deployed to real-world clinical field within three months, while classical terminologies like SNOMED CT have gone through more than twenty years development. Our experiments show that the MedCT system achieves state-of-the-art (SOTA) performance in semantic matching and entity linking tasks, not only for Chinese but also for English. We also conducted a longitudinal field experiment by applying MedCT and LLMs in a representative spectrum of clinical tasks, including electronic health record (EHR) auto-generation and medical document search for diagnostic decision making. Our study shows a multitude of values of MedCT for clinical workflows and patient outcomes, especially in the new genre of clinical LLM applications. We present our approach in sufficient engineering detail, such that implementing a clinical terminology for other non-English societies should be readily reproducible. We openly release our terminology, models and algorithms, along with real-world clinical datasets for the development.         ",
    "url": "https://arxiv.org/abs/2501.06465",
    "authors": [
      "Ye Chen",
      "Dongdong Huang",
      "Haoyun Xu",
      "Cong Fu",
      "Lin Sheng",
      "Qingli Zhou",
      "Yuqiang Shen",
      "Kai Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.15576",
    "title": "First Real-Time Detection of Ambient Backscatters using Uplink Sounding Reference Signals of a Commercial 4G Smartphone",
    "abstract": "           Recently, cellular Ambient Backscattering has been proposed for cellular networks. Up to now an Ambient backscatter device, called zero-energy device or tag, broadcasted its message by backscattering ambient downlink waves from the closest Base Station (BS) according to a predefined pattern. A tag was detected by smartphones nearby. This paper presents, for the first time, a novel ambient backscatter communication system exploiting uplink ambient waves from smartphones instead of downlink waves. In this novel system, a BS connected to a smartphone monitors the uplink pilot signals and detects TAGs in proximity. The proposed system is implemented and tested with one prototype of TAG, a commercial off-the shelf 4G smartphone and a 4G Software Defined Radio (SDR) BS. Indoor and outdoor experiments were conducted to assess the proposed technique. These very preliminary experiments exhibit a promising potential. In indoor, a detection probability of more than 90% has been achieved without false alarm when the TAG was 3 meters from the UE, and the BS 20 meters away of them, behind walls and obstacles.         ",
    "url": "https://arxiv.org/abs/2501.15576",
    "authors": [
      "Ahmed ElSanhoury",
      "Islam Galal",
      "Khaled AlKady",
      "Aml ElKhodary",
      "Dinh-Thuy Phan-Huy",
      "Ayman M. Hassan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2501.18987",
    "title": "Better late, then? The hardness of choosing delays to meet passenger demands in temporal graphs",
    "abstract": "           In train networks, carefully-chosen delays may be beneficial for certain passengers, who would otherwise miss some connection. Given a simple (directed or undirected) temporal graph and a set of passengers (each specifying a starting vertex, an ending vertex, and a desired arrival time), we ask whether it is possible to delay some of the edges of the temporal graph to realize all the passengers' demands. We call this problem DelayBetter (DB), and study it along with two variants: in $\\delta$-DelayBetter, each delay must be of at most $\\delta$; in ($\\delta$-)Path DB, passengers also fully specify the vertices they should visit on their journey. On the positive side, we give a polynomial-time algorithm for Path DB and $\\delta$-Path DB, and obtain as a corollary a polynomial-time algorithm for DB and $\\delta$-DB on trees. We also provide an fpt algorithm for both problems parameterized by the size of the graph's Feedback Edge Set together with the number of passengers. On the negative side, we show NP-completeness of ($1$-)DB on bounded-degree temporal graphs even when the lifetime is $2$, and of ($10$-)DB on bounded-degree planar temporal graphs of lifetime $19$. Our results complement previous work studying reachability problems in temporal graphs with delaying operations. This is to our knowledge the first such problem in which the aim is to facilitate travel between specific points (as opposed to facilitating or impeding a broadcast from one or many sources).         ",
    "url": "https://arxiv.org/abs/2501.18987",
    "authors": [
      "David C. Kutner",
      "Anouk Sommer"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2502.04106",
    "title": "The Gradient Puppeteer: Adversarial Domination in Gradient Leakage Attacks through Model Poisoning",
    "abstract": "           In Federated Learning (FL), clients share gradients with a central server while keeping their data local. However, malicious servers could deliberately manipulate the models to reconstruct clients' data from shared gradients, posing significant privacy risks. Although such active gradient leakage attacks (AGLAs) have been widely studied, they suffer from two severe limitations: (i) coverage: no existing AGLAs can reconstruct all samples in a batch from the shared gradients; (ii) stealthiness: no existing AGLAs can evade principled checks of clients. In this paper, we address these limitations with two core contributions. First, we introduce a new theoretical analysis approach, which uniformly models AGLAs as backdoor poisoning. This analysis approach reveals that the core principle of AGLAs is to bias the gradient space to prioritize the reconstruction of a small subset of samples while sacrificing the majority, which theoretically explains the above limitations of existing AGLAs. Second, we propose Enhanced Gradient Global Vulnerability (EGGV), the first AGLA that achieves complete attack coverage while evading client-side detection. In particular, EGGV employs a gradient projector and a jointly optimized discriminator to assess gradient vulnerability, steering the gradient space toward the point most prone to data leakage. Extensive experiments show that EGGV achieves complete attack coverage and surpasses state-of-the-art (SOTA) with at least a 43% increase in reconstruction quality (PSNR) and a 45% improvement in stealthiness (D-SNR).         ",
    "url": "https://arxiv.org/abs/2502.04106",
    "authors": [
      "Kunlan Xiang",
      "Haomiao Yang",
      "Meng Hao",
      "Shaofeng Li",
      "Haoxin Wang",
      "Zikang Ding",
      "Wenbo Jiang",
      "Tianwei Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.05780",
    "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
    "abstract": "           Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines.         ",
    "url": "https://arxiv.org/abs/2502.05780",
    "authors": [
      "Danny Wang",
      "Ruihong Qiu",
      "Guangdong Bai",
      "Zi Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.06685",
    "title": "No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers",
    "abstract": "           We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. This motivates the pursuit of simulation-free training procedures of neural samplers. In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. However, it ultimately suffers from severe mode collapse. On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target. Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers.         ",
    "url": "https://arxiv.org/abs/2502.06685",
    "authors": [
      "Jiajun He",
      "Yuanqi Du",
      "Francisco Vargas",
      "Dinghuai Zhang",
      "Shreyas Padhy",
      "RuiKang OuYang",
      "Carla Gomes",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2502.09846",
    "title": "Robust Event-Triggered Integrated Communication and Control with Graph Information Bottleneck Optimization",
    "abstract": "           Integrated communication and control serves as a critical ingredient in Multi-Agent Reinforcement Learning. However, partial observability limitations will impair collaboration effectiveness, and a potential solution is to establish consensus through well-calibrated latent variables obtained from neighboring agents. Nevertheless, the rigid transmission of less informative content can still result in redundant information exchanges. Therefore, we propose a Consensus-Driven Event-Based Graph Information Bottleneck (CDE-GIB) method, which integrates the communication graph and information flow through a GIB regularizer to extract more concise message representations while avoiding the high computational complexity of inner-loop operations. To further minimize the communication volume required for establishing consensus during interactions, we also develop a variable-threshold event-triggering mechanism. By simultaneously considering historical data and current observations, this mechanism capably evaluates the importance of information to determine whether an event should be triggered. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods in terms of both efficiency and adaptability.         ",
    "url": "https://arxiv.org/abs/2502.09846",
    "authors": [
      "Ziqiong Wang",
      "Xiaoxue Yu",
      "Rongpeng Li",
      "Zhifeng Zhao"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2502.10122",
    "title": "Modern Hopfield Networks with Continuous-Time Memories",
    "abstract": "           Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets.         ",
    "url": "https://arxiv.org/abs/2502.10122",
    "authors": [
      "Saul Santos",
      "Ant\u00f3nio Farinhas",
      "Daniel C. McNamee",
      "Andr\u00e9 F.T. Martins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.14792",
    "title": "RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation",
    "abstract": "           Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention as a useful representation of the environment to tackle assisted and autonomous driving tasks. However, most of the existing work focuses on the fully supervised setting, training networks on large annotated datasets. In this work, we present RendBEV, a new method for the self-supervised training of BEV semantic segmentation networks, leveraging differentiable volumetric rendering to receive supervision from semantic perspective views computed by a 2D semantic segmentation model. Our method enables zero-shot BEV semantic segmentation, and already delivers competitive results in this challenging setting. When used as pretraining to then fine-tune on labeled BEV ground-truth, our method significantly boosts performance in low-annotation regimes, and sets a new state of the art when fine-tuning on all available labels.         ",
    "url": "https://arxiv.org/abs/2502.14792",
    "authors": [
      "Henrique Pi\u00f1eiro Monteagudo",
      "Leonardo Taccari",
      "Aurel Pjetri",
      "Francesco Sambo",
      "Samuele Salti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.00961",
    "title": "CAGN-GAT Fusion: A Hybrid Contrastive Attentive Graph Neural Network for Network Intrusion Detection",
    "abstract": "           Cybersecurity threats are growing, making network intrusion detection essential. Traditional machine learning models remain effective in resource-limited environments due to their efficiency, requiring fewer parameters and less computational time. However, handling short and highly imbalanced datasets remains challenging. In this study, we propose the fusion of a Contrastive Attentive Graph Network and Graph Attention Network (CAGN-GAT Fusion) and benchmark it against 15 other models, including both Graph Neural Networks (GNNs) and traditional ML models. Our evaluation is conducted on four benchmark datasets (KDD-CUP-1999, NSL-KDD, UNSW-NB15, and CICIDS2017) using a short and proportionally imbalanced dataset with a constant size of 5000 samples to ensure fairness in comparison. Results show that CAGN-GAT Fusion demonstrates stable and competitive accuracy, recall, and F1-score, even though it does not achieve the highest performance in every dataset. Our analysis also highlights the impact of adaptive graph construction techniques, including small changes in connections (edge perturbation) and selective hiding of features (feature masking), improving detection performance. The findings confirm that GNNs, particularly CAGN-GAT Fusion, are robust and computationally efficient, making them well-suited for resource-constrained environments. Future work will explore GraphSAGE layers and multiview graph construction techniques to further enhance adaptability and detection accuracy.         ",
    "url": "https://arxiv.org/abs/2503.00961",
    "authors": [
      "Md Abrar Jahin",
      "Shahriar Soudeep",
      "M. F. Mridha",
      "Raihan Kabir",
      "Md Rashedul Islam",
      "Yutaka Watanobe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.01284",
    "title": "Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention",
    "abstract": "           Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional machine learning models ($\\le77.05\\%$). Ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research.         ",
    "url": "https://arxiv.org/abs/2503.01284",
    "authors": [
      "Md Abrar Jahin",
      "Soudeep Shahriar",
      "M. F. Mridha",
      "Md. Jakir Hossen",
      "Nilanjan Dey"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.06965",
    "title": "SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks",
    "abstract": "           When discussing the Aerial-Ground Person Re-identification (AGPReID) task, we face the main challenge of the significant appearance variations caused by different viewpoints, making identity matching difficult. To address this issue, previous methods attempt to reduce the differences between viewpoints by critical attributes and decoupling the viewpoints. While these methods can mitigate viewpoint differences to some extent, they still face two main issues: (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, covering $4,231$ unique identities and containing $63,841$ high-quality images; the latter is reconstructed from the person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a feasible and effective solution for the AGPReID task. The datasets and source code available on this https URL.         ",
    "url": "https://arxiv.org/abs/2503.06965",
    "authors": [
      "Shining Wang",
      "Yunlong Wang",
      "Ruiqi Wu",
      "Bingliang Jiao",
      "Wenxuan Wang",
      "Peng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13366",
    "title": "Optimal Bounds for Adversarial Constrained Online Convex Optimization",
    "abstract": "           Constrained Online Convex Optimization (COCO) can be seen as a generalization of the standard Online Convex Optimization (OCO) framework. At each round, a cost function and constraint function are revealed after a learner chooses an action. The goal is to minimize both the regret and cumulative constraint violation (CCV) against an adaptive adversary. We show for the first time that is possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV, improving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\tilde{O} \\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively. Based on a new surrogate loss function enforcing a minimum penalty on the constraint function, we demonstrate that both the Follow-the-Regularized-Leader and the Online Gradient Descent achieve the optimal bounds.         ",
    "url": "https://arxiv.org/abs/2503.13366",
    "authors": [
      "Ricardo N. Ferreira",
      "Cl\u00e1udia Soares"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.13911",
    "title": "Incorporating Attributes and Multi-Scale Structures for Heterogeneous Graph Contrastive Learning",
    "abstract": "           Heterogeneous graphs (HGs) are composed of multiple types of nodes and edges, making it more effective in capturing the complex relational structures inherent in the real world. However, in real-world scenarios, labeled data is often difficult to obtain, which limits the applicability of semi-supervised approaches. Self-supervised learning aims to enable models to automatically learn useful features from data, effectively addressing the challenge of limited labeling data. In this paper, we propose a novel contrastive learning framework for heterogeneous graphs (ASHGCL), which incorporates three distinct views, each focusing on node attributes, high-order and low-order structural information, respectively, to effectively capture attribute information, high-order structures, and low-order structures for node representation learning. Furthermore, we introduce an attribute-enhanced positive sample selection strategy that combines both structural information and attribute information, effectively addressing the issue of sampling bias. Extensive experiments on four real-world datasets show that ASHGCL outperforms state-of-the-art unsupervised baselines and even surpasses some supervised benchmarks.         ",
    "url": "https://arxiv.org/abs/2503.13911",
    "authors": [
      "Ruobing Jiang",
      "Yacong Li",
      "Haobing Liu",
      "Yanwei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2503.18445",
    "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness",
    "abstract": "           Multi-modal semantic segmentation (MMSS) addresses the limitations of single-modality data by integrating complementary information across modalities. Despite notable progress, a significant gap persists between research and real-world deployment due to variability and uncertainty in multi-modal data quality. Robustness has thus become essential for practical MMSS applications. However, the absence of standardized benchmarks for evaluating robustness hinders further advancement. To address this, we first survey existing MMSS literature and categorize representative methods to provide a structured overview. We then introduce a robustness benchmark that evaluates MMSS models under three scenarios: Entire-Missing Modality (EMM), Random-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic standpoint, we model modality failure under two conditions: (1) all damaged combinations are equally probable; (2) each modality fails independently following a Bernoulli distribution. Based on these, we propose four metrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and $mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work provides the first dedicated benchmark for MMSS robustness, offering new insights and tools to advance the field. Source code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2503.18445",
    "authors": [
      "Chenfei Liao",
      "Kaiyu Lei",
      "Xu Zheng",
      "Junha Moon",
      "Zhixiong Wang",
      "Yixuan Wang",
      "Danda Pani Paudel",
      "Luc Van Gool",
      "Xuming Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.22418",
    "title": "Robustness quantification: a new method for assessing the reliability of the predictions of a classifier",
    "abstract": "           Based on existing ideas in the field of imprecise probabilities, we present a new approach for assessing the reliability of the individual predictions of a generative probabilistic classifier. We call this approach robustness quantification, compare it to uncertainty quantification, and demonstrate that it continues to work well even for classifiers that are learned from small training sets that are sampled from a shifted distribution.         ",
    "url": "https://arxiv.org/abs/2503.22418",
    "authors": [
      "Adri\u00e1n Detavernier",
      "Jasper De Bock"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2503.23495",
    "title": "Embedding Shift Dissection on CLIP: Effects of Augmentations on VLM's Representation Learning",
    "abstract": "           Understanding the representation shift on Vision Language Models like CLIP under different augmentations provides valuable insights on Mechanistic Interpretability. In this study, we show the shift on CLIP's embeddings on 9 common augmentation techniques: noise, blur, color jitter, scale and rotate, flip, elastic and perspective transforms, random brightness and contrast, and coarse dropout of pixel blocks. We scrutinize the embedding shifts under similarity on attention map, patch, edge, detail preservation, cosine similarity, L2 distance, pairwise distance and dendrogram clusters and provide qualitative analysis on sample images. Our findings suggest certain augmentations like noise, perspective transform and shift scaling have higher degree of drastic impact on embedding shift. This study provides a concrete foundation for future work on VLM's robustness for mechanical interpretation and adversarial data defense. The code implementation for this study can be found on \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2503.23495",
    "authors": [
      "Ashim Dahal",
      "Saydul Akbar Murad",
      "Nick Rahimi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.00493",
    "title": "Perturbation-Based Pinning Control Strategy for Enhanced Synchronization in Complex Networks",
    "abstract": "           Synchronization is essential for the stability and coordinated operation of complex networked systems. Pinning control, which selectively controls a subset of nodes, provides a scalable solution to enhance network synchronizability. However, existing strategies face key limitations: heuristic centrality-based methods lack a direct connection to synchronization dynamics, while spectral approaches, though effective, are computationally intensive. To address these challenges, we propose a perturbation-based optimized strategy (PBO) that dynamically evaluates each node's spectral impact on the Laplacian matrix, achieving improved synchronizability with significantly reduced computational costs (with complexity O(kM)). Extensive experiments demonstrate that the proposed method outperforms traditional strategies in synchronizability, convergence rate, and pinning robustness to node failures. Notably, in all the empirical networks tested and some generated networks, PBO significantly outperforms the brute-force greedy strategy, demonstrating its ability to avoid local optima and adapt to complex connectivity patterns. Our study establishes the theoretical relationship between network synchronizability and convergence rate, offering new insights into efficient synchronization strategies for large-scale complex networks.         ",
    "url": "https://arxiv.org/abs/2504.00493",
    "authors": [
      "Ziang Mao",
      "Tianlong Fan",
      "Linyuan L\u00fc"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.02670",
    "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
    "abstract": "           Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants.         ",
    "url": "https://arxiv.org/abs/2504.02670",
    "authors": [
      "Maciej Besta",
      "Lorenzo Paleari",
      "Jia Hao Andrea Jiang",
      "Robert Gerstenberger",
      "You Wu",
      "Patrick Iff",
      "Ales Kubicek",
      "Piotr Nyczyk",
      "Diana Khimey",
      "J\u00f3n Gunnar Hannesson",
      "Grzegorz Kwa\u015bniewski",
      "Marcin Copik",
      "Hubert Niewiadomski",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05500",
    "title": "Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search",
    "abstract": "           The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, test coverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations.         ",
    "url": "https://arxiv.org/abs/2504.05500",
    "authors": [
      "Vahid Majdinasab",
      "Amin Nikanjam",
      "Foutse Khomh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.05761",
    "title": "AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams",
    "abstract": "           Concept drift and extreme verification latency pose significant challenges in data stream learning, particularly when dealing with recurring concept changes in dynamic environments. This work introduces a novel method based on the Growing Neural Gas (GNG) algorithm, designed to effectively handle abrupt recurrent drifts while adapting to incrementally evolving data distributions (incremental drifts). Leveraging the self-organizing and topological adaptability of GNG, the proposed approach maintains a compact yet informative memory structure, allowing it to efficiently store and retrieve knowledge of past or recurring concepts, even under conditions of delayed or sparse stream supervision. Our experiments highlight the superiority of our approach over existing data stream learning methods designed to cope with incremental non-stationarities and verification latency, demonstrating its ability to quickly adapt to new drifts, robustly manage recurring patterns, and maintain high predictive accuracy with a minimal memory footprint. Unlike other techniques that fail to leverage recurring knowledge, our proposed approach is proven to be a robust and efficient online learning solution for unsupervised drifting data flows.         ",
    "url": "https://arxiv.org/abs/2504.05761",
    "authors": [
      "Maria Arostegi",
      "Miren Nekane Bilbao",
      "Jesus L. Lobo",
      "Javier Del Ser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.06212",
    "title": "NNN: Next-Generation Neural Networks for Marketing Mix Modeling",
    "abstract": "           We present NNN, a Transformer-based neural network approach to Marketing Mix Modeling (MMM) designed to address key limitations of traditional methods. Unlike conventional MMMs which rely on scalar inputs and parametric decay functions, NNN uses rich embeddings to capture both quantitative and qualitative aspects of marketing and organic channels (e.g., search queries, ad creatives). This, combined with its attention mechanism, enables NNN to model complex interactions, capture long-term effects, and potentially improve sales attribution accuracy. We show that L1 regularization permits the use of such expressive models in typical data-constrained settings. Evaluating NNN on simulated and real-world data demonstrates its efficacy, particularly through considerable improvement in predictive power. Beyond attribution, NNN provides valuable, complementary insights through model probing, such as evaluating keyword or creative effectiveness, enhancing model interpretability.         ",
    "url": "https://arxiv.org/abs/2504.06212",
    "authors": [
      "Thomas Mulc",
      "Mike Anderson",
      "Paul Cubre",
      "Huikun Zhang",
      "Ivy Liu",
      "Saket Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2504.06575",
    "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning",
    "abstract": "           Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria: high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text-transforming it into hate speech-while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.06575",
    "authors": [
      "Li An",
      "Yujian Liu",
      "Yepeng Liu",
      "Yang Zhang",
      "Yuheng Bu",
      "Shiyu Chang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.06643",
    "title": "AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection",
    "abstract": "           Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets.         ",
    "url": "https://arxiv.org/abs/2504.06643",
    "authors": [
      "Tiange Huang",
      "Yongjun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.06742",
    "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection",
    "abstract": "           Landmark detection plays a crucial role in medical imaging tasks that rely on precise spatial localization, including specific applications in diagnosis, treatment planning, image registration, and surgical navigation. However, manual annotation is labor-intensive and requires expert knowledge. While deep learning shows promise in automating this task, progress is hindered by limited public datasets, inconsistent benchmarks, and non-standardized baselines, restricting reproducibility, fair comparisons, and model generalizability. This work introduces nnLandmark, a self-configuring deep learning framework for 3D medical landmark detection, adapting nnU-Net to perform heatmap-based regression. By leveraging nnU-Net's automated configuration, nnLandmark eliminates the need for manual parameter tuning, offering out-of-the-box usability. It achieves state-of-the-art accuracy across two public datasets, with a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML) dental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset (AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm. With its strong generalization, reproducibility, and ease of deployment, nnLandmark establishes a reliable baseline for 3D landmark detection, supporting research in anatomical localization and clinical workflows that depend on precise landmark identification. The code will be available soon.         ",
    "url": "https://arxiv.org/abs/2504.06742",
    "authors": [
      "Alexandra Ertl",
      "Shuhan Xiao",
      "Stefan Denner",
      "Robin Peretzke",
      "David Zimmerer",
      "Peter Neher",
      "Fabian Isensee",
      "Klaus Maier-Hein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.06801",
    "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection",
    "abstract": "           Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.         ",
    "url": "https://arxiv.org/abs/2504.06801",
    "authors": [
      "Rishubh Parihar",
      "Srinjay Sarkar",
      "Sarthak Vora",
      "Jogendra Kundu",
      "R. Venkatesh Babu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2210.13300",
    "title": "Designing Universal Causal Deep Learning Models: The Case of Infinite-Dimensional Dynamical Systems from Stochastic Analysis",
    "abstract": "           Several non-linear operators in stochastic analysis, such as solution maps to stochastic differential equations, depend on a temporal structure which is not leveraged by contemporary neural operators designed to approximate general maps between Banach space. This paper therefore proposes an operator learning solution to this open problem by introducing a deep learning model-design framework that takes suitable infinite-dimensional linear metric spaces, e.g. Banach spaces, as inputs and returns a universal \\textit{sequential} deep learning model adapted to these linear geometries specialized for the approximation of operators encoding a temporal structure. We call these models \\textit{Causal Neural Operators}. Our main result states that the models produced by our framework can uniformly approximate on compact sets and across arbitrarily finite-time horizons H\u00f6lder or smooth trace class operators, which causally map sequences between given linear metric spaces. Our analysis uncovers new quantitative relationships on the latent state-space dimension of Causal Neural Operators, which even have new implications for (classical) finite-dimensional Recurrent Neural Networks. In addition, our guarantees for recurrent neural networks are tighter than the available results inherited from feedforward neural networks when approximating dynamical systems between finite-dimensional spaces.         ",
    "url": "https://arxiv.org/abs/2210.13300",
    "authors": [
      "Luca Galimberti",
      "Anastasis Kratsios",
      "Giulia Livieri"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ]
  },
  {
    "id": "arXiv:2211.01032",
    "title": "Random Embeddings of Graphs: The Expected Number of Faces in Most Graphs is Logarithmic",
    "abstract": "           A random 2-cell embedding of a connected graph $G$ in some orientable surface is obtained by choosing a random local rotation around each vertex. Under this setup, the number of faces or the genus of the corresponding 2-cell embedding becomes a random variable. Random embeddings of two particular graph classes, those of a bouquet of $n$ loops and those of $n$ parallel edges connecting two vertices, have been extensively studied and are well-understood. However, little is known about more general graphs. The results of this paper explain why Monte Carlo methods cannot work for approximating the minimum genus of graphs. In his breakthrough work [Permutation-partition pairs, JCTB 1991], Stahl developed the foundation of \"random topological graph theory\". Most of his results have been unsurpassed until today. In our work, we analyze the expected number of faces of random embeddings (equivalently, the average genus) of a graph $G$. It was very recently shown that for any graph $G$, the expected number of faces is at most linear. We show that the actual expected number of faces $F(G)$ is almost always much smaller. In particular, we prove: 1) $\\frac{1}{2}\\ln n - 2 < \\mathbb{E}[F(K_n)] \\le 3.65 \\ln n +o(1)$. 2) For random graphs $G(n,p)$ ($p=p(n)$), we have $\\mathbb{E}[F(G(n,p))] \\le \\ln^2 n+\\frac{1}{p}$. 3) For random models $B(n,\\Delta)$ containing only graphs, whose maximum degree is at most $\\Delta$, we obtain stronger bounds by showing that the expected number of faces is $\\Theta(\\log n)$.         ",
    "url": "https://arxiv.org/abs/2211.01032",
    "authors": [
      "Jesse Campion Loth",
      "Kevin Halasz",
      "Tom\u00e1\u0161 Masa\u0159\u00edk",
      "Bojan Mohar",
      "Robert \u0160\u00e1mal"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2307.02284",
    "title": "Universal Scaling Laws of Absorbing Phase Transitions in Artificial Deep Neural Networks",
    "abstract": "           We demonstrate that conventional artificial deep neural networks operating near the phase boundary of the signal propagation dynamics, also known as the edge of chaos, exhibit universal scaling laws of absorbing phase transitions in non-equilibrium statistical mechanics. We exploit the fully deterministic nature of the propagation dynamics to elucidate an analogy between a signal collapse in the neural networks and an absorbing state (a state that the system can enter but cannot escape from). Our numerical results indicate that the multilayer perceptrons and the convolutional neural networks belong to the mean-field and the directed percolation universality classes, respectively. Also, the finite-size scaling is successfully applied, suggesting a potential connection to the depth-width trade-off in deep learning. Furthermore, our analysis of the training dynamics under the gradient descent reveals that hyperparameter tuning to the phase boundary is necessary but insufficient for achieving optimal generalization in deep networks. Remarkably, nonuniversal metric factors associated with the scaling laws are shown to play a significant role in concretizing the above observations. These findings highlight the usefulness of the notion of criticality for analyzing the behavior of artificial deep neural networks and offer new insights toward a unified understanding of the essential relationship between criticality and intelligence.         ",
    "url": "https://arxiv.org/abs/2307.02284",
    "authors": [
      "Keiichi Tamai",
      "Tsuyoshi Okubo",
      "Truong Vinh Truong Duy",
      "Naotake Natori",
      "Synge Todo"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.07891",
    "title": "A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks",
    "abstract": "           Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By precisely analyzing the improvement in the training and test errors, we demonstrate that these non-linear features can enhance learning.         ",
    "url": "https://arxiv.org/abs/2310.07891",
    "authors": [
      "Behrad Moniri",
      "Donghwan Lee",
      "Hamed Hassani",
      "Edgar Dobriban"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.13706",
    "title": "Multi-view Hybrid Graph Convolutional Network for Volume-to-mesh Reconstruction in Cardiovascular MRI",
    "abstract": "           Cardiovascular magnetic resonance imaging is emerging as a crucial tool to examine cardiac morphology and function. Essential to this endeavour are anatomical 3D surface and volumetric meshes derived from CMR images, which facilitate computational anatomy studies, biomarker discovery, and in-silico simulations. Traditional approaches typically follow complex multi-step pipelines, first segmenting images and then reconstructing meshes, making them time-consuming and prone to error propagation. In response, we introduce HybridVNet, a novel architecture for direct image-to-mesh extraction seamlessly integrating standard convolutional neural networks with graph convolutions, which we prove can efficiently handle surface and volumetric meshes by encoding them as graph structures. To further enhance accuracy, we propose a multi-view HybridVNet architecture which processes both long axis and short axis CMR, showing that it can increase the performance of cardiac MR mesh generation. Our model combines traditional convolutional networks with variational graph generative models, deep supervision and mesh-specific regularisation. Experiments on a comprehensive dataset from the UK Biobank confirm the potential of HybridVNet to significantly advance cardiac imaging and computational cardiology by efficiently generating high-fidelity meshes from CMR images. Multi-view HybridVNet outperforms the state-of-the-art, achieving improvements of up to $\\sim$27\\% reduction in Mean Contour Distance (from 1.86 mm to 1.35 mm for the LV Myocardium), up to $\\sim$18\\% improvement in Hausdorff distance (from 4.74 mm to 3.89mm, for the LV Endocardium), and up to $\\sim$8\\% in Dice Coefficient (from 0.78 to 0.84, for the LV Myocardium), highlighting its superior accuracy.         ",
    "url": "https://arxiv.org/abs/2311.13706",
    "authors": [
      "Nicol\u00e1s Gaggion",
      "Benjamin A. Matheson",
      "Yan Xia",
      "Rodrigo Bonazzola",
      "Nishant Ravikumar",
      "Zeike A. Taylor",
      "Diego H. Milone",
      "Alejandro F. Frangi",
      "Enzo Ferrante"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.05936",
    "title": "Planar graphs in blowups of fans",
    "abstract": "           We show that every $n$-vertex planar graph is contained in the graph obtained from a fan by blowing up each vertex by a complete graph of order $O(\\sqrt{n}\\log^2 n)$. Equivalently, every $n$-vertex planar graph $G$ has a set $X$ of $O(\\sqrt{n}\\log^2 n)$ vertices such that $G-X$ has bandwidth $O(\\sqrt{n}\\log^2 n)$. We in fact prove the same result for any proper minor-closed class, and we prove more general results that explore the trade-off between $X$ and the bandwidth of $G-X$. The proofs use three key ingredients. The first is a new local sparsification lemma, which shows that every $n$-vertex planar graph $G$ has a set of $O((n\\log n)/\\delta)$ vertices whose removal results in a graph with local density at most $\\delta$. The second is a generalization of a method of Feige and Rao that relates bandwidth and local density using volume-preserving Euclidean embeddings. The third ingredient is graph products, which are a key tool in the extension to any proper minor-closed class.         ",
    "url": "https://arxiv.org/abs/2407.05936",
    "authors": [
      "Marc Distel",
      "Vida Dujmovi\u0107",
      "Gwena\u00ebl Joret",
      "Piotr Micek",
      "Pat Morin",
      "David R. Wood"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  }
]