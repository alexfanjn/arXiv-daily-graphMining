[
  {
    "id": "arXiv:2504.05322",
    "title": "Balancing Benefits and Risks: RL Approaches for Addiction-Aware Social Media Recommenders",
    "abstract": "           Social media platforms provide valuable opportunities for users to gather information, interact with friends, and enjoy entertainment. However, their addictive potential poses significant challenges, including overuse and negative psycho-logical or behavioral impacts [4, 2, 8]. This study explores strategies to mitigate compulsive social media usage while preserving its benefits and ensuring economic sustainability, focusing on recommenders that promote balanced usage. We analyze user behaviors arising from intrinsic diversities and environmental interactions, offering insights for next-generation social media recommenders that prioritize well-being. Specifically, we examine the temporal predictability of overuse and addiction using measures available to recommenders, aiming to inform mechanisms that prevent addiction while avoiding user disengagement [7]. Building on RL-based computational frameworks for addiction modelling [6], our study introduces: - A recommender system adapting to user preferences, introducing non-stationary and non-Markovian dynamics. - Differentiated state representations for users and recommenders to capture nuanced interactions. - Distinct usage conditions-light and heavy use-addressing RL's limitations in distinguishing prolonged from healthy engagement. - Complexity in overuse impacts, highlighting their role in user adaptation [7]. Simulations demonstrate how model-based (MB) and model-free (MF) decision-making interact with environmental dynamics to influence user behavior and addiction. Results reveal the significant role of recommender systems in shaping addiction tendencies or fostering healthier engagement. These findings support ethical, adaptive recommender design, advancing sustainable social media ecosystems [9, 1]. Keywords: multi-agent systems, recommender systems, addiction, social media         ",
    "url": "https://arxiv.org/abs/2504.05322",
    "authors": [
      "Luca Bolis",
      "Stefano Livella",
      "Sabrina Patania",
      "Dimitri Ognibene",
      "Matteo Papini",
      "Kenji Morita"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2504.05338",
    "title": "Improving Early Prediction of Type 2 Diabetes Mellitus with ECG-DiaNet: A Multimodal Neural Network Leveraging Electrocardiogram and Clinical Risk Factors",
    "abstract": "           Type 2 Diabetes Mellitus (T2DM) remains a global health challenge, underscoring the need for early and accurate risk prediction. This study presents ECG-DiaNet, a multimodal deep learning model that integrates electrocardiogram (ECG) features with clinical risk factors (CRFs) to enhance T2DM onset prediction. Using data from Qatar Biobank (QBB), we trained and validated models on a development cohort (n=2043) and evaluated performance on a longitudinal test set (n=395) with five-year follow-up. ECG-DiaNet outperformed unimodal ECG-only and CRF-only models, achieving a higher AUROC (0.845 vs 0.8217) than the CRF-only model, with statistical significance (DeLong p<0.001). Reclassification metrics further confirmed improvements: Net Reclassification Improvement (NRI=0.0153) and Integrated Discrimination Improvement (IDI=0.0482). Risk stratification into low-, medium-, and high-risk groups showed ECG-DiaNet achieved superior positive predictive value (PPV) in high-risk individuals. The model's reliance on non-invasive and widely available ECG signals supports its feasibility in clinical and community health settings. By combining cardiac electrophysiology and systemic risk profiles, ECG-DiaNet addresses the multifactorial nature of T2DM and supports precision prevention. These findings highlight the value of multimodal AI in advancing early detection and prevention strategies for T2DM, particularly in underrepresented Middle Eastern populations.         ",
    "url": "https://arxiv.org/abs/2504.05338",
    "authors": [
      "Farida Mohsen",
      "Zubair Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05341",
    "title": "Three-Factor Learning in Spiking Neural Networks: An Overview of Methods and Trends from a Machine Learning Perspective",
    "abstract": "           Three-factor learning rules in Spiking Neural Networks (SNNs) have emerged as a crucial extension to traditional Hebbian learning and Spike-Timing-Dependent Plasticity (STDP), incorporating neuromodulatory signals to improve adaptation and learning efficiency. These mechanisms enhance biological plausibility and facilitate improved credit assignment in artificial neural systems. This paper takes a view on this topic from a machine learning perspective, providing an overview of recent advances in three-factor learning, discusses theoretical foundations, algorithmic implementations, and their relevance to reinforcement learning and neuromorphic computing. In addition, we explore interdisciplinary approaches, scalability challenges, and potential applications in robotics, cognitive modeling, and AI systems. Finally, we highlight key research gaps and propose future directions for bridging the gap between neuroscience and artificial intelligence.         ",
    "url": "https://arxiv.org/abs/2504.05341",
    "authors": [
      "Szymon Mazurek",
      "Jakub Caputa",
      "Jan K. Argasi\u0144ski",
      "Maciej Wielgosz"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05345",
    "title": "ZeroED: Hybrid Zero-shot Error Detection through Large Language Model Reasoning",
    "abstract": "           Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid zero-shot error detection framework, which combines LLM reasoning ability with the manual label-based ED pipeline. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to label errors holistically through in-context learning, guided by a two-step reasoning process for detailed error detection guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven public datasets demonstrate that, ZeroED substantially outperforms state-of-the-art methods by a maximum 30% improvement in F1 score and up to 90% token cost reduction.         ",
    "url": "https://arxiv.org/abs/2504.05345",
    "authors": [
      "Wei Ni",
      "Kaihang Zhang",
      "Xiaoye Miao",
      "Xiangyu Zhao",
      "Yangyang Wu",
      "Yaoshu Wang",
      "Jianwei Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2504.05356",
    "title": "DyTTP: Trajectory Prediction with Normalization-Free Transformers",
    "abstract": "           Accurate trajectory prediction is a cornerstone for the safe operation of autonomous driving systems, where understanding the dynamic behavior of surrounding agents is crucial. Transformer-based architectures have demonstrated significant promise in capturing complex spatio-temporality dependencies. However, their reliance on normalization layers can lead to computation overhead and training instabilities. In this work, we present a two-fold approach to address these challenges. First, we integrate DynamicTanh (DyT), which is the latest method to promote transformers, into the backbone, replacing traditional layer normalization. This modification simplifies the network architecture and improves the stability of the inference. We are the first work to deploy the DyT to the trajectory prediction task. Complementing this, we employ a snapshot ensemble strategy to further boost trajectory prediction performance. Using cyclical learning rate scheduling, multiple model snapshots are captured during a single training run. These snapshots are then aggregated via simple averaging at inference time, allowing the model to benefit from diverse hypotheses without incurring substantial additional computational cost. Extensive experiments on Argoverse datasets demonstrate that our combined approach significantly improves prediction accuracy, inference speed and robustness in diverse driving scenarios. This work underscores the potential of normalization-free transformer designs augmented with lightweight ensemble techniques in advancing trajectory forecasting for autonomous vehicles.         ",
    "url": "https://arxiv.org/abs/2504.05356",
    "authors": [
      "Yunxiang Liu",
      "Hongkuo Niu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05358",
    "title": "Debate-Feedback: A Multi-Agent Framework for Efficient Legal Judgment Prediction",
    "abstract": "           The use of AI in legal analysis and prediction (LegalAI) has gained widespread attention, with past research focusing on retrieval-based methods and fine-tuning large models. However, these approaches often require large datasets and underutilize the capabilities of modern large language models (LLMs). In this paper, inspired by the debate phase of real courtroom trials, we propose a novel legal judgment prediction model based on the Debate-Feedback architecture, which integrates LLM multi-agent debate and reliability evaluation models. Unlike traditional methods, our model achieves significant improvements in efficiency by minimizing the need for large historical datasets, thus offering a lightweight yet robust solution. Comparative experiments show that it outperforms several general-purpose and domain-specific legal models, offering a dynamic reasoning process and a promising direction for future LegalAI research.         ",
    "url": "https://arxiv.org/abs/2504.05358",
    "authors": [
      "Xi Chen",
      "Mao Mao",
      "Shuo Li",
      "Haotian Shangguan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05366",
    "title": "Handling Weather Uncertainty in Air Traffic Prediction through an Inverse Approach",
    "abstract": "           Adverse weather conditions, particularly convective phenomena, pose significant challenges to Air Traffic Management, often requiring real-time rerouting decisions that impact efficiency and safety. This study introduces a 3-D Gaussian Mixture Model to predict long lead-time flight trajectory changes, incorporating comprehensive weather and traffic data. Utilizing high-resolution meteorological datasets, including convective weather maps and wind data, alongside traffic records, the model demonstrates robust performance in forecasting reroutes up to 60 minutes. The novel 3-D Gaussian Mixture Model framework employs a probabilistic approach to capture uncertainty while providing accurate forecasts of altitude, latitude, and longitude. Extensive evaluation revealed a Mean Absolute Percentage Error below 0.02 across varying lead times, highlighting the model's accuracy and scalability. By integrating explainability techniques such as the Vanilla Gradient algorithm, the study provides insights into feature contributions, showing that they contribute to improving Air Traffic Management strategies to mitigate weather-induced disruptions.         ",
    "url": "https://arxiv.org/abs/2504.05366",
    "authors": [
      "G. Lancia",
      "D. Falanga",
      "S. Alam",
      "G. Lulli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05397",
    "title": "Physics-informed Modularized Neural Network for Advanced Building Control by Deep Reinforcement Learning",
    "abstract": "           Physics-informed machine learning (PIML) provides a promising solution for building energy modeling and can serve as a virtual environment to enable reinforcement learning (RL) agents to interact and learn. However, challenges remain in efficiently integrating physics priors, evaluating the effectiveness of physics constraints, balancing model accuracy and physics consistency, and enabling real-world implementation. To address these gaps, this study introduces a Physics-Informed Modularized Neural Network (PI-ModNN), which incorporates physics priors through a physics-informed model structure, loss functions, and hard constraints. A new evaluation metric called \"temperature response violation\" is developed to quantify the physical consistency of data-driven building dynamic models under varying control inputs and training data sizes. Additionally, a physics prior evaluation framework based on rule importance is proposed to assess the contribution of each individual physics prior, offering guidance on selecting appropriate PIML techniques. Results indicate that incorporating physical priors does not always improve model performance; inappropriate priors may decrease model accuracy and consistency. However, hard constraints are effective in enforcing model consistency. Furthermore, we present a general workflow for developing control-oriented PIML models and integrating them with deep reinforcement learning (DRL). Following this framework, a case study implementing DRL in an office space over three months demonstrates potential energy savings of 31.4%. Finally, we provide a general guideline for integrating data-driven models with advanced building control through a four-step evaluation framework, paving the way for reliable and scalable deployment of advanced building controls.         ",
    "url": "https://arxiv.org/abs/2504.05397",
    "authors": [
      "Zixin Jiang",
      "Xuezheng Wang",
      "Bing Dong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05398",
    "title": "CRDT Emulation, Simulation, and Representation Independence",
    "abstract": "           Conflict-free replicated data types (CRDTs) are distributed data structures designed for fault tolerance and high availability. CRDTs can be taxonomized into state-based CRDTs, in which replicas apply updates locally and periodically broadcast their local state to other replicas, and operation-based (op-based) CRDTs, in which every state-updating operation is individually broadcast and applied at each replica. In the literature, state-based and op-based CRDTs are considered equivalent due to the existence of algorithms that transform one kind of CRDT into the other. In particular, verification techniques and results for one kind of CRDT are often said to be applicable to the other kind, thanks to this equivalence. However, what it means for state-based and op-based CRDTs to emulate each other has never been made fully precise. In particular, emulation is nontrivial since state-based and op-based CRDTs place different requirements on the behavior of the underlying network with regard to both the causal ordering of message delivery, and the granularity of the messages themselves. In this paper, we specify and formalize CRDT emulation in terms of simulation by modeling CRDTs and their interactions with the network as formal transition systems. We show that emulation can be understood as weak simulations between the transition systems of the original and emulating CRDT systems, thus closing a gap in the CRDT literature. We precisely characterize which properties of CRDT systems are preserved by our weak simulations, and therefore which properties can be said to be applicable to state-based CRDTs as long as they are applicable to op-based CRDTs and vice versa. Finally, we leverage our emulation results to obtain a general representation independence result for CRDTs: intuitively, clients of a CRDT cannot tell whether they are interacting with a state-based or op-based CRDT in particular.         ",
    "url": "https://arxiv.org/abs/2504.05398",
    "authors": [
      "Nathan Liittschwager",
      "Jonathan Castello",
      "Stelios Tsampas",
      "Lindsey Kuper"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2504.05411",
    "title": "Less but Better: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection",
    "abstract": "           Personality detection automatically identifies an individual's personality from various data sources, such as social media texts. However, as the parameter scale of language models continues to grow, the computational cost becomes increasingly difficult to manage. Fine-tuning also grows more complex, making it harder to justify the effort and reliably predict outcomes. We introduce a novel parameter-efficient fine-tuning framework, PersLLM, to address these challenges. In PersLLM, a large language model (LLM) extracts high-dimensional representations from raw data and stores them in a dynamic memory layer. PersLLM then updates the downstream layers with a replaceable output network, enabling flexible adaptation to various personality detection scenarios. By storing the features in the memory layer, we eliminate the need for repeated complex computations by the LLM. Meanwhile, the lightweight output network serves as a proxy for evaluating the overall effectiveness of the framework, improving the predictability of results. Experimental results on key benchmark datasets like Kaggle and Pandora show that PersLLM significantly reduces computational cost while maintaining competitive performance and strong adaptability.         ",
    "url": "https://arxiv.org/abs/2504.05411",
    "authors": [
      "Lingzhi Shen",
      "Yunfei Long",
      "Xiaohao Cai",
      "Guanming Chen",
      "Imran Razzak",
      "Shoaib Jameel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05422",
    "title": "EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations",
    "abstract": "           As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints can be found here: this https URL.         ",
    "url": "https://arxiv.org/abs/2504.05422",
    "authors": [
      "Yue Yao",
      "Mohamed-Khalil Bouzidi",
      "Daniel Goehring",
      "Joerg Reichardt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.05424",
    "title": "Safe Automated Refactoring for Efficient Migration of Imperative Deep Learning Programs to Graph Execution",
    "abstract": "           Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code -- supporting symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. Though hybrid approaches aim for the \"best of both worlds,\" using them effectively requires subtle considerations to make code amenable to safe, accurate, and efficient graph execution. We present an automated refactoring approach that assists developers in specifying whether their otherwise eagerly-executed imperative DL code could be reliably and efficiently executed as graphs while preserving semantics. The approach, based on a novel imperative tensor analysis, automatically determines when it is safe and potentially advantageous to migrate imperative DL code to graph execution. The approach is implemented as a PyDev Eclipse IDE plug-in that integrates the WALA Ariadne analysis framework and evaluated on 19 Python projects consisting of 132.05 KLOC. We found that 326 of 766 candidate functions (42.56%) were refactorable, and an average speedup of 2.16 on performance tests was observed. The results indicate that the approach is useful in optimizing imperative DL code to its full potential.         ",
    "url": "https://arxiv.org/abs/2504.05424",
    "authors": [
      "Raffi Khatchadourian",
      "Tatiana Castro V\u00e9lez",
      "Mehdi Bagherzadeh",
      "Nan Jia",
      "Anita Raja"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2504.05425",
    "title": "A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%",
    "abstract": "           Predicting player behavior in strategic games, especially complex ones like chess, presents a significant challenge. The difficulty arises from several factors. First, the sheer number of potential outcomes stemming from even a single position, starting from the initial setup, makes forecasting a player's next move incredibly complex. Second, and perhaps even more challenging, is the inherent unpredictability of human behavior. Unlike the optimized play of engines, humans introduce a layer of variability due to differing playing styles and decision-making processes. Each player approaches the game with a unique blend of strategic thinking, tactical awareness, and psychological tendencies, leading to diverse and often unexpected actions. This stylistic variation, combined with the capacity for creativity and even irrational moves, makes predicting human play difficult. Chess, a longstanding benchmark of artificial intelligence research, has seen significant advancements in tools and automation. Engines like Deep Blue, AlphaZero, and Stockfish can defeat even the most skilled human players. However, despite their exceptional ability to outplay top-level grandmasters, predicting the moves of non-grandmaster players, who comprise most of the global chess community -- remains complicated for these engines. This paper proposes a novel approach combining expert knowledge with machine learning techniques to predict human players' next moves. By applying feature engineering grounded in domain expertise, we seek to uncover the patterns in the moves of intermediate-level chess players, particularly during the opening phase of the game. Our methodology offers a promising framework for anticipating human behavior, advancing both the fields of AI and human-computer interaction.         ",
    "url": "https://arxiv.org/abs/2504.05425",
    "authors": [
      "Benny Skidanov",
      "Daniel Erbesfeld",
      "Gera Weiss",
      "Achiya Elyasaf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05442",
    "title": "Broadcast via Mobile Agents in a Dynamic Network: Interplay of Graph Properties & Agents",
    "abstract": "           In this paper, we revisit the problem of \\textsc{Broadcast}, introduced by Das, Giachoudis, Luccio, and Markou [OPODIS, 2020], where $k+1$ agents are initially placed on an $n$ node dynamic graph, where $1$ agent has a message that must be broadcast to the remaining $k$ ignorant agents. The original paper studied the relationship between the number of agents needed to solve the problem and the edge density of the graph. The paper presented strong evidence that edge density of a graph, or the number of redundant edges within the graph, may be the correct graph property to accurately differentiate whether $k= o(n)$ agents (low edge density) or $k = \\Omega(n)$ agents (high edge density) are needed to solve the problem. In this paper, we show that surprisingly, edge density may not in fact be the correct differentiating property. The original paper presents graphs with edge density $1.1\\overline{6}$ that require $\\Omega(n)$ agents, however, we construct graphs with edge density $> 1.1\\overline{6}$ and develop an algorithm to solve the problem on those graphs using only $o(n)$ agents. We subsequently show that the relationship between edge density and number of agents is fairly weak by first constructing graphs with edge density tending to $1$ from above that require $\\Omega(n/f(n))$ agents to solve, for any function $f(n) \\to \\infty$ as $n \\to \\infty$. We then construct an infinite family of graphs with edge density $< \\rho$ requiring exactly $k$ ignorant agents to solve \\textsc{Broadcast}, for any $k>0$ and $\\rho>1$.         ",
    "url": "https://arxiv.org/abs/2504.05442",
    "authors": [
      "William K. Moses Jr.",
      "Amanda Redlich",
      "Frederick Stock"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2504.05454",
    "title": "GraphPINE: Graph Importance Propagation for Interpretable Drug Response Prediction",
    "abstract": "           Explainability is necessary for many tasks in biomedical research. Recent explainability methods have focused on attention, gradient, and Shapley value. These do not handle data with strong associated prior knowledge and fail to constrain explainability results based on known relationships between predictive features. We propose GraphPINE, a graph neural network (GNN) architecture leveraging domain-specific prior knowledge to initialize node importance optimized during training for drug response prediction. Typically, a manual post-prediction step examines literature (i.e., prior knowledge) to understand returned predictive features. While node importance can be obtained for gradient and attention after prediction, node importance from these methods lacks complementary prior knowledge; GraphPINE seeks to overcome this limitation. GraphPINE differs from other GNN gating methods by utilizing an LSTM-like sequential format. We introduce an importance propagation layer that unifies 1) updates for feature matrix and node importance and 2) uses GNN-based graph propagation of feature values. This initialization and updating mechanism allows for informed feature learning and improved graph representation. We apply GraphPINE to cancer drug response prediction using drug screening and gene data collected for over 5,000 gene nodes included in a gene-gene graph with a drug-target interaction (DTI) graph for initial importance. The gene-gene graph and DTIs were obtained from curated sources and weighted by article count discussing relationships between drugs and genes. GraphPINE achieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.05454",
    "authors": [
      "Yoshitaka Inoue",
      "Tianfan Fu",
      "Augustin Luna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Genomics (q-bio.GN)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2504.05456",
    "title": "Generative Adversarial Networks with Limited Data: A Survey and Benchmarking",
    "abstract": "           Generative Adversarial Networks (GANs) have shown impressive results in various image synthesis tasks. Vast studies have demonstrated that GANs are more powerful in feature and expression learning compared to other generative models and their latent space encodes rich semantic information. However, the tremendous performance of GANs heavily relies on the access to large-scale training data and deteriorates rapidly when the amount of data is limited. This paper aims to provide an overview of GANs, its variants and applications in various vision tasks, focusing on addressing the limited data issue. We analyze state-of-the-art GANs in limited data regime with designed experiments, along with presenting various methods attempt to tackle this problem from different perspectives. Finally, we further elaborate on remaining challenges and trends for future research.         ",
    "url": "https://arxiv.org/abs/2504.05456",
    "authors": [
      "Omar De Mitri",
      "Ruyu Wang",
      "Marco F. Huber"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05463",
    "title": "REVEAL: Relation-based Video Representation Learning for Video-Question-Answering",
    "abstract": "           Video-Question-Answering (VideoQA) comprises the capturing of complex visual relation changes over time, remaining a challenge even for advanced Video Language Models (VLM), i.a., because of the need to represent the visual content to a reasonably sized input for those models. To address this problem, we propose RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed to capture visual relation information by encoding them into structured, decomposed representations. Specifically, inspired by spatiotemporal scene graphs, we propose to encode video sequences as sets of relation triplets in the form of (\\textit{subject-predicate-object}) over time via their language embeddings. To this end, we extract explicit relations from video captions and introduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a Q-Former architecture to align an unordered set of video-derived queries with corresponding text-based relation descriptions. At inference, the resulting Q-former produces an efficient token representation that can serve as input to a VLM for VideoQA. We evaluate the proposed framework on five challenging benchmarks: NeXT-QA, Intent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video representation is able to outperform global alignment-based CLS or patch token representations and achieves competitive results against state-of-the-art models, particularly on tasks requiring temporal reasoning and relation comprehension. The code and models will be publicly released.         ",
    "url": "https://arxiv.org/abs/2504.05463",
    "authors": [
      "Sofian Chaybouti",
      "Walid Bousselham",
      "Moritz Wolter",
      "Hilde Kuehne"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05471",
    "title": "Graph Neural Networks for Enhancing Ensemble Forecasts of Extreme Rainfall",
    "abstract": "           Climate change is increasing the occurrence of extreme precipitation events, threatening infrastructure, agriculture, and public safety. Ensemble prediction systems provide probabilistic forecasts but exhibit biases and difficulties in capturing extreme weather. While post-processing techniques aim to enhance forecast accuracy, they rarely focus on precipitation, which exhibits complex spatial dependencies and tail behavior. Our novel framework leverages graph neural networks to post-process ensemble forecasts, specifically modeling the extremes of the underlying distribution. This allows to capture spatial dependencies and improves forecast accuracy for extreme events, thus leading to more reliable forecasts and mitigating risks of extreme precipitation and flooding.         ",
    "url": "https://arxiv.org/abs/2504.05471",
    "authors": [
      "Christopher B\u00fclte",
      "Sohir Maskey",
      "Philipp Scholl",
      "Jonas von Berg",
      "Gitta Kutyniok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05477",
    "title": "Trust Through Transparency: Explainable Social Navigation for Autonomous Mobile Robots via Vision-Language Models",
    "abstract": "           Service and assistive robots are increasingly being deployed in dynamic social environments; however, ensuring transparent and explainable interactions remains a significant challenge. This paper presents a multimodal explainability module that integrates vision language models and heat maps to improve transparency during navigation. The proposed system enables robots to perceive, analyze, and articulate their observations through natural language summaries. User studies (n=30) showed a preference of majority for real-time explanations, indicating improved trust and understanding. Our experiments were validated through confusion matrix analysis to assess the level of agreement with human expectations. Our experimental and simulation results emphasize the effectiveness of explainability in autonomous navigation, enhancing trust and interpretability.         ",
    "url": "https://arxiv.org/abs/2504.05477",
    "authors": [
      "Oluwadamilola Sotomi",
      "Devika Kodi",
      "Aliasghar Arab"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.05478",
    "title": "GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases",
    "abstract": "           Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q\\&As on large text-attributed KGs.         ",
    "url": "https://arxiv.org/abs/2504.05478",
    "authors": [
      "Alfred Clemedtson",
      "Borun Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.05483",
    "title": "Secure Diagnostics: Adversarial Robustness Meets Clinical Interpretability",
    "abstract": "           Deep neural networks for medical image classification often fail to generalize consistently in clinical practice due to violations of the i.i.d. assumption and opaque decision-making. This paper examines interpretability in deep neural networks fine-tuned for fracture detection by evaluating model performance against adversarial attack and comparing interpretability methods to fracture regions annotated by an orthopedic surgeon. Our findings prove that robust models yield explanations more aligned with clinically meaningful areas, indicating that robustness encourages anatomically relevant feature prioritization. We emphasize the value of interpretability for facilitating human-AI collaboration, in which models serve as assistants under a human-in-the-loop paradigm: clinically plausible explanations foster trust, enable error correction, and discourage reliance on AI for high-stakes decisions. This paper investigates robustness and interpretability as complementary benchmarks for bridging the gap between benchmark performance and safe, actionable clinical deployment.         ",
    "url": "https://arxiv.org/abs/2504.05483",
    "authors": [
      "Mohammad Hossein Najafi",
      "Mohammad Morsali",
      "Mohammadreza Pashanejad",
      "Saman Soleimani Roudi",
      "Mohammad Norouzi",
      "Saeed Bagheri Shouraki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05493",
    "title": "Neural network-enhanced integrators for simulating ordinary differential equations",
    "abstract": "           Numerous applications necessitate the computation of numerical solutions to differential equations across a wide range of initial conditions and system parameters, which feeds the demand for efficient yet accurate numerical integration this http URL study proposes a neural network (NN) enhancement of classical numerical integrators. NNs are trained to learn integration errors, which are then used as additive correction terms in numerical schemes. The performance of these enhanced integrators is compared with well-established methods through numerical studies, with a particular emphasis on computational efficiency. Analytical properties are examined in terms of local errors and backward error analysis. Embedded Runge-Kutta schemes are then employed to develop enhanced integrators that mitigate generalization risk, ensuring that the neural network's evaluation in previously unseen regions of the state space does not destabilize the integrator. It is guaranteed that the enhanced integrators perform at least as well as the desired classical Runge-Kutta schemes. The effectiveness of the proposed approaches is demonstrated through extensive numerical studies using a realistic model of a wind turbine, with parameters derived from the established simulation framework OpenFast.         ",
    "url": "https://arxiv.org/abs/2504.05493",
    "authors": [
      "Amine Othmane",
      "Kathrin Fla\u00dfkamp"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05499",
    "title": "Few-shot Personalized Scanpath Prediction",
    "abstract": "           A personalized model for scanpath prediction provides insights into the visual preferences and attention patterns of individual subjects. However, existing methods for training scanpath prediction models are data-intensive and cannot be effectively personalized to new individuals with only a few available examples. In this paper, we propose few-shot personalized scanpath prediction task (FS-PSP) and a novel method to address it, which aims to predict scanpaths for an unseen subject using minimal support data of that subject's scanpath behavior. The key to our method's adaptability is the Subject-Embedding Network (SE-Net), specifically designed to capture unique, individualized representations for each subject's scanpaths. SE-Net generates subject embeddings that effectively distinguish between subjects while minimizing variability among scanpaths from the same individual. The personalized scanpath prediction model is then conditioned on these subject embeddings to produce accurate, personalized results. Experiments on multiple eye-tracking datasets demonstrate that our method excels in FS-PSP settings and does not require any fine-tuning steps at test time. Code is available at: this https URL ",
    "url": "https://arxiv.org/abs/2504.05499",
    "authors": [
      "Ruoyu Xue",
      "Jingyi Xu",
      "Sounak Mondal",
      "Hieu Le",
      "Gregory Zelinsky",
      "Minh Hoai",
      "Dimitris Samaras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05500",
    "title": "Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search",
    "abstract": "           The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, test coverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations.         ",
    "url": "https://arxiv.org/abs/2504.05500",
    "authors": [
      "Vahid Majdinasab",
      "Amin Nikanjam",
      "Foutse Khomh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.05504",
    "title": "SelfMAD: Enhancing Generalization and Robustness in Morphing Attack Detection via Self-Supervised Learning",
    "abstract": "           With the continuous advancement of generative models, face morphing attacks have become a significant challenge for existing face verification systems due to their potential use in identity fraud and other malicious activities. Contemporary Morphing Attack Detection (MAD) approaches frequently rely on supervised, discriminative models trained on examples of bona fide and morphed images. These models typically perform well with morphs generated with techniques seen during training, but often lead to sub-optimal performance when subjected to novel unseen morphing techniques. While unsupervised models have been shown to perform better in terms of generalizability, they typically result in higher error rates, as they struggle to effectively capture features of subtle artifacts. To address these shortcomings, we present SelfMAD, a novel self-supervised approach that simulates general morphing attack artifacts, allowing classifiers to learn generic and robust decision boundaries without overfitting to the specific artifacts induced by particular face morphing methods. Through extensive experiments on widely used datasets, we demonstrate that SelfMAD significantly outperforms current state-of-the-art MADs, reducing the detection error by more than 64% in terms of EER when compared to the strongest unsupervised competitor, and by more than 66%, when compared to the best performing discriminative MAD model, tested in cross-morph settings. The source code for SelfMAD is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.05504",
    "authors": [
      "Marija Ivanovska",
      "Leon Todorov",
      "Naser Damer",
      "Deepak Kumar Jain",
      "Peter Peer",
      "Vitomir \u0160truc"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05515",
    "title": "How Do Solidity Versions Affect Vulnerability Detection Tools? An Empirical Study",
    "abstract": "           Context: Smart contract vulnerabilities pose significant security risks for the Ethereum ecosystem, driving the development of automated tools for detection and mitigation. Smart contracts are written in Solidity, a programming language that is rapidly evolving to add features and improvements to enhance smart contract security. New versions of Solidity change the compilation process, potentially affecting how tools interpret and analyze smart contract code. Objective: In such a continuously evolving landscape, we aim to investigate the compatibility of detection tools with Solidity versions. More specifically, we present a plan to study detection tools by empirically assessing (i) their compatibility with the Solidity pragma directives, (ii) their detection effectiveness, and (iii) their execution time across different versions of Solidity. Method: We will conduct an exploratory study by running several tools and collecting a large number of real-world smart contracts to create a balanced dataset. We will track and analyze the tool execution through SmartBugs, a framework that facilitates the tool execution and allows the integration of new tools.         ",
    "url": "https://arxiv.org/abs/2504.05515",
    "authors": [
      "Gerardo Iuliano",
      "Davide Corradini",
      "Michele Pasqua",
      "Mariano Ceccato",
      "Dario Di Nucci"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.05518",
    "title": "Evaluating the Generalization Capabilities of Large Language Models on Code Reasoning",
    "abstract": "           We assess how the code reasoning abilities of large language models (LLMs) generalize to different kinds of programs. We present techniques for obtaining in- and out-of-distribution programs with different characteristics: code sampled from a domain-specific language, code automatically generated by an LLM, code collected from competitive programming contests, and mutated versions of these programs. We also present an experimental methodology for evaluating LLM generalization by comparing their performance on these programs. We perform an extensive evaluation across 10 state-of-the-art models from the past year, obtaining insights into their generalization capabilities over time and across different classes of programs. Our results highlight that while earlier models exhibit behavior consistent with pattern matching, the latest models exhibit strong generalization abilities on code reasoning.         ",
    "url": "https://arxiv.org/abs/2504.05518",
    "authors": [
      "Rem Yang",
      "Julian Dai",
      "Nikos Vasilakis",
      "Martin Rinard"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05530",
    "title": "FORCE: Feature-Oriented Representation with Clustering and Explanation",
    "abstract": "           Learning about underlying patterns in data using latent unobserved structures to improve the accuracy of predictive models has become an active avenue of deep learning research. Most approaches cluster the original features to capture certain latent structures. However, the information gained in the process can often be implicitly derived by sufficiently complex models. Thus, such approaches often provide minimal benefits. We propose a SHAP (Shapley Additive exPlanations) based supervised deep learning framework FORCE which relies on two-stage usage of SHAP values in the neural network architecture, (i) an additional latent feature to guide model training, based on clustering SHAP values, and (ii) initiating an attention mechanism within the architecture using latent information. This approach gives a neural network an indication about the effect of unobserved values that modify feature importance for an observation. The proposed framework is evaluated on three real life datasets. Our results demonstrate that FORCE led to dramatic improvements in overall performance as compared to networks that did not incorporate the latent feature and attention framework (e.g., F1 score for presence of heart disease 0.80 vs 0.72). Using cluster assignments and attention based on SHAP values guides deep learning, enhancing latent pattern learning and overall discriminative capability.         ",
    "url": "https://arxiv.org/abs/2504.05530",
    "authors": [
      "Rishav Mukherjee",
      "Jeffrey Ahearn Thompson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2504.05556",
    "title": "Novel Data-Driven Indices for Early Detection and Quantification of Short-Term Voltage Instability from Voltage Trajectories",
    "abstract": "           This paper presents a novel Short-Term Voltage Stability Index (STVSI), which leverages Lyapunov Exponent-based detection to assess and quantify short-term stability triggered by Over Excitation Limiters (OELs) or undamped oscillations in voltage. The proposed method is measurement-based and decomposes the voltage trajectory into two key components using Empirical Mode Decomposition (EMD): a residual part, which indicates delayed voltage recovery, and an oscillatory part, which captures oscillations. The residual component is critical, as it can detect activation of OELs in synchronous generators or Low Voltage Ride-Through (LVRT) relays in inverter-based resources, potentially leading to instability within the quasisteady-state time frame. Meanwhile, the oscillatory component may indicate either a stable or unstable state in the short term. To accurately assess stability, STVSI employs an entropy-based metric to measure the proximity of the system to instability, with specific indices for short-term voltage stability based on oscillations and recovery. Simulations on the Nordic power system demonstrate that STVSI effectively identifies and categorizes voltage stability issues. Moreover, STVSI not only detects voltage stability conditions but also qualitatively assesses the extent of stability, providing a nuanced measure of stability.         ",
    "url": "https://arxiv.org/abs/2504.05556",
    "authors": [
      "Mohammad Almomani",
      "Muhammad Sarwar",
      "Venkataramana Ajjarapu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05579",
    "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
    "abstract": "           Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.         ",
    "url": "https://arxiv.org/abs/2504.05579",
    "authors": [
      "Artem Zholus",
      "Carl Doersch",
      "Yi Yang",
      "Skanda Koppula",
      "Viorica Patraucean",
      "Xu Owen He",
      "Ignacio Rocco",
      "Mehdi S. M. Sajjadi",
      "Sarath Chandar",
      "Ross Goroshin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05588",
    "title": "Multi-fidelity Reinforcement Learning Control for Complex Dynamical Systems",
    "abstract": "           Controlling instabilities in complex dynamical systems is challenging in scientific and engineering applications. Deep reinforcement learning (DRL) has seen promising results for applications in different scientific applications. The many-query nature of control tasks requires multiple interactions with real environments of the underlying physics. However, it is usually sparse to collect from the experiments or expensive to simulate for complex dynamics. Alternatively, controlling surrogate modeling could mitigate the computational cost issue. However, a fast and accurate learning-based model by offline training makes it very hard to get accurate pointwise dynamics when the dynamics are chaotic. To bridge this gap, the current work proposes a multi-fidelity reinforcement learning (MFRL) framework that leverages differentiable hybrid models for control tasks, where a physics-based hybrid model is corrected by limited high-fidelity data. We also proposed a spectrum-based reward function for RL learning. The effect of the proposed framework is demonstrated on two complex dynamics in physics. The statistics of the MFRL control result match that computed from many-query evaluations of the high-fidelity environments and outperform other SOTA baselines.         ",
    "url": "https://arxiv.org/abs/2504.05588",
    "authors": [
      "Luning Sun",
      "Xin-Yang Liu",
      "Siyan Zhao",
      "Aditya Grover",
      "Jian-Xun Wang",
      "Jayaraman J. Thiagarajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05601",
    "title": "AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes",
    "abstract": "           Object detection in Unmanned Aerial Vehicle (UAV) images poses significant challenges due to complex scale variations and class imbalance among objects. Existing methods often address these challenges separately, overlooking the intricate nature of UAV images and the potential synergy between them. In response, this paper proposes AD-Det, a novel framework employing a coherent coarse-to-fine strategy that seamlessly integrates two pivotal components: Adaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste (DCC). ASOE utilizes a high-resolution feature map to identify and cluster regions containing small objects. These regions are subsequently enlarged and processed by a fine-grained detector. On the other hand, DCC conducts object-level resampling by dynamically pasting tail classes around the cluster centers obtained by ASOE, main-taining a dynamic memory bank for each tail class. This approach enables AD-Det to not only extract regions with small objects for precise detection but also dynamically perform reasonable resampling for tail-class objects. Consequently, AD-Det enhances the overall detection performance by addressing the challenges of scale variations and class imbalance in UAV images through a synergistic and adaptive framework. We extensively evaluate our approach on two public datasets, i.e., VisDrone and UAVDT, and demonstrate that AD-Det significantly outperforms existing competitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision (AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.         ",
    "url": "https://arxiv.org/abs/2504.05601",
    "authors": [
      "Zhenteng Li",
      "Sheng Lian",
      "Dengfeng Pan",
      "Youlin Wang",
      "Wei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05634",
    "title": "Simplifying Data Integration: SLM-Driven Systems for Unified Semantic Queries Across Heterogeneous Databases",
    "abstract": "           The integration of heterogeneous databases into a unified querying framework remains a critical challenge, particularly in resource-constrained environments. This paper presents a novel Small Language Model(SLM)-driven system that synergizes advancements in lightweight Retrieval-Augmented Generation (RAG) and semantic-aware data structuring to enable efficient, accurate, and scalable query resolution across diverse data formats. By integrating MiniRAG's semantic-aware heterogeneous graph indexing and topology-enhanced retrieval with SLM-powered structured data extraction, our system addresses the limitations of traditional methods in handling Multi-Entity Question Answering (Multi-Entity QA) and complex semantic queries. Experimental results demonstrate superior performance in accuracy and efficiency, while the introduction of semantic entropy as an unsupervised evaluation metric provides robust insights into model uncertainty. This work pioneers a cost-effective, domain-agnostic solution for next-generation database systems.         ",
    "url": "https://arxiv.org/abs/2504.05634",
    "authors": [
      "Teng Lin"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.05649",
    "title": "POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud",
    "abstract": "           LiDAR-based 3D object detection is a fundamental task in the field of autonomous driving. This paper explores the unique advantage of Frequency Modulated Continuous Wave (FMCW) LiDAR in autonomous perception. Given a single frame FMCW point cloud with radial velocity measurements, we expect that our object detector can detect the short-term future locations of objects using only the current frame sensor data and demonstrate a fast ability to respond to intermediate danger. To achieve this, we extend the standard object detection task to a novel task named predictive object detection (POD), which aims to predict the short-term future location and dimensions of objects based solely on current observations. Typically, a motion prediction task requires historical sensor information to process the temporal contexts of each object, while our detector's avoidance of multi-frame historical information enables a much faster response time to potential dangers. The core advantage of FMCW LiDAR lies in the radial velocity associated with every reflected point. We propose a novel POD framework, the core idea of which is to generate a virtual future point using a ray casting mechanism, create virtual two-frame point clouds with the current and virtual future frames, and encode these two-frame voxel features with a sparse 4D encoder. Subsequently, the 4D voxel features are separated by temporal indices and remapped into two Bird's Eye View (BEV) features: one decoded for standard current frame object detection and the other for future predictive object detection. Extensive experiments on our in-house dataset demonstrate the state-of-the-art standard and predictive detection performance of the proposed POD framework.         ",
    "url": "https://arxiv.org/abs/2504.05649",
    "authors": [
      "Yining Shi",
      "Kun Jiang",
      "Xin Zhao",
      "Kangan Qian",
      "Chuchu Xie",
      "Tuopu Wen",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05662",
    "title": "Reconstruction-Free Anomaly Detection with Diffusion Models via Direct Latent Likelihood Evaluation",
    "abstract": "           Diffusion models, with their robust distribution approximation capabilities, have demonstrated excellent performance in anomaly detection. However, conventional reconstruction-based approaches rely on computing the reconstruction error between the original and denoised images, which requires careful noise-strength tuning and over ten network evaluations per input-leading to significantly slower detection speeds. To address these limitations, we propose a novel diffusion-based anomaly detection method that circumvents the need for resource-intensive reconstruction. Instead of reconstructing the input image, we directly infer its corresponding latent variables and measure their density under the Gaussian prior distribution. Remarkably, the prior density proves effective as an anomaly score even when using a short partial diffusion process of only 2-5 steps. We evaluate our method on the MVTecAD dataset, achieving an AUC of 0.991 at 15 FPS, thereby setting a new state-of-the-art speed-AUC anomaly detection trade-off.         ",
    "url": "https://arxiv.org/abs/2504.05662",
    "authors": [
      "Shunsuke Sakai",
      "Tatsuhito Hasegawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05670",
    "title": "Dual Boost-Driven Graph-Level Clustering Network",
    "abstract": "           Graph-level clustering remains a pivotal yet formidable challenge in graph learning. Recently, the integration of deep learning with representation learning has demonstrated notable advancements, yielding performance enhancements to a certain degree. However, existing methods suffer from at least one of the following issues: 1. the original graph structure has noise, and 2. during feature propagation and pooling processes, noise is gradually aggregated into the graph-level embeddings through information propagation. Consequently, these two limitations mask clustering-friendly information, leading to suboptimal graph-level clustering performance. To this end, we propose a novel Dual Boost-Driven Graph-Level Clustering Network (DBGCN) to alternately promote graph-level clustering and filtering out interference information in a unified framework. Specifically, in the pooling step, we evaluate the contribution of features at the global and optimize them using a learnable transformation matrix to obtain high-quality graph-level representation, such that the model's reasoning capability can be improved. Moreover, to enable reliable graph-level clustering, we first identify and suppress information detrimental to clustering by evaluating similarities between graph-level representations, providing more accurate guidance for multi-view fusion. Extensive experiments demonstrated that DBGCN outperforms the state-of-the-art graph-level clustering methods on six benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2504.05670",
    "authors": [
      "John Smith",
      "Wenxuan Tu",
      "Junlong Wu",
      "Wenxin Zhang",
      "Jingxin Liu",
      "Haotian Wang",
      "Jieren Cheng",
      "Huajie Lei",
      "Guangzhen Yao",
      "Lingren Wang",
      "Mengfei Li",
      "Renda Han",
      "Yu Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05672",
    "title": "Contrastive Decoupled Representation Learning and Regularization for Speech-Preserving Facial Expression Manipulation",
    "abstract": "           Speech-preserving facial expression manipulation (SPFEM) aims to modify a talking head to display a specific reference emotion while preserving the mouth animation of source spoken contents. Thus, emotion and content information existing in reference and source inputs can provide direct and accurate supervision signals for SPFEM models. However, the intrinsic intertwining of these elements during the talking process poses challenges to their effectiveness as supervisory signals. In this work, we propose to learn content and emotion priors as guidance augmented with contrastive learning to learn decoupled content and emotion representation via an innovative Contrastive Decoupled Representation Learning (CDRL) algorithm. Specifically, a Contrastive Content Representation Learning (CCRL) module is designed to learn audio feature, which primarily contains content information, as content priors to guide learning content representation from the source input. Meanwhile, a Contrastive Emotion Representation Learning (CERL) module is proposed to make use of a pre-trained visual-language model to learn emotion prior, which is then used to guide learning emotion representation from the reference input. We further introduce emotion-aware and emotion-augmented contrastive learning to train CCRL and CERL modules, respectively, ensuring learning emotion-independent content representation and content-independent emotion representation. During SPFEM model training, the decoupled content and emotion representations are used to supervise the generation process, ensuring more accurate emotion manipulation together with audio-lip synchronization. Extensive experiments and evaluations on various benchmarks show the effectiveness of the proposed algorithm.         ",
    "url": "https://arxiv.org/abs/2504.05672",
    "authors": [
      "Tianshui Chen",
      "Jianman Lin",
      "Zhijing Yang",
      "Chumei Qing",
      "Yukai Shi",
      "Liang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2504.05684",
    "title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis",
    "abstract": "           This paper introduces Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning (TARO), a novel framework for high-fidelity and temporally coherent video-to-audio synthesis. Built upon flow-based transformers, which offer stable training and continuous transformations for enhanced synchronization and audio quality, TARO introduces two key innovations: (1) Timestep-Adaptive Representation Alignment (TRA), which dynamically aligns latent representations by adjusting alignment strength based on the noise schedule, ensuring smooth evolution and improved fidelity, and (2) Onset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp event-driven markers of audio-relevant visual moments to enhance synchronization with dynamic visual events. Extensive experiments on the VGGSound and Landscape datasets demonstrate that TARO outperforms prior methods, achieving relatively 53\\% lower Frechet Distance (FD), 29% lower Frechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its superior audio quality and synchronization precision.         ",
    "url": "https://arxiv.org/abs/2504.05684",
    "authors": [
      "Tri Ton",
      "Ji Woo Hong",
      "Chang D. Yoo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05686",
    "title": "kNN-SVC: Robust Zero-Shot Singing Voice Conversion with Additive Synthesis and Concatenation Smoothness Optimization",
    "abstract": "           Robustness is critical in zero-shot singing voice conversion (SVC). This paper introduces two novel methods to strengthen the robustness of the kNN-VC framework for SVC. First, kNN-VC's core representation, WavLM, lacks harmonic emphasis, resulting in dull sounds and ringing artifacts. To address this, we leverage the bijection between WavLM, pitch contours, and spectrograms to perform additive synthesis, integrating the resulting waveform into the model to mitigate these issues. Second, kNN-VC overlooks concatenative smoothness, a key perceptual factor in SVC. To enhance smoothness, we propose a new distance metric that filters out unsuitable kNN candidates and optimize the summing weights of the candidates during inference. Although our techniques are built on the kNN-VC framework for implementation convenience, they are broadly applicable to general concatenative neural synthesis models. Experimental results validate the effectiveness of these modifications in achieving robust SVC. Demo: this http URL Code: this https URL ",
    "url": "https://arxiv.org/abs/2504.05686",
    "authors": [
      "Keren Shao",
      "Ke Chen",
      "Matthew Baas",
      "Shlomo Dubnov"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2504.05695",
    "title": "Architecture independent generalization bounds for overparametrized deep ReLU networks",
    "abstract": "           We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.         ",
    "url": "https://arxiv.org/abs/2504.05695",
    "authors": [
      "Thomas Chen",
      "Chun-Kai Kevin Chien",
      "Patricia Mu\u00f1oz Ewald",
      "Andrew G. Moore"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Analysis of PDEs (math.AP)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2504.05697",
    "title": "VADIS: A Visual Analytics Pipeline for Dynamic Document Representation and Information-Seeking",
    "abstract": "           In the biomedical domain, visualizing the document embeddings of an extensive corpus has been widely used in information-seeking tasks. However, three key challenges with existing visualizations make it difficult for clinicians to find information efficiently. First, the document embeddings used in these visualizations are generated statically by pretrained language models, which cannot adapt to the user's evolving interest. Second, existing document visualization techniques cannot effectively display how the documents are relevant to users' interest, making it difficult for users to identify the most pertinent information. Third, existing embedding generation and visualization processes suffer from a lack of interpretability, making it difficult to understand, trust and use the result for decision-making. In this paper, we present a novel visual analytics pipeline for user driven document representation and iterative information seeking (VADIS). VADIS introduces a prompt-based attention model (PAM) that generates dynamic document embedding and document relevance adjusted to the user's query. To effectively visualize these two pieces of information, we design a new document map that leverages a circular grid layout to display documents based on both their relevance to the query and the semantic similarity. Additionally, to improve the interpretability, we introduce a corpus-level attention visualization method to improve the user's understanding of the model focus and to enable the users to identify potential oversight. This visualization, in turn, empowers users to refine, update and introduce new queries, thereby facilitating a dynamic and iterative information-seeking experience. We evaluated VADIS quantitatively and qualitatively on a real-world dataset of biomedical research papers to demonstrate its effectiveness.         ",
    "url": "https://arxiv.org/abs/2504.05697",
    "authors": [
      "Rui Qiu",
      "Yamei Tu",
      "Po-Yin Yen",
      "Han-Wei Shen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2504.05706",
    "title": "SEVERE++: Evaluating Benchmark Sensitivity in Generalization of Video Representation Learning",
    "abstract": "           Continued advances in self-supervised learning have led to significant progress in video representation learning, offering a scalable alternative to supervised approaches by removing the need for manual annotations. Despite strong performance on standard action recognition benchmarks, video self-supervised learning methods are largely evaluated under narrow protocols, typically pretraining on Kinetics-400 and fine-tuning on similar datasets, limiting our understanding of their generalization in real world scenarios. In this work, we present a comprehensive evaluation of modern video self-supervised models, focusing on generalization across four key downstream factors: domain shift, sample efficiency, action granularity, and task diversity. Building on our prior work analyzing benchmark sensitivity in CNN-based contrastive learning, we extend the study to cover state-of-the-art transformer-based video-only and video-text models. Specifically, we benchmark 12 transformer-based methods (7 video-only, 5 video-text) and compare them to 10 CNN-based methods, totaling over 1100 experiments across 8 datasets and 7 downstream tasks. Our analysis shows that, despite architectural advances, transformer-based models remain sensitive to downstream conditions. No method generalizes consistently across all factors, video-only transformers perform better under domain shifts, CNNs outperform for fine-grained tasks, and video-text models often underperform despite large scale pretraining. We also find that recent transformer models do not consistently outperform earlier approaches. Our findings provide a detailed view of the strengths and limitations of current video SSL methods and offer a unified benchmark for evaluating generalization in video representation learning.         ",
    "url": "https://arxiv.org/abs/2504.05706",
    "authors": [
      "Fida Mohammad Thoker",
      "Letian Jiang",
      "Chen Zhao",
      "Piyush Bagad",
      "Hazel Doughty",
      "Bernard Ghanem",
      "Cees G. M. Snoek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05720",
    "title": "QEMesh: Employing A Quadric Error Metrics-Based Representation for Mesh Generation",
    "abstract": "           Mesh generation plays a crucial role in 3D content creation, as mesh is widely used in various industrial applications. Recent works have achieved impressive results but still face several issues, such as unrealistic patterns or pits on surfaces, thin parts missing, and incomplete structures. Most of these problems stem from the choice of shape representation or the capabilities of the generative network. To alleviate these, we extend PoNQ, a Quadric Error Metrics (QEM)-based representation, and propose a novel model, QEMesh, for high-quality mesh generation. PoNQ divides the shape surface into tiny patches, each represented by a point with its normal and QEM matrix, which preserves fine local geometry information. In our QEMesh, we regard these elements as generable parameters and design a unique latent diffusion model containing a novel multi-decoder VAE for PoNQ parameters generation. Given the latent code generated by the diffusion model, three parameter decoders produce several PoNQ parameters within each voxel cell, and an occupancy decoder predicts which voxel cells containing parameters to form the final shape. Extensive evaluations demonstrate that our method generates results with watertight surfaces and is comparable to state-of-the-art methods in several main metrics.         ",
    "url": "https://arxiv.org/abs/2504.05720",
    "authors": [
      "Jiaqi Li",
      "Ruowei Wang",
      "Yu Liu",
      "Qijun Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05728",
    "title": "AI-Driven Prognostics for State of Health Prediction in Li-ion Batteries: A Comprehensive Analysis with Validation",
    "abstract": "           This paper presents a comprehensive review of AI-driven prognostics for State of Health (SoH) prediction in lithium-ion batteries. We compare the effectiveness of various AI algorithms, including FFNN, LSTM, and BiLSTM, across multiple datasets (CALCE, NASA, UDDS) and scenarios (e.g., varying temperatures and driving conditions). Additionally, we analyze the factors influencing SoH fluctuations, such as temperature and charge-discharge rates, and validate our findings through simulations. The results demonstrate that BiLSTM achieves the highest accuracy, with an average RMSE reduction of 15% compared to LSTM, highlighting its robustness in real-world applications.         ",
    "url": "https://arxiv.org/abs/2504.05728",
    "authors": [
      "Tianqi Ding",
      "Dawei Xiang",
      "Tianyao Sun",
      "YiJiashum Qi",
      "Zunduo Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05729",
    "title": "Robust and Efficient Average Consensus with Non-Coherent Over-the-Air Aggregation",
    "abstract": "           Non-coherent over-the-air (OTA) computation has garnered increasing attention for its advantages in facilitating information aggregation among distributed agents in resource-constrained networks without requiring precise channel estimation. A promising application scenario of this method is distributed average consensus in wireless multi-agent systems. However, in such scenario, non-coherent interference from concurrent OTA transmissions can introduce bias in the consensus value. To address this issue, we develop a robust distributed average consensus algorithm by formulating the consensus problem as a distributed optimization problem. Using decentralized projected gradient descent (D-PGD), our proposed algorithm can achieve unbiased mean square average consensus even in the presence of non-coherent interference and noise. Additionally, we implement transmit power control and receive scaling mechanisms to further accelerate convergence. Simulation results demonstrate that our method can significantly enhance the convergence speed of the D-PGD algorithm for OTA average consensus without compromising accuracy.         ",
    "url": "https://arxiv.org/abs/2504.05729",
    "authors": [
      "Yuhang Deng",
      "Zheng Chen",
      "Erik G. Larsson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05732",
    "title": "LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources",
    "abstract": "           Long-form generation is crucial for a wide range of practical applications, typically categorized into short-to-long and long-to-long generation. While short-to-long generations have received considerable attention, generating long texts from extremely long resources remains relatively underexplored. The primary challenge in long-to-long generation lies in effectively integrating and analyzing relevant information from extensive inputs, which remains difficult for current large language models (LLMs). In this paper, we propose LLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance the ability of LLMs to process extremely long inputs. Drawing inspiration from convolutional neural networks, which iteratively integrate local features into higher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked convolutional scaling layers to progressively expand the understanding of input materials. Both quantitative and qualitative experimental results demonstrate that our approach substantially enhances the ability of LLMs to process long inputs and generate coherent, informative long-form articles, outperforming several representative baselines.         ",
    "url": "https://arxiv.org/abs/2504.05732",
    "authors": [
      "Haoyu Wang",
      "Yujia Fu",
      "Zhu Zhang",
      "Shuo Wang",
      "Zirui Ren",
      "Xiaorong Wang",
      "Zhili Li",
      "Chaoqun He",
      "Bo An",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05746",
    "title": "Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven One-Shot Talking Head Animation",
    "abstract": "           The paramount challenge in audio-driven One-shot Talking Head Animation (ADOS-THA) lies in capturing subtle imperceptible changes between adjacent video frames. Inherently, the temporal relationship of adjacent audio clips is highly correlated with that of the corresponding adjacent video frames, offering supplementary information that can be pivotal for guiding and supervising talking head animations. In this work, we propose to learn audio-visual correlations and integrate the correlations to help enhance feature representation and regularize final generation by a novel Temporal Audio-Visual Correlation Embedding (TAVCE) framework. Specifically, it first learns an audio-visual temporal correlation metric, ensuring the temporal audio relationships of adjacent clips are aligned with the temporal visual relationships of corresponding adjacent video frames. Since the temporal audio relationship contains aligned information about the visual frame, we first integrate it to guide learning more representative features via a simple yet effective channel attention mechanism. During training, we also use the alignment correlations as an additional objective to supervise generating visual frames. We conduct extensive experiments on several publicly available benchmarks (i.e., HDTF, LRW, VoxCeleb1, and VoxCeleb2) to demonstrate its superiority over existing leading algorithms.         ",
    "url": "https://arxiv.org/abs/2504.05746",
    "authors": [
      "Zhihua Xu",
      "Tianshui Chen",
      "Zhijing Yang",
      "Siyuan Peng",
      "Keze Wang",
      "Liang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05747",
    "title": "SEA-LION: Southeast Asian Languages in One Network",
    "abstract": "           Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.         ",
    "url": "https://arxiv.org/abs/2504.05747",
    "authors": [
      "Raymond Ng",
      "Thanh Ngan Nguyen",
      "Yuli Huang",
      "Ngee Chia Tai",
      "Wai Yi Leong",
      "Wei Qi Leong",
      "Xianbin Yong",
      "Jian Gang Ngui",
      "Yosephine Susanto",
      "Nicholas Cheng",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Adithya Venkatadri Hulagadri",
      "Kok Wai Teng",
      "Yeo Yeow Tong",
      "Bryan Siow",
      "Wei Yi Teo",
      "Wayne Lau",
      "Choon Meng Tan",
      "Brandon Ong",
      "Zhi Hao Ong",
      "Jann Railey Montalan",
      "Adwin Chan",
      "Sajeban Antonyrex",
      "Ren Lee",
      "Esther Choa",
      "David Ong Tat-Wee",
      "Bing Jie Darius Liu",
      "William Chandra Tjhi",
      "Erik Cambria",
      "Leslie Teo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05756",
    "title": "Interpretable Non-linear Survival Analysis with Evolutionary Symbolic Regression",
    "abstract": "           Survival Regression (SuR) is a key technique for modeling time to event in important applications such as clinical trials and semiconductor manufacturing. Currently, SuR algorithms belong to one of three classes: non-linear black-box -- allowing adaptability to many datasets but offering limited interpretability (e.g., tree ensembles); linear glass-box -- being easier to interpret but limited to modeling only linear interactions (e.g., Cox proportional hazards); and non-linear glass-box -- allowing adaptability and interpretability, but empirically found to have several limitations (e.g., explainable boosting machines, survival trees). In this work, we investigate whether Symbolic Regression (SR), i.e., the automated search of mathematical expressions from data, can lead to non-linear glass-box survival models that are interpretable and accurate. We propose an evolutionary, multi-objective, and multi-expression implementation of SR adapted to SuR. Our empirical results on five real-world datasets show that SR consistently outperforms traditional glass-box methods for SuR in terms of accuracy per number of dimensions in the model, while exhibiting comparable accuracy with black-box methods. Furthermore, we offer qualitative examples to assess the interpretability potential of SR models for SuR. Code at: this https URL.         ",
    "url": "https://arxiv.org/abs/2504.05756",
    "authors": [
      "Luigi Rovito",
      "Marco Virgolin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.05759",
    "title": "RETROcode: Leveraging a Code Database for Improved Natural Language to Code Generation",
    "abstract": "           As text and code resources have expanded, large-scale pre-trained models have shown promising capabilities in code generation tasks, typically employing supervised fine-tuning with problem statement-program pairs. However, increasing model size and data volume for performance gains also raises computational demands and risks of overfitting. Addressing these challenges, we present RETROcode, a novel adaptation of the RETRO architecture \\cite{RETRO} for sequence-to-sequence models, utilizing a large code database as an auxiliary scaling method. This approach, diverging from simply enlarging model and dataset sizes, allows RETROcode to leverage a vast code database for prediction, enhancing the model's efficiency by integrating extensive memory. Our findings indicate that RETROcode not only outperforms similar-sized traditional architectures on test sets but also approaches the effectiveness of the much larger Codex model, despite being trained from scratch on a substantially smaller dataset.         ",
    "url": "https://arxiv.org/abs/2504.05759",
    "authors": [
      "Nathana\u00ebl Beau",
      "Beno\u00eet Crabb\u00e9"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05761",
    "title": "AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams",
    "abstract": "           Concept drift and extreme verification latency pose significant challenges in data stream learning, particularly when dealing with recurring concept changes in dynamic environments. This work introduces a novel method based on the Growing Neural Gas (GNG) algorithm, designed to effectively handle abrupt recurrent drifts while adapting to incrementally evolving data distributions (incremental drifts). Leveraging the self-organizing and topological adaptability of GNG, the proposed approach maintains a compact yet informative memory structure, allowing it to efficiently store and retrieve knowledge of past or recurring concepts, even under conditions of delayed or sparse stream supervision. Our experiments highlight the superiority of our approach over existing data stream learning methods designed to cope with incremental non-stationarities and verification latency, demonstrating its ability to quickly adapt to new drifts, robustly manage recurring patterns, and maintain high predictive accuracy with a minimal memory footprint. Unlike other techniques that fail to leverage recurring knowledge, our proposed approach is proven to be a robust and efficient online learning solution for unsupervised drifting data flows.         ",
    "url": "https://arxiv.org/abs/2504.05761",
    "authors": [
      "Maria Arostegi",
      "Miren Nekane Bilbao",
      "Jesus L. Lobo",
      "Javier Del Ser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.05764",
    "title": "Layer-Aware Embedding Fusion for LLMs in Text Classifications",
    "abstract": "           Embedding fusion has emerged as an effective approach for enhancing performance across various NLP tasks. However, systematic guidelines for selecting optimal layers and developing effective fusion strategies for the integration of LLMs remain underexplored. In this study, we propose a layer-aware embedding selection method and investigate how to quantitatively evaluate different layers to identify the most important ones for downstream NLP tasks, showing that the critical layers vary depending on the dataset. We also explore how combining embeddings from multiple LLMs, without requiring model fine-tuning, can improve performance. Experiments on four English text classification datasets (SST-2, MR, R8, and R52) demonstrate that different layers in LLMs exhibit varying degrees of representational strength for classification, and that combining embeddings from different models can enhance performance if the models exhibit complementary characteristics. Additionally, we discuss resources overhead (memory and inference time) to provide a balanced perspective on the real world feasibility of embedding fusion. Future work will explore multilingual and domain specific datasets, as well as techniques for automating layer selection, to improve both performance and scalability.         ",
    "url": "https://arxiv.org/abs/2504.05764",
    "authors": [
      "Jiho Gwak",
      "Yuchul Jung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05767",
    "title": "Cross-Document Contextual Coreference Resolution in Knowledge Graphs",
    "abstract": "           Coreference resolution across multiple documents poses a significant challenge in natural language processing, particularly within the domain of knowledge graphs. This study introduces an innovative method aimed at identifying and resolving references to the same entities that appear across differing texts, thus enhancing the coherence and collaboration of information. Our method employs a dynamic linking mechanism that associates entities in the knowledge graph with their corresponding textual mentions. By utilizing contextual embeddings along with graph-based inference strategies, we effectively capture the relationships and interactions among entities, thereby improving the accuracy of coreference resolution. Rigorous evaluations on various benchmark datasets highlight notable advancements in our approach over traditional methodologies. The results showcase how the contextual information derived from knowledge graphs enhances the understanding of complex relationships across documents, leading to better entity linking and information extraction capabilities in applications driven by knowledge. Our technique demonstrates substantial improvements in both precision and recall, underscoring its effectiveness in the area of cross-document coreference resolution.         ",
    "url": "https://arxiv.org/abs/2504.05767",
    "authors": [
      "Zhang Dong",
      "Mingbang Wang",
      "Songhang deng",
      "Le Dai",
      "Jiyuan Li",
      "Xingzu Liu",
      "Ruilin Nong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2504.05768",
    "title": "Temporal Dynamic Embedding for Irregularly Sampled Time Series",
    "abstract": "           In several practical applications, particularly healthcare, clinical data of each patient is individually recorded in a database at irregular intervals as required. This causes a sparse and irregularly sampled time series, which makes it difficult to handle as a structured representation of the prerequisites of neural network models. We therefore propose temporal dynamic embedding (TDE), which enables neural network models to receive data that change the number of variables over time. TDE regards each time series variable as an embedding vector evolving over time, instead of a conventional fixed structured representation, which causes a critical missing problem. For each time step, TDE allows for the selective adoption and aggregation of only observed variable subsets and represents the current status of patient based on current observations. The experiment was conducted on three clinical datasets: PhysioNet 2012, MIMIC-III, and PhysioNet 2019. The TDE model performed competitively or better than the imputation-based baseline and several recent state-of-the-art methods with reduced training runtime.         ",
    "url": "https://arxiv.org/abs/2504.05768",
    "authors": [
      "Mincheol Kim",
      "Soo-Yong Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05781",
    "title": "Building Proactive and Instant-Reactive Safety Designs to Address Harassment in Social Virtual Reality",
    "abstract": "           Social Virtual Reality (VR) games offer immersive socialization experiences but pose significant challenges of harassment. Common solutions, such as reporting and moderation, address harassment after it happens but fail to prevent or stop harassment in the moment. In this study, we explore and design proactive and instant-reactive safety designs to mitigate harassment in social VR. Proactive designs prevent harassment from occurring, while instant-reactive designs minimize harm during incidents. We explore three directions for design: user-initiated personal bubbles, clarifying social norms, and encouraging bystander intervention. Through an iterative process, we first conducted a formative interview study to determine design goals for making these features effective, fit user needs, and robust to manipulation. We then implemented Puffer, an integrated safety system that includes a suite of proactive and instant-reactive features, as a social VR prototype. From an evaluation using simulated scenarios with participants, we find evidence that Puffer can help protect players during emergencies, foster prosocial norms, and create more positive social interactions. We conclude by discussing how system safety features can be designed to complement existing proactive and instant-reactive strategies, particularly for people with marginalized identities.         ",
    "url": "https://arxiv.org/abs/2504.05781",
    "authors": [
      "Zhehui Liao",
      "Hanwen Zhao",
      "Ayush Kulkarni",
      "Shaan Singh Chattrath",
      "Amy X. Zhang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2504.05793",
    "title": "Negotiating Strict Latency Limits for Dynamic Real-Time Services in Vehicular Time-Sensitive Networks",
    "abstract": "           Future vehicles are expected to dynamically deploy in-vehicle applications within a Service-Oriented Architecture (SOA). Critical services operate under hard real-time constraints, which Time-Sensitive Networking (TSN) complements on the in-vehicle Ethernet layer. TSN ensures deterministic communication between critical services and its Credit-Based Shaper (CBS) supports dynamic resource reservations. However, the dynamic nature of service deployment challenges network resource configuration, since any new reservation may change the latency of already validated flows. In addition, standard methods of worst-case latency analysis for CBS have been found incorrect, and current TSN stream reservation procedures lack mechanisms to signal application layer Quality-of-Service (QoS) requirements or verify deadlines. In this paper, we propose a QoS negotiation scheme within the automotive SOA that interacts with the TSN network controller to reserve resources while ensuring latency bounds. We comparatively evaluate reservation schemes using worst-case analysis and simulations of a realistic In-Vehicle Network (IVN) for demonstrating their impact on QoS guarantees, resource utilization, and setup times. We find that only a reservation scheme utilizing per-queue delay budgets and network calculus provides valid configurations and guarantees acceptable latency bounds throughout the IVN. The proposed service negotiation mechanism efficiently establishes 450 vehicular network reservations in just 11ms.         ",
    "url": "https://arxiv.org/abs/2504.05793",
    "authors": [
      "Timo H\u00e4ckel",
      "Lisa Maile",
      "Philipp Meyer",
      "Franz Korf",
      "Thomas C. Schmidt"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.05795",
    "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
    "abstract": "           Current image fusion methods struggle to adapt to real-world environments encompassing diverse degradations with spatially varying characteristics. To address this challenge, we propose a robust fusion controller (RFC) capable of achieving degradation-aware image fusion through fine-grained language instructions, ensuring its reliable application in adverse environments. Specifically, RFC first parses language instructions to innovatively derive the functional condition and the spatial condition, where the former specifies the degradation type to remove, while the latter defines its spatial coverage. Then, a composite control priori is generated through a multi-condition coupling network, achieving a seamless transition from abstract language instructions to latent control variables. Subsequently, we design a hybrid attention-based fusion network to aggregate multi-modal information, in which the obtained composite control priori is deeply embedded to linearly modulate the intermediate fused features. To ensure the alignment between language instructions and control outcomes, we introduce a novel language-feature alignment loss, which constrains the consistency between feature-level gains and the composite control priori. Extensive experiments on publicly available datasets demonstrate that our RFC is robust against various composite degradations, particularly in highly challenging flare scenarios.         ",
    "url": "https://arxiv.org/abs/2504.05795",
    "authors": [
      "Hao Zhang",
      "Yanping Zha",
      "Qingwei Zhuang",
      "Zhenfeng Shao",
      "Jiayi Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05801",
    "title": "From Superficial to Deep: Integrating External Knowledge for Follow-up Question Generation Using Knowledge Graph and LLM",
    "abstract": "           In a conversational system, dynamically generating follow-up questions based on context can help users explore information and provide a better user experience. Humans are usually able to ask questions that involve some general life knowledge and demonstrate higher order cognitive skills. However, the questions generated by existing methods are often limited to shallow contextual questions that are uninspiring and have a large gap to the human level. In this paper, we propose a three-stage external knowledge-enhanced follow-up question generation method, which generates questions by identifying contextual topics, constructing a knowledge graph (KG) online, and finally combining these with a large language model to generate the final question. The model generates information-rich and exploratory follow-up questions by introducing external common sense knowledge and performing a knowledge fusion operation. Experiments show that compared to baseline models, our method generates questions that are more informative and closer to human questioning levels while maintaining contextual relevance.         ",
    "url": "https://arxiv.org/abs/2504.05801",
    "authors": [
      "Jianyu Liu",
      "Yi Huang",
      "Sheng Bi",
      "Junlan Feng",
      "Guilin Qi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05806",
    "title": "Meta-Continual Learning of Neural Fields",
    "abstract": "           Neural Fields (NF) have gained prominence as a versatile framework for complex data representation. This work unveils a new problem setting termed \\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel strategy that employs a modular architecture combined with optimization-based meta-learning. Focused on overcoming the limitations of existing methods for continual learning of neural fields, such as catastrophic forgetting and slow convergence, our strategy achieves high-quality reconstruction with significantly improved learning speed. We further introduce Fisher Information Maximization loss for neural radiance fields (FIM-NeRF), which maximizes information gains at the sample level to enhance learning generalization, with proved convergence guarantee and generalization bound. We perform extensive evaluations across image, audio, video reconstruction, and view synthesis tasks on six diverse datasets, demonstrating our method's superiority in reconstruction quality and speed over existing MCL and CL-NF approaches. Notably, our approach attains rapid adaptation of neural fields for city-scale NeRF rendering with reduced parameter requirement.         ",
    "url": "https://arxiv.org/abs/2504.05806",
    "authors": [
      "Seungyoon Woo",
      "Junhyeog Yun",
      "Gunhee Kim"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05807",
    "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery State Information in Energy Harvesting IoT Networks",
    "abstract": "           For a two-hop IoT system consisting of multiple energy harvesting sensors, a cache-enabled edge node, and multiple monitors, the status update control at the edge node, which has partial battery state information (pBSI) of the sensors, is formulated as a pBSI problem. The concept of inferred pBSI is introduced to reduce the noiseless single-sensor pBSI problem to a Markov decision process with a moderate state-space size, enabling the optimal policy to be obtained through a value iteration algorithm. A lower bound on the expected time-average on-demand age of information performance is established for the general single-sensor status update problem. For the single-sensor pBSI problem, a semi-closed-form policy called the current-next (CN) policy is proposed, along with an efficient post-update value iteration algorithm with a per-iteration time complexity proportional to the square of the battery capacity. A weighted-update-gain-competition (WUGC) approach is further leveraged to extend the CN policy to the multi-sensor case. Numerical results in the single-sensor case demonstrate the near-optimal performance of the CN policy across various energy arrival processes. Simulations for an IoT system with $100$ sensors reveal that the WUGC-CN policy outperforms the maximum-age-first policy and the random-scheduling-based CN policy under Bernoulli energy arrival processes.         ",
    "url": "https://arxiv.org/abs/2504.05807",
    "authors": [
      "Hao Wu",
      "Shengtian Yang",
      "Jun Chen",
      "Chao Chen",
      "Anding Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05815",
    "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models",
    "abstract": "           Recently, the diffusion model has gained significant attention as one of the most successful image generation models, which can generate high-quality images by iteratively sampling noise. However, recent studies have shown that diffusion models are vulnerable to backdoor attacks, allowing attackers to enter input data containing triggers to activate the backdoor and generate their desired output. Existing backdoor attack methods primarily focused on target noise-to-image and text-to-image tasks, with limited work on backdoor attacks in image-to-image tasks. Furthermore, traditional backdoor attacks often rely on a single, conspicuous trigger to generate a fixed target image, lacking concealability and flexibility. To address these limitations, we propose a novel backdoor attack method called \"Parasite\" for image-to-image tasks in diffusion models, which not only is the first to leverage steganography for triggers hiding, but also allows attackers to embed the target content as a backdoor trigger to achieve a more flexible attack. \"Parasite\" as a novel attack method effectively bypasses existing detection frameworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved a 0 percent backdoor detection rate against the mainstream defense frameworks. In addition, in the ablation study, we discuss the influence of different hiding coefficients on the attack results. You can find our code at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.05815",
    "authors": [
      "Jiahao Chen",
      "Yu Pan",
      "Yi Du",
      "Chunkai Wu",
      "Lin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.05824",
    "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency and Accuracy in Large-Scale Systems",
    "abstract": "           Large-scale coreference resolution presents a significant challenge in natural language processing, necessitating a balance between efficiency and accuracy. In response to this challenge, we introduce an End-to-End Neural Coreference Resolution system tailored for large-scale applications. Our system efficiently identifies and resolves coreference links in text, ensuring minimal computational overhead without compromising on performance. By utilizing advanced neural network architectures, we incorporate various contextual embeddings and attention mechanisms, which enhance the quality of predictions for coreference pairs. Furthermore, we apply optimization strategies to accelerate processing speeds, making the system suitable for real-world deployment. Extensive evaluations conducted on benchmark datasets demonstrate that our model achieves improved accuracy compared to existing approaches, while effectively maintaining rapid inference times. Rigorous testing confirms the ability of our system to deliver precise coreference resolutions efficiently, thereby establishing a benchmark for future advancements in this field.         ",
    "url": "https://arxiv.org/abs/2504.05824",
    "authors": [
      "Zhang Dong",
      "Songhang deng",
      "Mingbang Wang",
      "Le Dai",
      "Jiyuan Li",
      "Xingzu Liu",
      "Ruilin Nong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05831",
    "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts",
    "abstract": "           Large language models (LLMs) increasingly rely on preference alignment methods to steer outputs toward human values, yet these methods are often constrained by the scarcity of high-quality human-annotated data. To tackle this, recent approaches have turned to synthetic data generated by LLMs as a scalable alternative. However, synthetic data can introduce distribution shifts, compromising the nuanced human preferences that are essential for desirable outputs. In this paper, we propose a novel distribution-aware optimization framework that improves preference alignment in the presence of such shifts. Our approach first estimates the likelihood ratios between the target and training distributions leveraging a learned classifier, then it minimizes the worst-case loss over data regions that reflect the target human-preferred distribution. By explicitly prioritizing the target distribution during optimization, our method mitigates the adverse effects of distributional variation and enhances the generation of responses that faithfully reflect human values.         ",
    "url": "https://arxiv.org/abs/2504.05831",
    "authors": [
      "Mingye Zhu",
      "Yi Liu",
      "Junbo Guo",
      "Quan Wang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05832",
    "title": "Channel State Information Analysis for Jamming Attack Detection in Static and Dynamic UAV Networks -- An Experimental Study",
    "abstract": "           Networks built on the IEEE 802.11 standard have experienced rapid growth in the last decade. Their field of application is vast, including smart home applications, Internet of Things (IoT), and short-range high throughput static and dynamic inter-vehicular communication networks. Within such networks, Channel State Information (CSI) provides a detailed view of the state of the communication channel and represents the combined effects of multipath propagation, scattering, phase shift, fading, and power decay. In this work, we investigate the problem of jamming attack detection in static and dynamic vehicular networks. We utilize ESP32-S3 modules to set up a communication network between an Unmanned Aerial Vehicle (UAV) and a Ground Control Station (GCS), to experimentally test the combined effects of a constant jammer on recorded CSI parameters, and the feasibility of jamming detection through CSI analysis in static and dynamic communication scenarios.         ",
    "url": "https://arxiv.org/abs/2504.05832",
    "authors": [
      "Pavlo Mykytyn",
      "Ronald Chitauro",
      "Zoya Dyka",
      "Peter Langendoerfer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.05844",
    "title": "Adaptive Substructure-Aware Expert Model for Molecular Property Prediction",
    "abstract": "           Molecular property prediction is essential for applications such as drug discovery and toxicity assessment. While Graph Neural Networks (GNNs) have shown promising results by modeling molecules as molecular graphs, their reliance on data-driven learning limits their ability to generalize, particularly in the presence of data imbalance and diverse molecular substructures. Existing methods often overlook the varying contributions of different substructures to molecular properties, treating them uniformly. To address these challenges, we propose ASE-Mol, a novel GNN-based framework that leverages a Mixture-of-Experts (MoE) approach for molecular property prediction. ASE-Mol incorporates BRICS decomposition and significant substructure awareness to dynamically identify positive and negative substructures. By integrating a MoE architecture, it reduces the adverse impact of negative motifs while improving adaptability to positive motifs. Experimental results on eight benchmark datasets demonstrate that ASE-Mol achieves state-of-the-art performance, with significant improvements in both accuracy and interpretability.         ",
    "url": "https://arxiv.org/abs/2504.05844",
    "authors": [
      "Tianyi Jiang",
      "Zeyu Wang",
      "Shanqing Yu",
      "Qi Xuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05849",
    "title": "On the Importance of Conditioning for Privacy-Preserving Data Augmentation",
    "abstract": "           Latent diffusion models can be used as a powerful augmentation method to artificially extend datasets for enhanced training. To the human eye, these augmented images look very different to the originals. Previous work has suggested to use this data augmentation technique for data anonymization. However, we show that latent diffusion models that are conditioned on features like depth maps or edges to guide the diffusion process are not suitable as a privacy preserving method. We use a contrastive learning approach to train a model that can correctly identify people out of a pool of candidates. Moreover, we demonstrate that anonymization using conditioned diffusion models is susceptible to black box attacks. We attribute the success of the described methods to the conditioning of the latent diffusion model in the anonymization process. The diffusion model is instructed to produce similar edges for the anonymized images. Hence, a model can learn to recognize these patterns for identification.         ",
    "url": "https://arxiv.org/abs/2504.05849",
    "authors": [
      "Julian Lorenz",
      "Katja Ludwig",
      "Valentin Haug",
      "Rainer Lienhart"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05851",
    "title": "Identifying and Replicating Code Patterns Driving Performance Regressions in Software Systems",
    "abstract": "           Context: Performance regressions negatively impact execution time and memory usage of software systems. Nevertheless, there is a lack of systematic methods to evaluate the effectiveness of performance test suites. Performance mutation testing, which introduces intentional defects (mutants) to measure and enhance fault-detection capabilities, is promising but underexplored. A key challenge is understanding if generated mutants accurately reflect real-world performance issues. Goal: This study evaluates and extends mutation operators for performance testing. Its objectives include (i) collecting existing performance mutation operators, (ii) introducing new operators from real-world code changes that impact performance, and (iii) evaluating these operators on real-world systems to see if they effectively degrade performance. Method: To this aim, we will (i) review the literature to identify performance mutation operators, (ii) conduct a mining study to extract patterns of code changes linked to performance regressions, (iii) propose new mutation operators based on these patterns, and (iv) apply and evaluate the operators to assess their effectiveness in exposing performance degradations. Expected Outcomes: We aim to provide an enriched set of mutation operators for performance testing, helping developers and researchers identify harmful coding practices and design better strategies to detect and prevent performance regressions.         ",
    "url": "https://arxiv.org/abs/2504.05851",
    "authors": [
      "Denivan Campos",
      "Luana Martins",
      "Emanuela Guglielmi",
      "Michele Tucci",
      "Daniele Di Pompeo",
      "Simone Scalabrino",
      "Vittorio Cortellessa",
      "Dario Di Nucci",
      "Rocco Oliveto"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2504.05861",
    "title": "Sparse Bounded Hop-Spanners for Geometric Intersection Graphs",
    "abstract": "           We present new results on $2$- and $3$-hop spanners for geometric intersection graphs. These include improved upper and lower bounds for $2$- and $3$-hop spanners for many geometric intersection graphs in $\\mathbb{R}^d$. For example, we show that the intersection graph of $n$ balls in $\\mathbb{R}^d$ admits a $2$-hop spanner of size $O^*\\left(n^{\\frac{3}{2}-\\frac{1}{2(2\\lfloor d/2\\rfloor +1)}}\\right)$ and the intersection graph of $n$ fat axis-parallel boxes in $\\mathbb{R}^d$ admits a $2$-hop spanner of size $O(n \\log^{d+1}n)$. Furthermore, we show that the intersection graph of general semi-algebraic objects in $\\mathbb{R}^d$ admits a $3$-hop spanner of size $O^*\\left(n^{\\frac{3}{2}-\\frac{1}{2(2D-1)}}\\right)$, where $D$ is a parameter associated with the description complexity of the objects. For such families (or more specifically, for tetrahedra in $\\mathbb{R}^3$), we provide a lower bound of $\\Omega(n^{\\frac{4}{3}})$. For $3$-hop and axis-parallel boxes in $\\mathbb{R}^d$, we provide the upper bound $O(n \\log ^{d-1}n)$ and lower bound $\\Omega\\left(n (\\frac{\\log n}{\\log \\log n})^{d-2}\\right)$.         ",
    "url": "https://arxiv.org/abs/2504.05861",
    "authors": [
      "Sujoy Bhore",
      "Timothy M. Chan",
      "Zhengcheng Huang",
      "Shakhar Smorodinsky",
      "Csaba D. Toth"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2504.05868",
    "title": "Energy-Conserving Neural Network Closure Model for Long-Time Accurate and Stable LES",
    "abstract": "           Machine learning-based closure models for LES have shown promise in capturing complex turbulence dynamics but often suffer from instabilities and physical inconsistencies. In this work, we develop a novel skew-symmetric neural architecture as closure model that enforces stability while preserving key physical conservation laws. Our approach leverages a discretization that ensures mass, momentum, and energy conservation, along with a face-averaging filter to maintain mass conservation in coarse-grained velocity fields. We compare our model against several conventional data-driven closures (including unconstrained convolutional neural networks), and the physics-based Smagorinsky model. Performance is evaluated on decaying turbulence and Kolmogorov flow for multiple coarse-graining factors. In these test cases we observe that unconstrained machine learning models suffer from numerical instabilities. In contrast, our skew-symmetric model remains stable across all tests, though at the cost of increased dissipation. Despite this trade-off, we demonstrate that our model still outperforms the Smagorinsky model in unseen scenarios. These findings highlight the potential of structure-preserving machine learning closures for reliable long-time LES.         ",
    "url": "https://arxiv.org/abs/2504.05868",
    "authors": [
      "Toby van Gastelen",
      "Wouter Edeling",
      "Benjamin Sanderse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2504.05878",
    "title": "KAN-SAM: Kolmogorov-Arnold Network Guided Segment Anything Model for RGB-T Salient Object Detection",
    "abstract": "           Existing RGB-thermal salient object detection (RGB-T SOD) methods aim to identify visually significant objects by leveraging both RGB and thermal modalities to enable robust performance in complex scenarios, but they often suffer from limited generalization due to the constrained diversity of available datasets and the inefficiencies in constructing multi-modal representations. In this paper, we propose a novel prompt learning-based RGB-T SOD method, named KAN-SAM, which reveals the potential of visual foundational models for RGB-T SOD tasks. Specifically, we extend Segment Anything Model 2 (SAM2) for RGB-T SOD by introducing thermal features as guiding prompts through efficient and accurate Kolmogorov-Arnold Network (KAN) adapters, which effectively enhance RGB representations and improve robustness. Furthermore, we introduce a mutually exclusive random masking strategy to reduce reliance on RGB data and improve generalization. Experimental results on benchmarks demonstrate superior performance over the state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2504.05878",
    "authors": [
      "Xingyuan Li",
      "Ruichao Hou",
      "Tongwei Ren",
      "Gangshan Wu"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05902",
    "title": "Defending Deep Neural Networks against Backdoor Attacks via Module Switching",
    "abstract": "           The exponential increase in the parameters of Deep Neural Networks (DNNs) has significantly raised the cost of independent training, particularly for resource-constrained entities. As a result, there is a growing reliance on open-source models. However, the opacity of training processes exacerbates security risks, making these models more vulnerable to malicious threats, such as backdoor attacks, while simultaneously complicating defense mechanisms. Merging homogeneous models has gained attention as a cost-effective post-training defense. However, we notice that existing strategies, such as weight averaging, only partially mitigate the influence of poisoned parameters and remain ineffective in disrupting the pervasive spurious correlations embedded across model parameters. We propose a novel module-switching strategy to break such spurious correlations within the model's propagation path. By leveraging evolutionary algorithms to optimize fusion strategies, we validate our approach against backdoor attacks targeting text and vision domains. Our method achieves effective backdoor mitigation even when incorporating a couple of compromised models, e.g., reducing the average attack success rate (ASR) to 22% compared to 31.9% with the best-performing baseline on SST-2.         ",
    "url": "https://arxiv.org/abs/2504.05902",
    "authors": [
      "Weijun Li",
      "Ansh Arora",
      "Xuanli He",
      "Mark Dras",
      "Qiongkai Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.05904",
    "title": "Intrinsic Saliency Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation",
    "abstract": "           Recent unsupervised video object segmentation (UVOS) methods predominantly adopt the motion-appearance paradigm. Mainstream motion-appearance approaches use either the two-encoder structure to separately encode motion and appearance features, or the single-encoder structure for joint encoding. However, these methods fail to properly balance the motion-appearance relationship. Consequently, even with complex fusion modules for motion-appearance integration, the extracted suboptimal features degrade the models' overall performance. Moreover, the quality of optical flow varies across scenarios, making it insufficient to rely solely on optical flow to achieve high-quality segmentation results. To address these challenges, we propose the Intrinsic Saliency guided Trunk-Collateral Net}work (ISTC-Net), which better balances the motion-appearance relationship and incorporates model's intrinsic saliency information to enhance segmentation performance. Specifically, considering that optical flow maps are derived from RGB images, they share both commonalities and differences. We propose a novel Trunk-Collateral structure. The shared trunk backbone captures the motion-appearance commonality, while the collateral branch learns the uniqueness of motion features. Furthermore, an Intrinsic Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the model's intrinsic saliency information to refine high-level features, and provide pixel-level guidance for motion-appearance fusion, thereby enhancing performance without additional input. Experimental results show that ISTC-Net achieved state-of-the-art performance on three UVOS datasets (89.2% J&F on DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS) and four standard video salient object detection (VSOD) benchmarks with the notable increase, demonstrating its effectiveness and superiority over previous methods.         ",
    "url": "https://arxiv.org/abs/2504.05904",
    "authors": [
      "Xiangyu Zheng",
      "Wanyun Li",
      "Songcheng He",
      "Xiaoqiang Li",
      "We Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05907",
    "title": "A Method for Generating Connected Erdos-Renyi Random Graphs",
    "abstract": "           We propose a novel and exact algorithm for generating connected Erdos-Renyi random graphs $G(n, p)$. Our approach exploits a link between the distribution of exploration process trajectories and an inhomogeneous random walk. In contrast to existing methods, our approach guarantees the correct distribution under the connectivity condition and achieves $O(n^2)$ runtime in the sparse case $p = c/n$. Furthermore, we show that our method can be extended to uniformly generate connected graphs $G(n, m)$ via an acceptance-rejection procedure.         ",
    "url": "https://arxiv.org/abs/2504.05907",
    "authors": [
      "Boris Chinyaev"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)",
      "Probability (math.PR)"
    ]
  },
  {
    "id": "arXiv:2504.05918",
    "title": "Deep RL-based Autonomous Navigation of Micro Aerial Vehicles (MAVs) in a complex GPS-denied Indoor Environment",
    "abstract": "           The Autonomy of Unmanned Aerial Vehicles (UAVs) in indoor environments poses significant challenges due to the lack of reliable GPS signals in enclosed spaces such as warehouses, factories, and indoor facilities. Micro Aerial Vehicles (MAVs) are preferred for navigating in these complex, GPS-denied scenarios because of their agility, low power consumption, and limited computational capabilities. In this paper, we propose a Reinforcement Learning based Deep-Proximal Policy Optimization (D-PPO) algorithm to enhance realtime navigation through improving the computation efficiency. The end-to-end network is trained in 3D realistic meta-environments created using the Unreal Engine. With these trained meta-weights, the MAV system underwent extensive experimental trials in real-world indoor environments. The results indicate that the proposed method reduces computational latency by 91\\% during training period without significant degradation in performance. The algorithm was tested on a DJI Tello drone, yielding similar results.         ",
    "url": "https://arxiv.org/abs/2504.05918",
    "authors": [
      "Amit Kumar Singh",
      "Prasanth Kumar Duba",
      "P. Rajalakshmi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05945",
    "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
    "abstract": "           In this paper, we propose CKGAN, a novel generative adversarial network (GAN) variant based on an integral probability metrics framework with characteristic kernel (CKIPM). CKIPM, as a distance between two probability distributions, is designed to optimize the lowerbound of the maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN mitigates the notorious problem of mode collapse by mapping the generated images back to random noise. To save the effort of selecting the kernel function manually, we propose a soft selection method to automatically learn a characteristic kernel function. The experimental evaluation conducted on a set of synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that CKGAN generally outperforms other MMD-based GANs. The results also show that at the cost of moderately more training time, the automatically selected kernel function delivers very close performance to the best of manually fine-tuned one on real image benchmarks and is able to improve the performances of other MMD-based GANs.         ",
    "url": "https://arxiv.org/abs/2504.05945",
    "authors": [
      "Kuntian Zhang",
      "Simin Yu",
      "Yaoshu Wang",
      "Makoto Onizuka",
      "Chuan Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05957",
    "title": "Drought forecasting using a hybrid neural architecture for integrating time series and static data",
    "abstract": "           Reliable forecasting is critical for early warning systems and adaptive drought management. Most previous deep learning approaches focus solely on homogeneous regions and rely on single-structured data. This paper presents a hybrid neural architecture that integrates time series and static data, achieving state-of-the-art performance on the DroughtED dataset. Our results illustrate the potential of designing neural models for the treatment of heterogeneous data in climate related tasks and present reliable prediction of USDM categories, an expert-informed drought metric. Furthermore, this work validates the potential of DroughtED for enabling location-agnostic training of deep learning models.         ",
    "url": "https://arxiv.org/abs/2504.05957",
    "authors": [
      "Julian Agudelo",
      "Vincent Guigue",
      "Cristina Manfredotti",
      "Hadrien Piot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05958",
    "title": "Hybrid Control as a Proxy for Detection and Mitigation of Sensor Attacks in Cooperative Driving",
    "abstract": "           To enhance the robustness of cooperative driving against cyberattacks, we propose a hybrid controller scheme to detect and mitigate False-Data Injection (FDI) attacks in real-time. The core of our method builds on a given Cooperative Adaptive Cruise Control (CACC) algorithm and exploits sensor redundancy to construct equivalent controllers, each driven by a distinct, non-overlapping subset of sensors (equivalent controller realizations). By construction, these controller realizations generate the same control input in the absence of an attack, allowing us to devise an algorithm that compares control signals and measurements to pinpoint anomalous behavior via a majority vote. This allows us to: 1) decide in real-time which subset of sensors is compromised; and 2) switch to a healthy subset, mitigating thus sensor FDI attacks. We model the latter logic as a hybrid dynamic controller that decides in real-time what realization to use, builds on attack-dependent flow and jump sets, and employs controller resets (to return the state of previously compromised controller realizations to a correct value after the attack stops). We demonstrate the performance of our scheme through simulation experiments.         ",
    "url": "https://arxiv.org/abs/2504.05958",
    "authors": [
      "Mischa Huisman",
      "Carlos Murguia",
      "Erjen Lefeber",
      "Nathan van de Wouw"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.05962",
    "title": "Autoencoder-Based Detection of Anomalous Stokes V Spectra in the Flare-Producing Active Region 13663 Using Hinode/SP Observations",
    "abstract": "           Detecting unusual signals in observational solar spectra is crucial for understanding the features associated with impactful solar events, such as solar flares. However, existing spectral analysis techniques face challenges, particularly when relying on pre-defined, physics-based calculations to process large volumes of noisy and complex observational data. To address these limitations, we applied deep learning to detect anomalies in the Stokes V spectra from the Hinode/SP instrument. Specifically, we developed an autoencoder model for spectral compression, which serves as an anomaly detection method. Our model effectively identifies anomalous spectra within spectro-polarimetric maps captured prior to the onset of the X1.3 flare on May 5, 2024, in NOAA AR 13663. These atypical spectral points exhibit highly complex profiles and spatially align with polarity inversion lines in magnetogram images, indicating their potential as sites of magnetic energy storage and possible triggers for flares. Notably, the detected anomalies are highly localized, making them particularly challenging to identify in magnetogram images using current manual methods.         ",
    "url": "https://arxiv.org/abs/2504.05962",
    "authors": [
      "Jargalmaa Batmunkh",
      "Yusuke Iida",
      "Takayoshi Oba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Solar and Stellar Astrophysics (astro-ph.SR)"
    ]
  },
  {
    "id": "arXiv:2504.05964",
    "title": "Context-aware Rate Adaptation for Predictive Flying Networks using Contextual Bandits",
    "abstract": "           The increasing complexity of wireless technologies, such as Wi-Fi, presents significant challenges for Rate Adaptation (RA) due to the large configuration space of transmission parameters. While extensive research has been conducted on RA for low-mobility networks, existing solutions fail to adapt in flying networks, where high mobility and dynamic wireless conditions introduce additional uncertainty. We propose Linear Upper Confidence Bound for RA (LinRA), a novel Contextual Bandit-based approach that leverages real-time link context to optimize transmission rates. Designed for predictive flying networks, where future trajectories are known, LinRA proactively adapts to obstacles affecting channel quality. Simulation results demonstrate that LinRA converges $\\mathbf{5.2\\times}$ faster than state-of-the-art benchmarks and improves throughput by 80\\% in Non Line-of-Sight (NLoS) conditions, matching the performance of ideal algorithms. With low time complexity, LinRA is a scalable and efficient RA solution for predictive flying networks.         ",
    "url": "https://arxiv.org/abs/2504.05964",
    "authors": [
      "Ruben Queiros",
      "Megumi Kaneko",
      "Helder Fontes",
      "Rui Campos"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.05966",
    "title": "AVP-AP: Self-supervised Automatic View Positioning in 3D cardiac CT via Atlas Prompting",
    "abstract": "           Automatic view positioning is crucial for cardiac computed tomography (CT) examinations, including disease diagnosis and surgical planning. However, it is highly challenging due to individual variability and large 3D search space. Existing work needs labor-intensive and time-consuming manual annotations to train view-specific models, which are limited to predicting only a fixed set of planes. However, in real clinical scenarios, the challenge of positioning semantic 2D slices with any orientation into varying coordinate space in arbitrary 3D volume remains unsolved. We thus introduce a novel framework, AVP-AP, the first to use Atlas Prompting for self-supervised Automatic View Positioning in the 3D CT volume. Specifically, this paper first proposes an atlas prompting method, which generates a 3D canonical atlas and trains a network to map slices into their corresponding positions in the atlas space via a self-supervised manner. Then, guided by atlas prompts corresponding to the given query images in a reference CT, we identify the coarse positions of slices in the target CT volume using rigid transformation between the 3D atlas and target CT volume, effectively reducing the search space. Finally, we refine the coarse positions by maximizing the similarity between the predicted slices and the query images in the feature space of a given foundation model. Our framework is flexible and efficient compared to other methods, outperforming other methods by 19.8% average structural similarity (SSIM) in arbitrary view positioning and achieving 9% SSIM in two-chamber view compared to four radiologists. Meanwhile, experiments on a public dataset validate our framework's generalizability.         ",
    "url": "https://arxiv.org/abs/2504.05966",
    "authors": [
      "Xiaolin Fan",
      "Yan Wang",
      "Yingying Zhang",
      "Mingkun Bao",
      "Bosen Jia",
      "Dong Lu",
      "Yifan Gu",
      "Jian Cheng",
      "Haogang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05970",
    "title": "MLPROP -- an open interactive web interface for thermophysical property prediction with machine learning",
    "abstract": "           Machine learning (ML) enables the development of powerful methods for predicting thermophysical properties with unprecedented scope and accuracy. However, technical barriers like cumbersome implementation in established workflows hinder their application in practice. With MLPROP, we provide an interactive web interface for directly applying advanced ML methods to predict thermophysical properties without requiring ML expertise, thereby substantially increasing the accessibility of novel models. MLPROP currently includes models for predicting the vapor pressure of pure components (GRAPPA), activity coefficients and vapor-liquid equilibria in binary mixtures (UNIFAC 2.0, mod. UNIFAC 2.0, and HANNA), and a routine to fit NRTL parameters to the model predictions. MLPROP will be continuously updated and extended and is accessible free of charge via this https URL. MLPROP removes the barrier to learning and experimenting with new ML-based methods for predicting thermophysical properties. The source code of all models is available as open source, which allows integration into existing workflows.         ",
    "url": "https://arxiv.org/abs/2504.05970",
    "authors": [
      "Marco Hoffmann",
      "Thomas Specht",
      "Nicolas Hayer",
      "Hans Hasse",
      "Fabian Jirasek"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05985",
    "title": "Adaptive RISE Control for Dual-Arm Unmanned Aerial Manipulator Systems with Deep Neural Networks",
    "abstract": "           The unmanned aerial manipulator system, consisting of a multirotor UAV (unmanned aerial vehicle) and a manipulator, has attracted considerable interest from researchers. Nevertheless, the operation of a dual-arm manipulator poses a dynamic challenge, as the CoM (center of mass) of the system changes with manipulator movement, potentially impacting the multirotor UAV. Additionally, unmodeled effects, parameter uncertainties, and external disturbances can significantly degrade control performance, leading to unforeseen dangers. To tackle these issues, this paper proposes a nonlinear adaptive RISE (robust integral of the sign of the error) controller based on DNN (deep neural network). The first step involves establishing the kinematic and dynamic model of the dual-arm aerial manipulator. Subsequently, the adaptive RISE controller is proposed with a DNN feedforward term to effectively address both internal and external challenges. By employing Lyapunov techniques, the asymptotic convergence of the tracking error signals are guaranteed rigorously. Notably, this paper marks a pioneering effort by presenting the first DNN-based adaptive RISE controller design accompanied by a comprehensive stability analysis. To validate the practicality and robustness of the proposed control approach, several groups of actual hardware experiments are conducted. The results confirm the efficacy of the developed methodology in handling real-world scenarios, thereby offering valuable insights into the performance of the dual-arm aerial manipulator system.         ",
    "url": "https://arxiv.org/abs/2504.05985",
    "authors": [
      "Yang Wang",
      "Hai Yu",
      "Shizhen Wu",
      "Zhichao Yang",
      "Jianda Han",
      "Yongchun Fang",
      "Xiao Liang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.06004",
    "title": "FedFeat+: A Robust Federated Learning Framework Through Federated Aggregation and Differentially Private Feature-Based Classifier Retraining",
    "abstract": "           In this paper, we propose the FedFeat+ framework, which distinctively separates feature extraction from classification. We develop a two-tiered model training process: following local training, clients transmit their weights and some features extracted from the feature extractor from the final local epochs to the server. The server aggregates these models using the FedAvg method and subsequently retrains the global classifier utilizing the shared features. The classifier retraining process enhances the model's understanding of the holistic view of the data distribution, ensuring better generalization across diverse datasets. This improved generalization enables the classifier to adaptively influence the feature extractor during subsequent local training epochs. We establish a balance between enhancing model accuracy and safeguarding individual privacy through the implementation of differential privacy mechanisms. By incorporating noise into the feature vectors shared with the server, we ensure that sensitive data remains confidential. We present a comprehensive convergence analysis, along with theoretical reasoning regarding performance enhancement and privacy preservation. We validate our approach through empirical evaluations conducted on benchmark datasets, including CIFAR-10, CIFAR-100, MNIST, and FMNIST, achieving high accuracy while adhering to stringent privacy guarantees. The experimental results demonstrate that the FedFeat+ framework, despite using only a lightweight two-layer CNN classifier, outperforms the FedAvg method in both IID and non-IID scenarios, achieving accuracy improvements ranging from 3.92 % to 12.34 % across CIFAR-10, CIFAR-100, and Fashion-MNIST datasets.         ",
    "url": "https://arxiv.org/abs/2504.06004",
    "authors": [
      "Mrityunjoy Gain",
      "Kitae Kim",
      "Avi Deb Raha",
      "Apurba Adhikary",
      "Eui-Nam Huh",
      "Zhu Han",
      "Choong Seon Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.06006",
    "title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
    "abstract": "           Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of using large language models (LLMs) for hyperparameter optimization by employing a fine-tuned version of Code Llama. Through parameter-efficient fine-tuning using LoRA, we adapt the LLM to generate accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional methods such as Optuna, which rely on exhaustive trials, the proposed approach achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our approach highlights that LLM-based optimization not only matches state-of-the-art methods like Tree-structured Parzen Estimators but also accelerates the tuning process. This positions LLMs as a promising alternative to conventional optimization techniques, particularly for rapid experimentation. Furthermore, the ability to generate hyperparameters in a single inference step makes this method particularly well-suited for resource-constrained environments such as edge devices and mobile applications, where computational efficiency is paramount. The results confirm that LLMs, beyond their efficiency, offer substantial time savings and comparable stability, underscoring their value in advancing machine learning workflows. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research.         ",
    "url": "https://arxiv.org/abs/2504.06006",
    "authors": [
      "Roman Kochnev",
      "Arash Torabi Goodarzi",
      "Zofia Antonina Bentyn",
      "Dmitry Ignatov",
      "Radu Timofte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.06010",
    "title": "Latent Multimodal Reconstruction for Misinformation Detection",
    "abstract": "           Multimodal misinformation, such as miscaptioned images, where captions misrepresent an image's origin, context, or meaning, poses a growing challenge in the digital age. To support fact-checkers, researchers have been focusing on creating datasets and developing methods for multimodal misinformation detection (MMD). Due to the scarcity of large-scale annotated MMD datasets, recent studies leverage synthetic training data via out-of-context image-caption pairs or named entity manipulations; altering names, dates, and locations. However, these approaches often produce simplistic misinformation that fails to reflect real-world complexity, limiting the robustness of detection models trained on them. Meanwhile, despite recent advancements, Large Vision-Language Models (LVLMs) remain underutilized for generating diverse, realistic synthetic training data for MMD. To address this gap, we introduce \"MisCaption This!\", a training dataset comprising LVLM-generated miscaptioned images. Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR), a network trained to reconstruct the embeddings of truthful captions, providing a strong auxiliary signal to the detection process. To optimize LAMAR, we explore different training strategies (end-to-end training and large-scale pre-training) and integration approaches (direct, mask, gate, and attention). Extensive experiments show that models trained on \"MisCaption This!\" generalize better on real-world misinformation, while LAMAR sets new state-of-the-art on both NewsCLIPpings and VERITE benchmarks; highlighting the potential of LVLM-generated data and reconstruction-based approaches for advancing MMD. We release our code at: this https URL ",
    "url": "https://arxiv.org/abs/2504.06010",
    "authors": [
      "Stefanos-Iordanis Papadopoulos",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2504.06015",
    "title": "Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights into Handling Faulty GNSS Measurements in Field Robotics",
    "abstract": "           This paper presents research findings on handling faulty measurements (i.e., outliers) of global navigation satellite systems (GNSS) for robot localization under adverse signal conditions in field applications, where raw GNSS data are frequently corrupted due to environmental interference such as multipath, signal blockage, or non-line-of-sight conditions. In this context, we investigate three strategies applied specifically to GNSS pseudorange observations: robust statistics for error mitigation, machine learning for faulty measurement prediction, and Bayesian inference for noise distribution approximation. Since previous studies have provided limited insight into the theoretical foundations and practical evaluations of these three methodologies within a unified problem statement (i.e., state estimation using ranging sensors), we conduct extensive experiments using real-world sensor data collected in diverse urban environments. Our goal is to examine both established techniques and newly proposed methods, thereby advancing the understanding of how to handle faulty range measurements, such as GNSS, for robust, long-term robot localization. In addition to presenting successful results, this work highlights critical observations and open questions to motivate future research in robust state estimation.         ",
    "url": "https://arxiv.org/abs/2504.06015",
    "authors": [
      "Haoming Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.06039",
    "title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning Strategies",
    "abstract": "           Capsule endoscopy is a method to capture images of the gastrointestinal tract and screen for diseases which might remain hidden if investigated with standard endoscopes. Due to the limited size of a video capsule, embedding AI models directly into the capsule demands careful consideration of the model size and thus complicates anomaly detection in this field. Furthermore, the scarcity of available data in this domain poses an ongoing challenge to achieving effective anomaly detection. Thus, this work introduces an ensemble strategy to address this challenge in anomaly detection tasks in video capsule endoscopies, requiring only a small number of individual neural networks during both the training and inference phases. Ensemble learning combines the predictions of multiple independently trained neural networks. This has shown to be highly effective in enhancing both the accuracy and robustness of machine learning models. However, this comes at the cost of higher memory usage and increased computational effort, which quickly becomes prohibitive in many real-world applications. Instead of applying the same training algorithm to each individual network, we propose using various loss functions, drawn from the anomaly detection field, to train each network. The methods are validated on the two largest publicly available datasets for video capsule endoscopy images, the Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on the Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our approach outperforms current baselines with significantly fewer parameters across all models, which is a crucial step towards incorporating artificial intelligence into capsule endoscopies.         ",
    "url": "https://arxiv.org/abs/2504.06039",
    "authors": [
      "Julia Werner",
      "Christoph Gerum",
      "Jorg Nick",
      "Maxime Le Floch",
      "Franz Brinkmann",
      "Jochen Hampe",
      "Oliver Bringmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.06067",
    "title": "GPU-accelerated Evolutionary Many-objective Optimization Using Tensorized NSGA-III",
    "abstract": "           NSGA-III is one of the most widely adopted algorithms for tackling many-objective optimization problems. However, its CPU-based design severely limits scalability and computational efficiency. To address the limitations, we propose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that leverages GPU parallelism for large-scale many-objective optimization. Unlike conventional GPU-accelerated evolutionary algorithms that rely on heuristic approximations to improve efficiency, TensorNSGA-III maintains the exact selection and variation mechanisms of NSGA-III while achieving significant acceleration. By reformulating the selection process with tensorized data structures and an optimized caching strategy, our approach effectively eliminates computational bottlenecks inherent in traditional CPU-based and na\u00efve GPU implementations. Experimental results on widely used numerical benchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$ over the CPU version of NSGA-III. Additionally, we validate its effectiveness in multiobjective robotic control tasks, where it discovers diverse and high-quality behavioral solutions. Furthermore, we investigate the critical role of large population sizes in many-objective optimization and demonstrate the scalability of TensorNSGA-III in such scenarios. The source code is available at this https URL ",
    "url": "https://arxiv.org/abs/2504.06067",
    "authors": [
      "Hao Li",
      "Zhenyu Liang",
      "Ran Cheng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2504.06070",
    "title": "PINP: Physics-Informed Neural Predictor with latent estimation of fluid flows",
    "abstract": "           Accurately predicting fluid dynamics and evolution has been a long-standing challenge in physical sciences. Conventional deep learning methods often rely on the nonlinear modeling capabilities of neural networks to establish mappings between past and future states, overlooking the fluid dynamics, or only modeling the velocity field, neglecting the coupling of multiple physical quantities. In this paper, we propose a new physics-informed learning approach that incorporates coupled physical quantities into the prediction process to assist with forecasting. Central to our method lies in the discretization of physical equations, which are directly integrated into the model architecture and loss function. This integration enables the model to provide robust, long-term future predictions. By incorporating physical equations, our model demonstrates temporal extrapolation and spatial generalization capabilities. Experimental results show that our approach achieves the state-of-the-art performance in spatiotemporal prediction across both numerical simulations and real-world extreme-precipitation nowcasting benchmarks.         ",
    "url": "https://arxiv.org/abs/2504.06070",
    "authors": [
      "Huaguan Chen",
      "Yang Liu",
      "Hao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.06099",
    "title": "Towards Varroa destructor mite detection using a narrow spectra illumination",
    "abstract": "           This paper focuses on the development and modification of a beehive monitoring device and Varroa destructor detection on the bees with the help of hyperspectral imagery while utilizing a U-net, semantic segmentation architecture, and conventional computer vision methods. The main objectives were to collect a dataset of bees and mites, and propose the computer vision model which can achieve the detection between bees and mites.         ",
    "url": "https://arxiv.org/abs/2504.06099",
    "authors": [
      "Samuel Bielik",
      "Simon Bilik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.06102",
    "title": "Sherlock: A Dataset for Process-aware Intrusion Detection Research on Power Grid Networks",
    "abstract": "           Physically distributed components and legacy protocols make the protection of power grids against increasing cyberattack threats challenging. Infamously, the 2015 and 2016 blackouts in Ukraine were caused by cyberattacks, and the German Federal Office for Information Security (BSI) recorded over 200 cyber incidents against the German energy sector between 2023 and 2024. Intrusion detection promises to quickly detect such attacks and mitigate the worst consequences. However, public datasets of realistic scenarios are vital to evaluate these systems. This paper introduces Sherlock, a dataset generated with the co-simulator Wattson. In total, Sherlock covers three scenarios with various attacks manipulating the process state by injecting malicious commands or manipulating measurement values. We additionally test five recently-published intrusion detection systems on Sherlock, highlighting specific challenges for intrusion detection in power grids. Dataset and documentation are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2504.06102",
    "authors": [
      "Eric Wagner",
      "Lennart Bader",
      "Konrad Wolsing",
      "Martin Serror"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.06121",
    "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions",
    "abstract": "           Lane detection is a critical component of Advanced Driver Assistance Systems (ADAS). Existing lane detection algorithms generally perform well under favorable weather conditions. However, their performance degrades significantly in adverse conditions, such as fog, which increases the risk of traffic accidents. This challenge is compounded by the lack of specialized datasets and methods designed for foggy environments. To address this, we introduce the FoggyLane dataset, captured in real-world foggy scenarios, and synthesize two additional datasets, FoggyCULane and FoggyTusimple, from existing popular lane detection datasets. Furthermore, we propose a robust Fog-Enhanced Network for lane detection, incorporating a Global Feature Fusion Module (GFFM) to capture global relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to model the structural and positional relationships of lane instances, and a Low-level Edge Enhanced Module (LEEM) to address missing edge details in foggy conditions. Comprehensive experiments demonstrate that our method achieves state-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on FoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT acceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA Jetson AGX Orin, confirming its real-time capabilities and robustness in foggy environments.         ",
    "url": "https://arxiv.org/abs/2504.06121",
    "authors": [
      "Ronghui Zhang",
      "Yuhang Ma",
      "Tengfei Li",
      "Ziyu Lin",
      "Yueying Wu",
      "Junzhou Chen",
      "Lin Zhang",
      "Jia Hu",
      "Tony Z. Qiu",
      "Konghui Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.06129",
    "title": "Knowledge Graph Completion with Relation-Aware Anchor Enhancement",
    "abstract": "           Text-based knowledge graph completion methods take advantage of pre-trained language models (PLM) to enhance intrinsic semantic connections of raw triplets with detailed text descriptions. Typical methods in this branch map an input query (textual descriptions associated with an entity and a relation) and its candidate entities into feature vectors, respectively, and then maximize the probability of valid triples. These methods are gaining promising performance and increasing attention for the rapid development of large language models. According to the property of the language models, the more related and specific context information the input query provides, the more discriminative the resultant embedding will be. In this paper, through observation and validation, we find a neglected fact that the relation-aware neighbors of the head entities in queries could act as effective contexts for more precise link prediction. Driven by this finding, we propose a relation-aware anchor enhanced knowledge graph completion method (RAA-KGC). Specifically, in our method, to provide a reference of what might the target entity be like, we first generate anchor entities within the relation-aware neighborhood of the head entity. Then, by pulling the query embedding towards the neighborhoods of the anchors, it is tuned to be more discriminative for target entity matching. The results of our extensive experiments not only validate the efficacy of RAA-KGC but also reveal that by integrating our relation-aware anchor enhancement strategy, the performance of current leading methods can be notably enhanced without substantial modifications.         ",
    "url": "https://arxiv.org/abs/2504.06129",
    "authors": [
      "Duanyang Yuan",
      "Sihang Zhou",
      "Xiaoshu Chen",
      "Dong Wang",
      "Ke Liang",
      "Xinwang Liu",
      "Jian Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.06134",
    "title": "SpikeStream: Accelerating Spiking Neural Network Inference on RISC-V Clusters with Sparse Computation Extensions",
    "abstract": "           Spiking Neural Network (SNN) inference has a clear potential for high energy efficiency as computation is triggered by events. However, the inherent sparsity of events poses challenges for conventional computing systems, driving the development of specialized neuromorphic processors, which come with high silicon area costs and lack the flexibility needed for running other computational kernels, limiting widespread adoption. In this paper, we explore the low-level software design, parallelization, and acceleration of SNNs on general-purpose multicore clusters with a low-overhead RISC-V ISA extension for streaming sparse computations. We propose SpikeStream, an optimization technique that maps weights accesses to affine and indirect register-mapped memory streams to enhance performance, utilization, and efficiency. Our results on the end-to-end Spiking-VGG11 model demonstrate a significant 4.39x speedup and an increase in utilization from 9.28% to 52.3% compared to a non-streaming parallel baseline. Additionally, we achieve an energy efficiency gain of 3.46x over LSMCore and a performance gain of 2.38x over Loihi.         ",
    "url": "https://arxiv.org/abs/2504.06134",
    "authors": [
      "Simone Manoni",
      "Paul Scheffler",
      "Luca Zanatta",
      "Andrea Acquaviva",
      "Luca Benini",
      "Andrea Bartolini"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2504.06141",
    "title": "Adversarial Training of Reward Models",
    "abstract": "           Reward modeling has emerged as a promising approach for the scalable alignment of language models. However, contemporary reward models (RMs) often lack robustness, awarding high rewards to low-quality, out-of-distribution (OOD) samples. This can lead to reward hacking, where policies exploit unintended shortcuts to maximize rewards, undermining alignment. To address this challenge, we introduce Adv-RM, a novel adversarial training framework that automatically identifies adversarial examples -- responses that receive high rewards from the target RM but are OOD and of low quality. By leveraging reinforcement learning, Adv-RM trains a policy to generate adversarial examples that reliably expose vulnerabilities in large state-of-the-art reward models such as Nemotron 340B RM. Incorporating these adversarial examples into the reward training process improves the robustness of RMs, mitigating reward hacking and enhancing downstream performance in RLHF. We demonstrate that Adv-RM significantly outperforms conventional RM training, increasing stability and enabling more effective RLHF training in both synthetic and real-data settings.         ",
    "url": "https://arxiv.org/abs/2504.06141",
    "authors": [
      "Alexander Bukharin",
      "Haifeng Qian",
      "Shengyang Sun",
      "Adithya Renduchintala",
      "Soumye Singhal",
      "Zhilin Wang",
      "Oleksii Kuchaiev",
      "Olivier Delalleau",
      "Tuo Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.06153",
    "title": "A Large-Scale Analysis on Contextual Self-Supervised Video Representation Learning",
    "abstract": "           Self-supervised learning has emerged as a powerful paradigm for label-free model pretraining, particularly in the video domain, where manual annotation is costly and time-intensive. However, existing self-supervised approaches employ diverse experimental setups, making direct comparisons challenging due to the absence of a standardized benchmark. In this work, we establish a unified benchmark that enables fair comparisons across different methods. Additionally, we systematically investigate five critical aspects of self-supervised learning in videos: (1) dataset size, (2) model complexity, (3) data distribution, (4) data noise, and (5) feature representations. To facilitate this study, we evaluate six self-supervised learning methods across six network architectures, conducting extensive experiments on five benchmark datasets and assessing performance on two distinct downstream tasks. Our analysis reveals key insights into the interplay between pretraining strategies, dataset characteristics, pretext tasks, and model architectures. Furthermore, we extend these findings to Video Foundation Models (ViFMs), demonstrating their relevance in large-scale video representation learning. Finally, leveraging these insights, we propose a novel approach that significantly reduces training data requirements while surpassing state-of-the-art methods that rely on 10% more pretraining data. We believe this work will guide future research toward a deeper understanding of self-supervised video representation learning and its broader implications.         ",
    "url": "https://arxiv.org/abs/2504.06153",
    "authors": [
      "Akash Kumar",
      "Ashlesha Kumar",
      "Vibhav Vineet",
      "Yogesh S Rawat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.06154",
    "title": "Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots",
    "abstract": "           Path planning algorithms, such as the search-based A*, are a critical component of autonomous mobile robotics, enabling robots to navigate from a starting point to a destination efficiently and safely. We investigated the resilience of the A* algorithm in the face of potential adversarial interventions known as obstacle attacks. The adversary's goal is to delay the robot's timely arrival at its destination by introducing obstacles along its original path. We developed malicious software to execute the attacks and conducted experiments to assess their impact, both in simulation using TurtleBot in Gazebo and in real-world deployment with the Unitree Go1 robot. In simulation, the attacks resulted in an average delay of 36\\%, with the most significant delays occurring in scenarios where the robot was forced to take substantially longer alternative paths. In real-world experiments, the delays were even more pronounced, with all attacks successfully rerouting the robot and causing measurable disruptions. These results highlight that the algorithm's robustness is not solely an attribute of its design but is significantly influenced by the operational environment. For example, in constrained environments like tunnels, the delays were maximized due to the limited availability of alternative routes.         ",
    "url": "https://arxiv.org/abs/2504.06154",
    "authors": [
      "Adrian Szvoren",
      "Jianwei Liu",
      "Dimitrios Kanoulas",
      "Nilufer Tuptuk"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.06160",
    "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
    "abstract": "           Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from sociological foundations of stigmatization theory, our stigmatization analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.         ",
    "url": "https://arxiv.org/abs/2504.06160",
    "authors": [
      "Rijul Magu",
      "Arka Dutta",
      "Sean Kim",
      "Ashiqur R. KhudaBukhsh",
      "Munmun De Choudhury"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.06165",
    "title": "Real-Time Pitch/F0 Detection Using Spectrogram Images and Convolutional Neural Networks",
    "abstract": "           This paper presents a novel approach to detect F0 through Convolutional Neural Networks and image processing techniques to directly estimate pitch from spectrogram images. Our new approach demonstrates a very good detection accuracy; a total of 92% of predicted pitch contours have strong or moderate correlations to the true pitch contours. Furthermore, the experimental comparison between our new approach and other state-of-the-art CNN methods reveals that our approach can enhance the detection rate by approximately 5% across various Signal-to-Noise Ratio conditions.         ",
    "url": "https://arxiv.org/abs/2504.06165",
    "authors": [
      "Xufang Zhao",
      "Omer Tsimhoni"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.06166",
    "title": "Assessing how hyperparameters impact Large Language Models' sarcasm detection performance",
    "abstract": "           Sarcasm detection is challenging for both humans and machines. This work explores how model characteristics impact sarcasm detection in OpenAI's GPT, and Meta's Llama-2 models, given their strong natural language understanding, and popularity. We evaluate fine-tuned and zero-shot models across various sizes, releases, and hyperparameters. Experiments were conducted on the political and balanced (pol-bal) portion of the popular Self-Annotated Reddit Corpus (SARC2.0) sarcasm dataset. Fine-tuned performance improves monotonically with model size within a model family, while hyperparameter tuning also impacts performance. In the fine-tuning scenario, full precision Llama-2-13b achieves state-of-the-art accuracy and $F_1$-score, both measured at 0.83, comparable to average human performance. In the zero-shot setting, one GPT-4 model achieves competitive performance to prior attempts, yielding an accuracy of 0.70 and an $F_1$-score of 0.75. Furthermore, a model's performance may increase or decline with each release, highlighting the need to reassess performance after each release.         ",
    "url": "https://arxiv.org/abs/2504.06166",
    "authors": [
      "Montgomery Gole",
      "Andriy Miranskyy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2504.06167",
    "title": "Signaling Human Intentions to Service Robots: Understanding the Use of Social Cues during In-Person Conversations",
    "abstract": "           As social service robots become commonplace, it is essential for them to effectively interpret human signals, such as verbal, gesture, and eye gaze, when people need to focus on their primary tasks to minimize interruptions and distractions. Toward such a socially acceptable Human-Robot Interaction, we conducted a study ($N=24$) in an AR-simulated context of a coffee chat. Participants elicited social cues to signal intentions to an anthropomorphic, zoomorphic, grounded technical, or aerial technical robot waiter when they were speakers or listeners. Our findings reveal common patterns of social cues over intentions, the effects of robot morphology on social cue position and conversational role on social cue complexity, and users' rationale in choosing social cues. We offer insights into understanding social cues concerning perceptions of robots, cognitive load, and social context. Additionally, we discuss design considerations on approaching, social cue recognition, and response strategies for future service robots.         ",
    "url": "https://arxiv.org/abs/2504.06167",
    "authors": [
      "Hanfang Lyu",
      "Xiaoyu Wang",
      "Nandi Zhang",
      "Shuai Ma",
      "Qian Zhu",
      "Yuhan Luo",
      "Fugee Tsung",
      "Xiaojuan Ma"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2504.06176",
    "title": "A Self-Supervised Framework for Space Object Behaviour Characterisation",
    "abstract": "           Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities.         ",
    "url": "https://arxiv.org/abs/2504.06176",
    "authors": [
      "Ian Groves",
      "Andrew Campbell",
      "James Fernandes",
      "Diego Rodriguez",
      "Paul Murray",
      "Massimiliano Vasile",
      "Victoria Nockles"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Space Physics (physics.space-ph)"
    ]
  },
  {
    "id": "arXiv:2504.06179",
    "title": "Plug and Play Distributed Control of Clustered Energy Hub Networks",
    "abstract": "           The transition to renewable energy is driving the rise of distributed multi-energy systems, in which individual energy hubs and prosumers (e.g., homes, industrial campuses) generate, store, and trade energy. Economic Model Predictive Control (MPC) schemes are widely used to optimize operation of energy hubs by efficiently dispatching resources and minimizing costs while ensuring operational constraints are met. Peer-to-peer (P2P) energy trading among hubs enhances network efficiency and reduces costs but also increases computational and privacy challenges, especially as the network scales. Additionally, current distributed control techniques require global recomputation whenever the network topology changes, limiting scalability. To address these challenges, we propose a clustering-based P2P trading framework that enables plug-and-play operation, allowing energy hubs to seamlessly join or leave without requiring network-wide controller updates. The impact is restricted to the hubs within the affected cluster. The energy trading problem is formulated as a bi-level bargaining game, where inter-cluster trading commitments are determined at the cluster level, while energy dispatch and cost-sharing among hubs within a cluster are refined at the hub level. Both levels are solved in a distributed manner using ADMM, ensuring computational feasibility and privacy preservation. Moreover, we develop plug-and-play procedures to handle dynamic topology changes at both the hub and cluster levels, minimizing disruptions across the network. Simulation results demonstrate that the proposed bi-level framework reduces operational costs, and enables scalable energy management under plug-and-play operation.         ",
    "url": "https://arxiv.org/abs/2504.06179",
    "authors": [
      "Varsha Behrunani",
      "Cara Koepele",
      "Jared Miller",
      "Ahmed Aboudonia",
      "Philipp Heer",
      "Roy S. Smith",
      "John Lygeros"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.06188",
    "title": "SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents",
    "abstract": "           AI agents are autonomous systems that can execute specific tasks based on predefined programming. Here, we present SkillFlow, a modular, technology-agnostic framework that allows agents to expand their functionality in an ad-hoc fashion by acquiring new skills from their environment or other agents. We present a theoretical model that examines under which conditions this framework would be beneficial, and we then explore SkillFlow's ability to accelerate task completion and lead to lower cumulative costs in a real-world application, namely scheduling agents for calendar events. We demonstrate that within a few iterations, SkillFlow leads to considerable (24.8%, p-value = $6.4\\times10^{-3}$) gains in time and cost, especially when the communication cost is high. Finally, we draw analogies from well-studied biological systems and compare this framework to that of lateral gene transfer, a significant process of adaptation and evolution in novel environments.         ",
    "url": "https://arxiv.org/abs/2504.06188",
    "authors": [
      "Pagkratios Tagkopoulos",
      "Fangzhou Li",
      "Ilias Tagkopoulos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2504.06193",
    "title": "Heuristic Methods are Good Teachers to Distill MLPs for Graph Link Prediction",
    "abstract": "           Link prediction is a crucial graph-learning task with applications including citation prediction and product recommendation. Distilling Graph Neural Networks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has emerged as an effective approach to achieve strong performance and reducing computational cost by removing graph dependency. However, existing distillation methods only use standard GNNs and overlook alternative teachers such as specialized model for link prediction (GNN4LP) and heuristic methods (e.g., common neighbors). This paper first explores the impact of different teachers in GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not always produce stronger students: MLPs distilled from GNN4LP can underperform those distilled from simpler GNNs, while weaker heuristic methods can teach MLPs to near-GNN performance with drastically reduced training costs. Building on these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which eliminates graph dependencies while effectively integrating complementary signals via a gating mechanism. Experiments on ten datasets show an average 7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less training time, indicating EHDM is an efficient and effective link prediction method.         ",
    "url": "https://arxiv.org/abs/2504.06193",
    "authors": [
      "Zongyue Qin",
      "Shichang Zhang",
      "Mingxuan Ju",
      "Tong Zhao",
      "Neil Shah",
      "Yizhou Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.06210",
    "title": "HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation",
    "abstract": "           We present Hierarchical Motion Representation (HiMoR), a novel deformation representation for 3D Gaussian primitives capable of achieving high-quality monocular dynamic 3D reconstruction. The insight behind HiMoR is that motions in everyday scenes can be decomposed into coarser motions that serve as the foundation for finer details. Using a tree structure, HiMoR's nodes represent different levels of motion detail, with shallower nodes modeling coarse motion for temporal smoothness and deeper nodes capturing finer motion. Additionally, our model uses a few shared motion bases to represent motions of different sets of nodes, aligning with the assumption that motion tends to be smooth and simple. This motion representation design provides Gaussians with a more structured deformation, maximizing the use of temporal relationships to tackle the challenging task of monocular dynamic 3D reconstruction. We also propose using a more reliable perceptual metric as an alternative, given that pixel-level metrics for evaluating monocular dynamic 3D reconstruction can sometimes fail to accurately reflect the true quality of reconstruction. Extensive experiments demonstrate our method's efficacy in achieving superior novel view synthesis from challenging monocular videos with complex motions.         ",
    "url": "https://arxiv.org/abs/2504.06210",
    "authors": [
      "Yiming Liang",
      "Tianhan Xu",
      "Yuta Kikuchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.06212",
    "title": "NNN: Next-Generation Neural Networks for Marketing Mix Modeling",
    "abstract": "           We present NNN, a Transformer-based neural network approach to Marketing Mix Modeling (MMM) designed to address key limitations of traditional methods. Unlike conventional MMMs which rely on scalar inputs and parametric decay functions, NNN uses rich embeddings to capture both quantitative and qualitative aspects of marketing and organic channels (e.g., search queries, ad creatives). This, combined with its attention mechanism, enables NNN to model complex interactions, capture long-term effects, and potentially improve sales attribution accuracy. We show that L1 regularization permits the use of such expressive models in typical data-constrained settings. Evaluating NNN on simulated and real-world data demonstrates its efficacy, particularly through considerable improvement in predictive power. Beyond attribution, NNN provides valuable, complementary insights through model probing, such as evaluating keyword or creative effectiveness, enhancing model interpretability.         ",
    "url": "https://arxiv.org/abs/2504.06212",
    "authors": [
      "Thomas Mulc",
      "Mike Anderson",
      "Paul Cubre",
      "Huikun Zhang",
      "Ivy Liu",
      "Saket Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2504.06241",
    "title": "A Case for Network-wide Orchestration of Host-based Intrusion Detection and Response",
    "abstract": "           Recent cyber incidents and the push for zero trust security underscore the necessity of monitoring host-level events. However, current host-level intrusion detection systems (IDS) lack the ability to correlate alerts and coordinate a network-wide response in real time. Motivated by advances in system-level extensions free of rebooting and network-wide orchestration of host actions, we propose using a central IDS orchestrator to remotely program the logic of each host IDS and collect the alerts generated in real time. In this paper, we make arguments for such a system concept and provide a high level design of the main system components. Furthermore, we have developed a system prototype and evaluated it using two experimental scenarios rooted from real-world attacks. The evaluation results show that the host-based IDS orchestration system is able to defend against the attacks effectively.         ",
    "url": "https://arxiv.org/abs/2504.06241",
    "authors": [
      "Mark Timmons",
      "Daniel Lukaszewski",
      "Geoffrey Xie"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2504.06242",
    "title": "Addressing Relative Degree Issues in Control Barrier Function Synthesis with Physics-Informed Neural Networks",
    "abstract": "           In robotics, control barrier function (CBF)-based safety filters are commonly used to enforce state constraints. A critical challenge arises when the relative degree of the CBF varies across the state space. This variability can create regions within the safe set where the control input becomes unconstrained. When implemented as a safety filter, this may result in chattering near the safety boundary and ultimately compromise system safety. To address this issue, we propose a novel approach for CBF synthesis by formulating it as solving a set of boundary value problems. The solutions to the boundary value problems are determined using physics-informed neural networks (PINNs). Our approach ensures that the synthesized CBFs maintain a constant relative degree across the set of admissible states, thereby preventing unconstrained control scenarios. We illustrate the approach in simulation and further verify it through real-world quadrotor experiments, demonstrating its effectiveness in preserving desired system safety properties.         ",
    "url": "https://arxiv.org/abs/2504.06242",
    "authors": [
      "Lukas Brunke",
      "Siqi Zhou",
      "Francesco D'Orazio",
      "Angela P. Schoellig"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.06257",
    "title": "PainNet: Statistical Relation Network with Episode-Based Training for Pain Estimation",
    "abstract": "           Despite the span in estimating pain from facial expressions, limited works have focused on estimating the sequence-level pain, which is reported by patients and used commonly in clinics. In this paper, we introduce a novel Statistical Relation Network, referred to as PainNet, designed for the estimation of the sequence-level pain. PainNet employs two key modules, the embedding and the relation modules, for comparing pairs of pain videos, and producing relation scores indicating if each pair belongs to the same pain category or not. At the core of the embedding module is a statistical layer mounted on the top of a RNN for extracting compact video-level features. The statistical layer is implemented as part of the deep architecture. Doing so, allows combining multiple training stages used in previous research, into a single end-to-end training stage. PainNet is trained using the episode-based training scheme, which involves comparing a query video with a set of videos representing the different pain categories. Experimental results show the benefit of using the statistical layer and the episode-based training in the proposed model. Furthermore, PainNet outperforms the state-of-the-art results on self-reported pain estimation.         ",
    "url": "https://arxiv.org/abs/2504.06257",
    "authors": [
      "Mina Bishay",
      "Graham Page",
      "Mohammad Mavadati"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05462",
    "title": "Quantum Mechanics and Neural Networks",
    "abstract": "           We demonstrate that any Euclidean-time quantum mechanical theory may be represented as a neural network, ensured by the Kosambi-Karhunen-Lo\u00e8ve theorem, mean-square path continuity, and finite two-point functions. The additional constraint of reflection positivity, which is related to unitarity, may be achieved by a number of mechanisms, such as imposing neural network parameter space splitting or the Markov property. Non-differentiability of the networks is related to the appearance of non-trivial commutators. Neural networks acting on Markov processes are no longer Markov, but still reflection positive, which facilitates the definition of deep neural network quantum systems. We illustrate these principles in several examples using numerical implementations, recovering classic quantum mechanical results such as Heisenberg uncertainty, non-trivial commutators, and the spectrum.         ",
    "url": "https://arxiv.org/abs/2504.05462",
    "authors": [
      "Christian Ferko",
      "James Halverson"
    ],
    "subjectives": [
      "High Energy Physics - Theory (hep-th)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2504.05591",
    "title": "Class Imbalance Correction for Improved Universal Lesion Detection and Tagging in CT",
    "abstract": "           Radiologists routinely detect and size lesions in CT to stage cancer and assess tumor burden. To potentially aid their efforts, multiple lesion detection algorithms have been developed with a large public dataset called DeepLesion (32,735 lesions, 32,120 CT slices, 10,594 studies, 4,427 patients, 8 body part labels). However, this dataset contains missing measurements and lesion tags, and exhibits a severe imbalance in the number of lesions per label category. In this work, we utilize a limited subset of DeepLesion (6\\%, 1331 lesions, 1309 slices) containing lesion annotations and body part label tags to train a VFNet model to detect lesions and tag them. We address the class imbalance by conducting three experiments: 1) Balancing data by the body part labels, 2) Balancing data by the number of lesions per patient, and 3) Balancing data by the lesion size. In contrast to a randomly sampled (unbalanced) data subset, our results indicated that balancing the body part labels always increased sensitivity for lesions >= 1cm for classes with low data quantities (Bone: 80\\% vs. 46\\%, Kidney: 77\\% vs. 61\\%, Soft Tissue: 70\\% vs. 60\\%, Pelvis: 83\\% vs. 76\\%). Similar trends were seen for three other models tested (FasterRCNN, RetinaNet, FoveaBox). Balancing data by lesion size also helped the VFNet model improve recalls for all classes in contrast to an unbalanced dataset. We also provide a structured reporting guideline for a ``Lesions'' subsection to be entered into the ``Findings'' section of a radiology report. To our knowledge, we are the first to report the class imbalance in DeepLesion, and have taken data-driven steps to address it in the context of joint lesion detection and tagging.         ",
    "url": "https://arxiv.org/abs/2504.05591",
    "authors": [
      "Peter D. Erickson",
      "Tejas Sudharshan Mathai",
      "Ronald M. Summers"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.05636",
    "title": "A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study",
    "abstract": "           Although digital breast tomosynthesis (DBT) improves diagnostic performance over full-field digital mammography (FFDM), false-positive recalls remain a concern in breast cancer screening. We developed a multi-modal artificial intelligence system integrating FFDM, synthetic mammography, and DBT to provide breast-level predictions and bounding-box localizations of suspicious findings. Our AI system, trained on approximately 500,000 mammography exams, achieved 0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls by 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity, underscoring its potential to improve clinical workflows. External validation confirmed strong generalizability, reducing the gap to a perfect AUROC by 35.31%-69.14% relative to strong baselines. In prospective deployment across 18 sites, the system reduced recall rates for low-risk cases. An improved version, trained on over 750,000 exams with additional labels, further reduced the gap by 18.86%-56.62% across large external datasets. Overall, these results underscore the importance of utilizing all available imaging modalities, demonstrate the potential for clinical impact, and indicate feasibility of further reduction of the test error with increased training set when using large-capacity neural networks.         ",
    "url": "https://arxiv.org/abs/2504.05636",
    "authors": [
      "Jungkyu Park",
      "Jan Witowski",
      "Yanqi Xu",
      "Hari Trivedi",
      "Judy Gichoya",
      "Beatrice Brown-Mulry",
      "Malte Westerhoff",
      "Linda Moy",
      "Laura Heacock",
      "Alana Lewin",
      "Krzysztof J. Geras"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05696",
    "title": "Diabetic Retinopathy Detection Based on Convolutional Neural Networks with SMOTE and CLAHE Techniques Applied to Fundus Images",
    "abstract": "           Diabetic retinopathy (DR) is one of the major complications in diabetic patients' eyes, potentially leading to permanent blindness if not detected timely. This study aims to evaluate the accuracy of artificial intelligence (AI) in diagnosing DR. The method employed is the Synthetic Minority Over-sampling Technique (SMOTE) algorithm, applied to identify DR and its severity stages from fundus images using the public dataset \"APTOS 2019 Blindness Detection.\" Literature was reviewed via ScienceDirect, ResearchGate, Google Scholar, and IEEE Xplore. Classification results using Convolutional Neural Network (CNN) showed the best performance for the binary classes normal (0) and DR (1) with an accuracy of 99.55%, precision of 99.54%, recall of 99.54%, and F1-score of 99.54%. For the multiclass classification No_DR (0), Mild (1), Moderate (2), Severe (3), Proliferate_DR (4), the accuracy was 95.26%, precision 95.26%, recall 95.17%, and F1-score 95.23%. Evaluation using the confusion matrix yielded results of 99.68% for binary classification and 96.65% for multiclass. This study highlights the significant potential in enhancing the accuracy of DR diagnosis compared to traditional human analysis         ",
    "url": "https://arxiv.org/abs/2504.05696",
    "authors": [
      "Sidhiq Mardianta",
      "Affandy",
      "Catur Supriyanto",
      "Catur Supriyanto",
      "Adi Wijaya"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2504.06250",
    "title": "Fractal and Regular Geometry of Deep Neural Networks",
    "abstract": "           We study the geometric properties of random neural networks by investigating the boundary volumes of their excursion sets for different activation functions, as the depth increases. More specifically, we show that, for activations which are not very regular (e.g., the Heaviside step function), the boundary volumes exhibit fractal behavior, with their Hausdorff dimension monotonically increasing with the depth. On the other hand, for activations which are more regular (e.g., ReLU, logistic and $\\tanh$), as the depth increases, the expected boundary volumes can either converge to zero, remain constant or diverge exponentially, depending on a single spectral parameter which can be easily computed. Our theoretical results are confirmed in some numerical experiments based on Monte Carlo simulations.         ",
    "url": "https://arxiv.org/abs/2504.06250",
    "authors": [
      "Simmaco Di Lillo",
      "Domenico Marinucci",
      "Michele Salvi",
      "Stefano Vigogna"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2210.15527",
    "title": "Exploiting Features and Logits in Heterogeneous Federated Learning",
    "abstract": "           Due to the rapid growth of IoT and artificial intelligence, deploying neural networks on IoT devices is becoming increasingly crucial for edge intelligence. Federated learning (FL) facilitates the management of edge devices to collaboratively train a shared model while maintaining training data local and private. However, a general assumption in FL is that all edge devices are trained on the same machine learning model, which may be impractical considering diverse device capabilities. For instance, less capable devices may slow down the updating process because they struggle to handle large models appropriate for ordinary devices. In this paper, we propose a novel data-free FL method that supports heterogeneous client models by managing features and logits, called Felo; and its extension with a conditional VAE deployed in the server, called Velo. Felo averages the mid-level features and logits from the clients at the server based on their class labels to provide the average features and logits, which are utilized for further training the client models. Unlike Felo, the server has a conditional VAE in Velo, which is used for training mid-level features and generating synthetic features according to the labels. The clients optimize their models based on the synthetic features and the average logits. We conduct experiments on two datasets and show satisfactory performances of our methods compared with the state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2210.15527",
    "authors": [
      "Yun-Hin Chan",
      "Edith C.-H. Ngai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2302.04406",
    "title": "Neural Architecture Search: Two Constant Shared Weights Initialisations",
    "abstract": "           In the last decade, zero-cost metrics have gained prominence in neural architecture search (NAS) due to their ability to evaluate architectures without training. These metrics are significantly faster and less computationally expensive than traditional NAS methods and provide insights into neural architectures' internal workings. This paper introduces epsinas, a novel zero-cost NAS metric that assesses architecture potential using two constant shared weight initialisations and the statistics of their outputs. We show that the dispersion of raw outputs, normalised by their average magnitude, strongly correlates with trained accuracy. This effect holds across image classification and language tasks on NAS-Bench-101, NAS-Bench-201, and NAS-Bench-NLP. Our method requires no data labels, operates on a single minibatch, and eliminates the need for gradient computation, making it independent of training hyperparameters, loss metrics, and human annotations. It evaluates a network in a fraction of a GPU second and integrates seamlessly into existing NAS frameworks. The code supporting this study can be found on GitHub at this https URL.         ",
    "url": "https://arxiv.org/abs/2302.04406",
    "authors": [
      "Ekaterina Gracheva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2304.02549",
    "title": "Self-Supervised Siamese Autoencoders",
    "abstract": "           In contrast to fully-supervised models, self-supervised representation learning only needs a fraction of data to be labeled and often achieves the same or even higher downstream performance. The goal is to pre-train deep neural networks on a self-supervised task, making them able to extract meaningful features from raw input data afterwards. Previously, autoencoders and Siamese networks have been successfully employed as feature extractors for tasks such as image classification. However, both have their individual shortcomings and benefits. In this paper, we combine their complementary strengths by proposing a new method called SidAE (Siamese denoising autoencoder). Using an image classification downstream task, we show that our model outperforms two self-supervised baselines across multiple data sets and scenarios. Crucially, this includes conditions in which only a small amount of labeled data is available. Empirically, the Siamese component has more impact, but the denoising autoencoder is nevertheless necessary to improve performance.         ",
    "url": "https://arxiv.org/abs/2304.02549",
    "authors": [
      "Friederike Baier",
      "Sebastian Mair",
      "Samuel G. Fadel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.15203",
    "title": "Frequency maps reveal the correlation between Adversarial Attacks and Implicit Bias",
    "abstract": "           Despite their impressive performance in classification tasks, neural networks are known to be vulnerable to adversarial attacks, subtle perturbations of the input data designed to deceive the model. In this work, we investigate the correlation between these perturbations and the implicit bias of neural networks trained with gradient-based algorithms. To this end, we analyse a representation of the network's implicit bias through the lens of the Fourier transform. Specifically, we identify unique fingerprints of implicit bias and adversarial attacks by calculating the minimal, essential frequencies needed for accurate classification of each image, as well as the frequencies that drive misclassification in its adversarially perturbed counterpart. This approach enables us to uncover and analyse the correlation between these essential frequencies, providing a precise map of how the network's biases align or contrast with the frequency components exploited by adversarial attacks. To this end, among other methods, we use a newly introduced technique capable of detecting nonlinear correlations between high-dimensional datasets. Our results provide empirical evidence that the network bias in Fourier space and the target frequencies of adversarial attacks are highly correlated and suggest new potential strategies for adversarial defence.         ",
    "url": "https://arxiv.org/abs/2305.15203",
    "authors": [
      "Lorenzo Basile",
      "Nikos Karantzas",
      "Alberto d'Onofrio",
      "Luca Manzoni",
      "Luca Bortolussi",
      "Alex Rodriguez",
      "Fabio Anselmi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2306.11950",
    "title": "Mitigating Communication Costs in Neural Networks: The Role of Dendritic Nonlinearity",
    "abstract": "           Our understanding of biological neuronal networks has profoundly influenced the development of artificial neural networks (ANNs). However, neurons utilized in ANNs differ considerably from their biological counterparts, primarily due to the absence of complex dendritic trees with local nonlinearities. Early studies have suggested that dendritic nonlinearities could substantially improve the learning capabilities of neural network models. In this study, we systematically examined the role of nonlinear dendrites within neural networks. Utilizing machine-learning methodologies, we assessed how dendritic nonlinearities influence neural network performance. Our findings demonstrate that dendritic nonlinearities do not substantially affect learning capacity; rather, their primary benefit lies in enabling network capacity expansion while minimizing communication costs through effective localized feature aggregation. This research provides critical insights with significant implications for designing future neural network accelerators aimed at reducing communication overhead during neural network training and inference.         ",
    "url": "https://arxiv.org/abs/2306.11950",
    "authors": [
      "Xundong Wu",
      "Pengfei Zhao",
      "Zilin Yu",
      "Lei Ma",
      "Ka-Wa Yip",
      "Huajin Tang",
      "Gang Pan",
      "Poirazi Panayiota",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2310.15117",
    "title": "Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference",
    "abstract": "           Large Language Models (LLMs) have been used as experts to infer causal graphs, often by repeatedly applying a pairwise prompt that asks about the causal relationship of each variable pair. However, such experts, including human domain experts, cannot distinguish between direct and indirect effects given a pairwise prompt. Therefore, instead of the graph, we propose that causal order be used as a more stable output interface for utilizing expert knowledge. Even when querying a perfect expert with a pairwise prompt, we show that the inferred graph can have significant errors whereas the causal order is always correct. In practice, however, LLMs are imperfect experts and we find that pairwise prompts lead to multiple cycles. Hence, we propose the triplet method, a novel querying strategy that introduces an auxiliary variable for every variable pair and instructs the LLM to avoid cycles within this triplet. It then uses a voting-based ensemble method that results in higher accuracy and fewer cycles while ensuring cost efficiency. Across multiple real-world graphs, such a triplet-based method yields a more accurate order than the pairwise prompt, using both LLMs and human annotators. The triplet method enhances robustness by repeatedly querying an expert with different auxiliary variables, enabling smaller models like Phi-3 and Llama-3 8B Instruct to surpass GPT-4 with pairwise prompting. For practical usage, we show how the expert-provided causal order from the triplet method can be used to reduce error in downstream graph discovery and effect inference tasks.         ",
    "url": "https://arxiv.org/abs/2310.15117",
    "authors": [
      "Aniket Vashishtha",
      "Abbavaram Gowtham Reddy",
      "Abhinav Kumar",
      "Saketh Bachu",
      "Vineeth N Balasubramanian",
      "Amit Sharma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2311.15832",
    "title": "Attacking at non-harmonic frequencies in screaming-channel attacks",
    "abstract": "           Screaming-channel attacks enable Electromagnetic (EM) Side-Channel Attacks (SCAs) at larger distances due to higher EM leakage energies than traditional SCAs, relaxing the requirement of close access to the victim. This attack can be mounted on devices integrating Radio Frequency (RF) modules on the same die as digital circuits, where the RF can unintentionally capture, modulate, amplify, and transmit the leakage along with legitimate signals. Leakage results from digital switching activity, so the hypothesis of previous works was that this leakage would appear at multiples of the digital clock frequency, i.e., harmonics. This work demonstrates that compromising signals appear not only at the harmonics and that leakage at non-harmonics can be exploited for successful attacks. Indeed, the transformations undergone by the leaked signal are complex due to propagation effects through the substrate and power and ground planes, so the leakage also appears at other frequencies. We first propose two methodologies to locate frequencies that contain leakage and demonstrate that it appears at non-harmonic frequencies. Then, our experimental results show that screaming-channel attacks at non-harmonic frequencies can be as successful as at harmonics when retrieving a 16-byte AES key. As the RF spectrum is polluted by interfering signals, we run experiments and show successful attacks in a more realistic, noisy environment where harmonic frequencies are contaminated by multi-path fading and interference. These attacks at non-harmonic frequencies increase the attack surface by providing attackers with an increased number of potential frequencies where attacks can succeed.         ",
    "url": "https://arxiv.org/abs/2311.15832",
    "authors": [
      "Jeremy Guillaume",
      "Maxime Pelcat",
      "Amor Nafkha",
      "Ruben Salvador"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2402.07601",
    "title": "Topic-aware Most Influential Community Search in Social Networks",
    "abstract": "           Influential community search (ICS) finds a set of densely connected and high-impact vertices from a social network. Although great effort has been devoted to ICS problems, most existing methods do not consider how relevant the influential community found is to specific topics. A few attempts at topic-aware ICS problems cannot capture the stochastic nature of community formation and influence propagation in social networks. To address these issues, we introduce a novel problem of topic-aware most influential community search (\\prob) to discover a set of vertices such that for a given topic vector q, they induce a $(k, l, \\eta)$-core in an uncertain directed interaction graph and have the highest influence scores under the independent cascade (IC) model. We propose an online algorithm to provide an approximate result for any \\prob query with bounded errors. Furthermore, we design two index structures and an index-based heuristic algorithm for efficient \\prob query processing. Finally, we experimentally evaluate the efficacy and efficiency of our proposed approaches on various real-world datasets. The results show that (1) the communities of \\prob have higher relevance and social influence w.r.t.~the query topics as well as structural cohesiveness than those of several state-of-the-art topic-aware and influential CS methods and (2) the index-based algorithm achieves speed-ups of up to three orders of magnitude over the online algorithm with an affordable overhead for index construction.         ",
    "url": "https://arxiv.org/abs/2402.07601",
    "authors": [
      "Long Teng",
      "Yanhao Wang",
      "Zhe Lin",
      "Fei Yu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.10206",
    "title": "Ising on the Graph: Task-specific Graph Subsampling via the Ising Model",
    "abstract": "           Reducing a graph while preserving its overall properties is an important problem with many applications. Typically, reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion without requiring a differentiable loss function for the task. We showcase the versatility of our approach on four distinct applications: image segmentation, explainability for graph classification, 3D shape sparsification, and sparse approximate matrix inverse determination.         ",
    "url": "https://arxiv.org/abs/2402.10206",
    "authors": [
      "Maria B\u00e5nkestad",
      "Jennifer R. Andersson",
      "Sebastian Mair",
      "Jens Sj\u00f6lund"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.17904",
    "title": "4CNet: A Diffusion Approach to Map Prediction for Decentralized Multi-Robot Exploration",
    "abstract": "           Mobile robots in unknown cluttered environments with irregularly shaped obstacles often face energy and communication challenges which directly affect their ability to explore these environments. In this paper, we introduce a novel deep learning architecture, Confidence-Aware Contrastive Conditional Consistency Model (4CNet), for robot map prediction during decentralized, resource-limited multi-robot exploration. 4CNet uniquely incorporates: 1) a conditional consistency model for map prediction in unstructured unknown regions, 2) a contrastive map-trajectory pretraining framework for a trajectory encoder that extracts spatial information from the trajectories of nearby robots during map prediction, and 3) a confidence network to measure the uncertainty of map prediction for effective exploration under resource constraints. We incorporate 4CNet within our proposed robot exploration with map prediction architecture, 4CNet-E. We then conduct extensive comparison studies with 4CNet-E and state-of-the-art heuristic and learning methods to investigate both map prediction and exploration performance in environments consisting of irregularly shaped obstacles and uneven terrain. Results showed that 4CNet-E obtained statistically significant higher prediction accuracy and area coverage with varying environment sizes, number of robots, energy budgets, and communication limitations when compared to database and learning-based methods. Hardware experiments were performed and validated the applicability and generalizability of 4CNet-E in both unstructured indoor and real natural outdoor environments.         ",
    "url": "https://arxiv.org/abs/2402.17904",
    "authors": [
      "Aaron Hao Tan",
      "Siddarth Narasimhan",
      "Goldie Nejat"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2403.06430",
    "title": "AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration",
    "abstract": "           Deep learning-based face restoration models, increasingly prevalent in smart devices, have become targets for sophisticated backdoor attacks. These attacks, through subtle trigger injection into input face images, can lead to unexpected restoration outcomes. Unlike conventional methods focused on classification tasks, our approach introduces a unique degradation objective tailored for attacking restoration models. Moreover, we propose the Adaptive Selective Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural network for input-specific trigger generation in the frequency domain, seamlessly blending triggers with benign images. This results in imperceptible yet effective attacks, guiding restoration predictions towards subtly degraded outputs rather than conspicuous targets. Extensive experiments demonstrate the efficacy of the degradation objective on state-of-the-art face restoration models. Additionally, it is notable that AS-FIBA can insert effective backdoors that are more imperceptible than existing backdoor attack methods, including WaNet, ISSBA, and FIBA.         ",
    "url": "https://arxiv.org/abs/2403.06430",
    "authors": [
      "Zhenbo Song",
      "Wenhao Gao",
      "Zhenyuan Zhang",
      "Jianfeng Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2403.18191",
    "title": "Measuring changes in polarisation using Singular Value Decomposition of network graphs",
    "abstract": "           In this paper we present new methods of measuring polarisation in social networks. We use Random Dot Product Graphs to embed social networks in metric spaces. Singular Value Decomposition of this social network then provider an embedded dimensionality which corresponds to the number of uncorrelated dimensions in the network. A decrease in the optimal dimensionality for the embedding of the network graph means that the dimensions in the network are becoming more correlated, and therefore the network is becoming more polarised. We demonstrate this method by analysing social networks such as communication interactions among New Zealand Twitter users discussing climate change issues and international social media discussions of the COP conferences. In both cases, the decreasing embedded dimensionality indicates that these networks have become more polarised over time. We also use networks generated by stochastic block models to explore how an increase of the isolation between distinct communities, or the increase of the predominance of one community over the other, in the social networks decrease the embedded dimensionality and are therefore identifiable as polarisation processes.         ",
    "url": "https://arxiv.org/abs/2403.18191",
    "authors": [
      "Sage Anastasi",
      "Giulio Dalla Riva"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2404.03543",
    "title": "CodeEditorBench: Evaluating Code Editing Capability of Large Language Models",
    "abstract": "           Large Language Models (LLMs) for code are rapidly evolving, with code editing emerging as a critical capability. We introduce CodeEditorBench, an evaluation framework designed to rigorously assess the performance of LLMs in code editing tasks, including debugging, translating, polishing, and requirement switching. Unlike existing benchmarks focusing solely on code generation, CodeEditorBench emphasizes real-world scenarios and practical aspects of software development. We curate diverse coding challenges and scenarios from five sources, covering various programming languages, complexity levels, and editing tasks. Evaluation of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and GPT-4), outperform open-source models in CodeEditorBench, highlighting differences in model performance based on problem types and prompt sensitivities. CodeEditorBench aims to catalyze advancements in LLMs by providing a robust platform for assessing code editing capabilities. We will release all prompts and datasets to enable the community to expand the dataset and benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to the advancement of LLMs in code editing and provide a valuable resource for researchers and practitioners.         ",
    "url": "https://arxiv.org/abs/2404.03543",
    "authors": [
      "Jiawei Guo",
      "Ziming Li",
      "Xueling Liu",
      "Kaijing Ma",
      "Tianyu Zheng",
      "Zhouliang Yu",
      "Ding Pan",
      "Yizhi LI",
      "Ruibo Liu",
      "Yue Wang",
      "Shuyue Guo",
      "Xingwei Qu",
      "Xiang Yue",
      "Ge Zhang",
      "Wenhu Chen",
      "Jie Fu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.08424",
    "title": "Comparing Apples to Oranges: LLM-powered Multimodal Intention Prediction in an Object Categorization Task",
    "abstract": "           Human intention-based systems enable robots to perceive and interpret user actions to interact with humans and adapt to their behavior proactively. Therefore, intention prediction is pivotal in creating a natural interaction with social robots in human-designed environments. In this paper, we examine using Large Language Models (LLMs) to infer human intention in a collaborative object categorization task with a physical robot. We propose a novel multimodal approach that integrates user non-verbal cues, like hand gestures, body poses, and facial expressions, with environment states and user verbal cues to predict user intentions in a hierarchical architecture. Our evaluation of five LLMs shows the potential for reasoning about verbal and non-verbal user cues, leveraging their context-understanding and real-world knowledge to support intention prediction while collaborating on a task with a social robot. Video: this https URL ",
    "url": "https://arxiv.org/abs/2404.08424",
    "authors": [
      "Hassan Ali",
      "Philipp Allgeuer",
      "Stefan Wermter"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2404.08624",
    "title": "Regularized Gradient Clipping Provably Trains Wide and Deep Neural Networks",
    "abstract": "           We present and analyze a novel regularized form of the gradient clipping algorithm, proving that it converges to global minima of the loss surface of deep neural networks under the squared loss, provided that the layers are of sufficient width. The algorithm presented here, dubbed $\\delta-$GClip, introduces a modification to gradient clipping that leads to a first-of-its-kind example of a step size scheduling for gradient descent that provably minimizes training losses of deep neural nets. We also present empirical evidence that our theoretically founded $\\delta-$GClip algorithm is competitive with the state-of-the-art deep learning heuristics on various neural architectures including modern transformer based architectures. The modification we do to standard gradient clipping is designed to leverage the PL* condition, a variant of the Polyak-Lojasiewicz inequality which was recently proven to be true for sufficiently wide neural networks at any depth within a neighbourhood of the initialization.         ",
    "url": "https://arxiv.org/abs/2404.08624",
    "authors": [
      "Matteo Tucat",
      "Anirbit Mukherjee",
      "Procheta Sen",
      "Mingfei Sun",
      "Omar Rivasplata"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2405.10577",
    "title": "DuoSpaceNet: Leveraging Both Bird's-Eye-View and Perspective View Representations for 3D Object Detection",
    "abstract": "           Multi-view camera-only 3D object detection largely follows two primary paradigms: exploiting bird's-eye-view (BEV) representations or focusing on perspective-view (PV) features, each with distinct advantages. Although several recent approaches explore combining BEV and PV, many rely on partial fusion or maintain separate detection heads. In this paper, we propose DuoSpaceNet, a novel framework that fully unifies BEV and PV feature spaces within a single detection pipeline for comprehensive 3D perception. Our design includes a decoder to integrate BEV and PV features into unified detection queries, as well as a feature enhancement strategy that enriches different feature representations. In addition, DuoSpaceNet can be extended to handle multi-frame inputs, enabling more robust temporal analysis. Extensive experiments on nuScenes dataset show that DuoSpaceNet surpasses both BEV-based baselines (e.g., BEVFormer) and PV-based baselines (e.g., Sparse4D) in 3D object detection and BEV map segmentation, verifying the effectiveness of our proposed design.         ",
    "url": "https://arxiv.org/abs/2405.10577",
    "authors": [
      "Zhe Huang",
      "Yizhe Zhao",
      "Hao Xiao",
      "Chenyan Wu",
      "Lingting Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2405.20445",
    "title": "Fully-inductive Node Classification on Arbitrary Graphs",
    "abstract": "           One fundamental challenge in graph machine learning is generalizing to new graphs. Many existing methods following the inductive setup can generalize to test graphs with new structures, but assuming the feature and label spaces remain the same as the training ones. This paper introduces a fully-inductive setup, where models should perform inference on arbitrary test graphs with new structures, feature and label spaces. We propose GraphAny as the first attempt at this challenging setup. GraphAny models inference on a new graph as an analytical solution to a LinearGNN, which can be naturally applied to graphs with any feature and label spaces. To further build a stronger model with learning capacity, we fuse multiple LinearGNN predictions with learned inductive attention scores. Specifically, the attention module is carefully parameterized as a function of the entropy-normalized distance features between pairs of LinearGNN predictions to ensure generalization to new graphs. Empirically, GraphAny trained on a single Wisconsin dataset with only 120 labeled nodes can generalize to 30 new graphs with an average accuracy of 67.26%, surpassing not only all inductive baselines, but also strong transductive methods trained separately on each of the 30 test graphs.         ",
    "url": "https://arxiv.org/abs/2405.20445",
    "authors": [
      "Jianan Zhao",
      "Zhaocheng Zhu",
      "Mikhail Galkin",
      "Hesham Mostafa",
      "Michael Bronstein",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2405.20769",
    "title": "Avoiding Pitfalls for Privacy Accounting of Subsampled Mechanisms under Composition",
    "abstract": "           We consider the problem of computing tight privacy guarantees for the composition of subsampled differentially private mechanisms. Recent algorithms can numerically compute the privacy parameters to arbitrary precision but must be carefully applied. Our main contribution is to address two common points of confusion. First, some privacy accountants assume that the privacy guarantees for the composition of a subsampled mechanism are determined by self-composing the worst-case datasets for the uncomposed mechanism. We show that this is not true in general. Second, Poisson subsampling is sometimes assumed to have similar privacy guarantees compared to sampling without replacement. We show that the privacy guarantees may in fact differ significantly between the two sampling schemes. In particular, we give an example of hyperparameters that result in $\\varepsilon \\approx 1$ for Poisson subsampling and $\\varepsilon > 10$ for sampling without replacement. This occurs for some parameters that could realistically be chosen for DP-SGD.         ",
    "url": "https://arxiv.org/abs/2405.20769",
    "authors": [
      "Christian Janos Lebeda",
      "Matthew Regehr",
      "Gautam Kamath",
      "Thomas Steinke"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.06984",
    "title": "On the H\u00f6lder Stability of Multiset and Graph Neural Networks",
    "abstract": "           Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of \"separable\" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough. In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and \\Holder{} stability to parametric functions. The proposed framework, which we name \\emph{\\Holder{} in expectation}, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-\\Holder{} in expectation, with an exponent that decays rapidly with the network's depth . Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks.         ",
    "url": "https://arxiv.org/abs/2406.06984",
    "authors": [
      "Yair Davidson",
      "Nadav Dym"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.07042",
    "title": "EFFOcc: Learning Efficient Occupancy Networks from Minimal Labels for Autonomous Driving",
    "abstract": "           3D occupancy prediction (3DOcc) is a rapidly rising and challenging perception task in the field of autonomous driving. Existing 3D occupancy networks (OccNets) are both computationally heavy and label-hungry. In terms of model complexity, OccNets are commonly composed of heavy Conv3D modules or transformers at the voxel level. Moreover, OccNets are supervised with expensive large-scale dense voxel labels. Model and data inefficiencies, caused by excessive network parameters and label annotation requirements, severely hinder the onboard deployment of OccNets. This paper proposes an EFFicient Occupancy learning framework, EFFOcc, that targets minimal network complexity and label requirements while achieving state-of-the-art accuracy. We first propose an efficient fusion-based OccNet that only uses simple 2D operators and improves accuracy to the state-of-the-art on three large-scale benchmarks: Occ3D-nuScenes, Occ3D-Waymo, and OpenOccupancy-nuScenes. On the Occ3D-nuScenes benchmark, the fusion-based model with ResNet-18 as the image backbone has 21.35M parameters and achieves 51.49 in terms of mean Intersection over Union (mIoU). Furthermore, we propose a multi-stage occupancy-oriented distillation to efficiently transfer knowledge to vision-only OccNet. Extensive experiments on occupancy benchmarks show state-of-the-art precision for both fusion-based and vision-based OccNets. For the demonstration of learning with limited labels, we achieve 94.38\\% of the performance (mIoU = 28.38) of a 100\\% labeled vision OccNet (mIoU = 30.07) using the same OccNet trained with only 40\\% labeled sequences and distillation from the fusion-based OccNet.         ",
    "url": "https://arxiv.org/abs/2406.07042",
    "authors": [
      "Yining Shi",
      "Kun Jiang",
      "Jinyu Miao",
      "Ke Wang",
      "Kangan Qian",
      "Yunlong Wang",
      "Jiusi Li",
      "Tuopu Wen",
      "Mengmeng Yang",
      "Yiliang Xu",
      "Diange Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07467",
    "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
    "abstract": "           Most log-based anomaly detectors assume logs are stable, though logs are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models -- decision tree, k-nearest neighbors, and a feedforward neural network -- with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points in F1 score while using 62.87 percentage points less labeled data. When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 percentage points increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog's key components: cache, RAG and ensemble learning.         ",
    "url": "https://arxiv.org/abs/2406.07467",
    "authors": [
      "Fatemeh Hadadi",
      "Qinghua Xu",
      "Domenico Bianculli",
      "Lionel Briand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2406.08092",
    "title": "Languages Transferred Within the Encoder: On Representation Transfer in Zero-Shot Multilingual Translation",
    "abstract": "           Understanding representation transfer in multilingual neural machine translation (MNMT) can reveal the reason for the zero-shot translation deficiency. In this work, we systematically analyze the representational issue of MNMT models. We first introduce the identity pair, translating a sentence to itself, to address the lack of the base measure in multilingual investigations, as the identity pair can reflect the representation of a language within the model. Then, we demonstrate that the encoder transfers the source language to the representational subspace of the target language instead of the language-agnostic state. Thus, the zero-shot translation deficiency arises because the representation of a translation is entangled with other languages and not transferred to the target language effectively. Based on our findings, we propose two methods: 1) low-rank language-specific embedding at the encoder, and 2) language-specific contrastive learning of the representation at the decoder. The experimental results on Europarl-15, TED-19, and OPUS-100 datasets show that our methods substantially enhance the performance of zero-shot translations without sacrifices in supervised directions by improving language transfer capacity, thereby providing practical evidence to support our conclusions. Codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.08092",
    "authors": [
      "Zhi Qu",
      "Chenchen Ding",
      "Taro Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2407.00936",
    "title": "Large Language Model Enhanced Knowledge Representation Learning: A Survey",
    "abstract": "           Knowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling KG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models (LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating textual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, encoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified Seq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge from large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of downstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging research directions in these evolving domains.         ",
    "url": "https://arxiv.org/abs/2407.00936",
    "authors": [
      "Xin Wang",
      "Zirui Chen",
      "Haofen Wang",
      "Leong Hou U",
      "Zhao Li",
      "Wenbin Guo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.05712",
    "title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices",
    "abstract": "           Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, we introduce a mixed representation of explicit and implicit keypoints for precise motion modeling and precomputed visual features for enhanced foreground and background synthesis. With these two key designs and using simple U-Nets as backbones, our method achieves state-of-the-art performance with less than one-tenth the computational demand. It has been validated to reach speeds of over 100 FPS on mobile devices and support both video and audio-driven inputs.         ",
    "url": "https://arxiv.org/abs/2407.05712",
    "authors": [
      "Jianwen Jiang",
      "Gaojie Lin",
      "Zhengkun Rong",
      "Chao Liang",
      "Yongming Zhu",
      "Jiaqi Yang",
      "Tianyun Zhong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.09550",
    "title": "CAPM: Fast and Robust Verification on Maxpool-based CNN via Dual Network",
    "abstract": "           This study uses CAPM (Convex Adversarial Polytope for Maxpool-based CNN) to improve the verified bound for general purpose maxpool-based convolutional neural networks (CNNs) under bounded norm adversarial perturbations. The maxpool function is decomposed as a series of ReLU functions to extend the convex relaxation technique to maxpool functions, by which the verified bound can be efficiently computed through a dual network. The experimental results demonstrate that this technique allows the state-of-the-art verification precision for maxpool-based CNNs and involves a much lower computational cost than current verification methods, such as DeepZ, DeepPoly and PRIMA. This method is also applicable to large-scale CNNs, which previous studies show to be often computationally prohibitively expensive. Under certain circumstances, CAPM is 40-times, 20-times or twice as fast and give a significantly higher verification bound (CAPM 98% vs. PRIMA 76%/DeepPoly 73%/DeepZ 8%) as compared to PRIMA/DeepPoly/DeepZ. Furthermore, we additionally present the time complexity of our algorithm as $O(W^2NK)$, where $W$ is the maximum width of the neural network, $N$ is the number of neurons, and $K$ is the size of the maxpool layer's kernel.         ",
    "url": "https://arxiv.org/abs/2407.09550",
    "authors": [
      "Jia-Hau Bai",
      "Chi-Ting Liu",
      "Yu Wang",
      "Fu-Chieh Chang",
      "Pei-Yuan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.20007",
    "title": "Rosetta Statements: Simplifying FAIR Knowledge Graph Construction with a User-Centered Approach",
    "abstract": "           Machines need data and metadata to be machine-actionable and FAIR (findable, accessible, interoperable, reusable) to manage increasing data volumes. Knowledge graphs and ontologies are key to this, but their use is hampered by high access barriers due to required prior knowledge in semantics and data modelling. The Rosetta Statement approach proposes modeling English natural language statements instead of a mind-independent reality. We propose a metamodel for creating semantic schema patterns for simple statement types. The approach supports versioning of statements and provides a detailed editing history. Each Rosetta Statement pattern has a dynamic label for displaying statements as natural language sentences. Implemented in the Open Research Knowledge Graph (ORKG) as a use case, this approach allows domain experts to define data schema patterns without needing semantic knowledge. Future plans include combining Rosetta Statements with semantic units to organize ORKG into meaningful subgraphs, improving usability. A search interface for querying statements without needing SPARQL or Cypher knowledge is also planned, along with tools for data entry and display using Large Language Models. The Rosetta Statement metamodel supports a three-step knowledge graph construction procedure. Domain experts can model semantic content without support from ontology engineers by using Wikidata, lowering entry barriers and increasing cognitive interoperability. The second level involves mapping Wikidata terms to established ontologies, and the third step developing semantic graph patterns for reasoning, requiring collaboration with ontology engineers.         ",
    "url": "https://arxiv.org/abs/2407.20007",
    "authors": [
      "Lars Vogt",
      "Kheir Eddine Farfar",
      "Pallavi Karanth",
      "Marcel Konrad",
      "Allard Oelen",
      "Manuel Prinz",
      "Philip Stroemert"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2408.13378",
    "title": "DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction",
    "abstract": "           Advancements in large language models (LLMs) allow them to address diverse questions using human-like interfaces. Still, limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. While drug-target interaction (DTI) prediction is important for drug discovery, existing approaches face challenges due to complex biological systems and the lack of interpretability needed for clinical applications. DrugAgent is a multi-agent LLM system for DTI prediction that combines multiple specialized perspectives with transparent reasoning. Our system adapts and extends existing multi-agent frameworks by (1) applying coordinator-based architecture to the DTI domain, (2) integrating domain-specific data sources, including ML predictions, knowledge graphs, and literature evidence, and (3) incorporating Chain-of-Thought (CoT) and ReAct (Reason+Act) frameworks for transparent DTI reasoning. We conducted comprehensive experiments using a kinase inhibitor dataset, where our multi-agent LLM method outperformed the non-reasoning multi-agent model (GPT-4o mini) by 45% in F1 score (0.514 vs 0.355). Through ablation studies, we demonstrated the contributions of each agent, with the AI agent being the most impactful, followed by the KG agent and search agent. Most importantly, our approach provides detailed, human-interpretable reasoning for each prediction by combining evidence from multiple sources - a critical feature for biomedical applications where understanding the rationale behind predictions is essential for clinical decision-making and regulatory compliance. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.13378",
    "authors": [
      "Yoshitaka Inoue",
      "Tianci Song",
      "Xinling Wang",
      "Augustin Luna",
      "Tianfan Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2409.04067",
    "title": "Preconditioned FEM-based Neural Networks for Solving Incompressible Fluid Flows and Related Inverse Problems",
    "abstract": "           The numerical simulation and optimization of technical systems described by partial differential equations is expensive, especially in multi-query scenarios in which the underlying equations have to be solved for different parameters. A comparatively new approach in this context is to combine the good approximation properties of neural networks (for parameter dependence) with the classical finite element method (for discretization). However, instead of considering the solution mapping of the PDE from the parameter space into the FEM-discretized solution space as a purely data-driven regression problem, so-called physically informed regression problems have proven to be useful. In these, the equation residual is minimized during the training of the neural network, i.e., the neural network \"learns\" the physics underlying the problem. In this paper, we extend this approach to saddle-point and non-linear fluid dynamics problems, respectively, namely stationary Stokes and stationary Navier-Stokes equations. In particular, we propose a modification of the existing approach: Instead of minimizing the plain vanilla equation residual during training, we minimize the equation residual modified by a preconditioner. By analogy with the linear case, this also improves the condition in the present non-linear case. Our numerical examples demonstrate that this approach significantly reduces the training effort and greatly increases accuracy and generalizability. Finally, we show the application of the resulting parameterized model to a related inverse problem.         ",
    "url": "https://arxiv.org/abs/2409.04067",
    "authors": [
      "Franziska Griese",
      "Fabian Hoppe",
      "Alexander R\u00fcttgers",
      "Philipp Knechtges"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2409.06705",
    "title": "HSR-KAN: Efficient Hyperspectral Image Super-Resolution via Kolmogorov-Arnold Networks",
    "abstract": "           Hyperspectral images (HSIs) have great potential in various visual tasks due to their rich spectral information. However, obtaining high-resolution hyperspectral images remains challenging due to limitations of physical imaging. Inspired by Kolmogorov-Arnold Networks (KANs), we propose an efficient HSI super-resolution (HSI-SR) model to fuse a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution HSI (HR-HSI). To achieve the effective integration of spatial information from HR-MSI, we design a fusion module based on KANs, called KAN-Fusion. Further inspired by the channel attention mechanism, we design a spectral channel attention module called KAN Channel Attention Block (KAN-CAB) for post-fusion feature extraction. As a channel attention module integrated with KANs, KAN-CAB not only enhances the fine-grained adjustment ability of deep networks, enabling networks to accurately simulate details of spectral sequences and spatial textures, but also effectively avoid Curse of Dimensionality. Extensive experiments show that, compared to current state-of-the-art HSI-SR methods, proposed HSR-KAN achieves the best performance in terms of both qualitative and quantitative assessments. Our code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2409.06705",
    "authors": [
      "Baisong Li",
      "Xingwang Wang",
      "Haixiao Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.18084",
    "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model",
    "abstract": "           With the increasing presence of service robots and autonomous vehicles in human environments, navigation systems need to evolve beyond simple destination reach to incorporate social awareness. This paper introduces GSON, a novel group-based social navigation framework that leverages Large Multimodal Models (LMMs) to enhance robots' social perception capabilities. Our approach uses visual prompting to enable zero-shot extraction of social relationships among pedestrians and integrates these results with robust pedestrian detection and tracking pipelines to overcome the inherent inference speed limitations of LMMs. The planning system incorporates a mid-level planner that sits between global path planning and local motion planning, effectively preserving both global context and reactive responsiveness while avoiding disruption of the predicted social group. We validate GSON through extensive real-world mobile robot navigation experiments involving complex social scenarios such as queuing, conversations, and photo sessions. Comparative results show that our system significantly outperforms existing navigation approaches in minimizing social perturbations while maintaining comparable performance on traditional navigation metrics.         ",
    "url": "https://arxiv.org/abs/2409.18084",
    "authors": [
      "Shangyi Luo",
      "Ji Zhu",
      "Peng Sun",
      "Yuhong Deng",
      "Cunjun Yu",
      "Anxing Xiao",
      "Xueqian Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.08105",
    "title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?",
    "abstract": "           Prompting techniques such as chain-of-thought have established themselves as a popular vehicle for improving the outputs of large language models (LLMs). For code generation, however, their exact mechanics and efficacy are under-explored. We thus investigate the effects of a wide range of prompting strategies with a focus on automatic re-prompting over multiple turns and computational requirements. After systematically decomposing reasoning, instruction, and execution feedback prompts, we conduct an extensive grid search on the competitive programming benchmarks CodeContests and TACO for multiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o). Our study reveals strategies that consistently improve performance across all models with small and large sampling budgets. We then show how finetuning with such an optimal configuration allows models to internalize the induced reasoning process and obtain improvements in performance and scalability for multi-turn code generation.         ",
    "url": "https://arxiv.org/abs/2410.08105",
    "authors": [
      "Kunhao Zheng",
      "Juliette Decugis",
      "Jonas Gehring",
      "Taco Cohen",
      "Benjamin Negrevergne",
      "Gabriel Synnaeve"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.13453",
    "title": "Adaptive Augmentation Policy Optimization with LLM Feedback",
    "abstract": "           Data augmentation is a critical component of deep learning pipelines, enhancing model generalization by increasing dataset diversity. Traditional augmentation strategies rely on manually designed transformations, stochastic sampling, or automated search-based approaches. Although automated methods improve performance, they often require extensive computational resources and are tailored to specific datasets. In this work, we propose a Large Language Model (LLM)-guided augmentation optimization strategy that refines augmentation policies based on model performance feedback. We introduce two approaches: (1) LLM-Guided Augmentation Policy Optimization, where augmentation policies are selected by an LLM prior to training and iteratively refined across multiple training cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization, where policies adapt in real-time based on performance metrics. This in-training approach eliminates the need for full model retraining before receiving LLM feedback, thereby reducing computational costs while improving performance. Our methodology employs an LLM to dynamically select augmentation transformations based on dataset characteristics, model architecture, and prior training outcomes. Unlike traditional search-based methods, our approach leverages the contextual knowledge of LLMs, particularly in specialized domains like medical imaging, to recommend augmentation strategies tailored to domain-specific data. We evaluate our approach on multiple domain-specific image classification datasets where augmentation is key to model robustness. Results show that LLM-guided augmentation optimization outperforms traditional methods, improving model accuracy. These findings highlight the potential of LLMs in automating and adapting deep learning training workflows.         ",
    "url": "https://arxiv.org/abs/2410.13453",
    "authors": [
      "Ant Duru",
      "Alptekin Temizel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2411.02770",
    "title": "A spectral mixture representation of isotropic kernels to generalize random Fourier features",
    "abstract": "           Rahimi and Recht (2007) introduced the idea of decomposing positive definite shift-invariant kernels by randomly sampling from their spectral distribution. This famous technique, known as Random Fourier Features (RFF), is in principle applicable to any such kernel whose spectral distribution can be identified and simulated. In practice, however, it is usually applied to the Gaussian kernel because of its simplicity, since its spectral distribution is also Gaussian. Clearly, simple spectral sampling formulas would be desirable for broader classes of kernels. In this paper, we show that the spectral distribution of positive definite isotropic kernels in $\\mathbb{R}^{d}$ for all $d\\geq1$ can be decomposed as a scale mixture of $\\alpha$-stable random vectors, and we identify the mixing distribution as a function of the kernel. This constructive decomposition provides a simple and ready-to-use spectral sampling formula for many multivariate positive definite shift-invariant kernels, including exponential power kernels, generalized Mat\u00e9rn kernels, generalized Cauchy kernels, as well as newly introduced kernels such as the Beta, Kummer, and Tricomi kernels. In particular, we retrieve the fact that the spectral distributions of these kernels are scale mixtures of the multivariate Gaussian distribution, along with an explicit mixing distribution formula. This result has broad applications for support vector machines, kernel ridge regression, Gaussian processes, and other kernel-based machine learning techniques for which the random Fourier features technique is applicable.         ",
    "url": "https://arxiv.org/abs/2411.02770",
    "authors": [
      "Nicolas Langren\u00e9",
      "Xavier Warin",
      "Pierre Gruet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.04794",
    "title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code",
    "abstract": "           Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual alignment. However, although LLMs show promising cross-lingual alignment in IE, a significant imbalance across languages persists, highlighting an underlying deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with advanced cross-lingual and multilingual capabilities for universal information extraction. Firstly, it standardizes the representation of multilingual schemas using Python classes, ensuring a consistent ontology across different languages. Then, IE across languages is formulated as a unified code generation task. Secondly, we enhance the model's cross-lingual transferability through IE cross-lingual alignment instruction tuning on a translated instance prediction task we proposed. During this phase, we also construct a high-quality and diverse bilingual IE parallel dataset with 257k samples, called ParallelNER, synthesized by our proposed robust three-stage pipeline, with manual annotation to ensure quality. Although without training in 29 unseen languages, KnowCoder-X surpasses ChatGPT by $30.17\\%$ and SoTA by $20.03\\%$, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 64 IE benchmarks in Chinese and English under various settings demonstrate that KnowCoder-X significantly enhances cross-lingual IE transfer through boosting the IE alignment. Our code and dataset are available at: this https URL ",
    "url": "https://arxiv.org/abs/2411.04794",
    "authors": [
      "Yuxin Zuo",
      "Wenxuan Jiang",
      "Wenxuan Liu",
      "Zixuan Li",
      "Long Bai",
      "Hanbin Wang",
      "Yutao Zeng",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.05659",
    "title": "Investigation of Holographic Beamforming via Dynamic Metasurface Antennas in QoS Guaranteed Power Efficient Networks",
    "abstract": "           This work focuses on designing a power-efficient network for Dynamic Metasurface Antennas (DMA)-aided multi-user multiple-input single-output (MISO) antenna systems. Power efficiency is achieved through holographic beamforming in a DMA-aided network, minimizing total transmission power while ensuring a guaranteed signal-to-noise-and-interference ratio (SINR) for multiple users in downlink. Unlike conventional MISO systems, which have well-explored beamforming solutions, DMA require specialized methods due to their unique physical constraints and wave-domain precoding capabilities. To achieve this, optimization algorithms relying on alternating optimization and semi-definite programming, are developed, including spherical-wave channel modelling of near-field communication. In this setup, the beamforming performance of DMA-aided precoding is analyzed in comparison to its optimal limits and traditional fully digital (FD) architectures, considering the effects of the Lorentzian constraints of metasurfaces and the degree of freedom (DoF) limitations due to a reduced number of RF chains. We demonstrate that the performance gap caused by DoF constraints becomes more significant as the number of users increases, highlighting the trade-offs of DMA in high-density wireless networks.         ",
    "url": "https://arxiv.org/abs/2411.05659",
    "authors": [
      "Askin Altinoklu",
      "Leila Musavian"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.10087",
    "title": "PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse",
    "abstract": "           Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar SSL method and a contrastive learning-based SSL method. Additionally, PFML is on par with the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse.         ",
    "url": "https://arxiv.org/abs/2411.10087",
    "authors": [
      "Einari Vaaras",
      "Manu Airaksinen",
      "Okko R\u00e4s\u00e4nen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2411.13951",
    "title": "PATH: A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series",
    "abstract": "           Benchmarking anomaly detection approaches for multivariate time series is a challenging task due to a lack of high-quality datasets. Current publicly available datasets are too small, not diverse and feature trivial anomalies, which hinders measurable progress in this research area. We propose a solution: a diverse, extensive, and non-trivial dataset generated via state-of-the-art simulation tools that reflects realistic behaviour of an automotive powertrain, including its multivariate, dynamic and variable-state properties. Additionally, our dataset represents a discrete-sequence problem, which remains unaddressed by previously-proposed solutions in literature. To cater for both unsupervised and semi-supervised anomaly detection settings, as well as time series generation and forecasting, we make different versions of the dataset available, where training and test subsets are offered in contaminated and clean versions, depending on the task. We also provide baseline results from a selection of approaches based on deterministic and variational autoencoders, as well as a non-parametric approach. As expected, the baseline experimentation shows that the approaches trained on the semi-supervised version of the dataset outperform their unsupervised counterparts, highlighting a need for approaches more robust to contaminated training data. Furthermore, results show that the threshold used can have a large influence on detection performance, hence more work needs to be invested in methods to find a suitable threshold without the need for labelled data.         ",
    "url": "https://arxiv.org/abs/2411.13951",
    "authors": [
      "Lucas Correia",
      "Jan-Christoph Goos",
      "Thomas B\u00e4ck",
      "Anna V. Kononova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2412.00114",
    "title": "SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments",
    "abstract": "           Large vision-language models (LVLMs) have shown remarkable capabilities in interpreting visual content. While existing works demonstrate these models' vulnerability to deliberately placed adversarial texts, such texts are often easily identifiable as anomalous. In this paper, we present the first approach to generate scene-coherent typographic adversarial attacks that mislead advanced LVLMs while maintaining visual naturalness through the capability of the LLM-based agent. Our approach addresses three critical questions: what adversarial text to generate, where to place it within the scene, and how to integrate it seamlessly. We propose a training-free, multi-modal LLM-driven scene-coherent typographic adversarial planning (SceneTAP) that employs a three-stage process: scene understanding, adversarial planning, and seamless integration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the scene, formulate effective adversarial text, strategically plan its placement, and provide detailed instructions for natural integration within the image. This is followed by a scene-coherent TextDiffuser that executes the attack using a local diffusion mechanism. We extend our method to real-world scenarios by printing and placing generated patches in physical environments, demonstrating its practical implications. Extensive experiments show that our scene-coherent adversarial text successfully misleads state-of-the-art LVLMs, including ChatGPT-4o, even after capturing new images of physical setups. Our evaluations demonstrate a significant increase in attack success rates while maintaining visual naturalness and contextual appropriateness. This work highlights vulnerabilities in current vision-language models to sophisticated, scene-coherent adversarial attacks and provides insights into potential defense mechanisms.         ",
    "url": "https://arxiv.org/abs/2412.00114",
    "authors": [
      "Yue Cao",
      "Yun Xing",
      "Jie Zhang",
      "Di Lin",
      "Tianwei Zhang",
      "Ivor Tsang",
      "Yang Liu",
      "Qing Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.08755",
    "title": "Proactive Adversarial Defense: Harnessing Prompt Tuning in Vision-Language Models to Detect Unseen Backdoored Images",
    "abstract": "           Backdoor attacks pose a critical threat by embedding hidden triggers into inputs, causing models to misclassify them into target labels. While extensive research has focused on mitigating these attacks in object recognition models through weight fine-tuning, much less attention has been given to detecting backdoored samples directly. Given the vast datasets used in training, manual inspection for backdoor triggers is impractical, and even state-of-the-art defense mechanisms fail to fully neutralize their impact. To address this gap, we introduce a groundbreaking method to detect unseen backdoored images during both training and inference. Leveraging the transformative success of prompt tuning in Vision Language Models (VLMs), our approach trains learnable text prompts to differentiate clean images from those with hidden backdoor triggers. Experiments demonstrate the exceptional efficacy of this method, achieving an impressive average accuracy of 86% across two renowned datasets for detecting unseen backdoor triggers, establishing a new standard in backdoor defense.         ",
    "url": "https://arxiv.org/abs/2412.08755",
    "authors": [
      "Kyle Stein",
      "Andrew Arash Mahyari",
      "Guillermo Francia",
      "Eman El-Sheikh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.09680",
    "title": "PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields",
    "abstract": "           We tackle the ill-posed inverse rendering problem in 3D reconstruction with a Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR) theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and 3D Gaussian Splatting approaches: they estimate view-dependent appearance without modeling scene materials and illumination. To address this limitation, we present an inverse rendering (IR) model capable of jointly estimating scene geometry, materials, and illumination. Our model builds upon recent NeRF-based IR approaches, but crucially introduces two novel physics-based priors that better constrain the IR estimation. Our priors are rigorously formulated as intuitive loss terms and achieve state-of-the-art material estimation without compromising novel view synthesis quality. Our method is easily adaptable to other inverse rendering and 3D reconstruction frameworks that require material estimation. We demonstrate the importance of extending current neural rendering approaches to fully model scene properties beyond geometry and view-dependent appearance. Code is publicly available at this https URL ",
    "url": "https://arxiv.org/abs/2412.09680",
    "authors": [
      "Sean Wu",
      "Shamik Basu",
      "Tim Broedermann",
      "Luc Van Gool",
      "Christos Sakaridis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.10778",
    "title": "Sample-efficient Unsupervised Policy Cloning from Ensemble Self-supervised Labeled Videos",
    "abstract": "           Current advanced policy learning methodologies have demonstrated the ability to develop expert-level strategies when provided enough information. However, their requirements, including task-specific rewards, action-labeled expert trajectories, and huge environmental interactions, can be expensive or even unavailable in many scenarios. In contrast, humans can efficiently acquire skills within a few trials and errors by imitating easily accessible internet videos, in the absence of any other supervision. In this paper, we try to let machines replicate this efficient watching-and-learning process through Unsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a novel framework to efficiently learn policies from action-free videos without rewards and any other expert supervision. UPESV trains a video labeling model to infer the expert actions in expert videos through several organically combined self-supervised tasks. Each task performs its duties, and they together enable the model to make full use of both action-free videos and reward-free interactions for robust dynamics understanding and advanced action prediction. Simultaneously, UPESV clones a policy from the labeled expert videos, in turn collecting environmental interactions for self-supervised tasks. After a sample-efficient, unsupervised, and iterative training process, UPESV obtains an advanced policy based on a robust video labeling model. Extensive experiments in sixteen challenging procedurally generated environments demonstrate that the proposed UPESV achieves state-of-the-art interaction-limited policy learning performance (outperforming five current advanced baselines on 12/16 tasks) without exposure to any other supervision except for videos.         ",
    "url": "https://arxiv.org/abs/2412.10778",
    "authors": [
      "Xin Liu",
      "Yaran Chen",
      "Haoran Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.11215",
    "title": "Neural Port-Hamiltonian Differential Algebraic Equations for Compositional Learning of Electrical Networks",
    "abstract": "           We develop compositional learning algorithms for coupled dynamical systems. While deep learning has proven effective at modeling complex relationships from data, compositional couplings between system components typically introduce algebraic constraints on state variables, posing challenges to many existing data-driven approaches to modeling dynamical systems. Towards developing deep learning models for constrained dynamical systems, we introduce neural port-Hamiltonian differential algebraic equations (N-PHDAEs), which use neural networks to parametrize unknown terms in both the differential and algebraic components of a port-Hamiltonian DAE. To train these models, we propose an algorithm that uses automatic differentiation to perform index reduction, automatically transforming the neural DAE into an equivalent system of neural ordinary differential equations (N-ODEs), for which established model inference and backpropagation methods exist. The proposed compositional modeling framework and learning algorithms may be applied broadly to learn control-oriented models of dynamical systems in a variety of application areas, however, in this work, we focus on their application to the modeling of electrical networks. Experiments simulating the dynamics of nonlinear circuits exemplify the benefits of our approach: the proposed N-PHDAE model achieves an order of magnitude improvement in prediction accuracy and constraint satisfaction when compared to a baseline N-ODE over long prediction time horizons. We also validate the compositional capabilities of our approach through experiments on a simulated D.C. microgrid: we train individual N-PHDAE models for separate grid components, before coupling them to accurately predict the behavior of larger-scale networks.         ",
    "url": "https://arxiv.org/abs/2412.11215",
    "authors": [
      "Cyrus Neary",
      "Nathan Tsao",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2412.11530",
    "title": "RoMeO: Robust Metric Visual Odometry",
    "abstract": "           Visual odometry (VO) aims to estimate camera poses from visual inputs -- a fundamental building block for many applications such as VR/AR and robotics. This work focuses on monocular RGB VO where the input is a monocular RGB video without IMU or 3D sensors. Existing approaches lack robustness under this challenging scenario and fail to generalize to unseen data (especially outdoors); they also cannot recover metric-scale poses. We propose Robust Metric Visual Odometry (RoMeO), a novel method that resolves these issues leveraging priors from pre-trained depth models. RoMeO incorporates both monocular metric depth and multi-view stereo (MVS) models to recover metric-scale, simplify correspondence search, provide better initialization and regularize optimization. Effective strategies are proposed to inject noise during training and adaptively filter noisy depth priors, which ensure the robustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the state-of-the-art (SOTA) by a large margin across 6 diverse datasets covering both indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO reduces the relative (align the trajectory scale with GT) and absolute trajectory errors both by >50%. The performance gain also transfers to the full SLAM pipeline (with global BA & loop closure). Code will be released upon acceptance.         ",
    "url": "https://arxiv.org/abs/2412.11530",
    "authors": [
      "Junda Cheng",
      "Zhipeng Cai",
      "Zhaoxing Zhang",
      "Wei Yin",
      "Matthias Muller",
      "Michael Paulitsch",
      "Xin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.03017",
    "title": "Convexity in ReLU Neural Networks: beyond ICNNs?",
    "abstract": "           Convex functions and their gradients play a critical role in mathematical imaging, from proximal optimization to Optimal Transport. The successes of deep learning has led many to use learning-based methods, where fixed functions or operators are replaced by learned neural networks. Regardless of their empirical superiority, establishing rigorous guarantees for these methods often requires to impose structural constraints on neural architectures, in particular convexity. The most popular way to do so is to use so-called Input Convex Neural Networks (ICNNs). In order to explore the expressivity of ICNNs, we provide necessary and sufficient conditions for a ReLU neural network to be convex. Such characterizations are based on product of weights and activations, and write nicely for any architecture in the path-lifting framework. As particular applications, we study our characterizations in depth for 1 and 2-hidden-layer neural networks: we show that every convex function implemented by a 1-hidden-layer ReLU network can be also expressed by an ICNN with the same architecture; however this property no longer holds with more layers. Finally, we provide a numerical procedure that allows an exact check of convexity for ReLU neural networks with a large number of affine regions.         ",
    "url": "https://arxiv.org/abs/2501.03017",
    "authors": [
      "Anne Gagneux",
      "Mathurin Massias",
      "Emmanuel Soubies",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.07766",
    "title": "Large Language Models for Knowledge Graph Embedding: A Survey",
    "abstract": "           Large language models (LLMs) have garnered significant attention for their superior performance in many knowledge-driven applications on the world wide this http URL models are designed to train hundreds of millions or more parameters on large amounts of text data, enabling them to understand and generate naturallanguage effectively. As the superior performance of LLMs becomes apparent,they are increasingly being applied to knowledge graph embedding (KGE) related tasks to improve the processing results. Traditional KGE representation learning methods map entities and relations into a low-dimensional vector space, enablingthe triples in the knowledge graph to satisfy a specific scoring function in thevector space. However, based on the powerful language understanding and seman-tic modeling capabilities of LLMs, that have recently been invoked to varying degrees in different types of KGE related scenarios such as multi-modal KGE andopen KGE according to their task characteristics. In this paper, we investigate awide range of approaches for performing LLMs-related tasks in different types of KGE scenarios. To better compare the various approaches, we summarize each KGE scenario in a classification. Finally, we discuss the applications in which the methods are mainly used and suggest several forward-looking directions for the development of this new research area.         ",
    "url": "https://arxiv.org/abs/2501.07766",
    "authors": [
      "Bingchen Liu",
      "Yuanyuan Fang",
      "Naixing Xu",
      "Shihao Hou",
      "Xin Li",
      "Qian Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2501.09446",
    "title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness",
    "abstract": "           This paper investigates the robustness of vision-language models against adversarial visual perturbations and introduces a novel ``double visual defense\" to enhance this robustness. Unlike previous approaches that resort to lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform large-scale adversarial vision-language pre-training from scratch using web-scale data. We then strengthen the defense by incorporating adversarial visual instruction tuning. The resulting models from each stage, $\\Delta$CLIP and $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a new state-of-the-art in adversarial defense for vision-language models. For example, the adversarial robustness of $\\Delta$CLIP surpasses that of the previous best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP surpasses the previous best models on ImageNet-1k by ~20% in terms of adversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA brings a ~30% robustness improvement to image captioning task and a ~20% robustness improvement to visual question answering task. Furthermore, our models exhibit stronger zero-shot recognition capability, fewer hallucinations, and superior reasoning performance compared to baselines. Our project page is this https URL.         ",
    "url": "https://arxiv.org/abs/2501.09446",
    "authors": [
      "Zeyu Wang",
      "Cihang Xie",
      "Brian Bartoldson",
      "Bhavya Kailkhura"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.17762",
    "title": "Improving Privacy Benefits of Redaction",
    "abstract": "           We propose a novel redaction methodology that can be used to sanitize natural text data. Our new technique provides better privacy benefits than other state of the art techniques while maintaining lower redaction levels.         ",
    "url": "https://arxiv.org/abs/2501.17762",
    "authors": [
      "Vaibhav Gusain",
      "Douglas Leith"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.17848",
    "title": "Improving Genetic Programming for Symbolic Regression with Equality Graphs",
    "abstract": "           The search for symbolic regression models with genetic programming (GP) has a tendency of revisiting expressions in their original or equivalent forms. Repeatedly evaluating equivalent expressions is inefficient, as it does not immediately lead to better solutions. However, evolutionary algorithms require diversity and should allow the accumulation of inactive building blocks that can play an important role at a later point. The equality graph is a data structure capable of compactly storing expressions and their equivalent forms allowing an efficient verification of whether an expression has been visited in any of their stored equivalent forms. We exploit the e-graph to adapt the subtree operators to reduce the chances of revisiting expressions. Our adaptation, called eggp, stores every visited expression in the e-graph, allowing us to filter out from the available selection of subtrees all the combinations that would create already visited expressions. Results show that, for small expressions, this approach improves the performance of a simple GP algorithm to compete with PySR and Operon without increasing computational cost. As a highlight, eggp was capable of reliably delivering short and at the same time accurate models for a selected set of benchmarks from SRBench and a set of real-world datasets.         ",
    "url": "https://arxiv.org/abs/2501.17848",
    "authors": [
      "Fabricio Olivetti de Franca",
      "Gabriel Kronberger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2501.18812",
    "title": "Estimating the Probability of Sampling a Trained Neural Network at Random",
    "abstract": "           We present and analyze an algorithm for estimating the size, under a Gaussian or uniform measure, of a localized neighborhood in neural network parameter space with behavior similar to an ``anchor'' point. We refer to this as the \"local volume\" of the anchor. We adapt an existing basin-volume estimator, which is very fast but in many cases only provides a lower bound. We show that this lower bound can be improved with an importance-sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of local volume can also be interpreted as a measure of the anchor network's information content. As expected for a measure of complexity, this quantity increases during language model training. We find that overfit, badly-generalizing neighborhoods are smaller, indicating a more complex learned behavior. This smaller volume can also be interpreted in an MDL sense as suboptimal compression. Our results are consistent with a picture of generalization we call the \"volume hypothesis\": that neural net training produces good generalization primarily because the architecture gives simple functions more volume in parameter space, and the optimizer samples from the low-loss manifold in a volume-sensitive way. We believe that fast local-volume estimators are a promising practical metric of network complexity and architectural inductive bias for interpretability purposes.         ",
    "url": "https://arxiv.org/abs/2501.18812",
    "authors": [
      "Adam Scherlis",
      "Nora Belrose"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.02514",
    "title": "Privacy Attacks on Image AutoRegressive Models",
    "abstract": "           Image autoregressive generation has emerged as a powerful new paradigm, with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns about their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to those of DMs as a reference point. Specifically, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images, with a True Positive Rate at False Positive Rate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using comparable attacks. We leverage our novel MIA to perform dataset inference (DI) for IARs and show that it requires as few as 6 samples to detect dataset membership, compared to 200 samples for DI in DMs. This confirms a higher level of information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. This trend suggests that incorporating techniques from DMs into IARs, such as modeling the per-token probability distribution using a diffusion procedure, could help mitigate IARs' vulnerability to privacy attacks. We make our code available at: this https URL ",
    "url": "https://arxiv.org/abs/2502.02514",
    "authors": [
      "Antoni Kowalczuk",
      "Jan Dubi\u0144ski",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.03251",
    "title": "RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry",
    "abstract": "           The foundation model has heralded a new era in artificial intelligence, pretraining a single model to offer cross-domain transferability on different datasets. Graph neural networks excel at learning graph data, the omnipresent non-Euclidean structure, but often lack the generalization capacity. Hence, graph foundation model is drawing increasing attention, and recent efforts have been made to leverage Large Language Models. On the one hand, existing studies primarily focus on text-attributed graphs, while a wider range of real graphs do not contain fruitful textual attributes. On the other hand, the sequential graph description tailored for the Large Language Model neglects the structural complexity, which is a predominant characteristic of the graph. Such limitations motivate an important question: Can we go beyond Large Language Models, and pretrain a universal model to learn the structural knowledge for any graph? The answer in the language or vision domain is a shared vocabulary. We observe the fact that there also exist shared substructures underlying graph domain, and thereby open a new opportunity of graph foundation model with structural vocabulary. The key innovation is the discovery of a simple yet effective structural vocabulary of trees and cycles, and we explore its inherent connection to Riemannian geometry. Herein, we present a universal pretraining model, RiemannGFM. Concretely, we first construct a novel product bundle to incorporate the diverse geometries of the vocabulary. Then, on this constructed space, we stack Riemannian layers where the structural vocabulary, regardless of specific graph, is learned in Riemannian manifold offering cross-domain transferability. Extensive experiments show the effectiveness of RiemannGFM on a diversity of real graphs.         ",
    "url": "https://arxiv.org/abs/2502.03251",
    "authors": [
      "Li Sun",
      "Zhenhao Huang",
      "Suyang Zhou",
      "Qiqi Wan",
      "Hao Peng",
      "Philip Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.04760",
    "title": "Graph Federated Learning Based Proactive Content Caching in Edge Computing",
    "abstract": "           With the rapid growth of mobile data traffic and the increasing prevalence of video streaming, proactive content caching in edge computing has become crucial for reducing latency and alleviating network congestion. However, traditional caching strategies such as FIFO, LRU, and LFU fail to effectively predict future content popularity, while existing proactive caching approaches often require users to upload data to a central server, raising concerns regarding privacy and scalability. To address these challenges, this paper proposes a Graph Federated Learning-based Proactive Content Caching (GFPCC) scheme that enhances caching efficiency while preserving user privacy. The proposed approach integrates federated learning and graph neural networks, enabling users to locally train Light Graph Convolutional Networks (LightGCN) to capture user-item relationships and predict content popularity. Instead of sharing raw data, only the trained model parameters are transmitted to the central server, where a federated averaging algorithm aggregates updates, refines the global model, and selects the most popular files for proactive caching. Experimental evaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC outperforms baseline caching algorithms by achieving higher cache efficiency through more accurate content popularity predictions. Moreover, the federated learning framework strengthens privacy protection while maintaining efficient model training; however, scalability remains a challenge in large-scale networks with dynamic user preferences.         ",
    "url": "https://arxiv.org/abs/2502.04760",
    "authors": [
      "Rui Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.07040",
    "title": "Robust high-order low-rank BUG integrators based on explicit Runge-Kutta methods",
    "abstract": "           In this work, we propose high-order basis-update & Galerkin (BUG) integrators based on explicit Runge-Kutta methods for large-scale matrix differential equations. These dynamical low-rank integrators are high-order extensions of the BUG integrator and are constructed by performing a BUG step at each stage of the Runge-Kutta method. In this way, the resulting Runge-Kutta BUG integrator is robust to the presence of small singular values and does not involve backward time-integration steps. We provide an error bound, which shows that the Runge-Kutta BUG integrator retains the order of convergence of the associated Runge-Kutta method until the error reaches a plateau corresponding to the low-rank truncation error and which vanishes as the rank becomes full. This error bound is finally validated experimentally on three numerical test cases. The results demonstrate the high-order convergence of the Runge-Kutta BUG integrator and its superior accuracy compared to other dynamical low-rank integrators proposed in the literature.         ",
    "url": "https://arxiv.org/abs/2502.07040",
    "authors": [
      "Fabio Nobile",
      "S\u00e9bastien Riffaud"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2502.12601",
    "title": "COPU: Conformal Prediction for Uncertainty Quantification in Natural Language Generation",
    "abstract": "           Uncertainty Quantification (UQ) for Natural Language Generation (NLG) is crucial for assessing the performance of Large Language Models (LLMs), as it reveals confidence in predictions, identifies failure modes, and gauges output reliability. Conformal Prediction (CP), a model-agnostic method that generates prediction sets with a specified error rate, has been adopted for UQ in classification tasks, where the size of the prediction set indicates the model's uncertainty. However, when adapting CP to NLG, the sampling-based method for generating candidate outputs cannot guarantee the inclusion of the ground truth, limiting its applicability across a wide range of error rates. To address this, we propose \\ourmethod, a method that explicitly adds the ground truth to the candidate outputs and uses logit scores to measure nonconformity. Our experiments with six LLMs on four NLG tasks show that \\ourmethod outperforms baseline methods in calibrating error rates and empirical cover rates, offering accurate UQ across a wide range of user-specified error rates.         ",
    "url": "https://arxiv.org/abs/2502.12601",
    "authors": [
      "Sean Wang",
      "Yicheng Jiang",
      "Yuxin Tang",
      "Lu Cheng",
      "Hanjie Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.13595",
    "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
    "abstract": "           Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.         ",
    "url": "https://arxiv.org/abs/2502.13595",
    "authors": [
      "Kenneth Enevoldsen",
      "Isaac Chung",
      "Imene Kerboua",
      "M\u00e1rton Kardos",
      "Ashwin Mathur",
      "David Stap",
      "Jay Gala",
      "Wissam Siblini",
      "Dominik Krzemi\u0144ski",
      "Genta Indra Winata",
      "Saba Sturua",
      "Saiteja Utpala",
      "Mathieu Ciancone",
      "Marion Schaeffer",
      "Gabriel Sequeira",
      "Diganta Misra",
      "Shreeya Dhakal",
      "Jonathan Rystr\u00f8m",
      "Roman Solomatin",
      "\u00d6mer \u00c7a\u011fatan",
      "Akash Kundu",
      "Martin Bernstorff",
      "Shitao Xiao",
      "Akshita Sukhlecha",
      "Bhavish Pahwa",
      "Rafa\u0142 Po\u015bwiata",
      "Kranthi Kiran GV",
      "Shawon Ashraf",
      "Daniel Auras",
      "Bj\u00f6rn Pl\u00fcster",
      "Jan Philipp Harries",
      "Lo\u00efc Magne",
      "Isabelle Mohr",
      "Mariya Hendriksen",
      "Dawei Zhu",
      "Hippolyte Gisserot-Boukhlef",
      "Tom Aarsen",
      "Jan Kostkan",
      "Konrad Wojtasik",
      "Taemin Lee",
      "Marek \u0160uppa",
      "Crystina Zhang",
      "Roberta Rocca",
      "Mohammed Hamdy",
      "Andrianos Michail",
      "John Yang",
      "Manuel Faysse",
      "Aleksei Vatolin",
      "Nandan Thakur",
      "Manan Dey",
      "Dipam Vasani",
      "Pranjal Chitale",
      "Simone Tedeschi",
      "Nguyen Tai",
      "Artem Snegirev",
      "Michael G\u00fcnther",
      "Mengzhou Xia",
      "Weijia Shi",
      "Xing Han L\u00f9",
      "Jordan Clive",
      "Gayatri Krishnakumar",
      "Anna Maksimova",
      "Silvan Wehrli",
      "Maria Tikhonova",
      "Henil Panchal",
      "Aleksandr Abramov",
      "Malte Ostendorff",
      "Zheng Liu",
      "Simon Clematide",
      "Lester James Miranda",
      "Alena Fenogenova",
      "Guangyu Song",
      "Ruqiya Bin Safi",
      "Wen-Ding Li",
      "Alessia Borghini",
      "Federico Cassano",
      "Hongjin Su",
      "Jimmy Lin",
      "Howard Yen",
      "Lasse Hansen",
      "Sara Hooker",
      "Chenghao Xiao",
      "Vaibhav Adlakha",
      "Orion Weller",
      "Siva Reddy",
      "Niklas Muennighoff"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.13622",
    "title": "REFIND at SemEval-2025 Task 3: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models",
    "abstract": "           Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.13622",
    "authors": [
      "DongGeon Lee",
      "Hwanjo Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14558",
    "title": "Model Inversion Attack against Federated Unlearning",
    "abstract": "           With the introduction of regulations related to the ``right to be forgotten\", federated learning (FL) is facing new privacy compliance challenges. To address these challenges, researchers have proposed federated unlearning (FU). However, existing FU research has primarily focused on improving the efficiency of unlearning, with less attention paid to the potential privacy vulnerabilities inherent in these methods. To address this gap, we draw inspiration from gradient inversion attacks in FL and propose the federated unlearning inversion attack (FUIA). The FUIA is specifically designed for the three types of FU (sample unlearning, client unlearning, and class unlearning), aiming to provide a comprehensive analysis of the privacy leakage risks associated with FU. In FUIA, the server acts as an honest-but-curious attacker, recording and exploiting the model differences before and after unlearning to expose the features and labels of forgotten data. FUIA significantly leaks the privacy of forgotten data and can target all types of FU. This attack contradicts the goal of FU to eliminate specific data influence, instead exploiting its vulnerabilities to recover forgotten data and expose its privacy flaws. Extensive experimental results show that FUIA can effectively reveal the private information of forgotten data. To mitigate this privacy leakage, we also explore two potential defense methods, although these come at the cost of reduced unlearning effectiveness and the usability of the unlearned model.         ",
    "url": "https://arxiv.org/abs/2502.14558",
    "authors": [
      "Lei Zhou",
      "Youwen Zhu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14926",
    "title": "DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for LoRaWAN-related engineering tasks",
    "abstract": "           This paper investigates the performance of 16 Large Language Models (LLMs) in automating LoRaWAN-related engineering tasks involving optimal placement of drones and received power calculation under progressively complex zero-shot, natural language prompts. The primary research question is whether lightweight, locally executed LLMs can generate correct Python code for these tasks. To assess this, we compared locally run models against state-of-the-art alternatives, such as GPT-4 and DeepSeek-V3, which served as reference points. By extracting and executing the Python functions generated by each model, we evaluated their outputs on a zero-to-five scale. Results show that while DeepSeek-V3 and GPT-4 consistently provided accurate solutions, certain smaller models -- particularly Phi-4 and LLaMA-3.3 -- also demonstrated strong performance, underscoring the viability of lightweight alternatives. Other models exhibited errors stemming from incomplete understanding or syntactic issues. These findings illustrate the potential of LLM-based approaches for specialized engineering applications while highlighting the need for careful model selection, rigorous prompt design, and targeted domain fine-tuning to achieve reliable outcomes.         ",
    "url": "https://arxiv.org/abs/2502.14926",
    "authors": [
      "Daniel Fernandes",
      "Jo\u00e3o P. Matos-Carvalho",
      "Carlos M. Fernandes",
      "Nuno Fachada"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2502.15641",
    "title": "A Deep Neural Network-based Frequency Predictor for Frequency-Constrained Optimal Power Flow",
    "abstract": "           Rate of change of frequency (RoCoF) and frequency nadir should be considered in real-time frequency-constrained optimal power flow (FCOPF) to ensure frequency stability of the modern power systems. Since calculating the frequency response is complex, deep neural network (DNN) could be adopted to capture the nonlinearities and estimate those two metrics accurately. Therefore, in this paper, a DNN-based frequency predictor is developed with the training data obtained from time-domain simulations using PSCAD/EMTDC. Subsequently, it is reformulated using a set of mixed-integer linear programming formulations and then embedded into the FCOPF framework as constraints to ensure grid frequency stability, creating the proposed DNN-FCOPF model. Two benchmark models, a traditional OPF without any frequency constraints and a linear system-wide RoCoF-constrained FCOPF, are also implemented to gauge the proposed DNN-FCOPF. Finally, the solutions obtained with these three models are compared and evaluated with time-domain simulations using PSCAD under various load profiles, demonstrating the effectiveness of the proposed DNN-FCOPF.         ",
    "url": "https://arxiv.org/abs/2502.15641",
    "authors": [
      "Fan Jiang",
      "Xingpeng Li",
      "Pascal Van Hentenryck"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2502.19679",
    "title": "Old Experience Helps: Leveraging Survey Methodology to Improve AI Text Annotation Reliability in Social Sciences",
    "abstract": "           This paper introduces a framework for assessing the reliability of Large Language Model (LLM) text annotations in social science research by adapting established survey methodology principles. Drawing parallels between survey respondent behavior and LLM outputs, the study implements three key interventions: option randomization, position randomization, and reverse validation. While traditional accuracy metrics may mask model instabilities, particularly in edge cases, the framework provides a more comprehensive reliability assessment. Using the F1000 dataset in biomedical science and three sizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates that these survey-inspired interventions can effectively identify unreliable annotations that might otherwise go undetected through accuracy metrics alone. The results show that 5-25% of LLM annotations change under these interventions, with larger models exhibiting greater stability. Notably, for rare categories approximately 50% of \"correct\" annotations demonstrate low reliability when subjected to this framework. The paper then introduce an information-theoretic reliability score (R-score) based on Kullback-Leibler divergence that quantifies annotation confidence and distinguishes between random guessing and meaningful annotations at the case level. This approach complements existing expert validation methods by providing a scalable way to assess internal annotation reliability and offers practical guidance for prompt design and downstream analysis.         ",
    "url": "https://arxiv.org/abs/2502.19679",
    "authors": [
      "Linzhuo li"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2503.22733",
    "title": "RBFleX-NAS: Training-Free Neural Architecture Search Using Radial Basis Function Kernel and Hyperparameter Detection",
    "abstract": "           Neural Architecture Search (NAS) is an automated technique to design optimal neural network architectures for a specific workload. Conventionally, evaluating candidate networks in NAS involves extensive training, which requires significant time and computational resources. To address this, training-free NAS has been proposed to expedite network evaluation with minimal search time. However, state-of-the-art training-free NAS algorithms struggle to precisely distinguish well-performing networks from poorly-performing networks, resulting in inaccurate performance predictions and consequently sub-optimal top-1 network accuracy. Moreover, they are less effective in activation function exploration. To tackle the challenges, this paper proposes RBFleX-NAS, a novel training-free NAS framework that accounts for both activation outputs and input features of the last layer with a Radial Basis Function (RBF) kernel. We also present a detection algorithm to identify optimal hyperparameters using the obtained activation outputs and input feature maps. We verify the efficacy of RBFleX-NAS over a variety of NAS benchmarks. RBFleX-NAS significantly outperforms state-of-the-art training-free NAS methods in terms of top-1 accuracy, achieving this with short search time in NAS-Bench-201 and NAS-Bench-SSS. In addition, it demonstrates higher Kendall correlation compared to layer-based training-free NAS algorithms. Furthermore, we propose NAFBee, a new activation design space that extends the activation type to encompass various commonly used functions. In this extended design space, RBFleX-NAS demonstrates its superiority by accurately identifying the best-performing network during activation function search, providing a significant advantage over other NAS algorithms.         ",
    "url": "https://arxiv.org/abs/2503.22733",
    "authors": [
      "Tomomasa Yamasaki",
      "Zhehui Wang",
      "Tao Luo",
      "Niangjun Chen",
      "Bo Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.00950",
    "title": "Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration",
    "abstract": "           Neural Radiance Fields (NeRF) have become a popular 3D reconstruction approach in recent years. While they produce high-quality results, they also demand lengthy training times, often spanning days. This paper studies neural pruning as a strategy to address these concerns. We compare pruning approaches, including uniform sampling, importance-based methods, and coreset-based techniques, to reduce the model size and speed up training. Our findings show that coreset-driven pruning can achieve a 50% reduction in model size and a 35% speedup in training, with only a slight decrease in accuracy. These results suggest that pruning can be an effective method for improving the efficiency of NeRF models in resource-limited settings.         ",
    "url": "https://arxiv.org/abs/2504.00950",
    "authors": [
      "Tianqi Ding",
      "Dawei Xiang",
      "Pablo Rivas",
      "Liang Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2504.02618",
    "title": "Variational Online Mirror Descent for Robust Learning in Schr\u00f6dinger Bridge",
    "abstract": "           Sch\u00f6dinger bridge (SB) has evolved into a universal class of probabilistic generative models. In practice, however, estimated learning signals are often uncertain, and the reliability promised by existing methods is often based on speculative optimal-case scenarios. Recent studies regarding the Sinkhorn algorithm through mirror descent (MD) have gained attention, revealing geometric insights into solution acquisition of the SB problems. In this paper, we propose a variational online MD (OMD) framework for the SB problems, which provides further stability to SB solvers. We formally prove convergence and a regret bound for the novel OMD formulation of SB acquisition. As a result, we propose a simulation-free SB algorithm called Variational Mirrored Schr\u00f6dinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of the Gaussian mixture parameterization for Schr\u00f6dinger potentials. Based on the Wasserstein gradient flow theory, the algorithm offers tractable learning dynamics that precisely approximate each OMD step. In experiments, we validate the performance of the proposed VMSB algorithm across an extensive suite of benchmarks. VMSB consistently outperforms contemporary SB solvers on a range of SB problems, demonstrating the robustness predicted by our theory.         ",
    "url": "https://arxiv.org/abs/2504.02618",
    "authors": [
      "Dong-Sig Han",
      "Jaein Kim",
      "Hee Bin Yoo",
      "Byoung-Tak Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2504.04325",
    "title": "Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)",
    "abstract": "           Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called false positives in Colombia, represents one of the most harrowing episodes of the Colombian armed conflict. This article proposes an innovative methodology based on natural language analysis and semantic co-occurrence models to explore, systematize, and visualize narrative patterns present in the public hearings of victims and appearing parties. By constructing skipgram networks and analyzing their modularity, the study identifies thematic clusters that reveal regional and procedural status differences, providing empirical evidence on dynamics of victimization, responsibility, and acknowledgment in this case. This computational approach contributes to the collective construction of both judicial and extrajudicial truth, offering replicable tools for other transitional justice cases. The work is grounded in the pillars of truth, justice, reparation, and non-repetition, proposing a critical and in-depth reading of contested memories.         ",
    "url": "https://arxiv.org/abs/2504.04325",
    "authors": [
      "Juan Sosa",
      "Alejandro Urrego-L\u00f3pez",
      "Cesar Prieto",
      "Emma J. Camargo-D\u00edaz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2504.04346",
    "title": "Crowdsourcing-Based Knowledge Graph Construction for Drug Side Effects Using Large Language Models with an Application on Semaglutide",
    "abstract": "           Social media is a rich source of real-world data that captures valuable patient experience information for pharmacovigilance. However, mining data from unstructured and noisy social media content remains a challenging task. We present a systematic framework that leverages large language models (LLMs) to extract medication side effects from social media and organize them into a knowledge graph (KG). We apply this framework to semaglutide for weight loss using data from Reddit. Using the constructed knowledge graph, we perform comprehensive analyses to investigate reported side effects across different semaglutide brands over time. These findings are further validated through comparison with adverse events reported in the FAERS database, providing important patient-centered insights into semaglutide's side effects that complement its safety profile and current knowledge base of semaglutide for both healthcare professionals and patients. Our work demonstrates the feasibility of using LLMs to transform social media data into structured KGs for pharmacovigilance.         ",
    "url": "https://arxiv.org/abs/2504.04346",
    "authors": [
      "Zhijie Duan",
      "Kai Wei",
      "Zhaoqian Xue",
      "Jiayan Zhou",
      "Shu Yang",
      "Siyuan Ma",
      "Jin Jin",
      "Lingyao li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.04650",
    "title": "Autono: A ReAct-Based Highly Robust Autonomous Agent Framework",
    "abstract": "           This paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve complex tasks through adaptive decision making and multi-agent collaboration. Unlike traditional frameworks that rely on fixed workflows generated by LLM-based planners, this framework dynamically generates next actions during agent execution based on prior trajectories, thereby enhancing its robustness. To address potential termination issues caused by adaptive execution paths, I propose a timely abandonment strategy incorporating a probabilistic penalty mechanism. For multi-agent collaboration, I introduce a memory transfer mechanism that enables shared and dynamically updated memory among agents. The framework's innovative timely abandonment strategy dynamically adjusts the probability of task abandonment via probabilistic penalties, allowing developers to balance conservative and exploratory tendencies in agent execution strategies by tuning hyperparameters. This significantly improves adaptability and task execution efficiency in complex environments. Additionally, agents can be extended through external tool integration, supported by modular design and MCP protocol compatibility, which enables flexible action space expansion. Through explicit division of labor, the multi-agent collaboration mechanism enables agents to focus on specific task components, thereby significantly improving execution efficiency and quality.         ",
    "url": "https://arxiv.org/abs/2504.04650",
    "authors": [
      "Zihao Wu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2504.04798",
    "title": "TabRep: a Simple and Effective Continuous Representation for Training Tabular Diffusion Models",
    "abstract": "           Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient.         ",
    "url": "https://arxiv.org/abs/2504.04798",
    "authors": [
      "Jacob Si",
      "Zijing Ou",
      "Mike Qu",
      "Zhengrui Xiang",
      "Yingzhen Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05045",
    "title": "Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation",
    "abstract": "           Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency.         ",
    "url": "https://arxiv.org/abs/2504.05045",
    "authors": [
      "Huilin Yin",
      "Zhikun Yang",
      "Daniel Watzenig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2504.05138",
    "title": "Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning",
    "abstract": "           Federated learning (FL) allows edge devices to collaboratively train models without sharing local data. As FL gains popularity, clients may need to train multiple unrelated FL models, but communication constraints limit their ability to train all models simultaneously. While clients could train FL models sequentially, opportunistically having FL clients concurrently train different models -- termed multi-model federated learning (MMFL) -- can reduce the overall training time. Prior work uses simple client-to-model assignments that do not optimize the contribution of each client to each model over the course of its training. Prior work on single-model FL shows that intelligent client selection can greatly accelerate convergence, but na\u00efve extensions to MMFL can violate heterogeneous resource constraints at both the server and the clients. In this work, we develop a novel convergence analysis of MMFL with arbitrary client sampling methods, theoretically demonstrating the strengths and limitations of previous well-established gradient-based methods. Motivated by this analysis, we propose MMFL-LVR, a loss-based sampling method that minimizes training variance while explicitly respecting communication limits at the server and reducing computational costs at the clients. We extend this to MMFL-StaleVR, which incorporates stale updates for improved efficiency and stability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead deployment. Experiments show our methods improve average accuracy by up to 19.1% over random sampling, with only a 5.4% gap from the theoretical optimum (full client participation).         ",
    "url": "https://arxiv.org/abs/2504.05138",
    "authors": [
      "Haoran Zhang",
      "Zejun Gong",
      "Zekai Li",
      "Marie Siew",
      "Carlee Joe-Wong",
      "Rachid El-Azouzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2504.05250",
    "title": "PEAKS: Selecting Key Training Examples Incrementally via Prediction Error Anchored by Kernel Similarity",
    "abstract": "           As deep learning continues to be driven by ever-larger datasets, understanding which examples are most important for generalization has become a critical question. While progress in data selection continues, emerging applications require studying this problem in dynamic contexts. To bridge this gap, we pose the Incremental Data Selection (IDS) problem, where examples arrive as a continuous stream, and need to be selected without access to the full data source. In this setting, the learner must incrementally build a training dataset of predefined size while simultaneously learning the underlying task. We find that in IDS, the impact of a new sample on the model state depends fundamentally on both its geometric relationship in the feature space and its prediction error. Leveraging this insight, we propose PEAKS (Prediction Error Anchored by Kernel Similarity), an efficient data selection method tailored for IDS. Our comprehensive evaluations demonstrate that PEAKS consistently outperforms existing selection strategies. Furthermore, PEAKS yields increasingly better performance returns than random selection as training data size grows on real-world datasets.         ",
    "url": "https://arxiv.org/abs/2504.05250",
    "authors": [
      "Mustafa Burak Gurbuz",
      "Xingyu Zheng",
      "Constantine Dovrolis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.04290",
    "title": "Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale Transformer Approach",
    "abstract": "           Pneumonia, a prevalent respiratory infection, remains a leading cause of morbidity and mortality worldwide, particularly among vulnerable populations. Chest X-rays serve as a primary tool for pneumonia detection; however, variations in imaging conditions and subtle visual indicators complicate consistent interpretation. Automated tools can enhance traditional methods by improving diagnostic reliability and supporting clinical decision-making. In this study, we propose a novel multi-scale transformer approach for pneumonia detection that integrates lung segmentation and classification into a unified framework. Our method introduces a lightweight transformer-enhanced TransUNet for precise lung segmentation, achieving a Dice score of 95.68% on the \"Chest X-ray Masks and Labels\" dataset with fewer parameters than traditional transformers. For classification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101) to extract multi-scale feature maps, which are then processed through a modified transformer module to enhance pneumonia detection. This integration of multi-scale feature extraction and lightweight transformer modules ensures robust performance, making our method suitable for resource-constrained clinical environments. Our approach achieves 93.75% accuracy on the \"Kermany\" dataset and 96.04% accuracy on the \"Cohen\" dataset, outperforming existing methods while maintaining computational efficiency. This work demonstrates the potential of multi-scale transformer architectures to improve pneumonia diagnosis, offering a scalable and accurate solution to global healthcare challenges.\"this https URL ",
    "url": "https://arxiv.org/abs/2408.04290",
    "authors": [
      "Alireza Saber",
      "Pouria Parhami",
      "Alimohammad Siahkarzadeh",
      "Mansoor Fateh",
      "Amirreza Fateh"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.05454",
    "title": "Meta-Dynamical State Space Models for Integrative Neural Data Analysis",
    "abstract": "           Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings. Existing approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family of related solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach on few-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks.         ",
    "url": "https://arxiv.org/abs/2410.05454",
    "authors": [
      "Ayesha Vermani",
      "Josue Nassar",
      "Hyungju Jeon",
      "Matthew Dowling",
      "Il Memming Park"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2503.22896",
    "title": "Representation and Stability Analysis of 1D PDEs with Periodic Boundary Conditions",
    "abstract": "           PDEs with periodic boundary conditions are frequently used to model processes in large spatial environments, assuming solutions to extend periodically beyond some bounded interval. However, solutions to these PDEs often do not converge to a unique equilibrium, but instead converge to non-stationary trajectories existing in the nullspace of the spatial differential operator (e.g. $\\frac{\\partial^2}{\\partial x^2}$). To analyse this convergence behaviour, in this paper, it is shown how such trajectories can be modeled for a broad class of linear, 2nd order, 1D PDEs with periodic as well as more general boundary conditions, using the Partial Integral Equation (PIE) representation. In particular, it is first shown how any PDE state satisfying these boundary conditions can be uniquely expressed in terms of two components, existing in the image and the nullspace of the differential operator $\\frac{\\partial^2}{\\partial x^2}$, respectively. An equivalent representation of linear PDEs is then derived as a PIE, explicitly defining the dynamics of both state components. Finally, a notion of exponential stability is defined that requires only one of the state components to converge to zero, and it is shown how this stability notion can be tested by solving a linear operator inequality. The proposed methodology is applied to two examples, demonstrating that exponential stability can be verified with tight bounds on the rate of decay.         ",
    "url": "https://arxiv.org/abs/2503.22896",
    "authors": [
      "Declan Jagt",
      "Sergei Chernyshenko",
      "Matthew Peet"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  }
]