[
  {
    "id": "arXiv:2107.09101",
    "title": "Accelerating deep neural networks for efficient scene understanding in  automotive cyber-physical systems",
    "abstract": "Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount of interest in the past few decades, while one of the most critical operations in these systems is the perception of the environment. Deep learning and, especially, the use of Deep Neural Networks (DNNs) provides impressive results in analyzing and understanding complex and dynamic scenes from visual data. The prediction horizons for those perception systems are very short and inference must often be performed in real time, stressing the need of transforming the original large pre-trained networks into new smaller models, by utilizing Model Compression and Acceleration (MCA) techniques. Our goal in this work is to investigate best practices for appropriately applying novel weight sharing techniques, optimizing the available variables and the training procedures towards the significant acceleration of widely adopted DNNs. Extensive evaluation studies carried out using various state-of-the-art DNN models in object detection and tracking experiments, provide details about the type of errors that manifest after the application of weight sharing techniques, resulting in significant acceleration gains with negligible accuracy losses. ",
    "url": "https://arxiv.org/abs/2107.09101",
    "authors": [
      "Stavros Nousias",
      "Erion-Vasilis Pikoulis",
      "Christos Mavrokefalidis",
      "Aris S. Lalos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.09333",
    "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical  report)",
    "abstract": "To increase performance and efficiency, systems use FPGAs as reconfigurable accelerators. A key challenge in designing these systems is partitioning computation between processors and an FPGA. An appropriate division of labor may be difficult to predict in advance and require experiments and measurements. When an investigation requires rewriting part of the system in a new language or with a new programming model, its high cost can retard the study of different configurations. A single-language system with an appropriate programming model and compiler that targets both platforms simplifies this exploration to a simple recompile with new compiler directives. This work introduces StreamBlocks, an open-source compiler and runtime that uses the CAL dataflow programming language to partition computations across heterogeneous (CPU/accelerator) platforms. Because of the dataflow model's semantics and the CAL language, StreamBlocks can exploit both thread parallelism in multi-core CPUs and the inherent parallelism of FPGAs. StreamBlocks supports exploring the design space with a profile-guided tool that helps identify the best hardware-software partitions. ",
    "url": "https://arxiv.org/abs/2107.09333",
    "authors": [
      "Endri Bezati",
      "Mahyar Emami",
      "J\u00f6rn Janneck",
      "James Larus"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Computation and Language (cs.CL)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2107.09422",
    "title": "Large-scale graph representation learning with very deep GNNs and  self-supervision",
    "abstract": "Effectively and efficiently deploying graph neural networks (GNNs) at scale remains one of the most challenging aspects of graph representation learning. Many powerful solutions have only ever been validated on comparatively small datasets, often with counter-intuitive outcomes -- a barrier which has been broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered the OGB-LSC with two large-scale GNNs: a deep transductive node classifier powered by bootstrapping, and a very deep (up to 50-layer) inductive graph regressor regularised by denoising objectives. Our models achieved an award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In doing so, we demonstrate evidence of scalable self-supervised graph representation learning, and utility of very deep GNNs -- both very important open issues. Our code is publicly available at: https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc. ",
    "url": "https://arxiv.org/abs/2107.09422",
    "authors": [
      "Ravichandra Addanki",
      "Peter W. Battaglia",
      "David Budden",
      "Andreea Deac",
      "Jonathan Godwin",
      "Thomas Keck",
      "Wai Lok Sibon Li",
      "Alvaro Sanchez-Gonzalez",
      "Jacklynn Stott",
      "Shantanu Thakoor",
      "Petar Veli\u010dkovi\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2107.09437",
    "title": "Edge of chaos as a guiding principle for modern neural network training",
    "abstract": "The success of deep neural networks in real-world problems has prompted many attempts to explain their training dynamics and generalization performance, but more guiding principles for the training of neural networks are still needed. Motivated by the edge of chaos principle behind the optimal performance of neural networks, we study the role of various hyperparameters in modern neural network training algorithms in terms of the order-chaos phase diagram. In particular, we study a fully analytical feedforward neural network trained on the widely adopted Fashion-MNIST dataset, and study the dynamics associated with the hyperparameters in back-propagation during the training process. We find that for the basic algorithm of stochastic gradient descent with momentum, in the range around the commonly used hyperparameter values, clear scaling relations are present with respect to the training time during the ordered phase in the phase diagram, and the model's optimal generalization power at the edge of chaos is similar across different training parameter combinations. In the chaotic phase, the same scaling no longer exists. The scaling allows us to choose the training parameters to achieve faster training without sacrificing performance. In addition, we find that the commonly used model regularization method - weight decay - effectively pushes the model towards the ordered phase to achieve better performance. Leveraging on this fact and the scaling relations in the other hyperparameters, we derived a principled guideline for hyperparameter determination, such that the model can achieve optimal performance by saturating it at the edge of chaos. Demonstrated on this simple neural network model and training algorithm, our work improves the understanding of neural network training dynamics, and can potentially be extended to guiding principles of more complex model architectures and algorithms. ",
    "url": "https://arxiv.org/abs/2107.09437",
    "authors": [
      "Lin Zhang",
      "Ling Feng",
      "Kan Chen",
      "Choy Heng Lai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Chaotic Dynamics (nlin.CD)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2107.09628",
    "title": "Saliency for free: Saliency prediction as a side-effect of object  recognition",
    "abstract": "Saliency is the perceptual capacity of our visual system to focus our attention (i.e. gaze) on relevant objects. Neural networks for saliency estimation require ground truth saliency maps for training which are usually achieved via eyetracking experiments. In the current paper, we demonstrate that saliency maps can be generated as a side-effect of training an object recognition deep neural network that is endowed with a saliency branch. Such a network does not require any ground-truth saliency maps for training.Extensive experiments carried out on both real and synthetic saliency datasets demonstrate that our approach is able to generate accurate saliency maps, achieving competitive results on both synthetic and real datasets when compared to methods that do require ground truth data. ",
    "url": "https://arxiv.org/abs/2107.09628",
    "authors": [
      "Carola Figueroa-Flores",
      "David Berga",
      "Joost van der Weijer",
      "Bogdan Raducanu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.09055",
    "title": "Stock price prediction using BERT and GAN",
    "abstract": "The stock market has been a popular topic of interest in the recent past. The growth in the inflation rate has compelled people to invest in the stock and commodity markets and other areas rather than saving. Further, the ability of Deep Learning models to make predictions on the time series data has been proven time and again. Technical analysis on the stock market with the help of technical indicators has been the most common practice among traders and investors. One more aspect is the sentiment analysis - the emotion of the investors that shows the willingness to invest. A variety of techniques have been used by people around the globe involving basic Machine Learning and Neural Networks. Ranging from the basic linear regression to the advanced neural networks people have experimented with all possible techniques to predict the stock market. It's evident from recent events how news and headlines affect the stock markets and cryptocurrencies. This paper proposes an ensemble of state-of-the-art methods for predicting stock prices. Firstly sentiment analysis of the news and the headlines for the company Apple Inc, listed on the NASDAQ is performed using a version of BERT, which is a pre-trained transformer model by Google for Natural Language Processing (NLP). Afterward, a Generative Adversarial Network (GAN) predicts the stock price for Apple Inc using the technical indicators, stock indexes of various countries, some commodities, and historical prices along with the sentiment scores. Comparison is done with baseline models like - Long Short Term Memory (LSTM), Gated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving Average (ARIMA) model. ",
    "url": "https://arxiv.org/abs/2107.09055",
    "authors": [
      "Priyank Sonkiya",
      "Vikas Bajpai",
      "Anukriti Bansal"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.09070",
    "title": "Dim but not entirely dark: Extracting the Galactic Center Excess'  source-count distribution with neural nets",
    "abstract": "The two leading hypotheses for the Galactic Center Excess (GCE) in the $\\textit{Fermi}$ data are an unresolved population of faint millisecond pulsars (MSPs) and dark-matter (DM) annihilation. The dichotomy between these explanations is typically reflected by modeling them as two separate emission components. However, point-sources (PSs) such as MSPs become statistically degenerate with smooth Poisson emission in the ultra-faint limit (formally where each source is expected to contribute much less than one photon on average), leading to an ambiguity that can render questions such as whether the emission is PS-like or Poissonian in nature ill-defined. We present a conceptually new approach that describes the PS and Poisson emission in a unified manner and only afterwards derives constraints on the Poissonian component from the so obtained results. For the implementation of this approach, we leverage deep learning techniques, centered around a neural network-based method for histogram regression that expresses uncertainties in terms of quantiles. We demonstrate that our method is robust against a number of systematics that have plagued previous approaches, in particular DM / PS misattribution. In the $\\textit{Fermi}$ data, we find a faint GCE described by a median source-count distribution (SCD) peaked at a flux of $\\sim4 \\times 10^{-11} \\ \\text{counts} \\ \\text{cm}^{-2} \\ \\text{s}^{-1}$ (corresponding to $\\sim3 - 4$ expected counts per PS), which would require $N \\sim \\mathcal{O}(10^4)$ sources to explain the entire excess (median value $N = \\text{29,300}$ across the sky). Although faint, this SCD allows us to derive the constraint $\\eta_P \\leq 66\\%$ for the Poissonian fraction of the GCE flux $\\eta_P$ at 95% confidence, suggesting that a substantial amount of the GCE flux is due to PSs. ",
    "url": "https://arxiv.org/abs/2107.09070",
    "authors": [
      "Florian List",
      "Nicholas L. Rodd",
      "Geraint F. Lewis"
    ],
    "subjectives": [
      "High Energy Astrophysical Phenomena (astro-ph.HE)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ]
  },
  {
    "id": "arXiv:2107.09145",
    "title": "Adaptive wavelet distillation from neural networks through  interpretations",
    "abstract": "Recent deep-learning models have achieved impressive prediction performance, but often sacrifice interpretability and computational efficiency. Interpretability is crucial in many disciplines, such as science and medicine, where models must be carefully vetted or where interpretation is the goal itself. Moreover, interpretable models are concise and often yield computational efficiency. Here, we propose adaptive wavelet distillation (AWD), a method which aims to distill information from a trained neural network into a wavelet transform. Specifically, AWD penalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predictive, concise, computationally efficient, and has properties (such as a multi-scale structure) which make it easy to interpret. In close collaboration with domain experts, we showcase how AWD addresses challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases, AWD yields a scientifically interpretable and concise model which gives predictive performance better than state-of-the-art neural networks. Moreover, AWD identifies predictive features that are scientifically meaningful in the context of respective domains. All code and models are released in a full-fledged package available on Github (https://github.com/Yu-Group/adaptive-wavelets). ",
    "url": "https://arxiv.org/abs/2107.09145",
    "authors": [
      "Wooseok Ha",
      "Chandan Singh",
      "Francois Lanusse",
      "Eli Song",
      "Song Dang",
      "Kangmin He",
      "Srigokul Upadhyayula",
      "Bin Yu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.09200",
    "title": "A quantum algorithm for training wide and deep classical neural networks",
    "abstract": "Given the success of deep learning in classical machine learning, quantum algorithms for traditional neural network architectures may provide one of the most promising settings for quantum machine learning. Considering a fully-connected feedforward neural network, we show that conditions amenable to classical trainability via gradient descent coincide with those necessary for efficiently solving quantum linear systems. We propose a quantum algorithm to approximately train a wide and deep neural network up to $O(1/n)$ error for a training set of size $n$ by performing sparse matrix inversion in $O(\\log n)$ time. To achieve an end-to-end exponential speedup over gradient descent, the data distribution must permit efficient state preparation and readout. We numerically demonstrate that the MNIST image dataset satisfies such conditions; moreover, the quantum algorithm matches the accuracy of the fully-connected network. Beyond the proven architecture, we provide empirical evidence for $O(\\log n)$ training of a convolutional neural network with pooling. ",
    "url": "https://arxiv.org/abs/2107.09200",
    "authors": [
      "Alexander Zlokapa",
      "Hartmut Neven",
      "Seth Lloyd"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.09360",
    "title": "A biophysical network model reveals the link between deficient  inhibitory cognitive control and major neurotransmitter and neural  connectivity hypotheses in schizophrenia",
    "abstract": "We address a biophysical network dynamical model to study how the modulation of dopamine (DA) activity and related N-methyl-d-aspartate (NMDA) glutamate receptor activity as well as the emerging Pre-Frontal Cortex (PFC) functional connectivity network (FCN) affect inhibitory cognitive function in schizophrenia in an antisaccade task. The values of the model parameters and the topology of the PFC-FCN were estimated by minimizing the differences between simulations and the observed distributions of reaction times (RT) during the performance of the antisaccade task in 30 patients with schizophrenia and 30 healthy controls. We show that the proposed model approximates remarkably well the predicted prefrontal cortical DA hypo-activity and the related NMDA receptor hypo-function as well as the FCN dysconnection pattern that are considered as the major etio-pathological hypotheses to explain cognitive deficits in schizophrenia. ",
    "url": "https://arxiv.org/abs/2107.09360",
    "authors": [
      "Konstantinos Spiliotis",
      "Giannis Kahramanoglou",
      "Jens Starke",
      "Nikolaos Smyrnis",
      "Constantinos Siettos"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2107.09487",
    "title": "On node ranking in graphs",
    "abstract": "The ranking of nodes in a network according to their ``importance'' is a classic problem that has attracted the interest of different scientific communities in the last decades. The current COVID-19 pandemic has recently rejuvenated the interest in this problem, as it is related to the selection of which individuals should be tested in a population of asymptomatic individuals, or which individuals should be vaccinated first. Motivated by the COVID-19 spreading dynamics, in this paper we review the most popular methods for node ranking in undirected unweighted graphs, and compare their performance in a benchmark realistic network, that takes into account the community-based structure of society. Also, we generalize a classic benchmark network originally proposed by Newman for ranking nodes in unweighted graphs, to show how ranks change in the weighted case. ",
    "url": "https://arxiv.org/abs/2107.09487",
    "authors": [
      "Ekaterina Dudkina",
      "Michelangelo Bin",
      "Jane Breen",
      "Emanuele Crisostomi",
      "Pietro Ferraro",
      "Steve Kirkland",
      "Jakub Marecek",
      "Roderick Murray-Smith",
      "Thomas Parisini",
      "Lewi Stone",
      "Serife Yilmaz",
      "Robert Shorten"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2107.09591",
    "title": "Hybrid neural network reduced order modelling for turbulent flows with  geometric parameters",
    "abstract": "Geometrically parametrized Partial Differential Equations are nowadays widely used in many different fields as, for example, shape optimization processes or patient specific surgery studies. The focus of this work is on some advances for this topic, capable of increasing the accuracy with respect to previous approaches while relying on a high cost-benefit ratio performance. The main scope of this paper is the introduction of a new technique mixing up a classical Galerkin-projection approach together with a data-driven method to obtain a versatile and accurate algorithm for the resolution of geometrically parametrized incompressible turbulent Navier-Stokes problems. The effectiveness of this procedure is demonstrated on two different test cases: a classical academic back step problem and a shape deformation Ahmed body application. The results show into details the properties of the architecture we developed while exposing possible future perspectives for this work. ",
    "url": "https://arxiv.org/abs/2107.09591",
    "authors": [
      "Matteo Zancanaro",
      "Markus Mrosek",
      "Giovanni Stabile",
      "Carsten Othmer",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2102.11289",
    "title": "Ps and Qs: Quantization-aware pruning for efficient low latency neural  network inference",
    "abstract": " Comments: 22 pages, 7 Figures, 1 Table ",
    "url": "https://arxiv.org/abs/2102.11289",
    "authors": [
      "Benjamin Hawks",
      "Javier Duarte",
      "Nicholas J. Fraser",
      "Alessandro Pappalardo",
      "Nhan Tran",
      "Yaman Umuroglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Instrumentation and Detectors (physics.ins-det)"
    ]
  },
  {
    "id": "arXiv:2103.00222",
    "title": "Variational Laplace for Bayesian neural networks",
    "abstract": " Comments: Accidental resubmission of new version of arXiv:2011.10443 ",
    "url": "https://arxiv.org/abs/2103.00222",
    "authors": [
      "Ali Unlu",
      "Laurence Aitchison"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2103.01124",
    "title": "Automated data-driven approach for gap filling in the time series using  evolutionary learning",
    "abstract": " Comments: The approach to experiments has been revised from the last text version. Additional experiments were performed on new time series ",
    "url": "https://arxiv.org/abs/2103.01124",
    "authors": [
      "Mikhail Sarafanov",
      "Nikolay O. Nikitin",
      "Anna V. Kalyuzhnaya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2103.08587",
    "title": "Network community structure and resilience to localized damage:  application to brain microcirculation",
    "abstract": " Comments: 14 pages, 11 figures ",
    "url": "https://arxiv.org/abs/2103.08587",
    "authors": [
      "Florian Goirand",
      "Bertrand Georgeot",
      "Olivier Giraud",
      "Sylvie Lorthois"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Social and Information Networks (cs.SI)",
      "Neurons and Cognition (q-bio.NC)",
      "Tissues and Organs (q-bio.TO)"
    ]
  },
  {
    "id": "arXiv:2107.08517",
    "title": "Decentralized federated learning of deep neural networks on non-iid data",
    "abstract": " Comments: 7 pages, 2 figures ",
    "url": "https://arxiv.org/abs/2107.08517",
    "authors": [
      "Noa Onoszko",
      "Gustav Karlsson",
      "Olof Mogren",
      "Edvin Listo Zec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  }
]