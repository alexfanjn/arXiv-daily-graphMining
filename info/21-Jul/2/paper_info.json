[
  {
    "id": "arXiv:2107.00090",
    "title": "Mesh-based graph convolutional neural network models of processes with  complex initial states",
    "abstract": "Predicting the evolution of a representative sample of a material with microstructure is a fundamental problem in homogenization. In this work we propose a graph convolutional neural network that utilizes the discretized representation of the initial microstructure directly, without segmentation or clustering. Compared to feature-based and pixel-based convolutional neural network models, the proposed method has a number of advantages: (a) it is deep in that it does not require featurization but can benefit from it, (b) it has a simple implementation with standard convolutional filters and layers, (c) it works natively on unstructured and structured grid data without interpolation (unlike pixel-based convolutional neural networks), and (d) it preserves rotational invariance like other graph-based convolutional neural networks. We demonstrate the performance of the proposed network and compare it to traditional pixel-based convolution neural network models and feature-based graph convolutional neural networks on three large datasets. ",
    "url": "https://arxiv.org/abs/2107.00090",
    "authors": [
      "Ari Frankel",
      "Cosmin Safta",
      "Coleman Alleman",
      "Reese Jones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.00309",
    "title": "Spotting adversarial samples for speaker verification by neural vocoders",
    "abstract": "Automatic speaker verification (ASV), one of the most important technology for biometric identification, has been widely adopted in security-critic applications, including transaction authentication and access control. However, previous works have shown ASV is seriously vulnerable to recently emerged adversarial attacks, yet effective countermeasures against them are limited. In this paper, we adopt neural vocoders to spot adversarial samples for ASV. We use neural vocoder to re-synthesize audio and find that the difference between the ASV scores for the original and re-synthesized audio is a good indicator to distinguish genuine and adversarial samples. As the very beginning work in this direction of detecting adversarial samples for ASV, there is no reliable baseline for comparison. So we first implement Griffin-Lim for detection and set it as our baseline. The proposed method accomplishes effective detection performance and outperforms all the baselines in all the settings. We also show the neural vocoder adopted in the detection framework is dataset independent. Our codes will be made open-source for future works to do comparison. ",
    "url": "https://arxiv.org/abs/2107.00309",
    "authors": [
      "Haibin Wu",
      "Po-chun Hsu",
      "Ji Gao",
      "Shanshan Zhang",
      "Shen Huang",
      "Jian Kang",
      "Zhiyong Wu",
      "Helen Meng",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2107.00559",
    "title": "SALYPATH: A Deep-Based Architecture for visual attention prediction",
    "abstract": "Human vision is naturally more attracted by some regions within their field of view than others. This intrinsic selectivity mechanism, so-called visual attention, is influenced by both high- and low-level factors; such as the global environment (illumination, background texture, etc.), stimulus characteristics (color, intensity, orientation, etc.), and some prior visual information. Visual attention is useful for many computer vision applications such as image compression, recognition, and captioning. In this paper, we propose an end-to-end deep-based method, so-called SALYPATH (SALiencY and scanPATH), that efficiently predicts the scanpath of an image through features of a saliency model. The idea is predict the scanpath by exploiting the capacity of a deep-based model to predict the saliency. The proposed method was evaluated through 2 well-known datasets. The results obtained showed the relevance of the proposed framework comparing to state-of-the-art models. ",
    "url": "https://arxiv.org/abs/2107.00559",
    "authors": [
      "Mohamed Amine Kerkouri",
      "Marouane Tliba",
      "Aladine Chetouani",
      "Rachid Harba"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.00584",
    "title": "On the functional graph of the power map over finite groups",
    "abstract": "In this paper we study the description of the digraph associated with the power map over finite groups. Our main motivation comes from the nice description of such digraphs in the case of cyclic groups. In particular, we derive results on abelian groups, and also on flower groups, which are introduced in this paper. The class of flower groups includes many non abelian groups such as dihedral and generalized quaternion groups, and the projective general linear group of order two over a finite field. In particular, we provide improvements on past works. ",
    "url": "https://arxiv.org/abs/2107.00584",
    "authors": [
      "Claudio Qureshi",
      "Lucas Reis"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2107.00320",
    "title": "Prediction of tone detection thresholds in interaurally delayed noise  based on interaural phase difference fluctuations",
    "abstract": "Differences between the interaural phase of a noise and a target tone improve detection thresholds. The maximum masking release is obtained for detecting an antiphasic tone (S$\\pi$) in diotic noise (N0). It has been shown in several studies that this benefit gradually declines as an interaural delay is applied to the N0S$\\pi$ complex. This decline has been attributed to the reduced interaural coherence of the noise. Here, we report detection thresholds for a 500 Hz tone in masking noise with up to 8 ms interaural delay and bandwidths from 25 to 1000 Hz. When reducing the noise bandwidth from 100 to 50 and 25 Hz, the masking release at 8 ms delay increases, as expected for increasing temporal coherence with decreasing bandwidth. For bandwidths of 100 to 1000 Hz, no significant difference was observed and detection thresholds with these noises have a delay dependence that is fully described by the temporal coherence imposed by the typical monaurally determined auditory filter bandwidth. A minimalistic binaural model is suggested based on interaural phase difference fluctuations without the assumption of delay lines. ",
    "url": "https://arxiv.org/abs/2107.00320",
    "authors": [
      "Mathias Dietz",
      "J\u00f6rg Encke",
      "Kristin I. Bracklo",
      "Stephan D. Ewert"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2107.00363",
    "title": "Well-calibrated prediction intervals for regression problems",
    "abstract": "Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. In this work, we review the above four classes of methods from a conceptual and experimental point of view. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step. ",
    "url": "https://arxiv.org/abs/2107.00363",
    "authors": [
      "Nicolas Dewolf",
      "Bernard De Baets",
      "Willem Waegeman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.00385",
    "title": "Morphological classification of compact and extended radio galaxies  using convolutional neural networks and data augmentation techniques",
    "abstract": "Machine learning techniques have been increasingly used in astronomical applications and have proven to successfully classify objects in image data with high accuracy. The current work uses archival data from the Faint Images of the Radio Sky at Twenty Centimeters (FIRST) to classify radio galaxies into four classes: Fanaroff-Riley Class I (FRI), Fanaroff-Riley Class II (FRII), Bent-Tailed (BENT), and Compact (COMPT). The model presented in this work is based on Convolutional Neural Networks (CNNs). The proposed architecture comprises three parallel blocks of convolutional layers combined and processed for final classification by two feed-forward layers. Our model classified selected classes of radio galaxy sources on an independent testing subset with an average of 96\\% for precision, recall, and F1 score. The best selected augmentation techniques were rotations, horizontal or vertical flips, and increase of brightness. Shifts, zoom and decrease of brightness worsened the performance of the model. The current results show that model developed in this work is able to identify different morphological classes of radio galaxies with a high efficiency and performance ",
    "url": "https://arxiv.org/abs/2107.00385",
    "authors": [
      "Viera Maslej-Kre\u0161\u0148\u00e1kov\u00e1",
      "Khadija El Bouchefry",
      "Peter Butka"
    ],
    "subjectives": [
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2107.00391",
    "title": "Explainable nonlinear modelling of multiple time series with invertible  neural networks",
    "abstract": "A method for nonlinear topology identification is proposed, based on the assumption that a collection of time series are generated in two steps: i) a vector autoregressive process in a latent space, and ii) a nonlinear, component-wise, monotonically increasing observation mapping. The latter mappings are assumed invertible, and are modelled as shallow neural networks, so that their inverse can be numerically evaluated, and their parameters can be learned using a technique inspired in deep learning. Due to the function inversion, the back-propagation step is not straightforward, and this paper explains the steps needed to calculate the gradients applying implicit differentiation. Whereas the model explainability is the same as that for linear VAR processes, preliminary numerical tests show that the prediction error becomes smaller. ",
    "url": "https://arxiv.org/abs/2107.00391",
    "authors": [
      "Luis Miguel Lopez-Ramos",
      "Kevin Roy",
      "Baltasar Beferull-Lozano"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.00594",
    "title": "Pretext Tasks selection for multitask self-supervised speech  representation learning",
    "abstract": "Through solving pretext tasks, self-supervised learning leverages unlabeled data to extract useful latent representations replacing traditional input features in the downstream task. In various application domains, including computer vision, natural language processing and audio/speech signal processing, a wide range of features where engineered through decades of research efforts. As it turns out, learning to predict such features has proven to be a particularly relevant pretext task leading to building useful self-supervised representations that prove to be effective for downstream tasks. However, methods and common practices for combining such pretext tasks, where each task targets a different group of features for better performance on the downstream task have not been explored and understood properly. In fact, the process relies almost exclusively on a computationally heavy experimental procedure, which becomes intractable with the increase of the number of pretext tasks. This paper introduces a method to select a group of pretext tasks among a set of candidates. The method we propose estimates properly calibrated weights for the partial losses corresponding to the considered pretext tasks during the self-supervised training process. The experiments conducted on speaker recognition and automatic speech recognition validate our approach, as the groups selected and weighted with our method perform better than classic baselines, thus facilitating the selection and combination of relevant pseudo-labels for self-supervised representation learning. ",
    "url": "https://arxiv.org/abs/2107.00594",
    "authors": [
      "Salah Zaiem",
      "Titouan Parcollet",
      "Slim Essid"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:1911.02728",
    "title": "Auto-encoding brain networks with applications to analyzing large-scale  brain imaging datasets",
    "abstract": " Comments: 31 pages, 12 figures, 5 tables ",
    "url": "https://arxiv.org/abs/1911.02728",
    "authors": [
      "Meimei Liu",
      "Zhengwu Zhang",
      "David B. Dunson"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2012.12696",
    "title": "NetworkDynamics.jl -- Composing and simulating complex networks in Julia",
    "abstract": " Comments: This article may be downloaded for personal use only. Any other use requires prior permission of the author and AIP Publishing. This article appeared in Chaos 31, 063133 (2021) and may be found at this https URL ",
    "url": "https://arxiv.org/abs/2012.12696",
    "authors": [
      "Michael Lindner",
      "Lucas Lincoln",
      "Fenja Drauschke",
      "Julia Monika Koulen",
      "Hans W\u00fcrfel",
      "Anton Plietzsch",
      "Frank Hellmann"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2102.00050",
    "title": "Sequential prediction under log-loss and misspecification",
    "abstract": " Title: Sequential prediction under log-loss and misspecification ",
    "url": "https://arxiv.org/abs/2102.00050",
    "authors": [
      "Meir Feder",
      "Yury Polyanskiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2102.04998",
    "title": "When does gradient descent with logistic loss interpolate using deep  networks with smoothed ReLU activations?",
    "abstract": " Title: When does gradient descent with logistic loss interpolate using deep  networks with smoothed ReLU activations? ",
    "url": "https://arxiv.org/abs/2102.04998",
    "authors": [
      "Niladri S. Chatterji",
      "Philip M. Long",
      "Peter L. Bartlett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2102.06900",
    "title": "Segmenting two-dimensional structures with strided tensor networks",
    "abstract": " Comments: Accepted to be presented at the 27th international conference on Information Processing in Medical Imaging (IPMI-2021), Bornholm, Denmark. Source code at this https URL Version 2: Minor fixes to notation in Eq.1 and typos ",
    "url": "https://arxiv.org/abs/2102.06900",
    "authors": [
      "Raghavendra Selvan",
      "Erik B Dam",
      "Jens Petersen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2104.07388",
    "title": "Conditional independence for pretext task selection in Self-supervised  speech representation learning",
    "abstract": " Comments: 5 pages, Accepted for presentation at Interspeech2021 ",
    "url": "https://arxiv.org/abs/2104.07388",
    "authors": [
      "Salah Zaiem",
      "Titouan Parcollet",
      "Slim Essid"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2105.13010",
    "title": "An error analysis of generative adversarial networks for learning  distributions",
    "abstract": " Title: An error analysis of generative adversarial networks for learning  distributions ",
    "url": "https://arxiv.org/abs/2105.13010",
    "authors": [
      "Jian Huang",
      "Yuling Jiao",
      "Zhen Li",
      "Shiao Liu",
      "Yang Wang",
      "Yunfei Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.10420",
    "title": "Finding critical edges in complex networks through local information",
    "abstract": " Title: Finding critical edges in complex networks through local information ",
    "url": "https://arxiv.org/abs/2106.10420",
    "authors": [
      "En-Yu Yu",
      "Yan Fu",
      "Jun-Lin Zhou",
      "Hong-Liang Sun",
      "Duan-Bing Chen"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2106.11756",
    "title": "Trinity: A No-Code AI platform for complex spatial datasets",
    "abstract": " Comments: 12 pages ",
    "url": "https://arxiv.org/abs/2106.11756",
    "authors": [
      "C.V.Krishnakumar Iyer",
      "Feili Hou",
      "Henry Wang",
      "Yonghong Wang",
      "Kay Oh",
      "Swetava Ganguli",
      "Vipul Pandey"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.16009",
    "title": "MissFormer: (In-)attention-based handling of missing observations for  trajectory filtering and prediction",
    "abstract": " Title: MissFormer: (In-)attention-based handling of missing observations for  trajectory filtering and prediction ",
    "url": "https://arxiv.org/abs/2106.16009",
    "authors": [
      "Stefan Becker",
      "Ronny Hug",
      "Wolfgang H\u00fcbner",
      "Michael Arens",
      "Brendan T. Morris"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2106.16239",
    "title": "Fixed points of monotonic and (weakly) scalable neural networks",
    "abstract": " Comments: 11 pages ",
    "url": "https://arxiv.org/abs/2106.16239",
    "authors": [
      "Tomasz Piotrowski",
      "Renato L. G. Cavalcante"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  }
]