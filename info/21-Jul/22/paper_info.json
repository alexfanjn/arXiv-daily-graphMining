[
  {
    "id": "arXiv:2107.09768",
    "title": "Checkovid: A COVID-19 misinformation detection system on Twitter using  network and content mining perspectives",
    "abstract": "During the COVID-19 pandemic, social media platforms were ideal for communicating due to social isolation and quarantine. Also, it was the primary source of misinformation dissemination on a large scale, referred to as the infodemic. Therefore, automatic debunking misinformation is a crucial problem. To tackle this problem, we present two COVID-19 related misinformation datasets on Twitter and propose a misinformation detection system comprising network-based and content-based processes based on machine learning algorithms and NLP techniques. In the network-based process, we focus on social properties, network characteristics, and users. On the other hand, we classify misinformation using the content of the tweets directly in the content-based process, which contains text classification models (paragraph-level and sentence-level) and similarity models. The evaluation results on the network-based process show the best results for the artificial neural network model with an F1 score of 88.68%. In the content-based process, our novel similarity models, which obtained an F1 score of 90.26%, show an improvement in the misinformation classification results compared to the network-based models. In addition, in the text classification models, the best result was achieved using the stacking ensemble-learning model by obtaining an F1 score of 95.18%. Furthermore, we test our content-based models on the Constraint@AAAI2021 dataset, and by getting an F1 score of 94.38%, we improve the baseline results. Finally, we develop a fact-checking website called Checkovid that uses each process to detect misinformative and informative claims in the domain of COVID-19 from different perspectives. ",
    "url": "https://arxiv.org/abs/2107.09768",
    "authors": [
      "Sajad Dadgar",
      "Mehdi Ghatee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2107.09851",
    "title": "Fundamental requirements for the design of ultra-reliable low-latency  mobile networks",
    "abstract": "The support for ultra-reliable communication is a key distinction between today's and tomorrow's mobile networks, enabling emerging critical-communication services such as factory automation, remote-controlled vessels, mobile cloud computing, and yet-to-come applications. In this paper, we study the fundamental requirements to design mobile networks capable of ultra-reliable communication. We consider two network design assets, bandwidth and network density, and network models by the 3GPP. Our findings indicate that required assets can be beyond what is typically found in today's networks. We study network sharing as an alternative approach to facilitate the provision of resources to meet ultra-reliability goals. ",
    "url": "https://arxiv.org/abs/2107.09851",
    "authors": [
      "Andr\u00e9 Gomes",
      "Jacek Kibi\u0142da",
      "Nicola Marchetti",
      "Luiz A. DaSilva"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2107.09887",
    "title": "An overview of mixing augmentation methods and augmentation strategies",
    "abstract": "Deep Convolutional Neural Networks have made an incredible progress in many Computer Vision tasks. This progress, however, often relies on the availability of large amounts of the training data, required to prevent over-fitting, which in many domains entails significant cost of manual data labeling. An alternative approach is application of data augmentation (DA) techniques that aim at model regularization by creating additional observations from the available ones. This survey focuses on two DA research streams: image mixing and automated selection of augmentation strategies. First, the presented methods are briefly described, and then qualitatively compared with respect to their key characteristics. Various quantitative comparisons are also included based on the results reported in recent DA literature. This review mainly covers the methods published in the materials of top-tier conferences and in leading journals in the years 2017-2021. ",
    "url": "https://arxiv.org/abs/2107.09887",
    "authors": [
      "Dominik Lewy",
      "Jacek Ma\u0144dziuk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.09951",
    "title": "Deep learning for temporal data representation in electronic health  records: A systematic review of challenges and methodologies",
    "abstract": "Objective: Temporal electronic health records (EHRs) can be a wealth of information for secondary uses, such as clinical events prediction or chronic disease management. However, challenges exist for temporal data representation. We therefore sought to identify these challenges and evaluate novel methodologies for addressing them through a systematic examination of deep learning solutions. Methods: We searched five databases (PubMed, EMBASE, the Institute of Electrical and Electronics Engineers [IEEE] Xplore Digital Library, the Association for Computing Machinery [ACM] digital library, and Web of Science) complemented with hand-searching in several prestigious computer science conference proceedings. We sought articles that reported deep learning methodologies on temporal data representation in structured EHR data from January 1, 2010, to August 30, 2020. We summarized and analyzed the selected articles from three perspectives: nature of time series, methodology, and model implementation. Results: We included 98 articles related to temporal data representation using deep learning. Four major challenges were identified, including data irregularity, data heterogeneity, data sparsity, and model opacity. We then studied how deep learning techniques were applied to address these challenges. Finally, we discuss some open challenges arising from deep learning. Conclusion: Temporal EHR data present several major challenges for clinical prediction modeling and data utilization. To some extent, current deep learning solutions can address these challenges. Future studies can consider designing comprehensive and integrated solutions. Moreover, researchers should incorporate additional clinical domain knowledge into study designs and enhance the interpretability of the model to facilitate its implementation in clinical practice. ",
    "url": "https://arxiv.org/abs/2107.09951",
    "authors": [
      "Feng Xie",
      "Han Yuan",
      "Yilin Ning",
      "Marcus Eng Hock Ong",
      "Mengling Feng",
      "Wynne Hsu",
      "Bibhas Chakraborty",
      "Nan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.10018",
    "title": "Formal method of synthesis of optimal topologies of computing systems  based on projective description of graphs",
    "abstract": "A deterministic method for synthesizing the interconnect topologies optimized for the required properties is proposed. The method is based on the original description of graphs by projections, on establishing the bijective correspondence of the required properties and the projection properties of the initial graph, on postulating the corresponding restrictions of modified projections and on iteratively applying these restrictions to them either until the projection system is solved and the projections of the desired graph are obtained, or until its incompatibility with the given initial conditions is revealed. ",
    "url": "https://arxiv.org/abs/2107.10018",
    "authors": [
      "V.A. Melent'ev"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2107.10089",
    "title": "Robust subgraph counting with distribution-free random graph analysis",
    "abstract": "Subgraphs such as cliques, loops and stars form crucial connections in the topologies of real-world networks. Random graph models provide estimates for how often certain subgraphs appear, which in turn can be tested against real-world networks. These subgraph counts, however, crucially depend on the assumed degree distribution. Fitting a degree distribution to network data is challenging, in particular for scale-free networks with power-law degrees. In this paper we develop robust subgraph counts that do not depend on the entire degree distribution, but only on the mean and mean absolute deviation (MAD), summary statistics that are easy to obtain for most real-world networks. By solving an optimization problem, we provide tight (the sharpest possible) bounds for the subgraph counts, for all possible subgraphs, and for all networks with degree distributions that share the same mean and MAD. We identify the extremal random graph that attains the tight bounds as the graph with a specific three-point degree distribution. We leverage the bounds to obtain robust scaling laws for how the numbers of subgraphs grow as function of the network size. The scaling laws indicate that sparse power-law networks are not the most extreme networks in terms of subgraph counts, but dense power-law networks are. The robust bounds are also shown to hold for several real-world data sets. ",
    "url": "https://arxiv.org/abs/2107.10089",
    "authors": [
      "Johan S. H. van Leeuwaarden",
      "Clara Stegehuis"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Probability (math.PR)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2107.10140",
    "title": "S4T: Source-free domain adaptation for semantic segmentation via  self-supervised selective self-training",
    "abstract": "Most modern approaches for domain adaptive semantic segmentation rely on continued access to source data during adaptation, which may be infeasible due to computational or privacy constraints. We focus on source-free domain adaptation for semantic segmentation, wherein a source model must adapt itself to a new target domain given only unlabeled target data. We propose Self-Supervised Selective Self-Training (S4T), a source-free adaptation algorithm that first uses the model's pixel-level predictive consistency across diverse views of each target image along with model confidence to classify pixel predictions as either reliable or unreliable. Next, the model is self-trained, using predicted pseudolabels for reliable predictions and pseudolabels inferred via a selective interpolation strategy for unreliable ones. S4T matches or improves upon the state-of-the-art in source-free adaptation on 3 standard benchmarks for semantic segmentation within a single epoch of adaptation. ",
    "url": "https://arxiv.org/abs/2107.10140",
    "authors": [
      "Viraj Prabhu",
      "Shivam Khare",
      "Deeksha Kartik",
      "Judy Hoffman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.09790",
    "title": "Non-existence of annular separators in geometric graphs",
    "abstract": "Benjamini and Papasoglou (2011) showed that planar graphs with uniform polynomial volume growth admit $1$-dimensional annular separators: The vertices at graph distance $R$ from any vertex can be separated from those at distance $2R$ by removing at most $O(R)$ vertices. They asked whether geometric $d$-dimensional graphs with uniform polynomial volume growth similarly admit $(d-1)$-dimensional annular separators when $d > 2$. We show that this fails in a strong sense: For any $d \\geq 3$ and every $s \\geq 1$, there is a collection of interior-disjoint spheres in $\\mathbb{R}^d$ whose tangency graph $G$ has uniform polynomial growth, but such that all annular separators in $G$ have cardinality at least $R^s$. ",
    "url": "https://arxiv.org/abs/2107.09790",
    "authors": [
      "Farzam Ebrahimnejad",
      "James R. Lee"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)",
      "Metric Geometry (math.MG)"
    ]
  },
  {
    "id": "arXiv:2107.10246",
    "title": "Sampling from Potts on random graphs of unbounded degree via  random-cluster dynamics",
    "abstract": "We consider the problem of sampling from the ferromagnetic Potts and random-cluster models on a general family of random graphs via the Glauber dynamics for the random-cluster model. The random-cluster model is parametrized by an edge probability $p \\in (0,1)$ and a cluster weight $q > 0$. We establish that for every $q\\ge 1$, the random-cluster Glauber dynamics mixes in optimal $\\Theta(n\\log n)$ steps on $n$-vertex random graphs having a prescribed degree sequence with bounded average branching $\\gamma$, throughout the uniqueness regime $p<p_u(q,\\gamma)$. Notably, this uniqueness threshold does not decay with the maximum degree and only depends on the average degree. In particular, $p_u(q,\\gamma)$ is expected to be sharp, in that for general $q$ and $\\gamma$, the random-cluster Glauber dynamics should slow down dramatically at $p_u(q,\\gamma)$. The family of random graph models we consider include the Erd\\H{o}s--R\\'enyi random graph $G(n,\\gamma/n)$, and so we provide the first polynomial-time sampling algorithm for the ferromagnetic Potts model on the Erd\\H{o}s--R\\'enyi random graphs that works for all $q$ in the full uniqueness regime. We accompany our results with mixing time lower bounds (exponential in the maximum degree) for the Potts Glauber dynamics, in the same settings where our $\\Theta(n \\log n)$ bounds for the random-cluster Glauber dynamics apply. This reveals a significant computational advantage of random-cluster based algorithms for sampling from the Potts Gibbs distribution at high temperatures in the presence of high-degree vertices. ",
    "url": "https://arxiv.org/abs/2107.10246",
    "authors": [
      "Antonio Blanca",
      "Reza Gheissari"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Discrete Mathematics (cs.DM)",
      "Mathematical Physics (math-ph)"
    ]
  },
  {
    "id": "arXiv:2005.08141",
    "title": "Neutral bots probe political bias on social media",
    "abstract": " Comments: 26 pages, 6 figures. Appendix: 10 pages, 5 figures and 4 tables ",
    "url": "https://arxiv.org/abs/2005.08141",
    "authors": [
      "Wen Chen",
      "Diogo Pacheco",
      "Kai-Cheng Yang",
      "Filippo Menczer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2009.05489",
    "title": "MRZ code extraction from visa and passport documents using convolutional  neural networks",
    "abstract": " Comments: This is a preprint of this https URL ",
    "url": "https://arxiv.org/abs/2009.05489",
    "authors": [
      "Yichuan Liu",
      "Hailey James",
      "Otkrist Gupta",
      "Dan Raviv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2102.05373",
    "title": "GuiltyWalker: Distance to illicit nodes in the Bitcoin network",
    "abstract": " Comments: 5 pages, 3 figures ",
    "url": "https://arxiv.org/abs/2102.05373",
    "authors": [
      "Catarina Oliveira",
      "Jo\u00e3o Torres",
      "Maria In\u00eas Silva",
      "David Apar\u00edcio",
      "Jo\u00e3o Tiago Ascens\u00e3o",
      "Pedro Bizarro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  }
]