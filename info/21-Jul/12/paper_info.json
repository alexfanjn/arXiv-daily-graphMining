[
  {
    "id": "arXiv:2107.04081",
    "title": "From local to global determinacy in concurrent graph games",
    "abstract": "In general, finite concurrent two-player reachability games are only determined in a weak sense: the supremum probability to win can be approached via stochastic strategies, but cannot be realized. We introduce a class of concurrent games that are determined in a much stronger sense, and in a way, it is the larger class with this property. To this end, we introduce the notion of \\emph{local interaction} at a state of a graph game: it is a \\emph{game form} whose outcomes (i.e. a table whose entries) are the next states, which depend on the concurrent actions of the players. By definition, a game form is \\emph{determined} iff it always yields games that are determined via deterministic strategies when used as a local interaction in a Nature-free, one-shot reachability game. We show that if all the local interactions of a graph game with Borel objective are determined game forms, the game itself is determined: if Nature does not play, one player has a winning strategy; if Nature plays, both players have deterministic strategies that maximize the probability to win. This constitutes a clear-cut separation: either a game form behaves poorly already when used alone with basic objectives, or it behaves well even when used together with other well-behaved game forms and complex objectives. Existing results for positional and finite-memory determinacy in turn-based games are extended this way to concurrent games with determined local interactions (CG-DLI). ",
    "url": "https://arxiv.org/abs/2107.04081",
    "authors": [
      "Benjamin Bordais",
      "Patricia Bouyer",
      "St\u00e9phane Le Roux"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2107.04238",
    "title": "Defense against DoS and load altering attacks via model-free control: A  proposal for a new cybersecurity setting",
    "abstract": "Defense against cyberattacks is an emerging topic related to fault-tolerant control. In order to avoid difficult mathematical modeling, model-free control (MFC) is suggested as an alternative to classical control. For illustration purpose a Load Frequency Control of multi-areas power network is considered. In the simulations, load altering attacks and Denial of Service (DoS) in the communication network are applied to the system. Our aim is to compare the impact of cyberattacks on control loops closed via respectively a classical controller in such situations and a model-free one. Computer experiments show impressive results with MFC. ",
    "url": "https://arxiv.org/abs/2107.04238",
    "authors": [
      "Michel Fliess",
      "C\u00e9dric Join",
      "Dominique Sauter"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2107.04296",
    "title": "Differentially private training of neural networks with Langevin  dynamics forcalibrated predictive uncertainty",
    "abstract": "We show that differentially private stochastic gradient descent (DP-SGD) can yield poorly calibrated, overconfident deep learning models. This represents a serious issue for safety-critical applications, e.g. in medical diagnosis. We highlight and exploit parallels between stochastic gradient Langevin dynamics, a scalable Bayesian inference technique for training deep neural networks, and DP-SGD, in order to train differentially private, Bayesian neural networks with minor adjustments to the original (DP-SGD) algorithm. Our approach provides considerably more reliable uncertainty estimates than DP-SGD, as demonstrated empirically by a reduction in expected calibration error (MNIST $\\sim{5}$-fold, Pediatric Pneumonia Dataset $\\sim{2}$-fold). ",
    "url": "https://arxiv.org/abs/2107.04296",
    "authors": [
      "Moritz Knolle",
      "Alexander Ziller",
      "Dmitrii Usynin",
      "Rickmer Braren",
      "Marcus R. Makowski",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2107.04380",
    "title": "Model compression as constrained optimization, with application to  neural nets. Part V: combining compressions",
    "abstract": "Model compression is generally performed by using quantization, low-rank approximation or pruning, for which various algorithms have been researched in recent years. One fundamental question is: what types of compression work better for a given model? Or even better: can we improve by combining compressions in a suitable way? We formulate this generally as a problem of optimizing the loss but where the weights are constrained to equal an additive combination of separately compressed parts; and we give an algorithm to learn the corresponding parts' parameters. Experimentally with deep neural nets, we observe that 1) we can find significantly better models in the error-compression space, indicating that different compression types have complementary benefits, and 2) the best type of combination depends exquisitely on the type of neural net. For example, we can compress ResNets and AlexNet using only 1 bit per weight without error degradation at the cost of adding a few floating point weights. However, VGG nets can be better compressed by combining low-rank with a few floating point weights. ",
    "url": "https://arxiv.org/abs/2107.04380",
    "authors": [
      "Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n",
      "Yerlan Idelbayev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.04466",
    "title": "An efficient greedy training algorithm for neural networks and  applications in PDEs",
    "abstract": "Recently, neural networks have been widely applied for solving partial differential equations. However, the resulting optimization problem brings many challenges for current training algorithms. This manifests itself in the fact that the convergence order that has been proven theoretically cannot be obtained numerically. In this paper, we develop a novel greedy training algorithm for solving PDEs which builds the neural network architecture adaptively. It is the first training algorithm that observes the convergence order of neural networks numerically. This innovative algorithm is tested on several benchmark examples in both 1D and 2D to confirm its efficiency and robustness. ",
    "url": "https://arxiv.org/abs/2107.04466",
    "authors": [
      "Wenrui Hao",
      "Xianlin Jin",
      "Jonathan W Siegel",
      "Jinchao Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2107.04479",
    "title": "Convergence analysis for gradient flows in the training of artificial  neural networks with ReLU activation",
    "abstract": "Gradient descent (GD) type optimization schemes are the standard methods to train artificial neural networks (ANNs) with rectified linear unit (ReLU) activation. Such schemes can be considered as discretizations of gradient flows (GFs) associated to the training of ANNs with ReLU activation and most of the key difficulties in the mathematical convergence analysis of GD type optimization schemes in the training of ANNs with ReLU activation seem to be already present in the dynamics of the corresponding GF differential equations. It is the key subject of this work to analyze such GF differential equations in the training of ANNs with ReLU activation and three layers (one input layer, one hidden layer, and one output layer). In particular, in this article we prove in the case where the target function is possibly multi-dimensional and continuous and in the case where the probability distribution of the input data is absolutely continuous with respect to the Lebesgue measure that the risk of every bounded GF trajectory converges to the risk of a critical point. In addition, in this article we show in the case of a 1-dimensional affine linear target function and in the case where the probability distribution of the input data coincides with the standard uniform distribution that the risk of every bounded GF trajectory converges to zero if the initial risk is sufficiently small. Finally, in the special situation where there is only one neuron on the hidden layer (1-dimensional hidden layer) we strengthen the above named result for affine linear target functions by proving that that the risk of every (not necessarily bounded) GF trajectory converges to zero if the initial risk is sufficiently small. ",
    "url": "https://arxiv.org/abs/2107.04479",
    "authors": [
      "Arnulf Jentzen",
      "Adrian Riekert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2107.04510",
    "title": "Hacking VMAF and VMAF NEG: metrics vulnerability to different  preprocessing",
    "abstract": "Video quality measurement plays a critical role in the development of video processing applications. In this paper, we show how popular quality metrics VMAF and its tuning-resistant version VMAF NEG can be artificially increased by video preprocessing. We propose a pipeline for tuning parameters of processing algorithms that allows increasing VMAF by up to 218.8%. A subjective comparison of preprocessed videos showed that with the majority of methods visual quality drops down or stays unchanged. We show that VMAF NEG scores can also be increased by some preprocessing methods by up to 23.6%. ",
    "url": "https://arxiv.org/abs/2107.04510",
    "authors": [
      "Maksim Siniukov",
      "Anastasia Antsiferova",
      "Dmitriy Kulikov",
      "Dmitriy Vatolin"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2103.08494",
    "title": "BrainNetGAN: Data augmentation of brain connectivity using generative  adversarial network for dementia classification",
    "abstract": " Title: BrainNetGAN: Data augmentation of brain connectivity using generative  adversarial network for dementia classification ",
    "url": "https://arxiv.org/abs/2103.08494",
    "authors": [
      "Chao Li",
      "Yiran Wei",
      "Xi Chen",
      "Carola-Bibiane Schonlieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2106.06988",
    "title": "NDPNet: A novel non-linear data projection network for few-shot  fine-grained image classification",
    "abstract": " Title: NDPNet: A novel non-linear data projection network for few-shot  fine-grained image classification ",
    "url": "https://arxiv.org/abs/2106.06988",
    "authors": [
      "Weichuan Zhang",
      "Xuefang Liu",
      "Zhe Xue",
      "Yongsheng Gao",
      "Changming Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2107.03230",
    "title": "Coastal water quality prediction based on machine learning with feature  interpretation and spatio-temporal analysis",
    "abstract": " Title: Coastal water quality prediction based on machine learning with feature  interpretation and spatio-temporal analysis ",
    "url": "https://arxiv.org/abs/2107.03230",
    "authors": [
      "Luka Grb\u010di\u0107",
      "Sini\u0161a Dru\u017eeta",
      "Goran Mau\u0161a",
      "Tomislav Lipi\u0107",
      "Darija Vuki\u0107 Lu\u0161i\u0107",
      "Marta Alvir",
      "Ivana Lu\u010din",
      "Ante Sikirica",
      "Davor Davidovi\u0107",
      "Vanja Trava\u0161",
      "Daniela Kalafatovi\u0107",
      "Kristina Pikelj",
      "Hana Fajkovi\u0107",
      "Toni Holjevi\u0107",
      "Lado Kranj\u010devi\u0107"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2107.03380",
    "title": "RRL: Resnet as representation for Reinforcement Learning",
    "abstract": " Comments: Published at ICML 2021 ",
    "url": "https://arxiv.org/abs/2107.03380",
    "authors": [
      "Rutav Shah",
      "Vikash Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2107.03904",
    "title": "A hybrid deep learning framework for Covid-19 detection via 3D Chest CT  Images",
    "abstract": " Comments: 5 pages, 1 figure, 2 tables ",
    "url": "https://arxiv.org/abs/2107.03904",
    "authors": [
      "Shuang Liang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  }
]