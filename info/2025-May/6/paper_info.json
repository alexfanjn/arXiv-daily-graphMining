[
  {
    "id": "arXiv:2505.01429",
    "title": "Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis",
    "abstract": "           Since mpox can spread from person to person, it is a zoonotic viral illness that poses a significant public health concern. It is difficult to make an early clinical diagnosis because of how closely its symptoms match those of measles and chickenpox. Medical imaging combined with deep learning (DL) techniques has shown promise in improving disease detection by analyzing affected skin areas. Our study explore the feasibility to train deep learning and vision transformer-based models from scratch with publicly available skin lesion image dataset. Our experimental results show dataset limitation as a major drawback to build better classifier models trained from scratch. We used transfer learning with the help of pre-trained models to get a better classifier. The MobileNet-v2 outperformed other state of the art pre-trained models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and ResNet-50 also achieved satisfactory performance compared to already available studies with accuracy 92.12% and 86.21% respectively. To further validate the performance of the models, we applied explainable AI techniques.         ",
    "url": "https://arxiv.org/abs/2505.01429",
    "authors": [
      "Md. Zahid Hossain",
      "Md. Rakibul Islam",
      "Most. Sharmin Sultana Samu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01436",
    "title": "Firewall Regulatory Networks for Autonomous Cyber Defense",
    "abstract": "           In this paper, we present the principles of designing new self-organising and autonomous management protocol to govern the dynamics of bio-inspired decentralized firewall architecture based on Biological Regularity Networks. The new architecture called Firewall Regulatory Networks (FRN) exhibits the following features (1) automatic rule policy configuration with provable utility-risk appetite guarantee, (2) resilient response for changing risks or new service requirements, and (3) globally optimized access control policy reconciliation. We present the FRN protocol and formalize the constraints to synthesize the undetermined components in the protocol to produce interactions that can achieve these objectives. We illustrate the feasibility of the FRN architecture in multiple case studies.         ",
    "url": "https://arxiv.org/abs/2505.01436",
    "authors": [
      "Qi Duan",
      "Ehab Al-Shaer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.01437",
    "title": "Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets",
    "abstract": "           The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for IoT-botnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes.         ",
    "url": "https://arxiv.org/abs/2505.01437",
    "authors": [
      "Hassan Wasswa",
      "Timothy Lynar",
      "Hussein Abbass"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01454",
    "title": "Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning",
    "abstract": "           Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet it faces significant challenges in communication efficiency and vulnerability to poisoning attacks. While sparsification techniques mitigate communication overhead by transmitting only critical model parameters, they inadvertently amplify security risks: adversarial clients can exploit sparse updates to evade detection and degrade model performance. Existing defense mechanisms, designed for standard FL communication scenarios, are ineffective in addressing these vulnerabilities within sparsified FL. To bridge this gap, we propose FLARE, a novel federated learning framework that integrates sparse index mask inspection and model update sign similarity analysis to detect and mitigate poisoning attacks in sparsified FL. Extensive experiments across multiple datasets and adversarial scenarios demonstrate that FLARE significantly outperforms existing defense strategies, effectively securing sparsified FL against poisoning attacks while maintaining communication efficiency.         ",
    "url": "https://arxiv.org/abs/2505.01454",
    "authors": [
      "Zhiyong Jin",
      "Runhua Xu",
      "Chao Li",
      "Yizhong Liu",
      "Jianxin Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01460",
    "title": "Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services",
    "abstract": "           Due to the increasing number of tasks that are solved on remote servers, identifying and classifying traffic is an important task to reduce the load on the server. There are various methods for classifying traffic. This paper discusses machine learning models for solving this problem. However, such ML models are also subject to attacks that affect the classification result of network traffic. To protect models, we proposed a solution based on an autoencoder         ",
    "url": "https://arxiv.org/abs/2505.01460",
    "authors": [
      "Denis Parfenov",
      "Anton Parfenov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01474",
    "title": "Watermark Overwriting Attack on StegaStamp algorithm",
    "abstract": "           This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS \"Erasing the invisible\" competition.         ",
    "url": "https://arxiv.org/abs/2505.01474",
    "authors": [
      "I.F.Serzhenko",
      "L.A.Khaertdinova",
      "M.A.Pautov",
      "A.V.Antsiferova"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01475",
    "title": "BiGSCoder: State Space Model for Code Understanding",
    "abstract": "           We present BiGSCoder, a novel encoder-only bidirectional state-space model (SSM) featuring a gated architecture, pre-trained for code understanding on a code dataset using masked language modeling. Our work aims to systematically evaluate SSMs' capabilities in coding tasks compared to traditional transformer architectures; BiGSCoder is built for this purpose. Through comprehensive experiments across diverse pre-training configurations and code understanding benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models, despite utilizing simpler pre-training strategies and much less training data. Our results indicate that BiGSCoder can serve as a more sample-efficient alternative to conventional transformer models. Furthermore, our study shows that SSMs perform better without positional embeddings and can effectively extrapolate to longer sequences during fine-tuning.         ",
    "url": "https://arxiv.org/abs/2505.01475",
    "authors": [
      "Shweta Verma",
      "Abhinav Anand",
      "Mira Mezini"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01479",
    "title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation",
    "abstract": "           Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.         ",
    "url": "https://arxiv.org/abs/2505.01479",
    "authors": [
      "Siheng Xiong",
      "Jieyu Zhou",
      "Zhangding Liu",
      "Yusen Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.01485",
    "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code",
    "abstract": "           Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning.         ",
    "url": "https://arxiv.org/abs/2505.01485",
    "authors": [
      "Tasnim Ahmed",
      "Salimur Choudhury"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.01524",
    "title": "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data",
    "abstract": "           Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.         ",
    "url": "https://arxiv.org/abs/2505.01524",
    "authors": [
      "Zexi Yao",
      "Nata\u0161a Kr\u010do",
      "Georgi Ganev",
      "Yves-Alexandre de Montjoye"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01548",
    "title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation",
    "abstract": "           Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.01548",
    "authors": [
      "Zhen Yao",
      "Xiaowen Ying",
      "Mooi Choo Chuah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01583",
    "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action",
    "abstract": "           Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.         ",
    "url": "https://arxiv.org/abs/2505.01583",
    "authors": [
      "Jen-Hao Cheng",
      "Vivian Wang",
      "Huayu Wang",
      "Huapeng Zhou",
      "Yi-Hao Peng",
      "Hou-I Liu",
      "Hsiang-Wei Huang",
      "Kuang-Ming Chen",
      "Cheng-Yen Yang",
      "Wenhao Chai",
      "Yi-Ling Chen",
      "Vibhav Vineet",
      "Qin Cai",
      "Jenq-Neng Hwang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01584",
    "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation",
    "abstract": "           Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions.         ",
    "url": "https://arxiv.org/abs/2505.01584",
    "authors": [
      "Zhiqiang He",
      "Zhi Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01624",
    "title": "Triangle-Decomposable Graphs for Isoperimetric Robots",
    "abstract": "           Isoperimetric robots are large scale, untethered inflatable robots that can undergo large shape changes, but have only been demonstrated in one 3D shape -- an octahedron. These robots consist of independent triangles that can change shape while maintaining their perimeter by moving the relative position of their joints. We introduce an optimization routine that determines if an arbitrary graph can be partitioned into unique triangles, and thus be constructed as an isoperimetric robotic system. We enumerate all minimally rigid graphs that can be constructed with unique triangles up to 9 nodes (7 triangles), and characterize the workspace of one node of each these robots. We also present a method for constructing larger graphs that can be partitioned by assembling subgraphs that are already partitioned into triangles. This enables a wide variety of isoperimetric robot configurations.         ",
    "url": "https://arxiv.org/abs/2505.01624",
    "authors": [
      "Nathan Usevitch",
      "Isaac Weaver",
      "James Usevitch"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.01637",
    "title": "Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression",
    "abstract": "           High-throughput neural network inference requires coordinating many optimization decisions, including parallel tiling, microkernel selection, and data layout. The product of these decisions forms a search space of programs which is typically intractably large. Existing approaches (e.g., auto-schedulers) often address this problem by sampling this space heuristically. In contrast, we introduce a dynamic-programming-based approach to explore more of the search space by iteratively decomposing large program specifications into smaller specifications reachable from a set of rewrites, then composing a final program from each rewrite that minimizes an affine cost model. To reduce memory requirements, we employ a novel memoization table representation, which indexes specifications by coordinates in $Z_{\\geq 0}$ and compresses identical, adjacent solutions. This approach can visit a much larger set of programs than prior work. To evaluate the approach, we developed Morello, a compiler which lowers specifications roughly equivalent to a few-node XLA computation graph to x86. Notably, we found that an affine cost model is sufficient to surface high-throughput programs. For example, Morello synthesized a collection of matrix multiplication benchmarks targeting a Zen 1 CPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply, which was integrated into Google's this http URL.         ",
    "url": "https://arxiv.org/abs/2505.01637",
    "authors": [
      "Samuel J. Kaufman",
      "Ren\u00e9 Just",
      "Rastislav Bodik"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01650",
    "title": "Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability",
    "abstract": "           The rapid expansion of advanced low-Earth orbit (LEO) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. A solution to the challenge is effective space object detection (SOD) for collision assessment and avoidance. In SOD, an LEO satellite must detect other satellites and objects with high precision and minimal delay. This paper investigates the feasibility and effectiveness of employing vision sensors for SOD tasks based on deep learning (DL) models. It introduces models based on the Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their performance under SOD scenarios. Experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (mAP50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to 0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from 0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\\%.         ",
    "url": "https://arxiv.org/abs/2505.01650",
    "authors": [
      "Wenxuan Zhang",
      "Peng Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.01652",
    "title": "Causally Fair Node Classification on Non-IID Graph Data",
    "abstract": "           Fair machine learning seeks to identify and mitigate biases in predictions against unfavorable populations characterized by demographic attributes, such as race and gender. Recently, a few works have extended fairness to graph data, such as social networks, but most of them neglect the causal relationships among data instances. This paper addresses the prevalent challenge in fairness-aware ML algorithms, which typically assume Independent and Identically Distributed (IID) data. We tackle the overlooked domain of non-IID, graph-based settings where data instances are interconnected, influencing the outcomes of fairness interventions. We base our research on the Network Structural Causal Model (NSCM) framework and posit two main assumptions: Decomposability and Graph Independence, which enable the computation of interventional distributions in non-IID settings using the $do$-calculus. Based on that, we develop the Message Passing Variational Autoencoder for Causal Inference (MPVA) to compute interventional distributions and facilitate causally fair node classification through estimated interventional distributions. Empirical evaluations on semi-synthetic and real-world datasets demonstrate that MPVA outperforms conventional methods by effectively approximating interventional distributions and mitigating bias. The implications of our findings underscore the potential of causality-based fairness in complex ML applications, setting the stage for further research into relaxing the initial assumptions to enhance model fairness.         ",
    "url": "https://arxiv.org/abs/2505.01652",
    "authors": [
      "Yucong Dai",
      "Lu Zhang",
      "Yaowei Hu",
      "Susan Gauch",
      "Yongkai Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01654",
    "title": "T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation",
    "abstract": "           T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic system developed for autonomous leaf localization, selection, and grasping in greenhouse environments. The system integrates a 6-degree-of-freedom manipulator with a stereo vision pipeline to identify and interact with target leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo provides dense depth maps, allowing the reconstruction of 3D leaf masks. These observations are processed through a leaf grasping algorithm that selects the optimal leaf based on clutter, visibility, and distance, and determines a grasp point by analyzing local surface flatness, top-down approachability, and margin from edges. The selected grasp point guides a trajectory executed by ROS-based motion controllers, driving a custom microneedle-equipped end-effector to clamp the leaf and simulate tissue sampling. Experiments conducted with artificial plants under varied poses demonstrate that the T-Rex system can consistently detect, plan, and perform physical interactions with plant-like targets, achieving a grasp success rate of 66.6\\%. This paper presents the system architecture, implementation, and testing of T-Rex as a step toward plant sampling automation in Controlled Environment Agriculture (CEA).         ",
    "url": "https://arxiv.org/abs/2505.01654",
    "authors": [
      "Srecharan Selvam",
      "Abhisesh Silwal",
      "George Kantor"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01656",
    "title": "A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory",
    "abstract": "           The pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. The current trunk and branch extraction technologies are mainly LiDAR-based or UAV-based. The former approaches obtain high-precision 3D data, but its equipment cost is high and the three-dimensional (3D) data processing is complex. The latter approaches efficiently capture canopy information, but they miss the 3-D structure of trees. In order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel WaveInst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. Experimental results of the proposed model show superior performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset. Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. The proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. Furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2D images directly. This study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding.         ",
    "url": "https://arxiv.org/abs/2505.01656",
    "authors": [
      "Chenyang Fan",
      "Xujie Zhu",
      "Taige Luo",
      "Sheng Xu",
      "Zhulin Chen",
      "Hongxin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01675",
    "title": "Enhanced Prediction Model for Time Series Characterized by GARCH via Interval Type-2 Fuzzy Inference System",
    "abstract": "           GARCH-type time series (characterized by Generalized Autoregressive Conditional Heteroskedasticity) exhibit pronounced volatility, autocorrelation, and heteroskedasticity. To address these challenges and enhance predictive accuracy, this study introduces a hybrid forecasting framework that integrates the Interval Type-2 Fuzzy Inference System (IT2FIS) with the GARCH model. Leveraging the interval-based uncertainty representation of IT2FIS and the volatility-capturing capability of GARCH, the proposed model effectively mitigates the adverse impact of heteroskedasticity on prediction reliability. Specifically, the GARCH component estimates conditional variance, which is subsequently incorporated into the Gaussian membership functions of IT2FIS. This integration transforms IT2FIS into an adaptive variable-parameter system, dynamically aligning with the time-varying volatility of the target series. Through systematic parameter optimization, the framework not only captures intricate volatility patterns but also accounts for heteroskedasticity and epistemic uncertainties during modeling, thereby improving both prediction precision and model robustness. Experimental validation employs diverse datasets, including air quality concentration, urban traffic flow, and energy consumption. Comparative analyses are conducted against models: the GARCH-Takagi-Sugeno-Kang (GARCH-TSK) model, fixed-variance time series models, the GARCH-Gated Recurrent Unit (GARCH-GRU), and Long Short-Term Memory (LSTM) networks. The results indicate that the proposed model achieves superior predictive performance across the majority of test scenarios in error metrics. These findings underscore the effectiveness of hybrid approaches in forecasting uncertainty for GARCH-type time series, highlighting their practical utility in real-world time series forecasting applications.         ",
    "url": "https://arxiv.org/abs/2505.01675",
    "authors": [
      "Shaohong Pei",
      "Da-Qing Zhang",
      "Feilong Lu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.01688",
    "title": "Sensing Safety Analysis for Vehicular Networks with Integrated Sensing and Communication (ISAC)",
    "abstract": "           Integrated sensing and communication (ISAC) emerged as a key feature of next-generation 6G wireless systems, allowing them to achieve high data rates and sensing accuracy. While prior research has primarily focused on addressing communication safety in ISAC systems, the equally critical issue of sensing safety remains largely ignored. In this paper, a novel threat to the sensing safety of ISAC vehicle networks is studied, whereby a malicious reconfigurable intelligent surface (RIS) is deployed to compromise the sensing functionality of a roadside unit (RSU). Specifically, a malicious attacker dynamically adjusts the phase shifts of an RIS to spoof the sensing outcomes of a vehicular user (VU)'s echo delay, Doppler shift, and angle-of-departure (AoD). To achieve spoofing on Doppler shift estimation, a time-varying phase shift design on the RIS is proposed. Furthermore, the feasible spoofing frequency set with respect to the Doppler shift is analytical derived. Analytical results also demonstrate that the maximum likelihood estimator (MLE) of the AoD can be significantly misled under spoofed Doppler shift estimation. Simulation results validate our theoretical findings, showing that the RIS can induce a spoofed velocity estimation from 0.1 m/s to 14.9 m/s for a VU with velocity of 10 m/s, and can cause an AoD estimation error of up to 65^{\\circ} with only a 5^{\\circ} beam misalignment.         ",
    "url": "https://arxiv.org/abs/2505.01688",
    "authors": [
      "Tingyu Shui",
      "Walid Saad",
      "Mingzhe Cheng"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.01689",
    "title": "Fragment-Level Macro-Diversity Reception in LoRaWAN Networks with LR-FHSS",
    "abstract": "           The rapid expansion of Internet of Things (IoT) deployments demands wireless protocols that combine high scalability with robust performance. Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) extends LoRaWAN by increasing capacity and resilience through frequency hopping and redundancy. However, current deployments require packet reconstruction at a single gateway, limiting the benefits of LR-FHSS. This paper proposes a macro-diversity reception strategy where multiple gateways collectively receive and combine payload fragments. We develop a stochastic geometry-based analytical model that captures the impact of header repetition, payload fragmentation, and coding redundancy. Closed-form expressions quantify success probabilities under interference, and numerical evaluations demonstrate significant capacity gains over nearest-gateway reception. These results highlight the potential of fragment-level macro-diversity to improve scalability and reliability in future LPWAN deployments.         ",
    "url": "https://arxiv.org/abs/2505.01689",
    "authors": [
      "Samer Lahoud",
      "Kinda Khawam"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2505.01693",
    "title": "High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers",
    "abstract": "           Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation.         ",
    "url": "https://arxiv.org/abs/2505.01693",
    "authors": [
      "Brian Wong",
      "Kaito Tanaka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.01695",
    "title": "SimAug: Enhancing Recommendation with Pretrained Language Models for Dense and Balanced Data Augmentation",
    "abstract": "           Deep Neural Networks (DNNs) are extensively used in collaborative filtering due to their impressive effectiveness. These systems depend on interaction data to learn user and item embeddings that are crucial for recommendations. However, the data often suffers from sparsity and imbalance issues: limited observations of user-item interactions can result in sub-optimal performance, and a predominance of interactions with popular items may introduce recommendation bias. To address these challenges, we employ Pretrained Language Models (PLMs) to enhance the interaction data with textual information, leading to a denser and more balanced dataset. Specifically, we propose a simple yet effective data augmentation method (SimAug) based on the textual similarity from PLMs, which can be seamlessly integrated to any systems as a lightweight, plug-and-play component in the pre-processing stage. Our experiments across nine datasets consistently demonstrate improvements in both utility and fairness when training with the augmented data generated by SimAug. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.01695",
    "authors": [
      "Yuying Zhao",
      "Xiaodong Yang",
      "Huiyuan Chen",
      "Xiran Fan",
      "Yu Wang",
      "Yiwei Cai",
      "Tyler Derr"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2505.01698",
    "title": "Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs",
    "abstract": "           The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.01698",
    "authors": [
      "Yuying Zhao",
      "Yu Wang",
      "Xueqi Cheng",
      "Anne Marie Tumlin",
      "Yunchao Liu",
      "Damin Xia",
      "Meng Jiang",
      "Tyler Derr"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.01706",
    "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm",
    "abstract": "           Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even \"good\" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present.         ",
    "url": "https://arxiv.org/abs/2505.01706",
    "authors": [
      "Sarvesh Shashidhar",
      "Ritik",
      "Nachiketa Patil",
      "Suraj Racha",
      "Ganesh Ramakrishnan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01712",
    "title": "World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks",
    "abstract": "           Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method.         ",
    "url": "https://arxiv.org/abs/2505.01712",
    "authors": [
      "Lingyi Wang",
      "Rashed Shelim",
      "Walid Saad",
      "Naren Ramakrishnan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2505.01726",
    "title": "Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes",
    "abstract": "           Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.         ",
    "url": "https://arxiv.org/abs/2505.01726",
    "authors": [
      "Jie Liu",
      "Pan Zhou",
      "Zehao Xiao",
      "Jiayi Shen",
      "Wenzhe Yin",
      "Jan-Jakob Sonke",
      "Efstratios Gavves"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01729",
    "title": "PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth",
    "abstract": "           Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.         ",
    "url": "https://arxiv.org/abs/2505.01729",
    "authors": [
      "Bu Jin",
      "Weize Li",
      "Baihan Yang",
      "Zhenxin Zhu",
      "Junpeng Jiang",
      "Huan-ang Gao",
      "Haiyang Sun",
      "Kun Zhan",
      "Hengtong Hu",
      "Xueyang Zhang",
      "Peng Jia",
      "Hao Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01735",
    "title": "Brain-Inspired Quantum Neural Architectures for Pattern Recognition: Integrating QSNN and QLSTM",
    "abstract": "           Recent advances in the fields of deep learning and quantum computing have paved the way for innovative developments in artificial intelligence. In this manuscript, we leverage these cutting-edge technologies to introduce a novel model that emulates the intricate functioning of the human brain, designed specifically for the detection of anomalies such as fraud in credit card transactions. Leveraging the synergies of Quantum Spiking Neural Networks (QSNN) and Quantum Long Short-Term Memory (QLSTM) architectures, our approach is developed in two distinct stages, closely mirroring the information processing mechanisms found in the brain's sensory and memory systems. In the initial stage, similar to the brain's hypothalamus, we extract low-level information from the data, emulating sensory data processing patterns. In the subsequent stage, resembling the hippocampus, we process this information at a higher level, capturing and memorizing correlated patterns. We will compare this model with other quantum models such as Quantum Neural Networks among others and their corresponding classical models.         ",
    "url": "https://arxiv.org/abs/2505.01735",
    "authors": [
      "Eva Andr\u00e9s",
      "Manuel Pegalajar Cu\u00e9llar",
      "Gabriel Navarro"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2505.01736",
    "title": "PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems",
    "abstract": "           Accurately modeling and forecasting complex systems governed by partial differential equations (PDEs) is crucial in various scientific and engineering domains. However, traditional numerical methods struggle in real-world scenarios due to incomplete or unknown physical laws. Meanwhile, machine learning approaches often fail to generalize effectively when faced with scarce observational data and the challenge of capturing local and global features. To this end, we propose the Physics-encoded Spectral Attention Network (PeSANet), which integrates local and global information to forecast complex systems with limited data and incomplete physical priors. The model consists of two key components: a physics-encoded block that uses hard constraints to approximate local differential operators from limited data, and a spectral-enhanced block that captures long-range global dependencies in the frequency domain. Specifically, we introduce a novel spectral attention mechanism to model inter-spectrum relationships and learn long-range spatial features. Experimental results demonstrate that PeSANet outperforms existing methods across all metrics, particularly in long-term forecasting accuracy, providing a promising solution for simulating complex systems with limited data and incomplete physics.         ",
    "url": "https://arxiv.org/abs/2505.01736",
    "authors": [
      "Han Wan",
      "Rui Zhang",
      "Qi Wang",
      "Yang Liu",
      "Hao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01749",
    "title": "Unified Steganography via Implicit Neural Representation",
    "abstract": "           Digital steganography is the practice of concealing for encrypted data transmission. Typically, steganography methods embed secret data into cover data to create stega data that incorporates hidden secret data. However, steganography techniques often require designing specific frameworks for each data type, which restricts their generalizability. In this paper, we present U-INR, a novel method for steganography via Implicit Neural Representation (INR). Rather than using the specific framework for each data format, we directly use the neurons of the INR network to represent the secret data and cover data across different data types. To achieve this idea, a private key is shared between the data sender and receivers. Such a private key can be used to determine the position of secret data in INR networks. To effectively leverage this key, we further introduce a key-based selection strategy that can be used to determine the position within the INRs for data storage. Comprehensive experiments across multiple data types, including images, videos, audio, and SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR, emphasizing its potential for improving data security and privacy in various applications.         ",
    "url": "https://arxiv.org/abs/2505.01749",
    "authors": [
      "Qi Song",
      "Ziyuan Luo",
      "Xiufeng Huang",
      "Sheng Li",
      "Renjie Wan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.01766",
    "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
    "abstract": "           Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.         ",
    "url": "https://arxiv.org/abs/2505.01766",
    "authors": [
      "Long Bai",
      "Boyi Ma",
      "Ruohan Wang",
      "Guankun Wang",
      "Beilei Cui",
      "Zhongliang Jiang",
      "Mobarakol Islam",
      "Zhe Min",
      "Jiewen Lai",
      "Nassir Navab",
      "Hongliang Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.01783",
    "title": "Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition",
    "abstract": "           Online anomaly detection is essential in fields such as cybersecurity, healthcare, and industrial monitoring, where promptly identifying deviations from expected behavior can avert critical failures or security breaches. While numerous anomaly scoring methods based on supervised or unsupervised learning have been proposed, current approaches typically rely on a continuous stream of real-world calibration data to provide assumption-free guarantees on the false discovery rate (FDR). To address the inherent challenges posed by limited real calibration data, we introduce context-aware prediction-powered conformal online anomaly detection (C-PP-COAD). Our framework strategically leverages synthetic calibration data to mitigate data scarcity, while adaptively integrating real data based on contextual cues. C-PP-COAD utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain rigorous and reliable anomaly detection performance over time. Experiments conducted on both synthetic and real-world datasets demonstrate that C-PP-COAD significantly reduces dependency on real calibration data without compromising guaranteed FDR control.         ",
    "url": "https://arxiv.org/abs/2505.01783",
    "authors": [
      "Amirmohammad Farzaneh",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.01788",
    "title": "Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning",
    "abstract": "           The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.         ",
    "url": "https://arxiv.org/abs/2505.01788",
    "authors": [
      "Md. Tanzib Hosain",
      "Asif Zaman",
      "Md. Shahriar Sajid",
      "Shadman Sakeeb Khan",
      "Shanjida Akter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.01796",
    "title": "Semantics-Aware Unified Terrestrial Non-Terrestrial 6G Networks",
    "abstract": "           The integration of Terrestrial and Non-Terrestrial Networks (TN-NTNs), which was introduced in 5G, is progressing toward a unified and seamless network of networks in Sixth-Generation (6G). This evolution leads to a significant increase in the volume of generated and communicated data, imposing technical and operational requirements accompanied by a higher cost and energy consumption. Efficiently managing the generation and transmission of data in these highly complex unified networks has become essential. In this article, we investigate the semantics-aware information handling problem within unified TN-NTNs, where data communication between the distant TN nodes is enabled via an NTN. To this end, an Internet of Things (IoT) monitoring system is employed, where status updates from a remote IoT device are communicated to a destination monitor via a constellation of Low Earth Orbit (LEO) satellites. We leverage semantic metrics that capture the timeliness, relevance, and utility of information to provide the most informative data for timely and informed decision-making and eventually reduce the volume of transmitted and processed data. The outcome is significantly lower energy consumption, memory, control, and processing requirements (up to 73% lower energy charging demands compared to the state-of-the-art), all without compromising the conveyed information.         ",
    "url": "https://arxiv.org/abs/2505.01796",
    "authors": [
      "Erfan Delfani",
      "Agapi Mesodiakaki",
      "Nikolaos Pappas"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.01810",
    "title": "Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees",
    "abstract": "           With the advancement of Internet of Things (IoT) technologies, high-precision indoor positioning has become essential for Location-Based Services (LBS) in complex indoor environments. Fingerprint-based localization is popular, but traditional algorithms and deep learning-based methods face challenges such as poor generalization, overfitting, and lack of interpretability. This paper applies conformal prediction (CP) to deep learning-based indoor positioning. CP transforms the uncertainty of the model into a non-conformity score, constructs prediction sets to ensure correctness coverage, and provides statistical guarantees. We also introduce conformal risk control for path navigation tasks to manage the false discovery rate (FDR) and the false negative rate (FNR).The model achieved an accuracy of approximately 100% on the training dataset and 85% on the testing dataset, effectively demonstrating its performance and generalization capability. Furthermore, we also develop a conformal p-value framework to control the proportion of position-error points. Experiments on the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19, MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction technique can effectively approximate the target coverage, and different models have different performance in terms of prediction set size and uncertainty quantification.         ",
    "url": "https://arxiv.org/abs/2505.01810",
    "authors": [
      "Zhiyi Zhou",
      "Hexin Peng",
      "Hongyu Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01811",
    "title": "Backdoor Attacks Against Patch-based Mixture of Experts",
    "abstract": "           As Deep Neural Networks (DNNs) continue to require larger amounts of data and computational power, Mixture of Experts (MoE) models have become a popular choice to reduce computational complexity. This popularity increases the importance of considering the security of MoE architectures. Unfortunately, the security of models using a MoE architecture has not yet gained much attention compared to other DNN models. In this work, we investigate the vulnerability of patch-based MoE (pMoE) models for image classification against backdoor attacks. We examine multiple trigger generation methods and Fine-Pruning as a defense. To better understand a pMoE model's vulnerability to backdoor attacks, we investigate which factors affect the model's patch selection. Our work shows that pMoE models are highly susceptible to backdoor attacks. More precisely, we achieve high attack success rates of up to 100% with visible triggers and a 2% poisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally, we show that pruning itself is ineffective as a defense but that fine-tuning can remove the backdoor almost completely. Our results show that fine-tuning the model for five epochs reduces the attack success rate to 2.1% whilst sacrificing 1.4% accuracy.         ",
    "url": "https://arxiv.org/abs/2505.01811",
    "authors": [
      "Cedric Chan",
      "Jona te Lintelo",
      "Stjepan Picek"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.01812",
    "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge",
    "abstract": "           Humans and intelligent animals can effortlessly internalize new information (\"news\") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\\textit{contexual shadowing effect}$, where training with the news $\\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.         ",
    "url": "https://arxiv.org/abs/2505.01812",
    "authors": [
      "Core Francisco Park",
      "Zechen Zhang",
      "Hidenori Tanaka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01816",
    "title": "Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp",
    "abstract": "           The Open Radio Access Network (O-RAN) architecture is revolutionizing cellular networks with its open, multi-vendor design and AI-driven management, aiming to enhance flexibility and reduce costs. Although it has many advantages, O-RAN is not threat-free. While previous studies have mainly examined vulnerabilities arising from O-RAN's intelligent components, this paper is the first to focus on the security challenges and vulnerabilities introduced by transitioning from single-operator to multi-operator RAN architectures. This shift increases the risk of untrusted third-party operators managing different parts of the network. To explore these vulnerabilities and their potential mitigation, we developed an open-access testbed environment that integrates a wireless network simulator with the official O-RAN Software Community (OSC) RAN intelligent component (RIC) cluster. This environment enables realistic, live data collection and serves as a platform for demonstrating APATE (adversarial perturbation against traffic efficiency), an evasion attack in which a malicious cell manipulates its reported key performance indicators (KPIs) and deceives the O-RAN traffic steering to gain unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate activity continues, we introduce MARRS (monitoring adversarial RAN reports), a detection framework based on a long-short term memory (LSTM) autoencoder (AE) that learns contextual features across the network to monitor malicious telemetry (also demonstrated in our testbed). Our evaluation showed that by executing APATE, an attacker can obtain a 248.5% greater UE allocation than it was supposed to in a benign scenario. In addition, the MARRS detection method was also shown to successfully classify malicious cell activity, achieving accuracy of 99.2% and an F1 score of 0.978.         ",
    "url": "https://arxiv.org/abs/2505.01816",
    "authors": [
      "Eran Aizikovich",
      "Dudu Mimran",
      "Edita Grolman",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01818",
    "title": "Adaptive DRL for IRS Mirror Orientation in Dynamic OWC Networks",
    "abstract": "           Intelligent reflecting surfaces (IRSs) have emerged as a promising solution to mitigate line-of-sight (LoS) blockages and enhance signal coverage in optical wireless communication (OWC) systems. In this work, we consider a mirror-based IRS to assist a dynamic indoor visible light communication (VLC) environment. We formulate an optimization problem that aims to maximize the sum rate by adjusting the orientation of the IRS mirrors. To enable real-time adaptability, the problem is modelled as a Markov decision process (MDP), and a deep reinforcement learning (DRL) algorithm, specifically deep deterministic policy gradient (DDPG), is employed to optimize mirror orientation toward mobile users under blockage and mobility constraints. Simulation results demonstrate that the proposed DDPG-based approach outperforms conventional DRL algorithms and achieves substantial improvements in sum rate compared to fixed-orientation IRS configurations.         ",
    "url": "https://arxiv.org/abs/2505.01818",
    "authors": [
      "Ahrar N. Hamad",
      "Ahmad Adnan Qidan",
      "Taisir E.H. El-Gorashi",
      "Jaafar M. H. Elmirghani"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.01837",
    "title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition",
    "abstract": "           Gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. While existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. Our experiments show up to 60\\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. Current CNN and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. To tackle this challenge, we propose CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. CVVNet employs a High-Low Frequency Extraction module (HLFE) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. The integration of our core Multi-Scale Attention Gated Aggregation (MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. Experimental results show that our CVVNet achieves state-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$ on Gait3D compared with the best existing methods.         ",
    "url": "https://arxiv.org/abs/2505.01837",
    "authors": [
      "Xiangru Li",
      "Wei Song",
      "Yingda Huang",
      "Wei Meng",
      "Le Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01869",
    "title": "Visual enhancement and 3D representation for underwater scenes: a review",
    "abstract": "           Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite the development of numerous enhancement algorithms, a comprehensive and systematic review covering both UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and 3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision.         ",
    "url": "https://arxiv.org/abs/2505.01869",
    "authors": [
      "Guoxi Huang",
      "Haoran Wang",
      "Brett Seymour",
      "Evan Kovacs",
      "John Ellerbrock",
      "Dave Blackham",
      "Nantheera Anantrasirichai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01870",
    "title": "ResiTok: A Resilient Tokenization-Enabled Framework for Ultra-Low-Rate and Robust Image Transmission",
    "abstract": "           Real-time transmission of visual data over wireless networks remains highly challenging, even when leveraging advanced deep neural networks, particularly under severe channel conditions such as limited bandwidth and weak connectivity. In this paper, we propose a novel Resilient Tokenization-Enabled (ResiTok) framework designed for ultra-low-rate image transmission that achieves exceptional robustness while maintaining high reconstruction quality. By reorganizing visual information into hierarchical token groups consisting of essential key tokens and supplementary detail tokens, ResiTok enables progressive encoding and graceful degradation of visual quality under constrained channel conditions. A key contribution is our resilient 1D tokenization method integrated with a specialized zero-out training strategy, which systematically simulates token loss during training, empowering the neural network to effectively compress and reconstruct images from incomplete token sets. Furthermore, the channel-adaptive coding and modulation design dynamically allocates coding resources according to prevailing channel conditions, yielding superior semantic fidelity and structural consistency even at extremely low channel bandwidth ratios. Evaluation results demonstrate that ResiTok outperforms state-of-the-art methods in both semantic similarity and visual quality, with significant advantages under challenging channel conditions.         ",
    "url": "https://arxiv.org/abs/2505.01870",
    "authors": [
      "Zhenyu Liu",
      "Yi Ma",
      "Rahim Tafazolli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.01873",
    "title": "An Approach for Handling Missing Attribute Values in Attribute-Based Access Control Policy Mining",
    "abstract": "           Attribute-Based Access Control (ABAC) enables highly expressive and flexible access decisions by considering a wide range of contextual attributes. ABAC policies use logical expressions that combine these attributes, allowing for precise and context-aware control. Algorithms that mine ABAC policies from legacy access control systems can significantly reduce the costs associated with migrating to ABAC. However, a major challenge in this process is handling incomplete entity information, where some attribute values are missing. This paper introduces an approach that enhances the policy mining process by predicting or inferring missing attribute values. This is accomplished by employing a contextual clustering technique that groups entities according to their known attributes, which are then used to analyze and refine authorization decisions. By effectively managing incomplete data, our approach provides security administrators with a valuable tool to improve their attribute data and ensure a smoother, more efficient transition to ABAC.         ",
    "url": "https://arxiv.org/abs/2505.01873",
    "authors": [
      "Thang Bui",
      "Elliot Shabram",
      "Anthony Matricia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.01879",
    "title": "What to Do When Privacy Is Gone",
    "abstract": "           Today's ethics of privacy is largely dedicated to defending personal information from big data technologies. This essay goes in the other direction. It considers the struggle to be lost, and explores two strategies for living after privacy is gone. First, total exposure embraces privacy's decline, and then contributes to the process with transparency. All personal information is shared without reservation. The resulting ethics is explored through a big data version of Robert Nozick's Experience Machine thought experiment. Second, transient existence responds to privacy's loss by ceaselessly generating new personal identities, which translates into constantly producing temporarily unviolated private information. The ethics is explored through Gilles Deleuze's metaphysics of difference applied in linguistic terms to the formation of the self. Comparing the exposure and transience alternatives leads to the conclusion that today's big data reality splits the traditional ethical link between authenticity and freedom. Exposure provides authenticity, but negates human freedom. Transience provides freedom, but disdains authenticity.         ",
    "url": "https://arxiv.org/abs/2505.01879",
    "authors": [
      "James Brusseau"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2505.01880",
    "title": "Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network",
    "abstract": "           Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.         ",
    "url": "https://arxiv.org/abs/2505.01880",
    "authors": [
      "Junyan Wu",
      "Wenbo Xu",
      "Wei Lu",
      "Xiangyang Luo",
      "Rui Yang",
      "Shize Guo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.01881",
    "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications",
    "abstract": "           Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.         ",
    "url": "https://arxiv.org/abs/2505.01881",
    "authors": [
      "Trisanth Srinivasan",
      "Santosh Patapati"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.01882",
    "title": "CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture",
    "abstract": "           Images used in real-world applications such as image or video retrieval, outdoor surveillance, and autonomous driving suffer from poor weather conditions. When designing robust computer vision systems, removing adverse weather such as haze, rain, and snow is a significant problem. Recently, deep-learning methods offered a solution for a single type of degradation. Current state-of-the-art universal methods struggle with combinations of degradations, such as haze and rain-streak. Few algorithms have been developed that perform well when presented with images containing multiple adverse weather conditions. This work focuses on developing an efficient solution for multiple adverse weather removal using a unified quaternion neural architecture called CMAWRNet. It is based on a novel texture-structure decomposition block, a novel lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. We also introduce a quaternion similarity loss function to preserve color information better. The quantitative and qualitative evaluation of the current state-of-the-art benchmarking datasets and real-world images shows the performance advantages of the proposed CMAWRNet compared to other state-of-the-art weather removal approaches dealing with multiple weather artifacts. Extensive computer simulations validate that CMAWRNet improves the performance of downstream applications such as object detection. This is the first time the decomposition approach has been applied to the universal weather removal task.         ",
    "url": "https://arxiv.org/abs/2505.01882",
    "authors": [
      "Vladimir Frants",
      "Sos Agaian",
      "Karen Panetta",
      "Peter Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01883",
    "title": "Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams",
    "abstract": "           We present a framework for large-scale sentiment and topic analysis of Twitter discourse. Our pipeline begins with targeted data collection using conflict-specific keywords, followed by automated sentiment labeling via multiple pre-trained models to improve annotation robustness. We examine the relationship between sentiment and contextual features such as timestamp, geolocation, and lexical content. To identify latent themes, we apply Latent Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and metadata attributes. Finally, we develop an interactive visualization interface to support exploration of sentiment trends and topic distributions across time and regions. This work contributes a scalable methodology for social media analysis in dynamic geopolitical contexts.         ",
    "url": "https://arxiv.org/abs/2505.01883",
    "authors": [
      "Yiwen Lu",
      "Siheng Xiong",
      "Zhaowei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.01893",
    "title": "DriveNetBench: An Affordable and Configurable Single-Camera Benchmarking System for Autonomous Driving Networks",
    "abstract": "           Validating autonomous driving neural networks often demands expensive equipment and complex setups, limiting accessibility for researchers and educators. We introduce DriveNetBench, an affordable and configurable benchmarking system designed to evaluate autonomous driving networks using a single-camera setup. Leveraging low-cost, off-the-shelf hardware, and a flexible software stack, DriveNetBench enables easy integration of various driving models, such as object detection and lane following, while ensuring standardized evaluation in real-world scenarios. Our system replicates common driving conditions and provides consistent, repeatable metrics for comparing network performance. Through preliminary experiments with representative vision models, we illustrate how DriveNetBench effectively measures inference speed and accuracy within a controlled test environment. The key contributions of this work include its affordability, its replicability through open-source software, and its seamless integration into existing workflows, making autonomous vehicle research more accessible.         ",
    "url": "https://arxiv.org/abs/2505.01893",
    "authors": [
      "Ali Al-Bustami",
      "Humberto Ruiz-Ochoa",
      "Jaerock Kwon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.01900",
    "title": "CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation",
    "abstract": "           Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\\% while preserving textual coherence and semantic equivalence to the original claims.         ",
    "url": "https://arxiv.org/abs/2505.01900",
    "authors": [
      "Mazal Bethany",
      "Nishant Vishwamitra",
      "Cho-Yu Jason Chiang",
      "Peyman Najafirad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.01902",
    "title": "From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup",
    "abstract": "           Accurate prediction of FIFA World Cup match outcomes holds significant value for analysts, coaches, bettors, and fans. This paper presents a machine learning framework specifically designed to forecast match winners in FIFA World Cup. By integrating both team-level historical data and player-specific performance metrics such as goals, assists, passing accuracy, and tackles, we capture nuanced interactions often overlooked by traditional aggregate models. Our methodology processes multi-year data to create year-specific team profiles that account for evolving rosters and player development. We employ classification techniques complemented by dimensionality reduction and hyperparameter optimization, to yield robust predictive models. Experimental results on data from the FIFA 2022 World Cup demonstrate our approach's superior accuracy compared to baseline method. Our findings highlight the importance of incorporating individual player attributes and team-level composition to enhance predictive performance, offering new insights into player synergy, strategic match-ups, and tournament progression scenarios. This work underscores the transformative potential of rich, player-centric data in sports analytics, setting a foundation for future exploration of advanced learning architectures such as graph neural networks to model complex team interactions.         ",
    "url": "https://arxiv.org/abs/2505.01902",
    "authors": [
      "Ali Al-Bustami",
      "Zaid Ghazal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01930",
    "title": "A Matrix Product State Representation of Boolean Functions",
    "abstract": "           We introduce a novel normal form representation of Boolean functions in terms of products of binary matrices, hereafter referred to as the Binary Matrix Product (BMP) representation. BMPs are analogous to the Tensor-Trains (TT) and Matrix Product States (MPS) used, respectively, in applied mathematics and in quantum many-body physics to accelerate computations that are usually inaccessible by more traditional approaches. BMPs turn out to be closely related to Binary Decision Diagrams (BDDs), a powerful compressed representation of Boolean functions invented in the late 80s by Bryant that has found a broad range of applications in many areas of computer science and engineering. We present a direct and natural translation of BMPs into Binary Decision Diagrams (BDDs), and derive an elementary set of operations used to manipulate and combine BMPs that are analogous to those introduced by Bryant for BDDs. Both BDDs and BMPs are practical tools when the complexity of these representations, as measured by the maximum bond dimension of a BMP (or the accumulated bond dimension across the BMP matrix train) and the number of nodes of a BDD, remains polynomial in the number of bits, $n$. In both cases, controlling the complexity hinges on optimizing the order of the Boolean variables. BMPs offer the advantage that their construction and manipulation rely on simple linear algebra -- a compelling feature that can facilitate the development of open-source libraries that are both more flexible and easier to use than those currently available for BDDs. An initial implementation of a BMP library is available on GitHub, with the expectation that the close conceptual connection to TT and MPS techniques will motivate further development of BMP methods by researchers in these fields, potentially enabling novel applications to classical and quantum computing.         ",
    "url": "https://arxiv.org/abs/2505.01930",
    "authors": [
      "Umut Eren Usturali",
      "Claudio Chamon",
      "Andrei E. Ruckenstein",
      "Eduardo R. Mucciolo"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2505.01938",
    "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder",
    "abstract": "           Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.01938",
    "authors": [
      "Qi Yang",
      "Le Yang",
      "Geert Van Der Auwera",
      "Zhu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.01946",
    "title": "Embedding based retrieval for long tail search queries in ecommerce",
    "abstract": "           In this abstract we present a series of optimizations we performed on the two-tower model architecture [14], training and evaluation datasets to implement semantic product search at Best Buy. Search queries on this http URL follow the pareto distribution whereby a minority of them account for most searches. This leaves us with a long tail of search queries that have low frequency of issuance. The queries in the long tail suffer from very spare interaction signals. Our current work focuses on building a model to serve the long tail queries. We present a series of optimizations we have done to this model to maximize conversion for the purpose of retrieval from the catalog. The first optimization we present is using a large language model to improve the sparsity of conversion signals. The second optimization is pretraining an off-the-shelf transformer-based model on the Best Buy catalog data. The third optimization we present is on the finetuning front. We use query-to-query pairs in addition to query-to-product pairs and combining the above strategies for finetuning the model. We also demonstrate how merging the weights of these finetuned models improves the evaluation metrics. Finally, we provide a recipe for curating an evaluation dataset for continuous monitoring of model performance with human-in-the-loop evaluation. We found that adding this recall mechanism to our current term match-based recall improved conversion by 3% in an online A/B test.         ",
    "url": "https://arxiv.org/abs/2505.01946",
    "authors": [
      "Akshay Kekuda",
      "Yuyang Zhang",
      "Arun Udayashankar"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2505.01947",
    "title": "Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach",
    "abstract": "           UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.         ",
    "url": "https://arxiv.org/abs/2505.01947",
    "authors": [
      "Ivan Tan",
      "Wei Minn",
      "Christopher M. Poskitt",
      "Lwin Khin Shar",
      "Lingxiao Jiang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.01948",
    "title": "Multi-Scale Graph Learning for Anti-Sparse Downscaling",
    "abstract": "           Water temperature can vary substantially even across short distances within the same sub-watershed. Accurate prediction of stream water temperature at fine spatial resolutions (i.e., fine scales, $\\leq$ 1 km) enables precise interventions to maintain water quality and protect aquatic habitats. Although spatiotemporal models have made substantial progress in spatially coarse time series modeling, challenges persist in predicting at fine spatial scales due to the lack of data at that this http URL address the problem of insufficient fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This method employs a multi-task learning framework where coarse-scale graph learning, bolstered by larger datasets, simultaneously enhances fine-scale graph learning. Although existing multi-scale or multi-resolution methods integrate data from different spatial scales, they often overlook the spatial correspondences across graph structures at various scales. To address this, our MSGL introduces an additional learning task, cross-scale interpolation learning, which leverages the hydrological connectedness of stream locations across coarse- and fine-scale graphs to establish cross-scale connections, thereby enhancing overall model performance. Furthermore, we have broken free from the mindset that multi-scale learning is limited to synchronous training by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL). Extensive experiments demonstrate the state-of-the-art performance of our method for anti-sparse downscaling of daily stream temperatures in the Delaware River Basin, USA, highlighting its potential utility for water resources monitoring and management.         ",
    "url": "https://arxiv.org/abs/2505.01948",
    "authors": [
      "Yingda Fan",
      "Runlong Yu",
      "Janet R. Barclay",
      "Alison P. Appling",
      "Yiming Sun",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.01967",
    "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview",
    "abstract": "           Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or \"worldviews\". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies.         ",
    "url": "https://arxiv.org/abs/2505.01967",
    "authors": [
      "Jiatao Li",
      "Yanheng Li",
      "Xiaojun Wan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2505.01969",
    "title": "MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection",
    "abstract": "           3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance.         ",
    "url": "https://arxiv.org/abs/2505.01969",
    "authors": [
      "Jiayi Cheng",
      "Can Gao",
      "Jie Zhou",
      "Jiajun Wen",
      "Tao Dai",
      "Jinbao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01976",
    "title": "A Survey on Privacy Risks and Protection in Large Language Models",
    "abstract": "           Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.         ",
    "url": "https://arxiv.org/abs/2505.01976",
    "authors": [
      "Kang Chen",
      "Xiuze Zhou",
      "Yuanguo Lin",
      "Shibo Feng",
      "Li Shen",
      "Pengcheng Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.01979",
    "title": "D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection",
    "abstract": "           Current Hierarchical Reinforcement Learning (HRL) algorithms excel in long-horizon sequential decision-making tasks but still face two challenges: delay effects and spurious correlations. To address them, we propose a causal HRL approach called D3HRL. First, D3HRL models delayed effects as causal relationships across different time spans and employs distributed causal discovery to learn these relationships. Second, it employs conditional independence testing to eliminate spurious correlations. Finally, D3HRL constructs and trains hierarchical policies based on the identified true causal relationships. These three steps are iteratively executed, gradually exploring the complete causal chain of the task. Experiments conducted in 2D-MineCraft and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects and accurately identifies causal relationships, leading to reliable decision-making in complex environments.         ",
    "url": "https://arxiv.org/abs/2505.01979",
    "authors": [
      "Chenran Zhao",
      "Dianxi Shi",
      "Mengzhu Wang",
      "Jianqiang Xia",
      "Huanhuan Yang",
      "Songchang Jin",
      "Shaowu Yang",
      "Chunping Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01988",
    "title": "Sparse Code Transceiver Design for Unsourced Random Access with Analytical Power Division in Gaussian MAC",
    "abstract": "           In this work, we discuss the problem of unsourced random access (URA) over a Gaussian multiple access channel (GMAC). To address the challenges posed by emerging massive machine-type connectivity, URA reframes multiple access as a coding-theoretic problem. The sparse code-oriented schemes are highly valued because they are widely used in existing protocols, making their implementation require only minimal changes to current networks. However, drawbacks such as the heavy reliance on extrinsic feedback from powerful channel codes and the lack of transmission robustness pose obstacles to the development of sparse codes. To address these drawbacks, a novel sparse code structure based on a universally applicable power division strategy is proposed. Comprehensive numerical results validate the effectiveness of the proposed scheme. Specifically, by employing the proposed power division method, which is derived analytically and does not require extensive simulations, a performance improvement of approximately 2.8 dB is achieved compared to schemes with identical channel code setups.         ",
    "url": "https://arxiv.org/abs/2505.01988",
    "authors": [
      "Zhentian Zhang",
      "Mohammad Javad Ahmadi",
      "Jian Dang",
      "Kai-Kit Wong",
      "Zaichen Zhang",
      "Christos Masouros"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.02005",
    "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields",
    "abstract": "           Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02005",
    "authors": [
      "Zhenxing Mi",
      "Ping Yin",
      "Xue Xiao",
      "Dan Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02018",
    "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation",
    "abstract": "           Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here.         ",
    "url": "https://arxiv.org/abs/2505.02018",
    "authors": [
      "Meng-Hao Guo",
      "Jiajun Xu",
      "Yi Zhang",
      "Jiaxi Song",
      "Haoyang Peng",
      "Yi-Xuan Deng",
      "Xinzhi Dong",
      "Kiyohiro Nakayama",
      "Zhengyang Geng",
      "Chen Wang",
      "Bolin Ni",
      "Guo-Wei Yang",
      "Yongming Rao",
      "Houwen Peng",
      "Han Hu",
      "Gordon Wetzstein",
      "Shi-min Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02027",
    "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning",
    "abstract": "           Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02027",
    "authors": [
      "Rui Lv",
      "Zaixi Zhang",
      "Kai Zhang",
      "Qi Liu",
      "Weibo Gao",
      "Jiawei Liu",
      "Jiaxia Yan",
      "Linan Yue",
      "Fangzhou Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.02043",
    "title": "Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction",
    "abstract": "           Recovering CAD models from point clouds, especially the sketch-extrusion process, can be seen as the process of rebuilding the topology and extrusion primitives. Previous methods utilize implicit fields for sketch representation, leading to shape reconstruction of curved edges. In this paper, we proposed a CAD reconstruction network that produces editable CAD models from input point clouds (Point2Primitive) by directly predicting every element of the extrusion primitives. Point2Primitive can directly detect and predict sketch curves (type and parameter) from point clouds based on an improved transformer. The sketch curve parameters are formulated as position queries and optimized in an autoregressive way, leading to high parameter accuracy. The topology is rebuilt by extrusion segmentation, and each extrusion parameter (sketch and extrusion operation) is recovered by combining the predicted curves and the computed extrusion operation. Extensive experiments demonstrate that our method is superior in primitive prediction accuracy and CAD reconstruction. The reconstructed shapes are of high geometrical fidelity.         ",
    "url": "https://arxiv.org/abs/2505.02043",
    "authors": [
      "Cheng Wang",
      "Xinzhu Ma",
      "Bin Wang",
      "Shixiang Tang",
      "Yuan Meng",
      "Ping Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02050",
    "title": "Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks",
    "abstract": "           Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems.         ",
    "url": "https://arxiv.org/abs/2505.02050",
    "authors": [
      "Kranthi Kumar Talluri",
      "Anders L. Madsen",
      "Galia Weidl"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.02069",
    "title": "Neural Logistic Bandits",
    "abstract": "           We study the problem of neural logistic bandits, where the main task is to learn an unknown reward function within a logistic link function using a neural network. Existing approaches either exhibit unfavorable dependencies on $\\kappa$, where $1/\\kappa$ represents the minimum variance of reward distributions, or suffer from direct dependence on the feature dimension $d$, which can be huge in neural network-based settings. In this work, we introduce a novel Bernstein-type inequality for self-normalized vector-valued martingales that is designed to bypass a direct dependence on the ambient dimension. This lets us deduce a regret upper bound that grows with the effective dimension $\\widetilde{d}$, not the feature dimension, while keeping a minimal dependence on $\\kappa$. Based on the concentration inequality, we propose two algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of order $\\widetilde{O}(\\widetilde{d}\\sqrt{\\kappa T})$ and $\\widetilde{O}(\\widetilde{d}\\sqrt{T/\\kappa})$, respectively, improving on the existing results. Lastly, we report numerical results on both synthetic and real datasets to validate our theoretical findings.         ",
    "url": "https://arxiv.org/abs/2505.02069",
    "authors": [
      "Seoungbin Bae",
      "Dabeen Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02072",
    "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction",
    "abstract": "           The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.         ",
    "url": "https://arxiv.org/abs/2505.02072",
    "authors": [
      "Eitan Wagner",
      "Omri Abend"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02073",
    "title": "Lightweight Defense Against Adversarial Attacks in Time Series Classification",
    "abstract": "           As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.         ",
    "url": "https://arxiv.org/abs/2505.02073",
    "authors": [
      "Yi Han"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02074",
    "title": "Learning Local Causal World Models with State Space Models and Attention",
    "abstract": "           World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Despite their impressive performance, many solutions fail to learn a causal representation of the environment they are trying to model, which would be necessary to gain a deep enough understanding of the world to perform complex tasks. With this work, we aim to broaden the research in the intersection of causality theory and neural world modelling by assessing the potential for causal discovery of the State Space Model (SSM) architecture, which has been shown to have several advantages over the widespread Transformer. We show empirically that, compared to an equivalent Transformer, a SSM can model the dynamics of a simple environment and learn a causal model at the same time with equivalent or better performance, thus paving the way for further experiments that lean into the strength of SSMs and further enhance them with causal awareness.         ",
    "url": "https://arxiv.org/abs/2505.02074",
    "authors": [
      "Francesco Petri",
      "Luigi Asprino",
      "Aldo Gangemi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02079",
    "title": "HandOcc: NeRF-based Hand Rendering with Occupancy Networks",
    "abstract": "           We propose HandOcc, a novel framework for hand rendering based upon occupancy. Popular rendering methods such as NeRF are often combined with parametric meshes to provide deformable hand models. However, in doing so, such approaches present a trade-off between the fidelity of the mesh and the complexity and dimensionality of the parametric model. The simplicity of parametric mesh structures is appealing, but the underlying issue is that it binds methods to mesh initialization, making it unable to generalize to objects where a parametric model does not exist. It also means that estimation is tied to mesh resolution and the accuracy of mesh fitting. This paper presents a pipeline for meshless 3D rendering, which we apply to the hands. By providing only a 3D skeleton, the desired appearance is extracted via a convolutional model. We do this by exploiting a NeRF renderer conditioned upon an occupancy-based representation. The approach uses the hand occupancy to resolve hand-to-hand interactions further improving results, allowing fast rendering, and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset, we achieved state-of-the-art results.         ",
    "url": "https://arxiv.org/abs/2505.02079",
    "authors": [
      "Maksym Ivashechkin",
      "Oscar Mendez",
      "Richard Bowden"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02094",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations",
    "abstract": "           We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.         ",
    "url": "https://arxiv.org/abs/2505.02094",
    "authors": [
      "Runyi Yu",
      "Yinhuai Wang",
      "Qihan Zhao",
      "Hok Wai Tsui",
      "Jingbo Wang",
      "Ping Tan",
      "Qifeng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02096",
    "title": "TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing",
    "abstract": "           Audio-Visual Video Parsing (AVVP) task aims to parse the event categories and occurrence times from audio and visual modalities in a given video. Existing methods usually focus on implicitly modeling audio and visual features through weak labels, without mining semantic relationships for different modalities and explicit modeling of event temporal dependencies. This makes it difficult for the model to accurately parse event information for each segment under weak supervision, especially when high similarity between segmental modal features leads to ambiguous event boundaries. Hence, we propose a multimodal optimization framework, TeMTG, that combines text enhancement and multi-hop temporal graph modeling. Specifically, we leverage pre-trained multimodal models to generate modality-specific text embeddings, and fuse them with audio-visual features to enhance the semantic representation of these features. In addition, we introduce a multi-hop temporal graph neural network, which explicitly models the local temporal relationships between segments, capturing the temporal continuity of both short-term and long-range events. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators in the LLP dataset.         ",
    "url": "https://arxiv.org/abs/2505.02096",
    "authors": [
      "Yaru Chen",
      "Peiliang Zhang",
      "Fei Li",
      "Faegheh Sardari",
      "Ruohao Guo",
      "Zhenbo Li",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2505.02105",
    "title": "Deep Representation Learning for Electronic Design Automation",
    "abstract": "           Representation learning has become an effective technique utilized by electronic design automation (EDA) algorithms, which leverage the natural representation of workflow elements as images, grids, and graphs. By addressing challenges related to the increasing complexity of circuits and stringent power, performance, and area (PPA) requirements, representation learning facilitates the automatic extraction of meaningful features from complex data formats, including images, grids, and graphs. This paper examines the application of representation learning in EDA, covering foundational concepts and analyzing prior work and case studies on tasks that include timing prediction, routability analysis, and automated placement. Key techniques, including image-based methods, graph-based approaches, and hybrid multimodal solutions, are presented to illustrate the improvements provided in routing, timing, and parasitic prediction. The provided advancements demonstrate the potential of representation learning to enhance efficiency, accuracy, and scalability in current integrated circuit design flows.         ",
    "url": "https://arxiv.org/abs/2505.02105",
    "authors": [
      "Pratik Shrestha",
      "Saran Phatharodom",
      "Alec Aversa",
      "David Blankenship",
      "Zhengfeng Wu",
      "Ioannis Savidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02118",
    "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets",
    "abstract": "           This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).         ",
    "url": "https://arxiv.org/abs/2505.02118",
    "authors": [
      "Wei Liu",
      "Zhongyu Niu",
      "Lang Gao",
      "Zhiying Deng",
      "Jun Wang",
      "Haozhao Wang",
      "Ruixuan Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02124",
    "title": "GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code",
    "abstract": "           Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a program that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.         ",
    "url": "https://arxiv.org/abs/2505.02124",
    "authors": [
      "Samidha Verma",
      "Arushi Goyal",
      "Ananya Mathur",
      "Ankit Anand",
      "Sayan Ranu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02133",
    "title": "Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency",
    "abstract": "           The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.         ",
    "url": "https://arxiv.org/abs/2505.02133",
    "authors": [
      "Nazmus Ashrafi",
      "Salah Bouktif",
      "Mohammed Mediani"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.02139",
    "title": "Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking",
    "abstract": "           The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02139",
    "authors": [
      "Muyao Zhong",
      "Yushi Lin",
      "Peng Yang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02154",
    "title": "Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions",
    "abstract": "           This reproducibility study analyzes and extends the paper \"Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models,\" which investigates how neural retrieval models encode task-relevant properties such as term frequency. We reproduce key experiments from the original paper, confirming that information on query terms is captured in the model encoding. We extend this work by applying activation patching to Spanish and Chinese datasets and by exploring whether document-length information is encoded in the model as well. Our results confirm that the designed activation patching method can isolate the behavior to specific components and tokens in neural retrieval models. Moreover, our findings indicate that the location of term frequency generalizes across languages and that in later layers, the information for sequence-level tasks is represented in the CLS token. The results highlight the need for further research into interpretability in information retrieval and reproducibility in machine learning research. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02154",
    "authors": [
      "Oliver Savolainen",
      "Dur e Najaf Amjad",
      "Roxana Petcu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02156",
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents",
    "abstract": "           Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach         ",
    "url": "https://arxiv.org/abs/2505.02156",
    "authors": [
      "Minzheng Wang",
      "Yongbin Li",
      "Haobo Wang",
      "Xinghua Zhang",
      "Nan Xu",
      "Bingli Wu",
      "Fei Huang",
      "Haiyang Yu",
      "Wenji Mao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02168",
    "title": "CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design",
    "abstract": "           The rapid advancements of AI rely on the support of ICs. However, the growing complexity of digital ICs makes the traditional IC design process costly and time-consuming. In recent years, AI-assisted IC design methods have demonstrated great potential, but most methods are task-specific or focus solely on the circuit structure in graph format, overlooking other circuit modalities with rich functional information. In this paper, we introduce CircuitFusion, the first multimodal and implementation-aware circuit encoder. It encodes circuits into general representations that support different downstream circuit design tasks. To learn from circuits, we propose to fuse three circuit modalities: hardware code, structural graph, and functionality summary. More importantly, we identify four unique properties of circuits: parallel execution, functional equivalent transformation, multiple design stages, and circuit reusability. Based on these properties, we propose new strategies for both the development and application of CircuitFusion: 1) During circuit preprocessing, utilizing the parallel nature of circuits, we split each circuit into multiple sub-circuits based on sequential-element boundaries, each sub-circuit in three modalities. 2) During CircuitFusion pre-training, we introduce three self-supervised tasks that utilize equivalent transformations both within and across modalities. 3) When applying CircuitFusion to downstream tasks, we propose a new retrieval-augmented inference method, which retrieves similar known circuits as a reference for predictions. It improves fine-tuning performance and even enables zero-shot inference. Evaluated on five different circuit design tasks, CircuitFusion consistently outperforms the SOTA supervised method specifically developed for every single task, demonstrating its generalizability and ability to learn circuits' inherent properties.         ",
    "url": "https://arxiv.org/abs/2505.02168",
    "authors": [
      "Wenji Fang",
      "Shang Liu",
      "Jing Wang",
      "Zhiyao Xie"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2505.02176",
    "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection",
    "abstract": "           Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated \"pseudosaliency,\" including minutiae-based, image quality-based, and autoencoder-based saliency maps. Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. Our findings demonstrate the effectiveness of saliency-guided training for fingerprint PAD in both limited and large data contexts, and we present a configuration capable of earning the first place on the LivDet-2021 benchmark. Our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint PAD. All collected saliency data and trained models are released with the paper to support reproducible research.         ",
    "url": "https://arxiv.org/abs/2505.02176",
    "authors": [
      "Samuel Webster",
      "Adam Czajka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02179",
    "title": "ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications",
    "abstract": "           Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP, demonstrating exceptional efficiency alongside state-of-the-art performance. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02179",
    "authors": [
      "Tao Zhu",
      "Qi Yu",
      "Xinru Dong",
      "Shiyu Li",
      "Yue Liu",
      "Jinlong Jiang",
      "Lei Shu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02182",
    "title": "Robust AI-Generated Face Detection with Imbalanced Data",
    "abstract": "           Deepfakes, created using advanced AI techniques such as Variational Autoencoder and Generative Adversarial Networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. Current deepfake detection techniques have evolved from CNN-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like CLIP, which capture global anomalies and improve cross-domain generalization. Despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. To address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02182",
    "authors": [
      "Yamini Sri Krubha",
      "Aryana Hou",
      "Braden Vester",
      "Web Walker",
      "Xin Wang",
      "Li Lin",
      "Shu Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02236",
    "title": "Improving Physical Object State Representation in Text-to-Image Generative Systems",
    "abstract": "           Current text-to-image generative models struggle to accurately represent object states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. Next, we fine-tune several open-source text-to-image models on this synthetic data. We evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using GPT4o-mini, and achieve an average absolute improvement of 8+% across four models on the public GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific focus on common objects in various physical states. We demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. We release all evaluation prompts and code.         ",
    "url": "https://arxiv.org/abs/2505.02236",
    "authors": [
      "Tianle Chen",
      "Chaitanya Chakka",
      "Deepti Ghadiyaram"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02238",
    "title": "Federated Causal Inference in Healthcare: Methods, Challenges, and Applications",
    "abstract": "           Federated causal inference enables multi-site treatment effect estimation without sharing individual-level data, offering a privacy-preserving solution for real-world evidence generation. However, data heterogeneity across sites, manifested in differences in covariate, treatment, and outcome, poses significant challenges for unbiased and efficient estimation. In this paper, we present a comprehensive review and theoretical analysis of federated causal effect estimation across both binary/continuous and time-to-event outcomes. We classify existing methods into weight-based strategies and optimization-based frameworks and further discuss extensions including personalized models, peer-to-peer communication, and model decomposition. For time-to-event outcomes, we examine federated Cox and Aalen-Johansen models, deriving asymptotic bias and variance under heterogeneity. Our analysis reveals that FedProx-style regularization achieves near-optimal bias-variance trade-offs compared to naive averaging and meta-analysis. We review related software tools and conclude by outlining opportunities, challenges, and future directions for scalable, fair, and trustworthy federated causal inference in distributed healthcare systems.         ",
    "url": "https://arxiv.org/abs/2505.02238",
    "authors": [
      "Haoyang Li",
      "Jie Xu",
      "Kyra Gan",
      "Fei Wang",
      "Chengxi Zang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02247",
    "title": "RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation",
    "abstract": "           3D Geometric Graph Neural Networks (GNNs) have emerged as transformative tools for modeling molecular data. Despite their predictive power, these models often suffer from limited interpretability, raising concerns for scientific applications that require reliable and transparent insights. While existing methods have primarily focused on explaining molecular substructures in 2D GNNs, the transition to 3D GNNs introduces unique challenges, such as handling the implicit dense edge structures created by a cut-off radius. To tackle this, we introduce a novel explanation method specifically designed for 3D GNNs, which localizes the explanation to the immediate neighborhood of each node within the 3D space. Each node is assigned an radius of influence, defining the localized region within which message passing captures spatial and structural interactions crucial for the model's predictions. This method leverages the spatial and geometric characteristics inherent in 3D graphs. By constraining the subgraph to a localized radius of influence, the approach not only enhances interpretability but also aligns with the physical and structural dependencies typical of 3D graph applications, such as molecular learning.         ",
    "url": "https://arxiv.org/abs/2505.02247",
    "authors": [
      "Jingxiang Qu",
      "Wenhan Gao",
      "Jiaxing Zhang",
      "Xufeng Liu",
      "Hua Wei",
      "Haibin Ling",
      "Yi Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2505.02252",
    "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models",
    "abstract": "           Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided.         ",
    "url": "https://arxiv.org/abs/2505.02252",
    "authors": [
      "Paloma Piot",
      "Patricia Mart\u00edn-Rodilla",
      "Javier Parapar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.02258",
    "title": "Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks",
    "abstract": "           Dielectric response (DR) of insulating materials is key input information for designing electrical insulation systems and defining safe operating conditions of various HV devices. In dielectric materials, different polarization and conduction processes occur at different time scales, making it challenging to physically interpret raw measured data. To analyze DR measurement results, equivalent circuit models (ECMs) are commonly used, reducing the complexity of the physical system to a number of circuit elements that capture the dominant response. This paper examines the use of physics-informed neural networks (PINNs) for inverse modeling of DR in time domain using parallel RC circuits. To assess their performance, we test PINNs on synthetic data generated from analytical solutions of corresponding ECMs, incorporating Gaussian noise to simulate measurement errors. Our results show that PINNs are highly effective at solving well-conditioned inverse problems, accurately estimating up to five unknown RC parameters with minimal requirements on neural network size, training duration, and hyperparameter tuning. Furthermore, we extend the ECMs to incorporate temperature dependence and demonstrate that PINNs can accurately recover embedded, nonlinear temperature functions from noisy DR data sampled at different temperatures. This case study in modeling DR in time domain presents a solution with wide-ranging potential applications in disciplines relying on ECMs, utilizing the latest technology in machine learning for scientific computation.         ",
    "url": "https://arxiv.org/abs/2505.02258",
    "authors": [
      "Emir Esenov",
      "Olof Hjortstam",
      "Yuriy Serdyuk",
      "Thomas Hammarstr\u00f6m",
      "Christian H\u00e4ger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2505.02268",
    "title": "Phantom Domain Finite Element Method: A novel approach for heterogeneous materials",
    "abstract": "           In this paper, we introduce the Phantom Domain Finite Element Method (PDFEM), a novel computational approach tailored for the efficient analysis of heterogeneous and composite materials. Inspired by fictitious domain methods, this method employs a structured mesh to discretize the entire material domain while utilizing separate, independent meshes for the inclusions. These inclusion meshes are coupled to the structured mesh via a substitution matrix, enabling them to act as phantom meshes that do not directly contribute to the final system of equations. This framework offers significant advantages, including enhanced flexibility in handling complex inclusion geometries and improved computational efficiency. To assess the accuracy and robustness of the proposed method, numerical experiments are conducted on structures containing inclusions of various geometries. In order to emphasize the efficiency of the PDFEM method, a numerical simulation is presented to highlight its advantages in the case of long natural fibers, such as flax and linen. These simulations are compared against FEM calculations, demonstrating the efficiency of PDFEM. Indeed, meshing such fine structures requires an extremely high number of elements, and in some cases, meshing becomes particularly challenging due to the complexity of the geometries.         ",
    "url": "https://arxiv.org/abs/2505.02268",
    "authors": [
      "Tianlong He",
      "Philippe Karamian-Surville",
      "Daniel Cho\u00ef"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.02272",
    "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots",
    "abstract": "           Quadruped robots are currently a widespread platform for robotics research, thanks to powerful Reinforcement Learning controllers and the availability of cheap and robust commercial platforms. However, to broaden the adoption of the technology in the real world, we require robust navigation stacks relying only on low-cost sensors such as depth cameras. This paper presents a first step towards a robust localization, mapping, and navigation system for low-cost quadruped robots. In pursuit of this objective we combine contact-aided kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing stability and accuracy of the system. Our results in simulation and two different real-world quadruped platforms show that our system can generate an accurate 2D map of the environment, robustly localize itself, and navigate autonomously. Furthermore, we present in-depth ablation studies of the important components of the system and their impact on localization accuracy. Videos, code, and additional experiments can be found on the project website: this https URL ",
    "url": "https://arxiv.org/abs/2505.02272",
    "authors": [
      "Dyuman Aditya",
      "Junning Huang",
      "Nico Bohlinger",
      "Piotr Kicki",
      "Krzysztof Walas",
      "Jan Peters",
      "Matteo Luperto",
      "Davide Tateo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02279",
    "title": "A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)",
    "abstract": "           Large language model (LLM)-powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in distinct deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP introduces REST-native messaging via multi-part messages and asynchronous streaming to support multimodal agent responses. A2A enables peer-to-peer task outsourcing through capability-based Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network agent discovery and secure collaboration using decentralized identifiers (DIDs) and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for multimodal messaging, A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.         ",
    "url": "https://arxiv.org/abs/2505.02279",
    "authors": [
      "Abul Ehtesham",
      "Aditi Singh",
      "Gaurav Kumar Gupta",
      "Saket Kumar"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02284",
    "title": "Conformal Prediction for Verifiable Learned Query Optimization",
    "abstract": "           Query optimization is critical in relational databases. Recently, numerous Learned Query Optimizers (LQOs) have been proposed, demonstrating superior performance over traditional hand-crafted query optimizers after short training periods. However, the opacity and instability of machine learning models have limited their practical applications. To address this issue, we are the first to formulate the LQO verification as a Conformal Prediction (CP) problem. We first construct the CP model and obtain user-controlled bounded ranges for the actual latency of LQO plans before execution. Then, we introduce CP-based runtime verification along with violation handling to ensure performance prior to execution. For both scenarios, we further extend our framework to handle distribution shifts in the dynamic environment using adaptive CP approaches. Finally, we present CP-guided plan search, which uses actual latency upper bounds from CP to heuristically guide query plan construction. We integrated our verification framework into three LQOs (Balsa, Lero, and RTOS) and conducted evaluations on the JOB and TPC-H workloads. Experimental results demonstrate that our method is both accurate and efficient. Our CP-based approaches achieve tight upper bounds, reliably detect and handle violations. Adaptive CP maintains accurate confidence levels even in the presence of distribution shifts, and the CP-guided plan search improves both query plan quality (up to 9.84x) and planning time, with a reduction of up to 74.4% for a single query and 9.96% across all test queries from trained LQOs.         ",
    "url": "https://arxiv.org/abs/2505.02284",
    "authors": [
      "Hanwen Liu",
      "Shashank Giridhara",
      "Ibrahim Sabek"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2505.02294",
    "title": "RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation",
    "abstract": "           Autonomous safe navigation in unstructured and novel environments poses significant challenges, especially when environment information can only be provided through low-cost vision sensors. Although safe reactive approaches have been proposed to ensure robot safety in complex environments, many base their theory off the assumption that the robot has prior knowledge on obstacle locations and geometries. In this paper, we present a real-time, vision-based framework that constructs continuous, first-order differentiable Signed Distance Fields (SDFs) of unknown environments without any pre-training. Our proposed method ensures full compatibility with established SDF-based reactive controllers. To achieve robust performance under practical sensing conditions, our approach explicitly accounts for noise in affordable RGB-D cameras, refining the neural SDF representation online for smoother geometry and stable gradient estimates. We validate the proposed method in simulation and real-world experiments using a Fetch robot.         ",
    "url": "https://arxiv.org/abs/2505.02294",
    "authors": [
      "Satyajeet Das",
      "Yifan Xue",
      "Haoming Li",
      "Nadia Figueroa"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.02299",
    "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
    "abstract": "           Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \\emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control.         ",
    "url": "https://arxiv.org/abs/2505.02299",
    "authors": [
      "Daisuke Yamada",
      "Harit Vishwakarma",
      "Ramya Korlakai Vinayak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02308",
    "title": "Enabling Local Neural Operators to perform Equation-Free System-Level Analysis",
    "abstract": "           Neural Operators (NOs) provide a powerful framework for computations involving physical laws that can be modelled by (integro-) partial differential equations (PDEs), directly learning maps between infinite-dimensional function spaces that bypass both the explicit equation identification and their subsequent numerical solving. Still, NOs have so far primarily been employed to explore the dynamical behavior as surrogates of brute-force temporal simulations/predictions. Their potential for systematic rigorous numerical system-level tasks, such as fixed-point, stability, and bifurcation analysis - crucial for predicting irreversible transitions in real-world phenomena - remains largely unexplored. Toward this aim, inspired by the Equation-Free multiscale framework, we propose and implement a framework that integrates (local) NOs with advanced iterative numerical methods in the Krylov subspace, so as to perform efficient system-level stability and bifurcation analysis of large-scale dynamical systems. Beyond fixed point, stability, and bifurcation analysis enabled by local in time NOs, we also demonstrate the usefulness of local in space as well as in space-time (\"patch\") NOs in accelerating the computer-aided analysis of spatiotemporal dynamics. We illustrate our framework via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE, which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN) model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node bifurcations.         ",
    "url": "https://arxiv.org/abs/2505.02308",
    "authors": [
      "Gianluca Fabiani",
      "Hannes Vandecasteele",
      "Somdatta Goswami",
      "Constantinos Siettos",
      "Ioannis G. Kevrekidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02331",
    "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection",
    "abstract": "           Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations.         ",
    "url": "https://arxiv.org/abs/2505.02331",
    "authors": [
      "Hao Cheng",
      "Zhiwei Zhao",
      "Yichao He",
      "Zhenzhen Hu",
      "Jia Li",
      "Meng Wang",
      "Richang Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2505.02343",
    "title": "Social Correction on Social Media: A Quantitative Analysis of Comment Behaviour and Reliability",
    "abstract": "           Corrections given by ordinary social media users, also referred to as Social Correction have emerged as a viable intervention against misinformation as per the recent literature. However, little is known about how often users give disputing or endorsing comments and how reliable those comments are. An online experiment was conducted to investigate how users' credibility evaluations of social media posts and their confidence in those evaluations combined with online reputational concerns affect their commenting behaviour. The study found that participants exhibited a more conservative approach when giving disputing comments compared to endorsing ones. Nevertheless, participants were more discerning in their disputing comments than endorsing ones. These findings contribute to a better understanding of social correction on social media and highlight the factors influencing comment behaviour and reliability.         ",
    "url": "https://arxiv.org/abs/2505.02343",
    "authors": [
      "Sameera S. Vithanage",
      "Keith Ransom",
      "Antonette Mendoza",
      "Shanika Karunasekera"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.02346",
    "title": "An Empirical Study on the Performance and Energy Usage of Compiled Python Code",
    "abstract": "           Python is a popular programming language known for its ease of learning and extensive libraries. However, concerns about performance and energy consumption have led to the development of compilers to enhance Python code efficiency. Despite the proven benefits of existing compilers on the efficiency of Python code, there is limited analysis comparing their performance and energy efficiency, particularly considering code characteristics and factors like CPU frequency and core count. Our study investigates how compilation impacts the performance and energy consumption of Python code, using seven benchmarks compiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython, Pyston-lite, and the experimental Python 3.13 version, compared to CPython. The benchmarks are single-threaded and executed on an NUC and a server, measuring energy usage, execution time, memory usage, and Last-Level Cache (LLC) miss rates at a fixed frequency and on a single core. The results show that compilation can significantly enhance execution time, energy and memory usage, with Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements. Nuitka optimizes memory usage consistently on both testbeds. The impact of compilation on LLC miss rate is not clear since it varies considerably across benchmarks for each compiler. Our study is important for researchers and practitioners focused on improving Python code performance and energy efficiency. We outline future research directions, such as exploring caching effects on energy usage. Our findings help practitioners choose the best compiler based on their efficiency benefits and accessibility.         ",
    "url": "https://arxiv.org/abs/2505.02346",
    "authors": [
      "Vincenzo Stoico",
      "Andrei Calin Dragomir",
      "Patricia Lago"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Performance (cs.PF)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.02349",
    "title": "A Slicing-Based Approach for Detecting and Patching Vulnerable Code Clones",
    "abstract": "           Code cloning is a common practice in software development, but it poses significant security risks by propagating vulnerabilities across cloned segments. To address this challenge, we introduce srcVul, a scalable, precise detection approach that combines program slicing with Locality-Sensitive Hashing to identify vulnerable code clones and recommend patches. srcVul builds a database of vulnerability-related slices by analyzing known vulnerable programs and their corresponding patches, indexing each slice's unique structural characteristics as a vulnerability slicing vector. During clone detection, srcVul efficiently matches slicing vectors from target programs with those in the database, recommending patches upon identifying similarities. Our evaluation of srcVul against three state-of-the-art vulnerable clone detectors demonstrates its accuracy, efficiency, and scalability, achieving 91% precision and 75% recall on established vulnerability databases and open-source repositories. These results highlight srcVul's effectiveness in detecting complex vulnerability patterns across diverse codebases.         ",
    "url": "https://arxiv.org/abs/2505.02349",
    "authors": [
      "Hakam Alomari",
      "Christopher Vendome",
      "Hilal Gyawali"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.02350",
    "title": "Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation",
    "abstract": "           Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using sparse ellipsoidal radial basis function networks, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02350",
    "authors": [
      "Bobo Lian",
      "Dandan Wang",
      "Chenjian Wu",
      "Minxin Chen"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02352",
    "title": "Social Biases in Knowledge Representations of Wikidata separates Global North from Global South",
    "abstract": "           Knowledge Graphs have become increasingly popular due to their wide usage in various downstream applications, including information retrieval, chatbot development, language model construction, and many others. Link prediction (LP) is a crucial downstream task for knowledge graphs, as it helps to address the problem of the incompleteness of the knowledge graphs. However, previous research has shown that knowledge graphs, often created in a (semi) automatic manner, are not free from social biases. These biases can have harmful effects on downstream applications, especially by leading to unfair behavior toward minority groups. To understand this issue in detail, we develop a framework -- AuditLP -- deploying fairness metrics to identify biased outcomes in LP, specifically how occupations are classified as either male or female-dominated based on gender as a sensitive attribute. We have experimented with the sensitive attribute of age and observed that occupations are categorized as young-biased, old-biased, and age-neutral. We conduct our experiments on a large number of knowledge triples that belong to 21 different geographies extracted from the open-sourced knowledge graph, Wikidata. Our study shows that the variance in the biased outcomes across geographies neatly mirrors the socio-economic and cultural division of the world, resulting in a transparent partition of the Global North from the Global South.         ",
    "url": "https://arxiv.org/abs/2505.02352",
    "authors": [
      "Paramita Das",
      "Sai Keerthana Karnam",
      "Aditya Soni",
      "Animesh Mukherjee"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02360",
    "title": "Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training",
    "abstract": "           Adversarial training is a cornerstone of robust deep learning, but fast methods like the Fast Gradient Sign Method (FGSM) often suffer from Catastrophic Overfitting (CO), where models become robust to single-step attacks but fail against multi-step variants. While existing solutions rely on noise injection, regularization, or gradient clipping, we propose a novel solution that purely controls the $l^p$ training norm to mitigate CO. Our study is motivated by the empirical observation that CO is more prevalent under the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we develop a framework for generalized $l^p$ attack as a fixed point problem and craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to $l^{\\infty}$. This leads to our core insight: CO emerges when highly concentrated gradients where information localizes in few dimensions interact with aggressive norm constraints. By quantifying gradient concentration through Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM that automatically tunes the training norm based on gradient information. Extensive experiments demonstrate that this approach achieves strong robustness without requiring additional regularization or noise injection, providing a novel and theoretically-principled pathway to mitigate the CO problem.         ",
    "url": "https://arxiv.org/abs/2505.02360",
    "authors": [
      "Fares B. Mehouachi",
      "Saif Eddin Jabari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02369",
    "title": "Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks",
    "abstract": "           Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.         ",
    "url": "https://arxiv.org/abs/2505.02369",
    "authors": [
      "Juyoung Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2505.02376",
    "title": "LAMeD: LLM-generated Annotations for Memory Leak Detection",
    "abstract": "           Static analysis tools are widely used to detect software bugs and vulnerabilities but often struggle with scalability and efficiency in complex codebases. Traditional approaches rely on manually crafted annotations -- labeling functions as sources or sinks -- to track data flows, e.g., ensuring that allocated memory is eventually freed, and code analysis tools such as CodeQL, Infer, or Cooddy can use function specifications, but manual annotation is laborious and error-prone, especially for large or third-party libraries. We present LAMeD (LLM-generated Annotations for Memory leak Detection), a novel approach that leverages large language models (LLMs) to automatically generate function-specific annotations. When integrated with analyzers such as Cooddy, LAMeD significantly improves memory leak detection and reduces path explosion. We also suggest directions for extending LAMeD to broader code analysis.         ",
    "url": "https://arxiv.org/abs/2505.02376",
    "authors": [
      "Ekaterina Shemetova",
      "Ilya Shenbin",
      "Ivan Smirnov",
      "Anton Alekseev",
      "Alexey Rukhovich",
      "Sergey Nikolenko",
      "Vadim Lomshakov",
      "Irina Piontkovskaya"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.02383",
    "title": "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret",
    "abstract": "           We address differentially private stochastic bandit problems from the angles of exploring the deep connections among Thompson Sampling with Gaussian priors, Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade off privacy and regret. DP-TS-UCB satisfies $ \\tilde{O} \\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O \\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$ controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB relies on anti-concentration bounds of Gaussian distributions and links exploration mechanisms in Thompson Sampling-based algorithms and Upper Confidence Bound-based algorithms, which may be of independent interest.         ",
    "url": "https://arxiv.org/abs/2505.02383",
    "authors": [
      "Bingshan Hu",
      "Zhiming Huang",
      "Tianyue H. Zhang",
      "Mathias L\u00e9cuyer",
      "Nidhi Hegde"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02384",
    "title": "On the Equivalence of Gaussian Graphical Models Defined on Complete Bipartite Graphs",
    "abstract": "           This paper introduces two Gaussian graphical models defined on complete bipartite graphs. We show that the determinants of the precision matrices associated with the models are equal up to scale, where the scale factor only depends on model parameters. In this context, we will introduce a notion of ``equivalence\" between the two Gaussian graphical models. This equivalence has two key applications: first, it can significantly reduce the complexity of computing the exact value of the determinant, and second, it enables the derivation of closed-form expressions for the determinants in certain special cases.         ",
    "url": "https://arxiv.org/abs/2505.02384",
    "authors": [
      "Mehdi Molkaraie"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computational Complexity (cs.CC)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2505.02392",
    "title": "Moneros Decentralized P2P Exchanges: Functionality, Adoption, and Privacy Risks",
    "abstract": "           Privacy-focused cryptocurrencies like Monero remain popular, despite increasing regulatory scrutiny that has led to their delisting from major centralized exchanges. The latter also explains the recent popularity of decentralized exchanges (DEXs) with no centralized ownership structures. These platforms typically leverage peer-to-peer (P2P) networks, promising secure and anonymous asset trading. However, questions of liability remain, and the academic literature lacks comprehensive insights into the functionality, trading activity, and privacy claims of these P2P platforms. In this paper, we provide an early systematization of the current landscape of decentralized peer-to-peer exchanges within the Monero ecosystem. We examine several recently developed DEX platforms, analyzing their popularity, functionality, architectural choices, and potential weaknesses. We further identify and report on a privacy vulnerability in the recently popularized Haveno exchange, demonstrating that certain Haveno trades could be detected, allowing transactions to be linked across the Monero and Bitcoin blockchains. We hope that our findings can nourish the discussion in the research community about more secure designs, and provide insights for regulators.         ",
    "url": "https://arxiv.org/abs/2505.02392",
    "authors": [
      "Yannik Kopyciok",
      "Friedhelm Victor",
      "Stefan Schmid"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.02393",
    "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection",
    "abstract": "           Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02393",
    "authors": [
      "Sungheon Jeong",
      "Jihong Park",
      "Mohsen Imani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02405",
    "title": "Estimating Commonsense Scene Composition on Belief Scene Graphs",
    "abstract": "           This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types.         ",
    "url": "https://arxiv.org/abs/2505.02405",
    "authors": [
      "Mario A.V. Saucedo",
      "Vignesh Kottayam Viswanathan",
      "Christoforos Kanellakis",
      "George Nikolakopoulos"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02413",
    "title": "Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks",
    "abstract": "           Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB, respectively.         ",
    "url": "https://arxiv.org/abs/2505.02413",
    "authors": [
      "Baoxia Du",
      "Hongyang Du",
      "Dusit Niyato",
      "Ruidong Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02420",
    "title": "Impact of Transceiver Selection on Synchronization Accuracy in White Rabbit Networks",
    "abstract": "           Achieving optimal synchronization accuracy between two White Rabbit devices hinges on the proper selection of transceivers, which act as electro-optical converters connecting WR devices to the optical network infrastructure. The correct choice of transceivers can significantly improve resilience to changes in the time offset between WR devices due to temperature fluctuations in the connecting optical fiber. To compare the performance of BiDi WDM and DWDM transceivers, an experimental setup was established under laboratory conditions to simulate a real optical network used for distributing precise time and frequency between two remote locations. The optical connection was emulated by integrating a 20 km G.652.D optical fiber into a climatic chamber, which provided variable environmental conditions similar to those experienced in real applications. The study compared BiDi WDM 1310/1550 nm transceivers with DWDM Ch33/Ch34 transceivers. Results showed that DWDM transceivers exhibited nearly thirteen times less sensitivity to temperature-induced changes in the optical connection, leading to a smaller time offset. Therefore, for achieving the highest accuracy in synchronizing WR devices in practical applications, DWDM transceiver technology is essential.         ",
    "url": "https://arxiv.org/abs/2505.02420",
    "authors": [
      "Michal \u0160pa\u010dek",
      "Josef Vojt\u011bch",
      "Jaroslav Rozto\u010dil"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.02433",
    "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning",
    "abstract": "           We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs.         ",
    "url": "https://arxiv.org/abs/2505.02433",
    "authors": [
      "Soumen Kumar Mondal",
      "Akshit Varmora",
      "Prateek Chanda",
      "Ganesh Ramakrishnan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02435",
    "title": "A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability",
    "abstract": "           Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.         ",
    "url": "https://arxiv.org/abs/2505.02435",
    "authors": [
      "Pouria Fatemi",
      "Ehsan Sharifian",
      "Mohammad Hossein Yassaee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02440",
    "title": "Cooperative ISAC Network for Off-Grid Imaging-based Low-Altitude Surveillance",
    "abstract": "           The low-altitude economy has emerged as a critical focus for future economic development, emphasizing the urgent need for flight activity surveillance utilizing the existing sensing capabilities of mobile cellular networks. Traditional monostatic or localization-based sensing methods, however, encounter challenges in fusing sensing results and matching channel parameters. To address these challenges, we propose an innovative approach that directly draws the radio images of the low-altitude space, leveraging its inherent sparsity with compressed sensing (CS)-based algorithms and the cooperation of multiple base stations. Furthermore, recognizing that unmanned aerial vehicles (UAVs) are randomly distributed in space, we introduce a physics-embedded learning method to overcome off-grid issues inherent in CS-based models. Additionally, an online hard example mining method is incorporated into the design of the loss function, enabling the network to adaptively concentrate on the samples bearing significant discrepancy with the ground truth, thereby enhancing its ability to detect the rare UAVs within the expansive low-altitude space. Simulation results demonstrate the effectiveness of the imaging-based low-altitude surveillance approach, with the proposed physics-embedded learning algorithm significantly outperforming traditional CS-based methods under off-grid conditions.         ",
    "url": "https://arxiv.org/abs/2505.02440",
    "authors": [
      "Yixuan Huang",
      "Jie Yang",
      "Chao-Kai Wen",
      "Shuqiang Xia",
      "Xiao Li",
      "Shi Jin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.02441",
    "title": "MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection",
    "abstract": "           Accurate identification of agricultural pests is essential for crop protection but remains challenging due to the large intra-class variance and fine-grained differences among pest species. While deep learning has advanced pest detection, most existing approaches rely solely on low-level visual features and lack effective multi-modal integration, leading to limited accuracy and poor interpretability. Moreover, the scarcity of high-quality multi-modal agricultural datasets further restricts progress in this field. To address these issues, we construct two novel multi-modal benchmarks-CTIP102 and STIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale Cross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach enhances visual quality via a super-resolution reconstruction module, and feeds both the original and reconstructed images into the network to improve clarity and detection performance. To better exploit semantic cues, we propose an Image-Text Fusion (ITF) module for joint modeling of visual and textual features, and an Image-Text Converter (ITC) that reconstructs fine-grained details across multiple scales to handle challenging backgrounds. Furthermore, we introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to generate a more complex and diverse pest detection dataset, MTIP102, improving the model's generalization to real-world scenarios. Extensive experiments demonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods on multiple pest detection benchmarks. All code and datasets will be made publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.02441",
    "authors": [
      "Jiaqi Zhang",
      "Zhuodong Liu",
      "Kejian Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02448",
    "title": "Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey",
    "abstract": "           Out-of-distribution detection (OOD) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (ID) data during testing. Recent advances in AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized OOD detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. This shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of ID images, adhering to a unimodal paradigm. To better align with CLIP's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. Specifically, we categorize existing methods based on how visual and textual information of OOD data is utilized within image + text modalities, and further divide them into four groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e., learnable vectors or class names) Known or Unknown, across two training strategies (i.e., train-free or training-required). More importantly, we discuss open problems in CLIP-like OOD detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.         ",
    "url": "https://arxiv.org/abs/2505.02448",
    "authors": [
      "Chaohua Li",
      "Enhao Zhang",
      "Chuanxing Geng",
      "Songcan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02450",
    "title": "Predicting the Dynamics of Complex System via Multiscale Diffusion Autoencoder",
    "abstract": "           Predicting the dynamics of complex systems is crucial for various scientific and engineering applications. The accuracy of predictions depends on the model's ability to capture the intrinsic dynamics. While existing methods capture key dynamics by encoding a low-dimensional latent space, they overlook the inherent multiscale structure of complex systems, making it difficult to accurately predict complex spatiotemporal evolution. Therefore, we propose a Multiscale Diffusion Prediction Network (MDPNet) that leverages the multiscale structure of complex systems to discover the latent space of intrinsic dynamics. First, we encode multiscale features through a multiscale diffusion autoencoder to guide the diffusion model for reliable reconstruction. Then, we introduce an attention-based graph neural ordinary differential equation to model the co-evolution across different scales. Extensive evaluations on representative systems demonstrate that the proposed method achieves an average prediction error reduction of 53.23% compared to baselines, while also exhibiting superior robustness and generalization.         ",
    "url": "https://arxiv.org/abs/2505.02450",
    "authors": [
      "Ruikun Li",
      "Jingwen Cheng",
      "Huandong Wang",
      "Qingmin Liao",
      "Yong Li"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2505.02463",
    "title": "Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda",
    "abstract": "           In this paper,we explore the application of Back translation (BT) as a semi-supervised technique to enhance Neural Machine Translation(NMT) models for the English-Luganda language pair, specifically addressing the challenges faced by low-resource languages. The purpose of our study is to demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora. Our methodology involves developing custom NMT models using both publicly available and web-crawled data, and applying Iterative and Incremental Back translation techniques. We strategically select datasets for incremental back translation across multiple small datasets, which is a novel element of our approach. The results of our study show significant improvements, with translation performance for the English-Luganda pair exceeding previous benchmarks by more than 10 BLEU score units across all translation directions. Additionally, our evaluation incorporates comprehensive assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced understanding of translation quality. The conclusion drawn from our research confirms the efficacy of BT when strategically curated datasets are utilized, establishing new performance benchmarks and demonstrating the potential of BT in enhancing NMT models for low-resource languages.         ",
    "url": "https://arxiv.org/abs/2505.02463",
    "authors": [
      "Richard Kimera",
      "Dongnyeong Heo",
      "Daniela N. Rim",
      "Heeyoul Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.02469",
    "title": "Efficient Continual Learning in Keyword Spotting using Binary Neural Networks",
    "abstract": "           Keyword spotting (KWS) is an essential function that enables interaction with ubiquitous smart devices. However, in resource-limited devices, KWS models are often static and can thus not adapt to new scenarios, such as added keywords. To overcome this problem, we propose a Continual Learning (CL) approach for KWS built on Binary Neural Networks (BNNs). The framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords over time. This study evaluates seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95% for a single additional keyword and up to 86% for four additional classes. Sensitivity to the amount of training samples in the CL phase, and differences in computational complexities are being evaluated. These evaluations demonstrate that batch-based algorithms are more sensitive to the CL dataset size, and that differences between the computational complexities are insignificant. These findings highlight the potential of developing an effective and computationally efficient technique for continuously integrating new keywords in KWS applications that is compatible with resource-constrained devices.         ",
    "url": "https://arxiv.org/abs/2505.02469",
    "authors": [
      "Quynh Nguyen-Phuong Vu",
      "Luciano Sebastian Martinez-Rau",
      "Yuxuan Zhang",
      "Nho-Duc Tran",
      "Bengt Oelmann",
      "Michele Magno",
      "Sebastian Bader"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2505.02476",
    "title": "Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation",
    "abstract": "           The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.         ",
    "url": "https://arxiv.org/abs/2505.02476",
    "authors": [
      "Hubert Padusinski",
      "Christian Steinhauser",
      "Christian Scherl",
      "Julian Gaal",
      "Jacob Langner"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.02490",
    "title": "Bayesian Robust Aggregation for Federated Learning",
    "abstract": "           Federated Learning enables collaborative training of machine learning models on decentralized data. This scheme, however, is vulnerable to adversarial attacks, when some of the clients submit corrupted model updates. In real-world scenarios, the total number of compromised clients is typically unknown, with the extent of attacks potentially varying over time. To address these challenges, we propose an adaptive approach for robust aggregation of model updates based on Bayesian inference. The mean update is defined by the maximum of the likelihood marginalized over probabilities of each client to be `honest'. As a result, the method shares the simplicity of the classical average estimators (e.g., sample mean or geometric median), being independent of the number of compromised clients. At the same time, it is as effective against attacks as methods specifically tailored to Federated Learning, such as Krum. We compare our approach with other aggregation schemes in federated setting on three benchmark image classification data sets. The proposed method consistently achieves state-of-the-art performance across various attack types with static and varying number of malicious clients.         ",
    "url": "https://arxiv.org/abs/2505.02490",
    "authors": [
      "Aleksandr Karakulev",
      "Usama Zafar",
      "Salman Toor",
      "Prashant Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02516",
    "title": "Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics",
    "abstract": "           Advanced neural interfaces are transforming applications ranging from neuroscience research to diagnostic tools (for mental state recognition, tremor and seizure detection) as well as prosthetic devices (for motor and communication recovery). By integrating complex functions into miniaturized neural devices, these systems unlock significant opportunities for personalized assistive technologies and adaptive therapeutic interventions. Leveraging high-density neural recordings, on-site signal processing, and machine learning (ML), these interfaces extract critical features, identify disease neuro-markers, and enable accurate, low-latency neural decoding. This integration facilitates real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. Moreover, the synergy between neural interfaces and ML has paved the way for self-sufficient, ubiquitous platforms capable of operating in diverse environments with minimal hardware costs and external dependencies. In this work, we review recent advancements in AI-driven decoding algorithms and energy-efficient System-on-Chip (SoC) platforms for next-generation miniaturized neural devices. These innovations highlight the potential for developing intelligent neural interfaces, addressing critical challenges in scalability, reliability, interpretability, and user adaptability.         ",
    "url": "https://arxiv.org/abs/2505.02516",
    "authors": [
      "MohammadAli Shaeri",
      "Jinhan Liu",
      "Mahsa Shoaran"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2505.02537",
    "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations",
    "abstract": "           Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures.         ",
    "url": "https://arxiv.org/abs/2505.02537",
    "authors": [
      "Davide Sartor",
      "Alberto Sinigaglia",
      "Gian Antonio Susto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02540",
    "title": "Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data",
    "abstract": "           In Federated Learning, heterogeneity in client data distributions often means that a single global model does not have the best performance for individual clients. Consider for example training a next-word prediction model for keyboards: user-specific language patterns due to demographics (dialect, age, etc.), language proficiency, and writing style result in a highly non-IID dataset across clients. Other examples are medical images taken with different machines, or driving data from different vehicle types. To address this, we propose a simple yet effective personalized federated learning framework (pFedLIA) that utilizes a computationally efficient influence approximation, called `Lazy Influence', to cluster clients in a distributed manner before model aggregation. Within each cluster, data owners collaborate to jointly train a model that captures the specific data patterns of the clients. Our method has been shown to successfully recover the global model's performance drop due to the non-IID-ness in various synthetic and real-world settings, specifically a next-word prediction task on the Nordic languages as well as several benchmark tasks. It matches the performance of a hypothetical Oracle clustering, and significantly improves on existing baselines, e.g., an improvement of 17% on CIFAR100.         ",
    "url": "https://arxiv.org/abs/2505.02540",
    "authors": [
      "Ljubomir Rokvic",
      "Panayiotis Danassis",
      "Boi Faltings"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02549",
    "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication",
    "abstract": "           Unsupervised visible-infrared person re-identification (UVI-ReID) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. Existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. In practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. To address this, we introduce a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. To this end, we propose a novel Robust Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning mechanism (RAL) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. Second, to alleviate error accumulation-where the model reinforces its own mistakes-RoDE employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. However, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. To resolve this, we introduce Cluster Consistency Matching (CCM), which aligns clusters across models and modalities by measuring cross-cluster similarity. Extensive experiments on three benchmarks demonstrate the effectiveness of RoDE.         ",
    "url": "https://arxiv.org/abs/2505.02549",
    "authors": [
      "Yongxiang Li",
      "Yuan Sun",
      "Yang Qin",
      "Dezhong Peng",
      "Xi Peng",
      "Peng Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2505.02565",
    "title": "Antifragility of RIS-assisted Communication Systems under Jamming Attacks",
    "abstract": "           Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain.         ",
    "url": "https://arxiv.org/abs/2505.02565",
    "authors": [
      "Mounir Bensalem",
      "Thomas R\u00f6thig",
      "Admela Jukan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.02566",
    "title": "Robustness questions the interpretability of graph neural networks: what to do?",
    "abstract": "           Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms. We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source. The results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications.         ",
    "url": "https://arxiv.org/abs/2505.02566",
    "authors": [
      "Kirill Lukyanov",
      "Georgii Sazonov",
      "Serafim Boyarsky",
      "Ilya Makarov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02573",
    "title": "Rethinking Federated Graph Learning: A Data Condensation Perspective",
    "abstract": "           Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client this http URL, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.         ",
    "url": "https://arxiv.org/abs/2505.02573",
    "authors": [
      "Hao Zhang",
      "Xunkai Li",
      "Yinlin Zhu",
      "Lianglin Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.02586",
    "title": "RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet",
    "abstract": "           This work introduces RGBX-DiffusionDet, an object detection framework extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB imagery via an adaptive multimodal encoder. To enable cross-modal interaction, we design the dynamic channel reduction within a convolutional block attention module (DCR-CBAM), which facilitates cross-talk between subnetworks by dynamically highlighting salient channel features. Furthermore, the dynamic multi-level aggregation block (DMLAB) is proposed to refine spatial feature representations through adaptive multiscale fusion. Finally, novel regularization losses that enforce channel saliency and spatial selectivity are introduced, leading to compact and discriminative feature embeddings. Extensive experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We demonstrate consistent superiority of the proposed approach over the baseline RGB-only DiffusionDet. The modular architecture maintains the original decoding complexity, ensuring efficiency. These results establish the proposed RGBX-DiffusionDet as a flexible multimodal object detection approach, providing new insights into integrating diverse 2D sensing modalities into diffusion-based detection pipelines.         ",
    "url": "https://arxiv.org/abs/2505.02586",
    "authors": [
      "Eliraz Orfaig",
      "Inna Stainvas",
      "Igal Bilik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02604",
    "title": "Low-Loss Space in Neural Networks is Continuous and Fully Connected",
    "abstract": "           Visualizations of the loss landscape in neural networks suggest that minima are isolated points. However, both theoretical and empirical studies indicate that it is possible to connect two different minima with a path consisting of intermediate points that also have low loss. In this study, we propose a new algorithm which investigates low-loss paths in the full parameter space, not only between two minima. Our experiments on LeNet5, ResNet18, and Compact Convolutional Transformer architectures consistently demonstrate the existence of such continuous paths in the parameter space. These results suggest that the low-loss region is a fully connected and continuous space in the parameter space. Our findings provide theoretical insight into neural network over-parameterization, highlighting that parameters collectively define a high-dimensional low-loss space, implying parameter redundancy exists only within individual models and not throughout the entire low-loss space. Additionally, our work also provides new visualization methods and opportunities to improve model generalization by exploring the low-loss space that is closer to the origin.         ",
    "url": "https://arxiv.org/abs/2505.02604",
    "authors": [
      "Yongding Tian",
      "Zaid Al-Ars",
      "Maksim Kitsak",
      "Peter Hofstee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02609",
    "title": "Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview",
    "abstract": "           Artificial intelligence is used at various stages of the recruitment process to automatically select the best candidate for a position, with companies guaranteeing unbiased recruitment. However, the algorithms used are either trained by humans or are based on learning from past experiences that were biased. In this article, we propose to generate data mimicking external (discrimination) and internal biases (self-censorship) in order to train five classic algorithms and to study the extent to which they do or do not find the best candidates according to objective criteria. In addition, we study the influence of the anonymisation of files on the quality of predictions.         ",
    "url": "https://arxiv.org/abs/2505.02609",
    "authors": [
      "Shuyu Wang",
      "Ang\u00e9lique Saillet",
      "Philom\u00e8ne Le Gall",
      "Alain Lacroux",
      "Christelle Martin-Lacroux",
      "Vincent Brault"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2505.02627",
    "title": "A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition",
    "abstract": "           Compositional generalization is a crucial property in artificial intelligence, enabling models to handle novel combinations of known components. While most deep learning models lack this capability, certain models succeed in specific tasks, suggesting the existence of governing conditions. This paper derives a necessary and sufficient condition for compositional generalization in neural networks. Conceptually, it requires that (i) the computational graph matches the true compositional structure, and (ii) components encode just enough information in training. The condition is supported by mathematical proofs. This criterion combines aspects of architecture design, regularization, and training data properties. A carefully designed minimal example illustrates an intuitive understanding of the condition. We also discuss the potential of the condition for assessing compositional generalization before training. This work is a fundamental theoretical study of compositional generalization in neural networks.         ",
    "url": "https://arxiv.org/abs/2505.02627",
    "authors": [
      "Yuanpeng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02629",
    "title": "Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for Automated Patch Correctness Assessment",
    "abstract": "           Automated program repair (APR) aims to automatically repair program errors without human intervention, and recent years have witnessed a growing interest on this research topic. While much progress has been made and techniques originating from different disciplines have been proposed, APR techniques generally suffer from the patch overfitting issue, i.e., the generated patches are not genuinely correct despite they pass the employed tests. To alleviate this issue, many research efforts have been devoted for automated patch correctness assessment (APCA). In particular, with the emergence of large language model (LLM) technology, researchers have employed LLM to assess the patch correctness and have obtained the state-of-the-art performance. The literature on APCA has demonstrated the importance of capturing patch semantic and explicitly considering certain code attributes in predicting patch correctness. However, existing LLM-based methods typically treat code as token sequences and ignore the inherent formal structure for code, making it difficult to capture the deep patch semantics. Moreover, these LLM-based methods also do not explicitly account for enough code attributes. To overcome these drawbacks, we in this paper design a novel patch graph representation named attributed patch semantic graph (APSG), which adequately captures the patch semantic and explicitly reflects important patch attributes. To effectively use graph information in APSG, we accordingly propose a new parameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA. Extensive evaluations have been conducted to evaluate our method, and the results show that compared to the state-of-the-art methods, our method improves accuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively.         ",
    "url": "https://arxiv.org/abs/2505.02629",
    "authors": [
      "Zhenyu Yang",
      "Jingwen Wu",
      "Zhen Yang",
      "Zhongxing Yu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.02639",
    "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning",
    "abstract": "           Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.         ",
    "url": "https://arxiv.org/abs/2505.02639",
    "authors": [
      "Xuan Lin",
      "Qingrui Liu",
      "Hongxin Xiang",
      "Daojian Zeng",
      "Xiangxiang Zeng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.02648",
    "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation",
    "abstract": "           Diffusion models have shown excellent performance in text-to-image generation. Nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing MLLMs to extract various scene elements effectively. In addition, Hierarchical Compositional diffusion utilizes a Gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.         ",
    "url": "https://arxiv.org/abs/2505.02648",
    "authors": [
      "Mingcheng Li",
      "Xiaolu Hou",
      "Ziyang Liu",
      "Dingkang Yang",
      "Ziyun Qian",
      "Jiawei Chen",
      "Jinjie Wei",
      "Yue Jiang",
      "Qingyao Xu",
      "Lihua Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02664",
    "title": "Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter",
    "abstract": "           Grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of Graph Neural Networks for efficient geometric reasoning from point cloud data. Building on the success of GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to efficiently produce 7-Dof grasp candidates. Candidates are assessed with an ensemble Graph Neural Network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). This improved representation boosts grasp detection performance over previous methods using the same generator. GtG 2.0 shows up to a 35% improvement in Average Precision on the GraspNet-1Billion benchmark compared to hypothesis-and-test and Graph Neural Network-based methods, ranking it among the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and Kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.         ",
    "url": "https://arxiv.org/abs/2505.02664",
    "authors": [
      "Ali Rashidi Moghadam",
      "Sayedmohammadreza Rastegari",
      "Mehdi Tale Masouleh",
      "Ahmad Kalhor"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02703",
    "title": "Structure Causal Models and LLMs Integration in Medical Visual Question Answering",
    "abstract": "           Medical Visual Question Answering (MedVQA) aims to answer medical questions according to medical images. However, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. Such cross-modal bias makes it challenging to infer medically meaningful answers. In this work, we propose a causal inference framework for the MedVQA task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (QA) session. We are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. During optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. In addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. Extensive experiments on three MedVQA datasets demonstrate that 1) our method significantly improves the accuracy of MedVQA, and 2) our method achieves true causal correlations in the face of complex medical data.         ",
    "url": "https://arxiv.org/abs/2505.02703",
    "authors": [
      "Zibo Xu",
      "Qiang Li",
      "Weizhi Nie",
      "Weijie Wang",
      "Anan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02712",
    "title": "Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework",
    "abstract": "           Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2505.02712",
    "authors": [
      "Andrzej Mizera",
      "Jakub Zarzycki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Molecular Networks (q-bio.MN)"
    ]
  },
  {
    "id": "arXiv:2505.02714",
    "title": "Less is More: Efficient Weight Farcasting with 1-Layer Neural Network",
    "abstract": "           Addressing the computational challenges inherent in training large-scale deep neural networks remains a critical endeavor in contemporary machine learning research. While previous efforts have focused on enhancing training efficiency through techniques such as gradient descent with momentum, learning rate scheduling, and weight regularization, the demand for further innovation continues to burgeon as model sizes keep expanding. In this study, we introduce a novel framework which diverges from conventional approaches by leveraging long-term time series forecasting techniques. Our method capitalizes solely on initial and final weight values, offering a streamlined alternative for complex model architectures. We also introduce a novel regularizer that is tailored to enhance the forecasting performance of our approach. Empirical evaluations conducted on synthetic weight sequences and real-world deep learning architectures, including the prominent large language model DistilBERT, demonstrate the superiority of our method in terms of forecasting accuracy and computational efficiency. Notably, our framework showcases improved performance while requiring minimal additional computational overhead, thus presenting a promising avenue for accelerating the training process across diverse tasks and architectures.         ",
    "url": "https://arxiv.org/abs/2505.02714",
    "authors": [
      "Xiao Shou",
      "Debarun Bhattacharjya",
      "Yanna Ding",
      "Chen Zhao",
      "Rui Li",
      "Jianxi Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02725",
    "title": "Acoustic Side-Channel Attacks on a Computer Mouse",
    "abstract": "           Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using audio emitted from a computing devices and their peripherals. Attacks targeting keyboards are popular and have been explored in the literature. However, similar attacks targeting other human interface peripherals, such as computer mice, are under-explored. To this end, this paper considers security leakage via acoustic signals emanating from normal mouse usage. We first confirm feasibility of such attacks by showing a proof-of-concept attack that classifies four mouse movements with 97% accuracy in a controlled environment. We then evolve the attack towards discerning twelve unique mouse movements using a smartphone to record the experiment. Using Machine Learning (ML) techniques, the model is trained on an experiment with six participants to be generalizable and discern among twelve movements with 94% accuracy. In addition, we experiment with an attack that detects a user action of closing a full-screen window on a laptop. Achieving an accuracy of 91%, this experiment highlights exploiting audio leakage from computer mouse movements in a realistic scenario.         ",
    "url": "https://arxiv.org/abs/2505.02725",
    "authors": [
      "Mauro Conti",
      "Marin Duroyon",
      "Gabriele Orazi",
      "Gene Tsudik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.02737",
    "title": "Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation",
    "abstract": "           Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entities' classes in a KG to gradually prune the candidate space as well as the entities' descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KG's semantic expressivity on the ED performance.         ",
    "url": "https://arxiv.org/abs/2505.02737",
    "authors": [
      "Pons Gerard",
      "Bilalli Besim",
      "Queralt Anna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2505.02741",
    "title": "dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs",
    "abstract": "           This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks.         ",
    "url": "https://arxiv.org/abs/2505.02741",
    "authors": [
      "Yihang Yuan",
      "Ali Aghdaei",
      "Zhuo Feng"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.02743",
    "title": "Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties",
    "abstract": "           Real-world data contains aleatoric uncertainty - irreducible noise arising from imperfect measurements or from incomplete knowledge about the data generation process. Mean variance estimation (MVE) networks can learn this type of uncertainty but require ad-hoc regularization strategies to avoid overfitting and are unable to predict epistemic uncertainty (model uncertainty). Conversely, Bayesian neural networks predict epistemic uncertainty but are notoriously difficult to train due to the approximate nature of Bayesian inference. We propose to cooperatively train a variance network with a Bayesian neural network and demonstrate that the resulting model disentangles aleatoric and epistemic uncertainties while improving the mean estimation. We demonstrate the effectiveness and scalability of this method across a diverse range of datasets, including a time-dependent heteroscedastic regression dataset we created where the aleatoric uncertainty is known. The proposed method is straightforward to implement, robust, and adaptable to various model architectures.         ",
    "url": "https://arxiv.org/abs/2505.02743",
    "authors": [
      "Jiaxiang Yi",
      "Miguel A. Bessa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02746",
    "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training",
    "abstract": "           Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.         ",
    "url": "https://arxiv.org/abs/2505.02746",
    "authors": [
      "Simon Ging",
      "Sebastian Walter",
      "Jelena Bratuli\u0107",
      "Johannes Dienert",
      "Hannah Bast",
      "Thomas Brox"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02770",
    "title": "Teaching the social media generation: rethinking learning without sacrificing quality",
    "abstract": "           The rise of social media and AI tools has reshaped how students engage with learning, process information, and build trust in educational content. This generation prefers short, visual materials and fast feedback but often struggles with focus, critical thinking, and deep learning. Educators face the challenge of adapting teaching methods to these habits without lowering academic standards. This study presents a blended learning redesign of a first-year technical course at a Dutch university. Key features included short whiteboard videos before class, hands-on teamwork during class, narrative-style handouts to reinforce learning, in-class draft assignments without AI, and weekly anonymous feedback to adjust in real time. The results were promising: attendance increased by nearly 50%, and none of the regularly attending students failed the exam. Students found the videos useful but emphasized that in-person sessions were essential for understanding the material. While some resisted the shift in expectations, most appreciated the structure, clarity, and opportunities for active learning. This case suggests that combining digital familiarity with clear expectations and active support can help meet students where they are, while still challenging them to grow.         ",
    "url": "https://arxiv.org/abs/2505.02770",
    "authors": [
      "Sepinoud Azimi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2505.02781",
    "title": "Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects",
    "abstract": "           Understanding and identifying controlled direct effects (CDEs) is crucial across numerous scientific domains, including public health. While existing methods can identify these effects from causal directed acyclic graphs (DAGs), the true underlying structure is often unknown in practice. Essential graphs, which represent a Markov equivalence class of DAGs characterized by the same set of d-separations, provide a more practical and realistic alternative. However, learning the full essential graph is computationally intensive and typically depends on strong, untestable assumptions. In this work, we characterize a local class of graphs, defined relative to a target variable, that share a specific subset of d-separations, and introduce a graphical representation of this class, called the local essential graph (LEG). We then present LocPC, a novel algorithm designed to recover the LEG from an observed distribution using only local conditional independence tests. Building on LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG that is sufficient to identify a CDE, bypassing the need of retrieving the full essential graph. Compared to global methods, our algorithms require less conditional independence tests and operate under weaker assumptions while maintaining theoretical guarantees.         ",
    "url": "https://arxiv.org/abs/2505.02781",
    "authors": [
      "Timoth\u00e9e Loranchet",
      "Charles K. Assaad"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02789",
    "title": "Recolorable Graph Exploration by an Oblivious Agent with Fewer Colors",
    "abstract": "           Recently, B\u00f6ckenhauer, Frei, Unger, and Wehner (SIROCCO 2023) introduced a novel variant of the graph exploration problem in which a single memoryless agent must visit all nodes of an unknown, undirected, and connected graph before returning to its starting node. Unlike the standard model for mobile agents, edges are not labeled with port numbers. Instead, the agent can color its current node and observe the color of each neighboring node. To move, it specifies a target color and then moves to an adversarially chosen neighbor of that color. B\u00f6ckenhauer~et al.~analyzed the minimum number of colors required for successful exploration and proposed an elegant algorithm that enables the agent to explore an arbitrary graph using only eight colors. In this paper, we present a novel graph exploration algorithm that requires only six colors. Furthermore, we prove that five colors are sufficient if we consider only a restricted class of graphs, which we call the $\\varphi$-free graphs, a class that includes every graph with maximum degree at most three and every cactus.         ",
    "url": "https://arxiv.org/abs/2505.02789",
    "authors": [
      "Shota Takahashi",
      "Haruki Kanaya",
      "Shoma Hiraoka",
      "Ryota Eguchi",
      "Yuichi Sudo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.02795",
    "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models",
    "abstract": "           Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.         ",
    "url": "https://arxiv.org/abs/2505.02795",
    "authors": [
      "Zheng Lin",
      "Yuxin Zhang",
      "Zhe Chen",
      "Zihan Fang",
      "Xianhao Chen",
      "Praneeth Vepakomma",
      "Wei Ni",
      "Jun Luo",
      "Yue Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.02797",
    "title": "DPNet: Dynamic Pooling Network for Tiny Object Detection",
    "abstract": "           In unmanned aerial systems, especially in complex environments, accurately detecting tiny objects is crucial. Resizing images is a common strategy to improve detection accuracy, particularly for small objects. However, simply enlarging images significantly increases computational costs and the number of negative samples, severely degrading detection performance and limiting its applicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny object detection to mitigate these issues. DPNet employs a flexible down-sampling strategy by introducing a factor (df) to relax the fixed downsampling process of the feature map to an adjustable one. Furthermore, we design a lightweight predictor to predict df for each input image, which is used to decrease the resolution of feature maps in the backbone. Thus, we achieve input-aware downsampling. We also design an Adaptive Normalization Module (ANM) to make a unified detector compatible with different dfs. A guidance loss supervises the predictor's training. DPNet dynamically allocates computing resources to trade off between detection accuracy and efficiency. Experiments on the TinyCOCO and TinyPerson datasets show that DPNet can save over 35% and 25% GFLOPs, respectively, while maintaining comparable detection performance. The code will be made publicly available.         ",
    "url": "https://arxiv.org/abs/2505.02797",
    "authors": [
      "Luqi Gong",
      "Haotian Chen",
      "Yikun Chen",
      "Tianliang Yao",
      "Chao Li",
      "Shuai Zhao",
      "Guangjie Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02798",
    "title": "Unifying Laplace Mechanism with Instance Optimality in Differential Privacy",
    "abstract": "           We adapt the canonical Laplace mechanism, widely used in differentially private data analysis, to achieve near instance optimality with respect to the hardness of the underlying dataset. In particular, we construct a piecewise Laplace distribution whereby we defy traditional assumptions and show that Laplace noise can in fact be drawn proportional to the local sensitivity when done in a piecewise manner. While it may initially seem counterintuitive that this satisfies (pure) differential privacy and can be sampled, we provide both through a simple connection to the exponential mechanism and inverse sensitivity along with the fact that the Laplace distribution is a two-sided exponential distribution. As a result, we prove that in the continuous setting our \\textit{piecewise Laplace mechanism} strictly dominates the inverse sensitivity mechanism, which was previously shown to both be nearly instance optimal and uniformly outperform the smooth sensitivity framework. Furthermore, in the worst-case where all local sensitivities equal the global sensitivity, our method simply reduces to a Laplace mechanism. We also complement this with an approximate local sensitivity variant to potentially ease the computational cost, which can also extend to higher dimensions.         ",
    "url": "https://arxiv.org/abs/2505.02798",
    "authors": [
      "David Durfee"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2505.02806",
    "title": "Cell-Free Massive MIMO-Assisted SWIPT for IoT Networks",
    "abstract": "           This paper studies cell-free massive multiple-input multiple-output (CF-mMIMO) systems that underpin simultaneous wireless information and power transfer (SWIPT) for separate information users (IUs) and energy users (EUs) in Internet of Things (IoT) networks. We propose a joint access point (AP) operation mode selection and power control design, wherein certain APs are designated for energy transmission to EUs, while others are dedicated to information transmission to IUs. The performance of the system, from both a spectral efficiency (SE) and energy efficiency (EE) perspective, is comprehensively analyzed. Specifically, we formulate two mixed-integer nonconvex optimization problems for maximizing the average sum-SE and EE, under realistic power consumption models and constraints on the minimum individual SE requirements for individual IUs, minimum HE for individual EUs, and maximum transmit power at each AP. The challenging optimization problems are solved using successive convex approximation (SCA) techniques. The proposed framework design is further applied to the average sum-HE maximization and energy harvesting fairness problems. Our numerical results demonstrate that the proposed joint AP operation mode selection and power control algorithm can achieve EE performance gains of up to $4$-fold and $5$-fold over random AP operation mode selection, with and without power control respectively.         ",
    "url": "https://arxiv.org/abs/2505.02806",
    "authors": [
      "Mohammadali Mohammadi",
      "Le-Nam Tran",
      "Zahra Mobini",
      "Hien Quoc Ngo",
      "Michail Matthaiou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.02809",
    "title": "Towards Quantifying the Hessian Structure of Neural Networks",
    "abstract": "           Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal two forces that shape the Hessian structure: a ``static force'' rooted in the architecture design, and a ``dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ``static force'' at random initialization. We study linear models and 1-hidden-layer networks with the mean-square (MSE) loss and the Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C \\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings reveal that $C$ is a primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.         ",
    "url": "https://arxiv.org/abs/2505.02809",
    "authors": [
      "Zhaorui Dong",
      "Yushun Zhang",
      "Zhi-Quan Luo",
      "Jianfeng Yao",
      "Ruoyu Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.02819",
    "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations",
    "abstract": "           We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.         ",
    "url": "https://arxiv.org/abs/2505.02819",
    "authors": [
      "Dmitriy Shopkhoev",
      "Ammar Ali",
      "Magauiya Zhussip",
      "Valentin Malykh",
      "Stamatios Lefkimmiatis",
      "Nikos Komodakis",
      "Sergey Zagoruyko"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.02824",
    "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
    "abstract": "           Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.         ",
    "url": "https://arxiv.org/abs/2505.02824",
    "authors": [
      "Kuofeng Gao",
      "Yufei Zhu",
      "Yiming Li",
      "Jiawang Bai",
      "Yong Yang",
      "Zhifeng Li",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.02828",
    "title": "Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review",
    "abstract": "           Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.         ",
    "url": "https://arxiv.org/abs/2505.02828",
    "authors": [
      "Sonal Allana",
      "Mohan Kankanhalli",
      "Rozita Dara"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2505.02831",
    "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves",
    "abstract": "           Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance generation quality of the diffusion transformers. However, existing approaches necessitate to either introduce an additional and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation A}lignment (SRA), a simple yet straightforward method that obtain representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in earlier layer with higher noise to that in later layer with lower noise to progressively enhance the overall representation learning during only generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that heavily dependent on powerful external representation priors.         ",
    "url": "https://arxiv.org/abs/2505.02831",
    "authors": [
      "Dengyang Jiang",
      "Mengmeng Wang",
      "Liuzhuozheng Li",
      "Lei Zhang",
      "Haoyu Wang",
      "Wei Wei",
      "Guang Dai",
      "Yanning Zhang",
      "Jingdong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01433",
    "title": "Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations",
    "abstract": "           Understanding the binding specificity between T-cell receptors (TCRs) and peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy and vaccine development. However, current predictive models struggle with generalization, especially in data-scarce settings and when faced with novel epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced Recognition Network), a deep learning framework that combines large-scale protein language models with chemical representations of peptides. By encoding TCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences into SMILES strings processed by MolFormer, LANTERN captures rich biological and chemical features critical for TCR-peptide recognition. Through extensive benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR, LANTERN demonstrates superior performance, particularly in zero-shot and few-shot learning scenarios. Our model also benefits from a robust negative sampling strategy and shows significant clustering improvements via embedding analysis. These results highlight the potential of LANTERN to advance TCR-pMHC binding prediction and support the development of personalized immunotherapies.         ",
    "url": "https://arxiv.org/abs/2505.01433",
    "authors": [
      "Cong Qi",
      "Hanzhang Fang",
      "Siqi jiang",
      "Tianxing Hu",
      "Wei Zhi"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01455",
    "title": "Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity",
    "abstract": "           Machine learning (ML) models are successful with weather forecasting and have shown progress in climate simulations, yet leveraging them for useful climate predictions needs exploration. Here we show this feasibility using NeuralGCM, a hybrid ML-physics atmospheric model, for seasonal predictions of large-scale atmospheric variability and Northern Hemisphere tropical cyclone (TC) activity. Inspired by physical model studies, we simplify boundary conditions, assuming sea surface temperature (SST) and sea ice follow their climatological cycle but persist anomalies present at initialization. With such forcings, NeuralGCM simulates realistic atmospheric circulation and TC climatology patterns. Furthermore, this configuration yields useful seasonal predictions (July-November) for the tropical atmosphere and various TC activity metrics. Notably, the prediction skill for TC frequency in the North Atlantic and East Pacific basins is comparable to existing physical models. These findings highlight the promise of leveraging ML models with physical insights to model TC risks and deliver seamless weather-climate predictions.         ",
    "url": "https://arxiv.org/abs/2505.01455",
    "authors": [
      "Gan Zhang",
      "Megha Rao",
      "Janni Yuval",
      "Ming Zhao"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01476",
    "title": "CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering",
    "abstract": "           Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.01476",
    "authors": [
      "Zhe Zhang",
      "Mingxiu Cai",
      "Hanxiao Wang",
      "Gaochang Wu",
      "Tianyou Chai",
      "Xiatian Zhu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01768",
    "title": "Continuous Filtered Backprojection by Learnable Interpolation Network",
    "abstract": "           Accurate reconstruction of computed tomography (CT) images is crucial in medical imaging field. However, there are unavoidable interpolation errors in the backprojection step of the conventional reconstruction methods, i.e., filtered-back-projection based methods, which are detrimental to the accurate reconstruction. In this study, to address this issue, we propose a novel deep learning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to enhance the reconstructed CT image quality, which achieves learnable interpolation in the backprojection step of filtered backprojection (FBP) and alleviates the interpolation errors. Specifically, in the proposed LInFBP, we formulate every local piece of the latent continuous function of discrete sinogram data as a linear combination of selected basis functions, and learn this continuous function by exploiting a deep network to predict the linear combination coefficients. Then, the learned latent continuous function is exploited for interpolation in backprojection step, which first time takes the advantage of deep learning for the interpolation in FBP. Extensive experiments, which encompass diverse CT scenarios, demonstrate the effectiveness of the proposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play ability and generalization capability.         ",
    "url": "https://arxiv.org/abs/2505.01768",
    "authors": [
      "Hui Lin",
      "Dong Zeng",
      "Qi Xie",
      "Zerui Mao",
      "Jianhua Ma",
      "Deyu Meng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01785",
    "title": "TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis",
    "abstract": "           Estimating the causal effect of time-varying treatments on survival outcomes is a challenging task in many domains, particularly in medicine where treatment protocols adapt over time. While recent advances in representation learning have improved causal inference for static treatments, extending these methods to dynamic treatment regimes with survival outcomes remains under-explored. In this paper, we introduce TV-SurvCaus, a novel framework that extends representation balancing techniques to the time-varying treatment setting for survival analysis. We provide theoretical guarantees through (1) a generalized bound for time-varying precision in estimation of heterogeneous effects, (2) variance control via sequential balancing weights, (3) consistency results for dynamic treatment regimes, (4) convergence rates for representation learning with temporal dependencies, and (5) a formal bound on the bias due to treatment-confounder feedback. Our neural architecture incorporates sequence modeling to handle temporal dependencies while balancing time-dependent representations. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that TV-SurvCaus outperforms existing methods in estimating individualized treatment effects with time-varying covariates and treatments. Our framework advances the field of causal inference by enabling more accurate estimation of treatment effects in dynamic, longitudinal settings with survival outcomes.         ",
    "url": "https://arxiv.org/abs/2505.01785",
    "authors": [
      "Ayoub Abraich"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01831",
    "title": "Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement",
    "abstract": "           High-quality fundus images provide essential anatomical information for clinical screening and ophthalmic disease diagnosis. Yet, due to hardware limitations, operational variability, and patient compliance, fundus images often suffer from low resolution and signal-to-noise ratio. Recent years have witnessed promising progress in fundus image enhancement. However, existing works usually focus on restoring structural details or global characteristics of fundus images, lacking a unified image enhancement framework to recover comprehensive multi-scale information. Moreover, few methods pinpoint the target of image enhancement, e.g., lesions, which is crucial for medical image-based diagnosis. To address these challenges, we propose a multi-scale target-aware representation learning framework (MTRL-FIE) for efficient fundus image enhancement. Specifically, we propose a multi-scale feature encoder (MFE) that employs wavelet decomposition to embed both low-frequency structural information and high-frequency details. Next, we design a structure-preserving hierarchical decoder (SHD) to fuse multi-scale feature embeddings for real fundus image restoration. SHD integrates hierarchical fusion and group attention mechanisms to achieve adaptive feature fusion while retaining local structural smoothness. Meanwhile, a target-aware feature aggregation (TFA) module is used to enhance pathological regions and reduce artifacts. Experimental results on multiple fundus image datasets demonstrate the effectiveness and generalizability of MTRL-FIE for fundus image enhancement. Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancement performance with a more lightweight architecture. Furthermore, our approach generalizes to other ophthalmic image processing tasks without supervised fine-tuning, highlighting its potential for clinical applications.         ",
    "url": "https://arxiv.org/abs/2505.01831",
    "authors": [
      "Haofan Wu",
      "Yin Huang",
      "Yuqing Wu",
      "Qiuyu Yang",
      "Bingfang Wang",
      "Li Zhang",
      "Muhammad Fahadullah Khan",
      "Ali Zia",
      "M.Saleh Memon",
      "Syed Sohail Bukhari",
      "Abdul Fattah Memon",
      "Daizong Ji",
      "Ya Zhang",
      "Ghulam Mustafa",
      "Yin Fang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.01884",
    "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images",
    "abstract": "           Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - this https URL)         ",
    "url": "https://arxiv.org/abs/2505.01884",
    "authors": [
      "Siddharth Kothari",
      "Srinivasan Murali",
      "Sankalp Kothari",
      "Ujjwal Verma",
      "Jaya Sreevalsan-Nair"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01985",
    "title": "Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate",
    "abstract": "           We can approximate a constraint or an objective function that is uncertain or nonlinear with a neural network that we embed in the optimization model. This approach, which is known as constraint learning, faces the challenge that optimization models with neural network surrogates are harder to solve. Such difficulties have motivated studies on model reformulation, specialized optimization algorithms, and - to a lesser extent - pruning of the embedded networks. In this work, we double down on the use of surrogates by applying network pruning to produce a surrogate of the neural network itself. In the context of using a Mixed-Integer Linear Programming (MILP) solver to verify neural networks, we obtained faster adversarial perturbations for dense neural networks by using sparse surrogates, especially - and surprisingly - if not taking the time to finetune the sparse network to make up for the loss in accuracy. In other words, we show that a pruned network with bad classification performance can still be a good - and more efficient - surrogate.         ",
    "url": "https://arxiv.org/abs/2505.01985",
    "authors": [
      "Hung Pham",
      "Aiden Ren",
      "Ibrahim Tahir",
      "Jiatai Tong",
      "Thiago Serra"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.01995",
    "title": "Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks",
    "abstract": "           Individual treatment effect estimation has gained significant attention in recent data science literature. This work introduces the Double Neural Network (Double-NN) method to address this problem within the framework of extended fiducial inference (EFI). In the proposed method, deep neural networks are used to model the treatment and control effect functions, while an additional neural network is employed to estimate their parameters. The universal approximation capability of deep neural networks ensures the broad applicability of this method. Numerical results highlight the superior performance of the proposed Double-NN method compared to the conformal quantile regression (CQR) method in individual treatment effect estimation. From the perspective of statistical inference, this work advances the theory and methodology for statistical inference of large models. Specifically, it is theoretically proven that the proposed method permits the model size to increase with the sample size $n$ at a rate of $O(n^{\\zeta})$ for some $0 \\leq \\zeta<1$, while still maintaining proper quantification of uncertainty in the model parameters. This result marks a significant improvement compared to the range $0\\leq \\zeta < \\frac{1}{2}$ required by the classical central limit theorem. Furthermore, this work provides a rigorous framework for quantifying the uncertainty of deep neural networks under the neural scaling law, representing a substantial contribution to the statistical understanding of large-scale neural network models.         ",
    "url": "https://arxiv.org/abs/2505.01995",
    "authors": [
      "Sehwan Kim",
      "Faming Liang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Computation (stat.CO)"
    ]
  },
  {
    "id": "arXiv:2505.02019",
    "title": "Learning the Simplest Neural ODE",
    "abstract": "           Since the advent of the ``Neural Ordinary Differential Equation (Neural ODE)'' paper, learning ODEs with deep learning has been applied to system identification, time-series forecasting, and related areas. Exploiting the diffeomorphic nature of ODE solution maps, neural ODEs has also enabled their use in generative modeling. Despite the rich potential to incorporate various kinds of physical information, training Neural ODEs remains challenging in practice. This study demonstrates, through the simplest one-dimensional linear model, why training Neural ODEs is difficult. We then propose a new stabilization method and provide an analytical convergence analysis. The insights and techniques presented here serve as a concise tutorial for researchers beginning work on Neural ODEs.         ",
    "url": "https://arxiv.org/abs/2505.02019",
    "authors": [
      "Yuji Okamoto",
      "Tomoya Takeuchi",
      "Yusuke Sakemi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2505.02155",
    "title": "Boundary value problem of magnetically insulated diode: existence of solutions and complex bifurcation",
    "abstract": "           The paper focuses on the stationary self-consistent problem of magnetic insulation for a vacuum diode with space-charge limitation, described by a singularly perturbed Vlasov-Maxwell system of dimension 1.5. The case of insulated diode when the electrons are deflected back towards the cathode at the point $x^{*}$ is considered. First, the initial VM system is reduced to the nonlinear singular limit system of ODEs for the potentials of electric and magnetic fields. The second step deals with the limit system's reduction to the new nonlinear singular ODE equation for effective potential $\\Theta(x)$. The existence of non-negative solutions is proved for the last equation on the interval $[0, x^{*})$ where $\\Theta(x)>0$. The most interesting and unexplored case is when $\\Theta(x)<0$ on the interval $(x^{*}, 1]$ and corresponds to the case of an insulated diode. For the first time, a numerical analysis of complex bifurcation of solutions in insulated diode is considered for $\\Theta(x)<0$ depending on parameters and boundary conditions. Bifurcation diagrams of the dependence of solution $\\Theta(x)$ on a free point (free boundary) $x^{*}$ were constructed. Insulated diode spacing is found.         ",
    "url": "https://arxiv.org/abs/2505.02155",
    "authors": [
      "Denis Sidorov",
      "Alexander Sinitsyn",
      "David Leguizamon",
      "Liguo Wang"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Mathematical Physics (math-ph)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.02211",
    "title": "CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images",
    "abstract": "           Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis.         ",
    "url": "https://arxiv.org/abs/2505.02211",
    "authors": [
      "Peiqi Li",
      "Yincheng Gao",
      "Renxing Li",
      "Haojie Yang",
      "Yunyun Liu",
      "Boji Liu",
      "Jiahui Ni",
      "Ying Zhang",
      "Yulu Wu",
      "Xiaowei Fang",
      "Lehang Guo",
      "Liping Sun",
      "Jiangang Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02347",
    "title": "Temporal Robustness in Discrete Time Linear Dynamical Systems",
    "abstract": "           Discrete time linear dynamical systems, including Markov chains, have found many applications. However, in some problems, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we propose to theoretically analyze a distributional robust cost estimation task in a Wasserstein ambiguity set, instead of learning a probability distribution from a few samples. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a fundamental result about Wasserstein distance based polytope.         ",
    "url": "https://arxiv.org/abs/2505.02347",
    "authors": [
      "Nilava Metya",
      "Arunesh Sinha"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02385",
    "title": "An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation",
    "abstract": "           The segmentation of cranial nerves (CNs) tract provides a valuable quantitative tool for the analysis of the morphology and trajectory of individual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which combine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have achieved promising segmentation performance. However, it is laborious or even infeasible to collect complete multimodal data in clinical practice due to limitations in equipment, user privacy, and working conditions. In this work, we propose a novel arbitrary-modal fusion network for volumetric CNs tract segmentation, called CNTSeg-v2, which trains one model to handle different combinations of available modalities. Instead of directly combining all the modalities, we select T1-weighted (T1w) images as the primary modality due to its simplicity in data acquisition and contribution most to the results, which supervises the information selection of other auxiliary modalities. Our model encompasses an Arbitrary-Modal Collaboration Module (ACM) designed to effectively extract informative features from other auxiliary modalities, guided by the supervision of T1w images. Meanwhile, we construct a Deep Distance-guided Multi-stage (DDM) decoder to correct small errors and discontinuities through signed distance maps to improve segmentation accuracy. We evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the clinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental results show that our CNTSeg-v2 achieves state-of-the-art segmentation performance, outperforming all competing methods.         ",
    "url": "https://arxiv.org/abs/2505.02385",
    "authors": [
      "Lei Xie",
      "Huajun Zhou",
      "Junxiong Huang",
      "Jiahao Huang",
      "Qingrun Zeng",
      "Jianzhong He",
      "Jiawei Zhang",
      "Baohua Fan",
      "Mingchu Li",
      "Guoqiang Xie",
      "Hao Chen",
      "Yuanjing Feng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02396",
    "title": "Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch",
    "abstract": "           Pneumonia Diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. This uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest X-rays, and the presence of co-existing respiratory conditions. This research proposes one of the supervised learning methods, CNN. Using MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using Keras API as the built from scratch model, for identifying lung diseases especially pneumonia. The datasets used in this research were obtained from the website through Kaggle. The result shows that by implementing CNN MobileNetV2 and CNN from scratch the result is promising. While validating data, MobileNetV2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is more stable. Although it takes more time to train each epoch. Meanwhile, after the 10th epoch, the Scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. With these results, ResNet101V2 offers stability, and the Scratch model offers high accuracy.         ",
    "url": "https://arxiv.org/abs/2505.02396",
    "authors": [
      "Kennard Norbert Sudiardjo",
      "Islam Nur Alam",
      "Wilson Wijaya",
      "Lili Ayu Wulandhari"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02445",
    "title": "Efficient Classical Algorithms for Simulating Gaussian Boson Sampling on Graphs",
    "abstract": "           Gaussian Boson Sampling (GBS) is a promising candidate for demonstrating quantum computational advantage and can be applied to solving graph-related problems. In this work, we propose Markov chain Monte Carlo-based algorithms to simulate GBS on undirected, unweighted graphs. Our main contribution is a double-loop variant of Glauber dynamics, whose stationary distribution matches the GBS distribution. We further prove that it mixes in polynomial time for dense graphs using a refined canonical path argument. Numerically, we conduct experiments on graphs with 256 vertices, larger than the scales in former GBS experiments as well as classical simulations. In particular, we show that both the single-loop and double-loop Glauber dynamics improve the performance of original random search and simulated annealing algorithms for the max-Hafnian and densest $k$-subgraph problems up to 10x. Overall, our approach offers both theoretical guarantees and practical advantages for classical simulations of GBS on graphs.         ",
    "url": "https://arxiv.org/abs/2505.02445",
    "authors": [
      "Yexin Zhang",
      "Shuo Zhou",
      "Xinzhao Wang",
      "Ziruo Wang",
      "Ziyi Yang",
      "Rui Yang",
      "Yecheng Xue",
      "Tongyang Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2505.02485",
    "title": "Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints",
    "abstract": "           The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization problem with the goal to design shifts to cover prearranged bus tours. The objective takes into account the operational cost as well as the satisfaction of drivers. This problem is heavily constrained due to strict legal rules and collective agreements. The objective of this article is to provide state-of-the-art exact and hybrid solution methods that can provide high-quality solutions for instances of different sizes. This work presents a comprehensive study of both an exact method, Branch and Price (B&P), as well as a Large Neighborhood Search (LNS) framework which uses B&P or Column Generation (CG) for the repair phase to solve the BDSP. It further proposes and evaluates a novel deeper integration of B&P and LNS, storing the generated columns from the LNS subproblems and reusing them for other subproblems, or to find better global solutions. The article presents a detailed analysis of several components of the solution methods and their impact, including general improvements for the B&P subproblem, which is a high-dimensional Resource Constrained Shortest Path Problem (RCSPP), and the components of the LNS. The evaluation shows that our approach provides new state-of-the-art results for instances of all sizes, including exact solutions for small instances, and low gaps to a known lower bound for mid-sized instances. Conclusions: We observe that B&P provides the best results for small instances, while the tight integration of LNS and CG can provide high-quality solutions for larger instances, further improving over LNS which just uses CG as a black box. The proposed methods are general and can also be applied to other rule sets and related optimization problems         ",
    "url": "https://arxiv.org/abs/2505.02485",
    "authors": [
      "Lucas Kletzander",
      "Tommaso Mannelli Mazzoli",
      "Nysret Musliu",
      "Pascal Van Hentenryck"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.02529",
    "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction",
    "abstract": "           Cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. Current approaches struggle to extract consistent features from heterogeneous CT and PET images, limiting their clinical applicability. We address these challenges by introducing RobSurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. The key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. This dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via Transformer-based processing. In extensive evaluations across three diverse datasets (HECKTOR, H\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. Most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These results, combined with strong generalization across different cancer types and imaging protocols, establish RobSurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care.         ",
    "url": "https://arxiv.org/abs/2505.02529",
    "authors": [
      "Aiman Farooq",
      "Azad Singh",
      "Deepak Mishra",
      "Santanu Chaudhury"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02530",
    "title": "Energy Efficiency Maximization for CR-NOMA based Smart Grid Communication Network",
    "abstract": "           Managing massive data flows effectively and resolving spectrum shortages are two challenges that Smart Grid Communication Networks (SGCN) must overcome. To address these problems, we provide a combined optimization approach that makes use of Cognitive Radio (CR) and Non-Orthogonal Multiple Access (NOMA) technologies. Our work focuses on using user pairing (UP) and power allocation (PA) techniques to maximize energy efficiency (EE) in SGCN, particularly within Neighbourhood Area Networks (NANs). We develop a joint optimization problem that takes into account the real-world limitations of a CR-NOMA setting. This problem is NP-hard, nonlinear, and nonconvex by nature. To address the computational complexity of the problem, we use the Block Coordinate Descent (BCD) method, which breaks the problem into UP and PA subproblems. Initially, we proposed the Zebra-Optimization User Pairing (ZOUP) algorithm to tackle the UP problem, which outperforms both Orthogonal Multiple Access (OMA) and non-optimized NOMA (UPWO) by 78.8\\% and 13.6\\%, respectively, at a SNR of 15 dB. Based on the ZOUP pairs, we subsequently proposed the PA approach, i.e., ZOUPPA, which significantly outperforms UPWO and ZOUP by 53.2\\% and 25.4\\%, respectively, at an SNR of 15 dB. A detailed analysis of key parameters, including varying SNRs, power allocation constants, path loss exponents, user density, channel availability, and coverage radius, underscores the superiority of our approach. By facilitating the effective use of communication resources in SGCN, our research opens the door to more intelligent and energy-efficient grid systems. Our work tackles important issues in SGCN and lays the groundwork for future developments in smart grid communication technologies by combining modern optimization approaches with CR-NOMA.         ",
    "url": "https://arxiv.org/abs/2505.02530",
    "authors": [
      "Mubashar Sarfraz",
      "Sheraz Alam",
      "Sajjad A. Ghauri",
      "Asad Mahmood"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2505.02613",
    "title": "Lane-Wise Highway Anomaly Detection",
    "abstract": "           This paper proposes a scalable and interpretable framework for lane-wise highway traffic anomaly detection, leveraging multi-modal time series data extracted from surveillance cameras. Unlike traditional sensor-dependent methods, our approach uses AI-powered vision models to extract lane-specific features, including vehicle count, occupancy, and truck percentage, without relying on costly hardware or complex road modeling. We introduce a novel dataset containing 73,139 lane-wise samples, annotated with four classes of expert-validated anomalies: three traffic-related anomalies (lane blockage and recovery, foreign object intrusion, and sustained congestion) and one sensor-related anomaly (camera angle shift). Our multi-branch detection system integrates deep learning, rule-based logic, and machine learning to improve robustness and precision. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in precision, recall, and F1-score, providing a cost-effective and scalable solution for real-world intelligent transportation systems.         ",
    "url": "https://arxiv.org/abs/2505.02613",
    "authors": [
      "Mei Qiu",
      "William Lorenz Reindl",
      "Yaobin Chen",
      "Stanley Chien",
      "Shu Hu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.02677",
    "title": "Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data",
    "abstract": "           Stroke is a major public health problem, affecting millions worldwide. Deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. However, existing methods rely on costly medical imaging modalities, such as computed tomography. Recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. Hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. We propose a multimodal deep neural network that processes Optical Coherence Tomography (OCT) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. We pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. Our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. The experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\\% AUROC improvement as compared to the unimodal image-only baseline, and $8$\\% improvement compared to an existing state-of-the-art foundation model. In conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes.         ",
    "url": "https://arxiv.org/abs/2505.02677",
    "authors": [
      "Saeed Shurrab",
      "Aadim Nepal",
      "Terrence J. Lee-St. John",
      "Nicola G. Ghazi",
      "Bartlomiej Piechowski-Jozwiak",
      "Farah E. Shamout"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.02768",
    "title": "Linear colorings of graphs",
    "abstract": "           Motivated by algorithmic applications, Kun, O'Brien, Pilipczuk, and Sullivan introduced the parameter linear chromatic number as a relaxation of treedepth and proved that the two parameters are polynomially related. They conjectured that treedepth could be bounded from above by twice the linear chromatic number. In this paper we investigate the properties of linear chromatic number and provide improved bounds in several graph classes.         ",
    "url": "https://arxiv.org/abs/2505.02768",
    "authors": [
      "Claire Hilaire",
      "Matja\u017e Krnc",
      "Martin Milani\u010d",
      "Jean-Florent Raymond"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2008.06255",
    "title": "From Attack to Protection: Leveraging Watermarking Attack Network for Advanced Add-on Watermarking",
    "abstract": "           Multi-bit watermarking (MW) has been designed to enhance resistance against watermarking attacks, such as signal processing operations and geometric distortions. Various benchmark tools exist to assess this robustness through simulated attacks on watermarked images. However, these tools often fail to capitalize on the unique attributes of the targeted MW and typically neglect the aspect of visual quality, a critical factor in practical applications. To overcome these shortcomings, we introduce a watermarking attack network (WAN), a fully trainable watermarking benchmark tool designed to exploit vulnerabilities within MW systems and induce watermark bit inversions, significantly diminishing watermark extractability. The proposed WAN employs an architecture based on residual dense blocks, which is adept at both local and global feature learning, thereby maintaining high visual quality while obstructing the extraction of embedded information. Our empirical results demonstrate that the WAN effectively undermines various block-based MW systems while minimizing visual degradation caused by attacks. This is facilitated by our novel watermarking attack loss, which is specifically crafted to compromise these systems. The WAN functions not only as a benchmarking tool but also as an add-on watermarking (AoW) mechanism, augmenting established universal watermarking schemes by enhancing robustness or imperceptibility without requiring detailed method context and adapting to dynamic watermarking requirements. Extensive experimental results show that AoW complements the performance of the targeted MW system by independently enhancing both imperceptibility and robustness.         ",
    "url": "https://arxiv.org/abs/2008.06255",
    "authors": [
      "Seung-Hun Nam",
      "Jihyeon Kang",
      "Daesik Kim",
      "Namhyuk Ahn",
      "Wonhyuk Ahn"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2302.06375",
    "title": "One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data",
    "abstract": "           There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.         ",
    "url": "https://arxiv.org/abs/2302.06375",
    "authors": [
      "Simone Luetto",
      "Fabrizio Garuti",
      "Enver Sangineto",
      "Lorenzo Forni",
      "Rita Cucchiara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2303.07546",
    "title": "Constrained Adversarial Learning for Automated Software Testing: a literature review",
    "abstract": "           It is imperative to safeguard computer applications and information systems against the growing number of cyber-attacks. Automated software testing tools can be developed to quickly analyze many lines of code and detect vulnerabilities by generating function-specific testing data. This process draws similarities to the constrained adversarial examples generated by adversarial machine learning methods, so there could be significant benefits to the integration of these methods in testing tools to identify possible attack vectors. Therefore, this literature review is focused on the current state-of-the-art of constrained data generation approaches applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance their software testing tools with adversarial testing methods and improve the resilience and robustness of their information systems. The found approaches were systematized, and the advantages and limitations of those specific for white-box, grey-box, and black-box testing were analyzed, identifying research gaps and opportunities to automate the testing tools with data generated by adversarial attacks.         ",
    "url": "https://arxiv.org/abs/2303.07546",
    "authors": [
      "Jo\u00e3o Vitorino",
      "Tiago Dias",
      "Tiago Fonseca",
      "Eva Maia",
      "Isabel Pra\u00e7a"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.13243",
    "title": "A Model Predictive Capture Point Control Framework for Robust Humanoid Balancing via Ankle, Hip, and Stepping Strategies",
    "abstract": "           The robust balancing capability of humanoids is essential for mobility in real environments. Many studies focus on implementing human-inspired ankle, hip, and stepping strategies to achieve human-level balance. In this paper, a robust balance control framework for humanoids is proposed. Firstly, a Model Predictive Control (MPC) framework is proposed for Capture Point (CP) tracking control, enabling the integration of ankle, hip, and stepping strategies within a single framework. Additionally, a variable weighting method is introduced that adjusts the weighting parameters of the Centroidal Angular Momentum damping control. Secondly, a hierarchical structure of the MPC and a stepping controller was proposed, allowing for the step time optimization. The robust balancing performance of the proposed method is validated through simulations and real robot experiments. Furthermore, a superior balancing performance is demonstrated compared to a state-of-the-art Quadratic Programming-based CP controller that employs the ankle, hip, and stepping strategies.         ",
    "url": "https://arxiv.org/abs/2307.13243",
    "authors": [
      "Myeong-Ju Kim",
      "Daegyu Lim",
      "Gyeongjae Park",
      "Kwanwoo Lee",
      "Jaeheung Park"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2308.04369",
    "title": "SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition",
    "abstract": "           Event camera-based pattern recognition is a newly arising research topic in recent years. Current researchers usually transform the event streams into images, graphs, or voxels, and adopt deep neural networks for event-based classification. Although good performance can be achieved on simple event recognition datasets, however, their results may be still limited due to the following two issues. Firstly, they adopt spatial sparse event streams for recognition only, which may fail to capture the color and detailed texture information well. Secondly, they adopt either Spiking Neural Networks (SNN) for energy-efficient recognition with suboptimal results, or Artificial Neural Networks (ANN) for energy-intensive, high-performance recognition. However, seldom of them consider achieving a balance between these two aspects. In this paper, we formally propose to recognize patterns by fusing RGB frames and event streams simultaneously and propose a new RGB frame-event recognition framework to address the aforementioned issues. The proposed method contains four main modules, i.e., memory support Transformer network for RGB frame encoding, spiking neural network for raw event stream encoding, multi-modal bottleneck fusion module for RGB-Event feature aggregation, and prediction head. Due to the scarce of RGB-Event based classification dataset, we also propose a large-scale PokerEvent dataset which contains 114 classes, and 27102 frame-event pairs recorded using a DVS346 event camera. Extensive experiments on two RGB-Event based classification datasets fully validated the effectiveness of our proposed framework. We hope this work will boost the development of pattern recognition by fusing RGB frames and event streams. Both our dataset and source code of this work will be released at this https URL ",
    "url": "https://arxiv.org/abs/2308.04369",
    "authors": [
      "Xiao Wang",
      "Yao Rong",
      "Zongzhen Wu",
      "Lin Zhu",
      "Bo Jiang",
      "Jin Tang",
      "Yonghong Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2309.15670",
    "title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
    "abstract": "           In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT), a well-known methodology of transformers, have been shown the best results of all methods implemented. Finally, a web application has been developed to demonstrate the performance of the pre-trained top-performer model (BERT) for multi-label ER in Bangla.         ",
    "url": "https://arxiv.org/abs/2309.15670",
    "authors": [
      "Sumit Kumar Banshal",
      "Sajal Das",
      "Shumaiya Akter Shammi",
      "Narayan Ranjan Chakraborty",
      "Aulia Luqman Aziz",
      "Mohammed Aljuaid",
      "Fazla Rabby",
      "Rohit Bansal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.01770",
    "title": "A simple connection from loss flatness to compressed neural representations",
    "abstract": "           Sharpness, a geometric measure in the parameter space that reflects the flatness of the loss landscape, has long been studied for its potential connections to neural network behavior. While sharpness is often associated with generalization, recent work highlights inconsistencies in this relationship, leaving its true significance unclear. In this paper, we investigate how sharpness influences the local geometric features of neural representations in feature space, offering a new perspective on its role. We introduce this problem and study three measures for compression: the Local Volumetric Ratio (LVR), based on volume compression, the Maximum Local Sensitivity (MLS), based on sensitivity to input changes, and the Local Dimensionality, based on how uniform the sensitivity is on different directions. We show that LVR and MLS correlate with the flatness of the loss around the local minima; and that this correlation is predicted by a relatively simple mathematical relationship: a flatter loss corresponds to a lower upper bound on the compression metrics of neural representations. Our work builds upon the linear stability insight by Ma and Ying, deriving inequalities between various compression metrics and quantities involving sharpness. Our inequalities readily extend to reparametrization-invariant sharpness as well. Through empirical experiments on various feedforward, convolutional, and transformer architectures, we find that our inequalities predict a consistently positive correlation between local representation compression and sharpness.         ",
    "url": "https://arxiv.org/abs/2310.01770",
    "authors": [
      "Shirui Chen",
      "Stefano Recanatesi",
      "Eric Shea-Brown"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.04391",
    "title": "On a Hierarchy of Spectral Invariants for Graphs",
    "abstract": "           We consider a hierarchy of graph invariants that naturally extends the spectral invariants defined by F\u00fcrer (Lin. Alg. Appl. 2010) based on the angles formed by the set of standard basis vectors and their projections onto eigenspaces of the adjacency matrix. We provide a purely combinatorial characterization of this hierarchy in terms of the walk counts. This allows us to give a complete answer to F\u00fcrer's question about the strength of his invariants in distinguishing non-isomorphic graphs in comparison to the 2-dimensional Weisfeiler-Leman algorithm, extending the recent work of Rattan and Seppelt (SODA 2023). As another application of the characterization, we prove that almost all graphs are determined up to isomorphism in terms of the spectrum and the angles, which is of interest in view of the long-standing open problem whether almost all graphs are determined by their eigenvalues alone. Finally, we describe the exact relationship between the hierarchy and the Weisfeiler-Leman algorithms for small dimensions, as also some other important spectral characteristics of a graph such as the generalized and the main spectra.         ",
    "url": "https://arxiv.org/abs/2310.04391",
    "authors": [
      "V. Arvind",
      "Frank Fuhlbr\u00fcck",
      "Johannes K\u00f6bler",
      "Oleg Verbitsky"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2310.06417",
    "title": "Supercharging Graph Transformers with Advective Diffusion",
    "abstract": "           The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.         ",
    "url": "https://arxiv.org/abs/2310.06417",
    "authors": [
      "Qitian Wu",
      "Chenxiao Yang",
      "Kaipeng Zeng",
      "Michael Bronstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2312.00933",
    "title": "Privacy Preserving Event Detection",
    "abstract": "           This paper presents a privacy-preserving event detection scheme based on measurements made by a network of sensors. A diameter-like decision statistic made up of the marginal types of the measurements observed by the sensors is employed. The proposed detection scheme can achieve the best type-I error exponent as the type-II error rate is required to be negligible. Detection performance with finite-length observations is also demonstrated through a simulation example of spectrum sensing. Privacy protection is achieved by obfuscating the sensors' marginal types with random zero-modulo-sum numbers that are generated and distributed via the exchange of encrypted messages among the sensors. The privacy-preserving performance against \"honest but curious\" adversaries, including colluding sensors, the fusion center, and external eavesdroppers, is analyzed through a series of cryptographic games. It is shown that the probability that any probabilistic polynomial time adversary successfully estimates the sensors' measured types cannot be much better than independent guessing, when there are at least two non-colluding sensors.         ",
    "url": "https://arxiv.org/abs/2312.00933",
    "authors": [
      "Xiaoshan Wang",
      "Tan F. Wong"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2312.17169",
    "title": "Improving Code Reviewer Recommendation: Accuracy, Latency, Workload, and Bystanders",
    "abstract": "           The code review team at Meta is continuously improving the code review process. To evaluate the new recommenders, we conduct three A/B tests which are a type of randomized controlled experimental trial. Expt 1. We developed a new recommender based on features that had been successfully used in the literature and that could be calculated with low latency. In an A/B test on 82k diffs in Spring of 2022, we found that the new recommender was more accurate and had lower latency. Expt 2. Reviewer workload is not evenly distributed, our goal was to reduce the workload of top reviewers. We then ran an A/B test on 28k diff authors in Winter 2023 on a workload balanced recommender. Our A/B test led to mixed results. Expt 3. We suspected the bystander effect might be slowing down reviews of diffs where only a team was assigned. We conducted an A/B test on 12.5k authors in Spring 2023 and found a large decrease in the amount of time it took for diffs to be reviewed when a recommended individual was explicitly assigned. Our findings also suggest there can be a discrepancy between historical back-testing and A/B test experimental findings.         ",
    "url": "https://arxiv.org/abs/2312.17169",
    "authors": [
      "Peter C. Rigby",
      "Seth Rogers",
      "Sadruddin Saleem",
      "Parth Suresh",
      "Daniel Suskin",
      "Patrick Riggs",
      "Chandra Maddila",
      "Nachiappan Nagappan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2401.00688",
    "title": "Inference and Visualization of Community Structure in Attributed Hypergraphs Using Mixed-Membership Stochastic Block Models",
    "abstract": "           Hypergraphs represent complex systems involving interactions among more than two entities and allow the investigation of higher-order structure and dynamics in complex systems. Node attribute data, which often accompanies network data, can enhance the inference of community structure in complex systems. While mixed-membership stochastic block models have been employed to infer community structure in hypergraphs, they complicate the visualization and interpretation of inferred community structure by assuming that nodes may possess soft community memberships. In this study, we propose a framework, HyperNEO, that combines mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods. Our approach generates a node layout that largely preserves the community memberships of nodes. We evaluate our framework on both synthetic and empirical hypergraphs with node attributes. We expect our framework will broaden the investigation and understanding of higher-order community structure in complex systems.         ",
    "url": "https://arxiv.org/abs/2401.00688",
    "authors": [
      "Kazuki Nakajima",
      "Takeaki Uno"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.14801",
    "title": "Mochi: Fast \\& Exact Collision Detection",
    "abstract": "           Collision Detection (CD) has several applications across the domains such as robotics, visual graphics, and fluid mechanics. Finding exact collisions between the objects in the scene is quite computationally intensive. To quickly filter the object pairs that do not result in a collision, bounding boxes are built on the objects, indexed using a Bounding Volume Hierarchy(BVH), and tested for intersection before performing the expensive object-object intersection tests. In state-of-the-art CD libraries, accelerators such as GPUs are used to accelerate BVH traversal by building specialized data structures. The recent addition of ray tracing architecture to GPU hardware is designed to do the same but in the context of implementing a Ray Tracing algorithm to render a graphical scene in real-time. We present Mochi, a fast and exact collision detection engine that accelerates both the broad and narrow phases by taking advantage of the capabilities of Ray Tracing cores. We introduce multiple new reductions to perform generic CD to support three types of objects for CD: simple spherical particles, objects describable by mathematical equations, and complex objects composed of a triangle mesh. By implementing our reductions, Mochi achieves several orders of magnitude speedups on synthetic datasets and 5x-28x speedups on real-world triangle mesh datasets. We further evaluate our reductions thoroughly and provide several architectural insights on the ray tracing cores that are otherwise unknown due to their proprietorship.         ",
    "url": "https://arxiv.org/abs/2402.14801",
    "authors": [
      "Durga Keerthi Mandarapu",
      "Nicholas James",
      "Milind Kulkarni"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2402.16310",
    "title": "REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories",
    "abstract": "           Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.5\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.         ",
    "url": "https://arxiv.org/abs/2402.16310",
    "authors": [
      "Bangchao Deng",
      "Bingqing Qu",
      "Pengyang Wang",
      "Dingqi Yang",
      "Benjamin Fankhauser",
      "Philippe Cudre-Mauroux"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.06432",
    "title": "Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain",
    "abstract": "           Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructing dynamic graphs, which enables the learning of higher-level semantic representations considering temporal perspectives, addressing the challenges in fMRI data representation learning. Utilizing the large-scale UK Biobank dataset for self-supervised learning, ST-JEMA shows exceptional representation learning performance on dynamic functional connectivity demonstrating superiority over previous methods in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI datasets even with limited samples and effectiveness of temporal reconstruction on missing data scenarios. These findings highlight the potential of our approach as a robust representation learning method for leveraging label-scarce fMRI data.         ",
    "url": "https://arxiv.org/abs/2403.06432",
    "authors": [
      "Jungwon Choi",
      "Hyungi Lee",
      "Byung-Hoon Kim",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2403.09782",
    "title": "Time-Constrained Erasure Correction for Data Recovery in UAV-LoRa-WuR Networks",
    "abstract": "           We described two erasure-correction schemes for data recovery in UAV-LoRa-WuR networks. Our results show that unless the maximum number for redundant frames a sensor can send per data-collection cycle is very small, erasure coding provides noticeable improvements over an uncoded transmissions. Whether to employ coding -- and if so, which type -- should be determined based on the sensors' energy budget (which dictates the maximum redundancy), the UAV's hovering time, and the node density. The analytical framework presented above aids in this decision making.         ",
    "url": "https://arxiv.org/abs/2403.09782",
    "authors": [
      "Kushwanth Sistu",
      "Siddhartha S. Borkotoky"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2404.02810",
    "title": "Generative-Contrastive Heterogeneous Graph Neural Network",
    "abstract": "           Heterogeneous Graphs (HGs) effectively model complex relationships in the real world through multi-type nodes and edges. In recent years, inspired by self-supervised learning (SSL), contrastive learning (CL)-based Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential in utilizing data augmentation and contrastive discriminators for downstream tasks. However, data augmentation remains limited due to the graph data's integrity. Furthermore, the contrastive discriminators suffer from sampling bias and lack local heterogeneous information. To tackle the above limitations, we propose a novel Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN). Specifically, we propose a heterogeneous graph generative learning method that enhances CL-based paradigm. This paradigm includes: 1) A contrastive view augmentation strategy using a masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generating hard negative samples. 3) A hierarchical contrastive learning strategy aimed at capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced contrastive discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest baselines on node classification and link prediction tasks.         ",
    "url": "https://arxiv.org/abs/2404.02810",
    "authors": [
      "Yu Wang",
      "Lei Sang",
      "Yi Zhang",
      "Yiwen Zhang",
      "Xindong Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2404.07878",
    "title": "LeapFrog: The Rowhammer Instruction Skip Attack",
    "abstract": "           Since its inception, Rowhammer exploits have rapidly evolved into increasingly sophisticated threats compromising data integrity and the control flow integrity of victim processes. Nevertheless, it remains a challenge for an attacker to identify vulnerable targets (i.e., Rowhammer gadgets), understand the outcome of the attempted fault, and formulate an attack that yields useful results. In this paper, we present a new type of Rowhammer gadget, called a LeapFrog gadget, which, when present in the victim code, allows an adversary to subvert code execution to bypass a critical piece of code (e.g., authentication check logic, encryption rounds, padding in security protocols). The LeapFrog gadget manifests when the victim code stores the Program Counter (PC) value in the user or kernel stack (e.g., a return address during a function call) which, when tampered with, repositions the return address to a location that bypasses a security-critical code pattern. This research also presents a systematic process to identify LeapFrog gadgets. This methodology enables the automated detection of susceptible targets and the determination of optimal attack parameters. We first show the attack on a decision tree algorithm to show the potential implications. Secondly, we employ the attack on OpenSSL to bypass the encryption and reveal the plaintext. We then use our tools to scan the Open Quantum Safe library and report on the number of LeapFrog gadgets in the code. Lastly, we demonstrate this new attack vector through a practical demonstration in a client/server TLS handshake scenario, successfully inducing an instruction skip in a client application. Our findings extend the impact of Rowhammer attacks on control flow and contribute to developing more robust defenses against these increasingly sophisticated threats.         ",
    "url": "https://arxiv.org/abs/2404.07878",
    "authors": [
      "Andrew Adiletta",
      "M. Caner Tol",
      "Kemal Derya",
      "Berk Sunar",
      "Saad Islam"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2404.12978",
    "title": "Strengthening Infrastructure Resilience to Hurricanes by Modeling Transportation and Electric Power Network Interdependencies",
    "abstract": "           This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.         ",
    "url": "https://arxiv.org/abs/2404.12978",
    "authors": [
      "Tasnuba Binte Jamal",
      "Samiul Hasan",
      "Omar I. Abdul-Aziz",
      "Pallab Mozumder",
      "Rounak Meyur"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2405.00318",
    "title": "Covariant spatio-temporal receptive fields for spiking neural networks",
    "abstract": "           Biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient. Neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it. There are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations. Here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine Gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time. Our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains. We use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision. This work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems. Our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control.         ",
    "url": "https://arxiv.org/abs/2405.00318",
    "authors": [
      "Jens Egholm Pedersen",
      "J\u00f6rg Conradt",
      "Tony Lindeberg"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.02646",
    "title": "Updating Windows Malware Detectors: Balancing Robustness and Regression against Adversarial EXEmples",
    "abstract": "           Adversarial EXEmples are carefully-perturbed programs tailored to evade machine learning Windows malware detectors, with an ongoing effort to develop robust models able to address detection effectiveness. However, even if robust models can prevent the majority of EXEmples, to maintain predictive power over time, models are fine-tuned to newer threats, leading either to partial updates or time-consuming retraining from scratch. Thus, even if the robustness against adversarial EXEmples is higher, the new models might suffer a regression in performance by misclassifying threats that were previously correctly detected. For these reasons, we study the trade-off between accuracy and regression when updating Windows malware detectors by proposing EXE-scanner, a plugin that can be chained to existing detectors to promptly stop EXEmples without causing regression. We empirically show that previously proposed hardening techniques suffer a regression of accuracy when updating non-robust models, exacerbating the gap when considering low false positives regimes and temporal drifts affecting data. Also, through EXE-scanner we gain evidence on the detectability of adversarial EXEmples, showcasing the presence of artifacts left inside while creating them. Due to its design, EXE-scanner can be chained to any classifier to obtain the best performance without the need for costly retraining. To foster reproducibility, we openly release the source code, along with the dataset of adversarial EXEmples based on state-of-the-art perturbation algorithms.         ",
    "url": "https://arxiv.org/abs/2405.02646",
    "authors": [
      "Matous Kozak",
      "Luca Demetrio",
      "Dmitrijs Trizna",
      "Fabio Roli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2405.09591",
    "title": "A Comprehensive Survey on Data Augmentation",
    "abstract": "           Data augmentation is a series of techniques that generate high-quality artificial data by manipulating existing data samples. By leveraging data augmentation techniques, AI models can achieve significantly improved applicability in tasks involving scarce or imbalanced datasets, thereby substantially enhancing AI models' generalization capabilities. Existing literature surveys only focus on a certain type of specific modality data, and categorize these methods from modality-specific and operation-centric perspectives, which lacks a consistent summary of data augmentation methods across multiple modalities and limits the comprehension of how existing data samples serve the data augmentation process. To bridge this gap, we propose a more enlightening taxonomy that encompasses data augmentation techniques for different common data modalities. Specifically, from a data-centric perspective, this survey proposes a modality-independent taxonomy by investigating how to take advantage of the intrinsic relationship between data samples, including single-wise, pair-wise, and population-wise sample data augmentation methods. Additionally, we categorize data augmentation methods across five data modalities through a unified inductive approach.         ",
    "url": "https://arxiv.org/abs/2405.09591",
    "authors": [
      "Zaitian Wang",
      "Pengfei Wang",
      "Kunpeng Liu",
      "Pengyang Wang",
      "Yanjie Fu",
      "Chang-Tien Lu",
      "Charu C. Aggarwal",
      "Jian Pei",
      "Yuanchun Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.13745",
    "title": "NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation",
    "abstract": "           Quadrilateral mesh generation plays a crucial role in numerical simulations within Computer-Aided Design and Engineering (CAD/E). Producing high-quality quadrangulation typically requires satisfying four key criteria. First, the quadrilateral mesh should closely align with principal curvature directions. Second, singular points should be strategically placed and effectively minimized. Third, the mesh should accurately conform to sharp feature edges. Lastly, quadrangulation results should exhibit robustness against noise and minor geometric variations. Existing methods generally involve first computing a regular cross field to represent quad element orientations across the surface, followed by extracting a quadrilateral mesh aligned closely with this cross field. A primary challenge with this approach is balancing the smoothness of the cross field with its alignment to pre-computed principal curvature directions, which are sensitive to small surface perturbations and often ill-defined in spherical or planar regions. To tackle this challenge, we propose NeurCross, a novel framework that simultaneously optimizes a cross field and a neural signed distance function (SDF), whose zero-level set serves as a proxy of the input shape. Our joint optimization is guided by three factors: faithful approximation of the optimized SDF surface to the input surface, alignment between the cross field and the principal curvature field derived from the SDF surface, and smoothness of the cross field. Acting as an intermediary, the neural SDF contributes in two essential ways. First, it provides an alternative, optimizable base surface exhibiting more regular principal curvature directions for guiding the cross field. Second, we leverage the Hessian matrix of the neural SDF to implicitly enforce cross field alignment with principal curvature directions...         ",
    "url": "https://arxiv.org/abs/2405.13745",
    "authors": [
      "Qiujie Dong",
      "Huibiao Wen",
      "Rui Xu",
      "Shuangmin Chen",
      "Jiaran Zhou",
      "Shiqing Xin",
      "Changhe Tu",
      "Taku Komura",
      "Wenping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.01183",
    "title": "Automatic Input Feature Relevance via Spectral Neural Networks",
    "abstract": "           In machine learning practice it is often useful to identify relevant input features, so as to obtain compact dataset for more efficient numerical handling. On the other hand, by isolating key input elements, ranked according their respective degree of relevance, can help to elaborate on the process of decision making. Here, we propose a novel method to estimate the relative importance of the input components for a Deep Neural Network. This is achieved by leveraging on a spectral re-parametrization of the optimization process. Eigenvalues associated to input nodes provide in fact a robust proxy to gauge the relevance of the supplied entry features. Notably, the spectral features ranking is performed automatically, as a byproduct of the network training, with no additional processing to be carried out. The technique is successfully challenged against both synthetic and real data.         ",
    "url": "https://arxiv.org/abs/2406.01183",
    "authors": [
      "Lorenzo Chicchi",
      "Lorenzo Buffoni",
      "Diego Febbe",
      "Lorenzo Giambagli",
      "Raffaele Marino",
      "Duccio Fanelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.07361",
    "title": "Deep Implicit Optimization enables Robust Learnable Features for Deformable Image Registration",
    "abstract": "           Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, existing DLIR methods forego many of the benefits and invariances of optimization methods. The lack of a task-specific inductive bias in DLIR methods leads to suboptimal performance, especially in the presence of domain shift. Our method aims to bridge this gap between statistical learning and optimization by explicitly incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, we explicitly exploit invariances of the correspondence matching problem induced by the optimization, while learning registration and label-aware features, and guaranteeing the warp functions to be a local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features and arbitrary test-time regularization, which is not possible with existing DLIR methods.         ",
    "url": "https://arxiv.org/abs/2406.07361",
    "authors": [
      "Rohit Jena",
      "Pratik Chaudhari",
      "James C. Gee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.07880",
    "title": "A Comprehensive Survey on Machine Learning Driven Material Defect Detection",
    "abstract": "           Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice.         ",
    "url": "https://arxiv.org/abs/2406.07880",
    "authors": [
      "Jun Bai",
      "Di Wu",
      "Tristan Shelley",
      "Peter Schubel",
      "David Twine",
      "John Russell",
      "Xuesen Zeng",
      "Ji Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2406.08918",
    "title": "Beyond the Calibration Point: Mechanism Comparison in Differential Privacy",
    "abstract": "           In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\\varepsilon, \\delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities.         ",
    "url": "https://arxiv.org/abs/2406.08918",
    "authors": [
      "Georgios Kaissis",
      "Stefan Kolek",
      "Borja Balle",
      "Jamie Hayes",
      "Daniel Rueckert"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2406.09355",
    "title": "Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models",
    "abstract": "           Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being \"hidden\" behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to \"steal\" these models for retrieval by training thief models on text-embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.         ",
    "url": "https://arxiv.org/abs/2406.09355",
    "authors": [
      "Manveer Singh Tamber",
      "Jasper Xian",
      "Jimmy Lin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2407.08159",
    "title": "Model-agnostic clean-label backdoor mitigation in cybersecurity environments",
    "abstract": "           The training phase of machine learning models is a delicate step, especially in cybersecurity contexts. Recent research has surfaced a series of insidious training-time attacks that inject backdoors in models designed for security classification tasks without altering the training labels. With this work, we propose new techniques that leverage insights in cybersecurity threat models to effectively mitigate these clean-label poisoning attacks, while preserving the model utility. By performing density-based clustering on a carefully chosen feature subspace, and progressively isolating the suspicious clusters through a novel iterative scoring procedure, our defensive mechanism can mitigate the attacks without requiring many of the common assumptions in the existing backdoor defense literature. To show the generality of our proposed mitigation, we evaluate it on two clean-label model-agnostic attacks on two different classic cybersecurity data modalities: network flows classification and malware classification, using gradient boosting and neural network models.         ",
    "url": "https://arxiv.org/abs/2407.08159",
    "authors": [
      "Giorgio Severi",
      "Simona Boboila",
      "John Holodnak",
      "Kendra Kratkiewicz",
      "Rauf Izmailov",
      "Michael J. De Lucia",
      "Alina Oprea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.19479",
    "title": "Commitment Attacks on Ethereum's Reward Mechanism",
    "abstract": "           Validators in permissionless, large-scale blockchains, such as Ethereum, are typically payoff-maximizing, rational actors. Ethereum relies on in-protocol incentives, like rewards for correct and timely votes, to induce honest behavior and secure the blockchain. However, external incentives, such as the block proposer's opportunity to capture maximal extractable value (MEV), may tempt validators to deviate from honest protocol participation. We show a series of commitment attacks on LMD GHOST, a core part of Ethereum's consensus mechanism. We demonstrate how a single adversarial block proposer can orchestrate long-range chain reorganizations by manipulating Ethereum's reward system for timely votes. These attacks disrupt the intended balance of power between proposers and voters: by leveraging credible threats, the adversarial proposer can coerce voters from previous slots into supporting blocks that conflict with the honest chain, enabling a chain reorganization. In response, we introduce a novel reward mechanism that restores the voters' role as a check against proposer power. Our proposed mitigation is fairer and more decentralized, not only in the context of these attacks, but also practical for implementation in Ethereum.         ",
    "url": "https://arxiv.org/abs/2407.19479",
    "authors": [
      "Roozbeh Sarenche",
      "Ertem Nusret Tas",
      "Barnabe Monnot",
      "Caspar Schwarz-Schilling",
      "Bart Preneel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.15186",
    "title": "Easy-access online social media metrics can foster the identification of misinformation sharing users",
    "abstract": "           Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.         ",
    "url": "https://arxiv.org/abs/2408.15186",
    "authors": [
      "J\u00falia Sz\u00e1mely",
      "Alessandro Galeazzi",
      "J\u00falia Koltai",
      "Elisa Omodei"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2409.05975",
    "title": "CoDiCast: Conditional Diffusion Model for Global Weather Prediction with Uncertainty Quantification",
    "abstract": "           Accurate weather forecasting is critical for science and society. Yet, existing methods have not managed to simultaneously have the properties of high accuracy, low uncertainty, and high computational efficiency. On one hand, to quantify the uncertainty in weather predictions, the strategy of ensemble forecast (i.e., generating a set of diverse predictions) is often employed. However, traditional ensemble numerical weather prediction (NWP) is computationally intensive. On the other hand, most existing machine learning-based weather prediction (MLWP) approaches are efficient and accurate. Nevertheless, they are deterministic and cannot capture the uncertainty of weather forecasting. In this work, we propose CoDiCast, a conditional diffusion model to generate accurate global weather prediction, while achieving uncertainty quantification with ensemble forecasts and modest computational cost. The key idea is to simulate a conditional version of the reverse denoising process in diffusion models, which starts from pure Gaussian noise to generate realistic weather scenarios for a future time point. Each denoising step is conditioned on observations from the recent past. Ensemble forecasts are achieved by repeatedly sampling from stochastic Gaussian noise to represent uncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMWF). Experimental results demonstrate that our approach outperforms several existing data-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can generate 6-day global weather forecasts, at 6-hour steps and $5.625^\\circ$ latitude-longitude resolution, for over 5 variables, in about 12 minutes on a commodity A100 GPU machine with 80GB memory. The open-souced code is provided at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.05975",
    "authors": [
      "Jimeng Shi",
      "Bowen Jin",
      "Jiawei Han",
      "Sundararaman Gopalakrishnan",
      "Giri Narasimhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2409.09256",
    "title": "Audio-text Retrieval with Transformer-based Hierarchical Alignment and Disentangled Cross-modal Representation",
    "abstract": "           Most existing audio-text retrieval (ATR) approaches typically rely on a single-level interaction to associate audio and text, limiting their ability to align different modalities and leading to suboptimal matches. In this work, we present a novel ATR framework that leverages two-stream Transformers in conjunction with a Hierarchical Alignment (THA) module to identify multi-level correspondences of different Transformer blocks between audio and text. Moreover, current ATR methods mainly focus on learning a global-level representation, missing out on intricate details to capture audio occurrences that correspond to textual semantics. To bridge this gap, we introduce a Disentangled Cross-modal Representation (DCR) approach that disentangles high-dimensional features into compact latent factors to grasp fine-grained audio-text semantic correlations. Additionally, we develop a confidence-aware (CA) module to estimate the confidence of each latent factor pair and adaptively aggregate cross-modal latent factors to achieve local semantic alignment. Experiments show that our THA effectively boosts ATR performance, with the DCR approach further contributing to consistent performance gains.         ",
    "url": "https://arxiv.org/abs/2409.09256",
    "authors": [
      "Yifei Xin",
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Xusheng Yang",
      "Yuexian Zou"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2409.09724",
    "title": "MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection",
    "abstract": "           The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations.         ",
    "url": "https://arxiv.org/abs/2409.09724",
    "authors": [
      "Yaning Zhang",
      "Tianyi Wang",
      "Zitong Yu",
      "Zan Gao",
      "Linlin Shen",
      "Shengyong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2409.11529",
    "title": "Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor Decompositions and Deep Unrolling",
    "abstract": "           Anomaly detection (AD) is increasingly recognized as a key component for ensuring the resilience of future communication systems. While deep learning has shown state-of-the-art AD performance, its application in critical systems is hindered by concerns regarding training data efficiency, domain adaptation and interpretability. This work considers AD in network flows using incomplete measurements, leveraging a robust tensor decomposition approach and deep unrolling techniques to address these challenges. We first propose a novel block-successive convex approximation algorithm based on a regularized model-fitting objective where the normal flows are modeled as low-rank tensors and anomalies as sparse. An augmentation of the objective is introduced to decrease the computational cost. We apply deep unrolling to derive a novel deep network architecture based on our proposed algorithm, treating the regularization parameters as learnable weights. Inspired by Bayesian approaches, we extend the model architecture to perform online adaptation to per-flow and per-time-step statistics, improving AD performance while maintaining a low parameter count and preserving the problem's permutation equivariances. To optimize the deep network weights for detection performance, we employ a homotopy optimization approach based on an efficient approximation of the area under the receiver operating characteristic curve. Extensive experiments on synthetic and real-world data demonstrate that our proposed deep network architecture exhibits a high training data efficiency, outperforms reference methods, and adapts seamlessly to varying network topologies.         ",
    "url": "https://arxiv.org/abs/2409.11529",
    "authors": [
      "Lukas Schynol",
      "Marius Pesavento"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.14535",
    "title": "Hyper-parameter Optimization for Wireless Network Traffic Prediction Models with A Novel Meta-Learning Framework",
    "abstract": "           This paper proposes a novel meta-learning based hyper-parameter optimization framework for wireless network traffic prediction (NTP) models. The primary objective is to accumulate and leverage the acquired hyper-parameter optimization experience, enabling the rapid determination of optimal hyper-parameters for new tasks. In this paper, an attention-based deep neural network (ADNN) is employed as the base-learner to address specific NTP tasks. The meta-learner is an innovative framework that integrates meta-learning with the k-nearest neighbor algorithm (KNN), genetic algorithm (GA), and gated residual network (GRN). Specifically, KNN is utilized to identify a set of candidate hyper-parameter selection strategies for a new task, which then serves as the initial population for GA, while a GRN-based chromosome screening module accelerates the validation of offspring chromosomes, ultimately determining the optimal hyper-parameters. Experimental results demonstrate that, compared to traditional methods such as Bayesian optimization (BO), GA, and particle swarm optimization (PSO), the proposed framework determines optimal hyper-parameters more rapidly, significantly reduces optimization time, and enhances the performance of the base-learner. It achieves an optimal balance between optimization efficiency and prediction accuracy.         ",
    "url": "https://arxiv.org/abs/2409.14535",
    "authors": [
      "Liangzhi Wang",
      "Jie Zhang",
      "Yuan Gao",
      "Jiliang Zhang",
      "Guiyi Wei",
      "Haibo Zhou",
      "Bin Zhuge",
      "Zitian Zhang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2410.05016",
    "title": "T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data",
    "abstract": "           Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging to construct for tabular data. This constitutes one of the main challenges of self-supervision for structured data. In the present work, we propose a novel augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive Architecture (JEPA) and is akin to mask reconstruction in the latent space. It involves predicting the latent representation of one subset of features from the latent representation of a different subset within the same sample, thereby learning rich representations without augmentations. We use our method as a pre-training technique and train several deep classifiers on the obtained representation. Our experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. Moreover, T-JEPA enables some methods to consistently outperform or match the performance of traditional methods likes Gradient Boosted Decision Trees. To understand why, we extensively characterize the obtained representations and show that T-JEPA effectively identifies relevant features for downstream tasks without access to the labels. Additionally, we introduce regularization tokens, a novel regularization method critical for training of JEPA-based models on structured data.         ",
    "url": "https://arxiv.org/abs/2410.05016",
    "authors": [
      "Hugo Thimonier",
      "Jos\u00e9 Lucas De Melo Costa",
      "Fabrice Popineau",
      "Arpad Rimmel",
      "Bich-Li\u00ean Doan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.07414",
    "title": "Bayes-Nash Generative Privacy Against Membership Inference Attacks",
    "abstract": "           Membership inference attacks (MIAs) expose significant privacy risks by determining whether an individual's data is in a dataset. While differential privacy (DP) mitigates such risks, it has several limitations in achieving an optimal balance between utility and privacy, include limited resolution in expressing this tradeoff in only a few privacy parameters, and intractable sensitivity calculations that may be necessary to provide tight privacy guarantees. We propose a game-theoretic framework that models privacy protection from MIA as a Bayesian game between a defender and an attacker. In this game, a dataset is the defender's private information, with privacy loss to the defender (which is gain to the attacker) captured in terms of the attacker's ability to infer membership of individuals in the dataset. To address the strategic complexity of this game, we represent the mixed strategy of the defender as a neural network generator which maps a private dataset to its public representation (for example, noisy summary statistics), while the mixed strategy of the attacker is captured by a discriminator which makes membership inference claims. We refer to the resulting computational approach as a general-sum Generative Adversarial Network, which is trained iteratively by alternating generator and discriminator updates akin to conventional GANs. We call the defender's data sharing policy thereby obtained Bayes-Nash Generative Privacy (BNGP). The BNGP strategy avoids sensitivity calculations, supports compositions of correlated mechanisms, is robust to the attacker's heterogeneous preferences over true and false positives, and yields provable differential privacy guarantees, albeit in an idealized setting.         ",
    "url": "https://arxiv.org/abs/2410.07414",
    "authors": [
      "Tao Zhang",
      "Rajagopal Venkatesaramani",
      "Rajat K. De",
      "Bradley A. Malin",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.10807",
    "title": "Hard-Constrained Neural Networks with Universal Approximation Guarantees",
    "abstract": "           Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction--an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Unlike approaches that modify outputs only at inference time, HardNet enables end-to-end training with hard constraint guarantees, leading to improved performance. To the best of our knowledge, HardNet is the first method with an efficient forward pass to enforce more than one input-dependent inequality constraint. It allows unconstrained optimization of the network parameters using standard algorithms by appending a differentiable closed-form enforcement layer to the network's output. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: learning with piecewise constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems.         ",
    "url": "https://arxiv.org/abs/2410.10807",
    "authors": [
      "Youngjae Min",
      "Navid Azizan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2410.14567",
    "title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions",
    "abstract": "           Retrieval-augmented generation (RAG) has become integral to large language models (LLMs), particularly for conversational AI systems where user questions may reference knowledge beyond the LLMs' training cutoff. However, many natural user questions lack well-defined answers, either due to limited domain knowledge or because the retrieval system returns documents that are relevant in appearance but uninformative in content. In such cases, LLMs often produce hallucinated answers without flagging them. While recent work has largely focused on questions with false premises, we study out-of-scope questions, where the retrieved document appears semantically similar to the question but lacks the necessary information to answer it. In this paper, we propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality. We use this dataset to evaluate several LLMs on their ability to detect out-of-scope questions and generate appropriate responses. Finally, we introduce an improved detection method that enhances the reliability of LLM-based question-answering systems in handling out-of-scope questions.         ",
    "url": "https://arxiv.org/abs/2410.14567",
    "authors": [
      "Zhiyuan Peng",
      "Jinming Nian",
      "Alexandre Evfimievski",
      "Yi Fang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2410.17052",
    "title": "On the Vulnerability of Text Sanitization",
    "abstract": "           Text sanitization, which employs differential privacy to replace sensitive tokens with new ones, represents a significant technique for privacy protection. Typically, its performance in preserving privacy is evaluated by measuring the attack success rate (ASR) of reconstruction attacks, where attackers attempt to recover the original tokens from the sanitized ones. However, current reconstruction attacks on text sanitization are developed empirically, making it challenging to accurately assess the effectiveness of sanitization. In this paper, we aim to provide a more accurate evaluation of sanitization effectiveness. Inspired by the works of Palamidessi et al., we implement theoretically optimal reconstruction attacks targeting text sanitization. We derive their bounds on ASR as benchmarks for evaluating sanitization performance. For real-world applications, we propose two practical reconstruction attacks based on these theoretical findings. Our experimental results underscore the necessity of reassessing these overlooked risks. Notably, one of our attacks achieves a 46.4% improvement in ASR over the state-of-the-art baseline, with a privacy budget of epsilon=4.0 on the SST-2 dataset. Our code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2410.17052",
    "authors": [
      "Meng Tong",
      "Kejiang Chen",
      "Xiaojian Yuan",
      "Jiayang Liu",
      "Weiming Zhang",
      "Nenghai Yu",
      "Jie Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2411.01642",
    "title": "Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination",
    "abstract": "           In high-energy physics, particle jet tagging plays a pivotal role in distinguishing quark from gluon jets using data from collider experiments. While graph-based deep learning methods have advanced this task beyond traditional feature-engineered approaches, the complex data structure and limited labeled samples present ongoing challenges. However, existing contrastive learning (CL) frameworks struggle to leverage rationale-aware augmentations effectively, often lacking supervision signals that guide the extraction of salient features and facing computational efficiency issues such as high parameter counts. In this study, we demonstrate that integrating a quantum rationale generator (QRG) within our proposed Quantum Rationale-aware Graph Contrastive Learning (QRGCL) framework significantly enhances jet discrimination performance, reducing reliance on labeled data and capturing discriminative features. Evaluated on the quark-gluon jet dataset, QRGCL achieves an AUC score of $77.53\\%$ while maintaining a compact architecture of only 45 QRG parameters, outperforming classical, quantum, and hybrid GCL and GNN benchmarks. These results highlight QRGCL's potential to advance jet tagging and other complex classification tasks in high-energy physics, where computational efficiency and feature extraction limitations persist.         ",
    "url": "https://arxiv.org/abs/2411.01642",
    "authors": [
      "Md Abrar Jahin",
      "Md. Akmol Masud",
      "M. F. Mridha",
      "Nilanjan Dey",
      "Zeyar Aung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ]
  },
  {
    "id": "arXiv:2411.11176",
    "title": "Infinite Width Limits of Self Supervised Neural Networks",
    "abstract": "           The NTK is a widely used tool in the theoretical analysis of deep learning, allowing us to look at supervised deep neural networks through the lenses of kernel regression. Recently, several works have investigated kernel models for self-supervised learning, hypothesizing that these also shed light on the behavior of wide neural networks by virtue of the NTK. However, it remains an open question to what extent this connection is mathematically sound -- it is a commonly encountered misbelief that the kernel behavior of wide neural networks emerges irrespective of the loss function it is trained on. In this paper, we bridge the gap between the NTK and self-supervised learning, focusing on two-layer neural networks trained under the Barlow Twins loss. We prove that the NTK of Barlow Twins indeed becomes constant as the width of the network approaches infinity. Our analysis technique is a bit different from previous works on the NTK and may be of independent interest. Overall, our work provides a first justification for the use of classic kernel theory to understand self-supervised learning of wide neural networks. Building on this result, we derive generalization error bounds for kernelized Barlow Twins and connect them to neural networks of finite width.         ",
    "url": "https://arxiv.org/abs/2411.11176",
    "authors": [
      "Maximilian Fleissner",
      "Gautham Govind Anil",
      "Debarghya Ghoshdastidar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.11718",
    "title": "Distributed Maximum Flow in Planar Graphs",
    "abstract": "           The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019]. We extend the distributed algorithmic toolkit to work on the dual graph $G^*$. These tools can then facilitate various algorithms on $G$ by solving a suitable dual problem on $G^*$. Given a directed planar graph $G$ with positive and negative edge-lengths and hop-diameter $D$, our key result is an $\\tilde{O}(D^2)$-round algorithm for Single Source Shortest Paths on $G^*$, which then implies $\\tilde{O}(D^2)$-round algorithms for Maximum $st$-Flow and Directed Global Min-Cut on $G$. Prior to our work, no $\\tilde{O}(\\text{poly}(D))$-round algorithm was known for those problems. We further obtain a $D\\cdot n^{o(1)}$-rounds $(1-\\epsilon)$-approximation algorithm for Maximum $st$-Flow on $G$ when $G$ is undirected and $st$-planar. Finally, we give a near optimal $\\tilde O(D)$-round algorithm for computing the weighted girth of $G$. The main challenges in our work are that $G^*$ is not the communication graph (e.g., a vertex of $G$ is mapped to multiple vertices of $G^*$), and that the diameter of $G^*$ can be much larger than $D$ (i.e., possibly by a linear factor). We overcome these challenges by carefully defining and maintaining subgraphs of the dual graph $G^*$ while applying the recursive decomposition on the primal graph $G$. The main technical difficulty, is that along the recursive decomposition, a face of $G$ gets shattered into (disconnected) components yet we still need to treat it as a dual node.         ",
    "url": "https://arxiv.org/abs/2411.11718",
    "authors": [
      "Yaseen Abd-Elhaleem",
      "Michal Dory",
      "Merav Parter",
      "Oren Weimann"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2411.11764",
    "title": "Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors",
    "abstract": "           Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease that impairs mobility and safety by increasing the risk of falls. An effective FOG detection system must be accurate, real-time, and deployable in free-living environments to enable timely interventions. However, existing detection methods face challenges due to (1) intra- and inter-patient variability, (2) subject-specific training, (3) using multiple sensors in FOG dominant locations (e.g., ankles) leading to high failure points, (4) centralized, non-adaptive learning frameworks that sacrifice patient privacy and prevent collaborative model refinement across populations and disease progression, and (5) most systems are tested in controlled settings, limiting their real-world applicability for continuous in-home monitoring. Addressing these gaps, we present FOGSense, a real-world deployable FOG detection system designed for uncontrolled, free-living conditions using only a single sensor. FOGSense uses Gramian Angular Field (GAF) transformations and privacy-preserving federated deep learning to capture temporal and spatial gait patterns missed by traditional methods with a low false positive rate. We evaluated our system using a public Parkinson's dataset collected in a free-living environment. FOGSense improves accuracy by 10.4% over a single-axis accelerometer, reduces failure points compared to multi-sensor systems, and demonstrates robustness to missing values. The federated architecture allows personalized model adaptation and efficient smartphone synchronization during off-peak hours, making it effective for long-term monitoring as symptoms evolve. Overall, FOGSense achieved a 22.2% improvement in F1-score and a 74.53% reduction in false positive rate compared to state-of-the-art methods, along with enhanced sensitivity for FOG episode detection.         ",
    "url": "https://arxiv.org/abs/2411.11764",
    "authors": [
      "Shovito Barua Soumma",
      "S M Raihanul Alam",
      "Rudmila Rahman",
      "Umme Niraj Mahi",
      "Abdullah Mamun",
      "Sayyed Mostafa Mostafavi",
      "Hassan Ghasemzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2411.16195",
    "title": "On the Robustness of the Successive Projection Algorithm",
    "abstract": "           The successive projection algorithm (SPA) is a workhorse algorithm to learn the $r$ vertices of the convex hull of a set of $(r-1)$-dimensional data points, a.k.a. a latent simplex, which has numerous applications in data science. In this paper, we revisit the robustness to noise of SPA and several of its variants. In particular, when $r \\geq 3$, we prove the tightness of the existing error bounds for SPA and for two more robust preconditioned variants of SPA. We also provide significantly improved error bounds for SPA, by a factor proportional to the conditioning of the $r$ vertices, in two special cases: for the first extracted vertex, and when $r \\leq 2$. We then provide further improvements for the error bounds of a translated version of SPA proposed by Arora et al. (''A practical algorithm for topic modeling with provable guarantees'', ICML, 2013) in two special cases: for the first two extracted vertices, and when $r \\leq 3$. Finally, we propose a new more robust variant of SPA that first shifts and lifts the data points in order to minimize the conditioning of the problem. We illustrate our results on synthetic data.         ",
    "url": "https://arxiv.org/abs/2411.16195",
    "authors": [
      "Giovanni Barbarino",
      "Nicolas Gillis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2412.02659",
    "title": "Adaptive Informed Deep Neural Networks for Power Flow Analysis",
    "abstract": "           This study introduces PINN4PF, an end-to-end deep learning architecture for power flow (PF) analysis that effectively captures the nonlinear dynamics of large-scale modern power systems. The proposed neural network (NN) architecture consists of two important advancements in the training pipeline: (A) a double-head feed-forward NN that aligns with PF analysis, including an activation function that adjusts to the net active and reactive power injections patterns, and (B) a physics-based loss function that partially incorporates power system topology information. The effectiveness of the proposed architecture is illustrated through 4-bus, 15-bus, 290-bus, and 2224-bus test systems and is evaluated against two baselines: a linear regression model (LR) and a black-box NN (MLP). The comparison is based on (i) generalization ability, (ii) robustness, (iii) impact of training dataset size on generalization ability, (iv) accuracy in approximating derived PF quantities (specifically line current, line active power, and line reactive power), and (v) scalability. Results demonstrate that PINN4PF outperforms both baselines across all test systems by up to two orders of magnitude not only in terms of direct criteria, e.g., generalization ability, but also in terms of approximating derived physical quantities.         ",
    "url": "https://arxiv.org/abs/2412.02659",
    "authors": [
      "Zeynab Kaseb",
      "Stavros Orfanoudakis",
      "Pedro P. Vergara",
      "Peter Palensky"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2412.06985",
    "title": "Ground Perturbation Detection via Lower-Limb Kinematic States During Locomotion",
    "abstract": "           Falls during daily ambulation activities are a leading cause of injury in older adults due to delayed physiological responses to disturbances of balance. Lower-limb exoskeletons have the potential to mitigate fall incidents by detecting and reacting to perturbations before the user. Although commonly used, the standard metric for perturbation detection, whole-body angular momentum, is poorly suited for exoskeleton applications due to computational delays and additional tunings. To address this, we developed a novel ground perturbation detector using lower-limb kinematic states during locomotion. To identify perturbations, we tracked deviations in the kinematic states from their nominal steady-state trajectories. Using a data-driven approach, we further optimized our detector with an open-source ground perturbation biomechanics dataset. A pilot experimental validation with five able-bodied subjects demonstrated that our model distinguished perturbed from unperturbed gait cycles with 98.8% accuracy and only a delay of 23.1% within the gait cycle, outperforming the benchmark by 47.7% in detection accuracy. The results of our study offer exciting promise for our detector and its potential utility to enhance the controllability of robotic assistive exoskeletons.         ",
    "url": "https://arxiv.org/abs/2412.06985",
    "authors": [
      "Maria T. Tagliaferri",
      "Leonardo Campeggi",
      "Owen N. Beck",
      "Inseung Kang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2412.08296",
    "title": "GDSG: Graph Diffusion-based Solution Generator for Optimization Problems in MEC Networks",
    "abstract": "           Optimization is crucial for MEC networks to function efficiently and reliably, most of which are NP-hard and lack efficient approximation algorithms. This leads to a paucity of optimal solution, constraining the effectiveness of conventional deep learning approaches. Most existing learning-based methods necessitate extensive optimal data and fail to exploit the potential benefits of suboptimal data that can be obtained with greater efficiency and effectiveness. Taking the multi-server multi-user computation offloading (MSCO) problem, which is widely observed in systems like Internet-of-Vehicles (IoV) and Unmanned Aerial Vehicle (UAV) networks, as a concrete scenario, we present a Graph Diffusion-based Solution Generation (GDSG) method. This approach is designed to work with suboptimal datasets while converging to the optimal solution large probably. We transform the optimization issue into distribution-learning and offer a clear explanation of learning from suboptimal training datasets. We build GDSG as a multi-task diffusion model utilizing a Graph Neural Network (GNN) to acquire the distribution of high-quality solutions. We use a simple and efficient heuristic approach to obtain a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the backbone GNN and achieve improved generalization. GDSG also reaches nearly 100\\% task orthogonality, ensuring no interference between the discrete and continuous generation tasks. We further reveal that this orthogonality arises from the diffusion-related training loss, rather than the neural network architecture itself. The experiments demonstrate that GDSG surpasses other benchmark methods on both the optimal and suboptimal training datasets. The MSCO datasets has open-sourced at this http URL, as well as the GDSG algorithm codes at this https URL.         ",
    "url": "https://arxiv.org/abs/2412.08296",
    "authors": [
      "Ruihuai Liang",
      "Bo Yang",
      "Pengyu Chen",
      "Xuelin Cao",
      "Zhiwen Yu",
      "M\u00e9rouane Debbah",
      "Dusit Niyato",
      "H. Vincent Poor",
      "Chau Yuen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.11142",
    "title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection",
    "abstract": "           Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.         ",
    "url": "https://arxiv.org/abs/2412.11142",
    "authors": [
      "Tiankai Yang",
      "Yi Nian",
      "Shawn Li",
      "Ruiyao Xu",
      "Yuangang Li",
      "Jiaqi Li",
      "Zhuo Xiao",
      "Xiyang Hu",
      "Ryan Rossi",
      "Kaize Ding",
      "Xia Hu",
      "Yue Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.11829",
    "title": "Robust Contact-rich Manipulation through Implicit Motor Adaptation",
    "abstract": "           Contact-rich manipulation plays an important role in daily human activities. However, uncertain physical parameters often pose significant challenges for both planning and control. A promising strategy is to develop policies that are robust across a wide range of parameters. Domain adaptation and domain randomization are widely used, but they tend to either limit generalization to new instances or perform conservatively due to neglecting instance-specific information. \\textit{Explicit motor adaptation} addresses these issues by estimating system parameters online and then retrieving the parameter-conditioned policy from a parameter-augmented base policy. However, it typically requires precise system identification or additional training of a student policy, both of which are challenging in contact-rich manipulation tasks with diverse physical parameters. In this work, we propose \\textit{implicit motor adaptation}, which enables parameter-conditioned policy retrieval given a roughly estimated parameter distribution instead of a single estimate. We leverage tensor train as an implicit representation of the base policy, facilitating efficient retrieval of the parameter-conditioned policy by exploiting the separable structure of tensor cores. This framework eliminates the need for precise system estimation and policy retraining while preserving optimal behavior and strong generalization. We provide a theoretical analysis to validate the approach, supported by numerical evaluations on three contact-rich manipulation primitives. Both simulation and real-world experiments demonstrate its ability to generate robust policies across diverse instances. Project website: \\href{this https URL}{this https URL}.         ",
    "url": "https://arxiv.org/abs/2412.11829",
    "authors": [
      "Teng Xue",
      "Amirreza Razmjoo",
      "Suhan Shetty",
      "Sylvain Calinon"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2412.12914",
    "title": "AoI in Context-Aware Hybrid Radio-Optical IoT Networks",
    "abstract": "           With the surge in IoT devices ranging from wearables to smart homes, prompt transmission is crucial. The Age of Information (AoI) emerges as a critical metric in this context, representing the freshness of the information transmitted across the network. This paper studies hybrid IoT networks that employ Optical Communication (OC) as a reinforcement medium to Radio Frequency (RF). We formulate a non-linear convex optimization that adopts a multi-objective optimization strategy to dynamically schedule the communication between devices and select their corresponding communication technology, aiming to balance the maximization of network throughput with the minimization of energy usage and the frequency of switching between technologies. To mitigate the impact of dominant sub-objectives and their scale disparity, the designed approach employs a regularization method that approximates adequate sub-objective scaling weights. Simulation results show that the OC supplementary integration alongside RF enhances the network's overall performances and significantly reduces the Mean AoI and Peak AoI, allowing the collection of the freshest possible data using the best available communication technology.         ",
    "url": "https://arxiv.org/abs/2412.12914",
    "authors": [
      "Aymen Hamrouni",
      "Sofie Pollin",
      "Hazem Sallouha"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2412.14803",
    "title": "Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations",
    "abstract": "           Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\\% increase in success rates for complex real-world dexterous manipulation tasks. Project page at this https URL ",
    "url": "https://arxiv.org/abs/2412.14803",
    "authors": [
      "Yucheng Hu",
      "Yanjiang Guo",
      "Pengchao Wang",
      "Xiaoyu Chen",
      "Yen-Jen Wang",
      "Jianke Zhang",
      "Koushil Sreenath",
      "Chaochao Lu",
      "Jianyu Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2501.00973",
    "title": "Defense Strategies for Autonomous Multi-agent Systems: Ensuring Safety and Resilience Under Exponentially Unbounded FDI Attacks",
    "abstract": "           False data injection attacks pose a significant threat to autonomous multi-agent systems (MASs). Existing attack-resilient control strategies generally have strict assumptions on the attack signals and overlook safety constraints, such as collision avoidance. In practical applications, leader agents equipped with advanced sensors or weaponry span a safe region to guide heterogeneous follower agents, ensuring coordinated operations while addressing collision avoidance to prevent financial losses and mission failures. This letter addresses these gaps by introducing and solving the safety-aware and attack-resilient (SAAR) control problem under exponentially unbounded false data injection (EU-FDI) attacks. Specifically, a novel attack-resilient observer layer (OL) is first designed to defend against EU-FDI attacks on the OL. Then, an attack-resilient compensational signal is designed to mitigate the adverse effects caused by the EU-FDI attack on control input layer (CIL). Finally, a SAAR controller is designed by solving a quadratic programming (QP) problem integrating control barrier function (CBF) certified collision-free safety constraints. Rigorous Lyapunov-based stability analysis certifies the SAAR controller's effectiveness in ensuring both safety and resilience. This study also pioneers a three-dimensional (3D) simulation of the SAAR containment control problem for heterogeneous MASs, demonstrating its applicability in realistic multi-agent scenarios.         ",
    "url": "https://arxiv.org/abs/2501.00973",
    "authors": [
      "Yichao Wang",
      "Mohamadamin Rajabinezhad",
      "Dimitra Panagou",
      "Shan Zuo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2501.01010",
    "title": "CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction",
    "abstract": "           Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.         ",
    "url": "https://arxiv.org/abs/2501.01010",
    "authors": [
      "Mohammad Shahab Sepehri",
      "Asal Mehradfar",
      "Mahdi Soltanolkotabi",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2501.11216",
    "title": "TigerVector: Supporting Vector Search in Graph Databases for Advanced RAGs",
    "abstract": "           In this paper, we introduce TigerVector, a system that integrates vector search and graph query within TigerGraph, a Massively Parallel Processing (MPP) native graph database. We extend the vertex attribute type with the embedding type. To support fast vector search, we devise an MPP index framework that interoperates efficiently with the graph engine. The graph query language GSQL is enhanced to support vector type expressions and enable query compositions between vector search results and graph query blocks. These advancements elevate the expressive power and analytical capabilities of graph databases, enabling seamless fusion of unstructured and structured data in ways previously unattainable. Through extensive experiments, we demonstrate TigerVector's hybrid search capability, scalability, and superior performance compared to other graph databases (including Neo4j and Amazon Neptune) and a highly optimized specialized vector database (Milvus). TigerVector was integrated into TigerGraph v4.2, the latest release of TigerGraph, in December 2024.         ",
    "url": "https://arxiv.org/abs/2501.11216",
    "authors": [
      "Shige Liu",
      "Zhifang Zeng",
      "Li Chen",
      "Adil Ainihaer",
      "Arun Ramasami",
      "Songting Chen",
      "Yu Xu",
      "Mingxi Wu",
      "Jianguo Wang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2501.15313",
    "title": "I Know What You Did Last Summer: Identifying VR User Activity Through VR Network Traffic",
    "abstract": "           Virtual Reality (VR) technology has gained substantial traction and has the potential to transform a number of industries, including education, entertainment, and professional sectors. Nevertheless, concerns have arisen about the security and privacy implications of VR applications and the impact that they might have on users. In this paper, we investigate the following overarching research question: can VR applications and VR user activities in the context of such applications (e.g., manipulating virtual objects, walking, talking, flying) be identified based on the (potentially encrypted) network traffic that is generated by VR headsets during the operation of VR applications? To answer this question, we collect network traffic data from 25 VR applications running on the Meta Quest Pro headset and identify characteristics of the generated network traffic, which we subsequently use to train off-the-shelf Machine Learning (ML) models. Our results indicate that through the use of ML models, we can identify the VR applications being used with an accuracy of 92.4% and the VR user activities performed with an accuracy of 91%. Furthermore, our results demonstrate that an attacker does not need to collect large amounts of network traffic data for each VR application to carry out such an attack. Specifically, an attacker only needs to collect less than 10 minutes of network traffic data for each VR application in order to identify applications with an accuracy higher than 90% and VR user activities with an accuracy higher than 88%.         ",
    "url": "https://arxiv.org/abs/2501.15313",
    "authors": [
      "Sheikh Samit Muhaimin",
      "Spyridon Mastorakis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2502.01710",
    "title": "DAGNet: A Dual-View Attention-Guided Network for Efficient X-ray Security Inspection",
    "abstract": "           With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray baggage scanner is widely deployed, they struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose a Dual-View Attention-Guided Network for Efficient X-ray Security Inspection (DAGNet). This study builds on a shared-weight backbone network as the foundation and constructs three key modules that work together: (1) Frequency Domain Interaction Module (FDIM) dynamically enhances features by adjusting frequency components based on inter-view relationships; (2) Dual-View Hierarchical Enhancement Module (DVHEM) employs cross-attention to align features between views and capture hierarchical associations; (3) Convolutional Guided Fusion Module (CGFM) fuses features to suppress redundancy while retaining critical discriminative information. Collectively, these modules substantially improve the performance of dual-view X-ray security inspection. Experimental results demonstrate that DAGNet outperforms existing state-of-the-art approaches across multiple backbone architectures. The code is available at:this https URL.         ",
    "url": "https://arxiv.org/abs/2502.01710",
    "authors": [
      "Shilong Hong",
      "Yanzhou Zhou",
      "Weichao Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.01713",
    "title": "Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool",
    "abstract": "           Algorithms are increasingly used to automate or aid human decisions, yet recent research shows that these algorithms may exhibit bias across legally protected demographic groups. However, data on these groups may be unavailable to organizations or external auditors due to privacy legislation. This paper studies bias detection using an unsupervised clustering tool when data on demographic groups are unavailable. We collaborate with the Dutch Executive Agency for Education to audit an algorithm that was used to assign risk scores to college students at the national level in the Netherlands between 2012-2023. Our audit covers more than 250,000 students from the whole country. The unsupervised clustering tool highlights known disparities between students with a non-European migration background and Dutch origin. Our contributions are three-fold: (1) we assess bias in a real-world, large-scale and high-stakes decision-making process by a governmental organization; (2) we use simulation studies to highlight potential pitfalls of using the unsupervised clustering tool to detect true bias when demographic group data are unavailable and provide recommendations for valid inferences; (3) we provide the unsupervised clustering tool in an open-source library. Our work serves as a starting point for a deliberative assessment by human experts to evaluate potential discrimination in algorithmic-supported decision-making processes.         ",
    "url": "https://arxiv.org/abs/2502.01713",
    "authors": [
      "Floris Holstege",
      "Mackenzie Jorgensen",
      "Kirtan Padh",
      "Jurriaan Parie",
      "Joel Persson",
      "Krsto Prorokovic",
      "Lukas Snoek"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.13506",
    "title": "Reproducing NevIR: Negation in Neural Information Retrieval",
    "abstract": "           Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category-listwise Large Language Model (LLM) re-rankers-outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalisability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM re-rankers achieve reasonable performance across both negation tasks.         ",
    "url": "https://arxiv.org/abs/2502.13506",
    "authors": [
      "Coen van den Elsen",
      "Francien Barkhof",
      "Thijmen Nijdam",
      "Simon Lupart",
      "Mohammad Aliannejadi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2502.14259",
    "title": "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records",
    "abstract": "           Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.         ",
    "url": "https://arxiv.org/abs/2502.14259",
    "authors": [
      "Sujeong Im",
      "Jungwoo Oh",
      "Edward Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.15251",
    "title": "SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training",
    "abstract": "           We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SimHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. Specifically, we collect over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. We then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method (PeCLR) in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.15251",
    "authors": [
      "Nie Lin",
      "Takehiko Ohkawa",
      "Yifei Huang",
      "Mingfang Zhang",
      "Minjie Cai",
      "Ming Li",
      "Ryosuke Furuta",
      "Yoichi Sato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2502.20490",
    "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
    "abstract": "           Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human bench of 92\\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \\dataset{} to enhance normative reasoning in VLMs.         ",
    "url": "https://arxiv.org/abs/2502.20490",
    "authors": [
      "MohammadHossein Rezaei",
      "Yicheng Fu",
      "Phil Cuvin",
      "Caleb Ziems",
      "Yanzhe Zhang",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.06337",
    "title": "Pretraining Generative Flow Networks with Inexpensive Rewards for Molecular Graph Generation",
    "abstract": "           Generative Flow Networks (GFlowNets) have recently emerged as a suitable framework for generating diverse and high-quality molecular structures by learning from rewards treated as unnormalized distributions. Previous works in this framework often restrict exploration by using predefined molecular fragments as building blocks, limiting the chemical space that can be accessed. In this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as building blocks to explore drug-like chemical space more comprehensively. We propose an unsupervised pre-training approach using drug-like molecule datasets, which teaches A-GFNs about inexpensive yet informative molecular descriptors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. We further implement a goal-conditioned finetuning process, which adapts A-GFNs to optimize for specific target properties. In this work, we pretrain A-GFN on a subset of ZINC dataset, and by employing robust evaluation metrics we show the effectiveness of our approach when compared to other relevant baseline methods for a wide range of drug design tasks.         ",
    "url": "https://arxiv.org/abs/2503.06337",
    "authors": [
      "Mohit Pandey",
      "Gopeshh Subbaraj",
      "Artem Cherkasov",
      "Martin Ester",
      "Emmanuel Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.00711",
    "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
    "abstract": "           The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.         ",
    "url": "https://arxiv.org/abs/2504.00711",
    "authors": [
      "Enjun Du",
      "Xunkai Li",
      "Tian Jin",
      "Zhihan Zhang",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.05197",
    "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
    "abstract": "           Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.         ",
    "url": "https://arxiv.org/abs/2504.05197",
    "authors": [
      "Yong Ren",
      "Jiangyan Yi",
      "Tao Wang",
      "Jianhua Tao",
      "Zheng Lian",
      "Zhengqi Wen",
      "Chenxing Li",
      "Ruibo Fu",
      "Ye Bai",
      "Xiaohui Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2504.05287",
    "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects",
    "abstract": "           The ability to robustly grasp a variety of objects is essential for dexterous robots. In this paper, we present a framework for zero-shot dynamic dexterous grasping using single-view visual inputs, designed to be resilient to various disturbances. Our approach utilizes a hand-centric object shape representation based on dynamic distance vectors between finger joints and object surfaces. This representation captures the local shape around potential contact regions rather than focusing on detailed global object geometry, thereby enhancing generalization to shape variations and uncertainties. To address perception limitations, we integrate a privileged teacher policy with a mixed curriculum learning approach, allowing the student policy to effectively distill grasping capabilities and explore for adaptation to disturbances. Trained in simulation, our method achieves success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects, demonstrating remarkable generalization. Quantitative and qualitative results validate the robustness of our policy against various disturbances.         ",
    "url": "https://arxiv.org/abs/2504.05287",
    "authors": [
      "Hui Zhang",
      "Zijian Wu",
      "Linyi Huang",
      "Sammy Christen",
      "Jie Song"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2504.06559",
    "title": "TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network",
    "abstract": "           Tabular data analysis presents unique challenges due to its heterogeneous feature types, missing values, and complex interactions. While traditional machine learning methods, such as gradient boosting, often outperform deep learning approaches, recent advancements in neural architectures offer promising alternatives. This paper introduces TabKAN, a novel framework that advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs leverge learnable activation functions on edges, which improve both interpretability and training efficiency. Our contributions include: (1) the introduction of modular KAN-based architectures for tabular data analysis, (2) the development of a transfer learning framework for KAN models that supports knowledge transfer between domains, (3) the development of model-specific interpretability for tabular data learning, which reduces dependence on post hoc and model-agnostic analysis, and (4) comprehensive evaluation of vanilla supervised learning across binary and multi-class classification tasks. Through extensive benchmarking on diverse public datasets, TabKAN demonstrates superior performance in supervised learning while significantly outperforming classical and Transformer-based models in transfer learning scenarios. Our findings highlight the advantage of KAN-based architectures in transferring knowledge across domains and narrowing the gap between traditional machine learning and deep learning for structured data.         ",
    "url": "https://arxiv.org/abs/2504.06559",
    "authors": [
      "Ali Eslamian",
      "Alireza Afzal Aghaei",
      "Qiang Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.12279",
    "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
    "abstract": "           We present a geometry-driven method for normalizing dysarthric speech by modeling time, frequency, and amplitude distortions as smooth, local Lie group transformations of spectrograms. Scalar fields generate these deformations via exponential maps, and a neural network is trained - using only synthetically warped healthy speech - to infer the fields and apply an approximate inverse at test time. We introduce a spontaneous-symmetry-breaking (SSB) potential that encourages the model to discover non-trivial field configurations. On real pathological speech, the system delivers consistent gains: up to 17 percentage-point WER reduction on challenging TORGO utterances and a 16 percent drop in WER variance, with no degradation on clean CommonVoice data. Character and phoneme error rates improve in parallel, confirming linguistic relevance. Our results demonstrate that geometrically structured warping provides consistent, zero-shot robustness gains for dysarthric ASR.         ",
    "url": "https://arxiv.org/abs/2504.12279",
    "authors": [
      "Mikhail Osipov"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2504.13034",
    "title": "Inference-friendly Graph Compression for Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have demonstrated promising performance in graph analysis. Nevertheless, the inference process of GNNs remains costly, hindering their applications for large graphs. This paper proposes inference-friendly graph compression (IFGC), a graph compression scheme to accelerate GNNs inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed graph $G_c$, to best preserve the inference results of $M$ over $G$, such that the result can be directly inferred by accessing $G_c$ with no or little decompression cost. (1) We characterize IFGC with a class of inference equivalence relation. The relation captures the node pairs in $G$ that are not distinguishable for GNN inference. (2) We introduce three practical specifications of IFGC for representative GNNs: structural preserving compression (SPGC), which computes $G_c$ that can be directly processed by GNN inference without decompression; ($\\alpha$, $r$)-compression, that allows for a configurable trade-off between compression ratio and inference quality, and anchored compression that preserves inference results for specific nodes of interest. For each scheme, we introduce compression and inference algorithms with guarantees of efficiency and quality of the inferred results. We conduct extensive experiments on diverse sets of large-scale graphs, which verifies the effectiveness and efficiency of our graph compression approaches.         ",
    "url": "https://arxiv.org/abs/2504.13034",
    "authors": [
      "Yangxin Fan",
      "Haolai Che",
      "Yinghui Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.15758",
    "title": "Observability conditions for neural state-space models with eigenvalues and their roots of unity",
    "abstract": "           We operate through the lens of ordinary differential equations and control theory to study the concept of observability in the context of neural state-space models and the Mamba architecture. We develop strategies to enforce observability, which are tailored to a learning context, specifically where the hidden states are learnable at initial time, in conjunction to over its continuum, and high-dimensional. We also highlight our methods emphasize eigenvalues, roots of unity, or both. Our methods effectuate computational efficiency when enforcing observability, sometimes at great scale. We formulate observability conditions in machine learning based on classical control theory and discuss their computational complexity. Our nontrivial results are fivefold. We discuss observability through the use of permutations in neural applications with learnable matrices without high precision. We present two results built upon the Fourier transform that effect observability with high probability up to the randomness in the learning. These results are worked with the interplay of representations in Fourier space and their eigenstructure, nonlinear mappings, and the observability matrix. We present a result for Mamba that is similar to a Hautus-type condition, but instead employs an argument using a Vandermonde matrix instead of eigenvectors. Our final result is a shared-parameter construction of the Mamba system, which is computationally efficient in high exponentiation. We develop a training algorithm with this coupling, showing it satisfies a Robbins-Monro condition under certain orthogonality, while a more classical training procedure fails to satisfy a contraction with high Lipschitz constant.         ",
    "url": "https://arxiv.org/abs/2504.15758",
    "authors": [
      "Andrew Gracyk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2504.15984",
    "title": "Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems",
    "abstract": "           Neuroadaptive haptics offers a path to more immersive extended reality (XR) experiences by dynamically tuning multisensory feedback to user preferences. We present a neuroadaptive haptics system that adapts XR feedback through reinforcement learning (RL) from explicit user ratings and brain-decoded neural signals. In a user study, participants interacted with virtual objects in VR while Electroencephalography (EEG) data were recorded. An RL agent adjusted haptic feedback based either on explicit ratings or on outputs from a neural decoder. Results show that the RL agent's performance was comparable across feedback sources, suggesting that implicit neural feedback can effectively guide personalization without requiring active user input. The EEG-based neural decoder achieved a mean F1 score of 0.8, supporting reliable classification of user experience. These findings demonstrate the feasibility of combining brain-computer interfaces (BCI) and RL to autonomously adapt XR interactions, reducing cognitive load and enhancing immersion.         ",
    "url": "https://arxiv.org/abs/2504.15984",
    "authors": [
      "Lukas Gehrke",
      "Aleksandrs Koselevs",
      "Marius Klug",
      "Klaus Gramann"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2504.18267",
    "title": "Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study",
    "abstract": "           This paper investigates the limitations of neural operators in learning solutions for a Hughes model, a first-order hyperbolic conservation law system for crowd dynamics. The model couples a Fokker-Planck equation representing pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes model belongs to the class of nonlinear hyperbolic systems that often exhibit complex solution structures, including shocks and discontinuities. In this study, we assess the performance of three state-of-the-art neural operators (Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural Operator) in various challenging scenarios. Specifically, we consider (1) discontinuous and Gaussian initial conditions and (2) diverse boundary conditions, while also examining the impact of different numerical schemes. Our results show that these neural operators perform well in easy scenarios with fewer discontinuities in the initial condition, yet they struggle in complex scenarios with multiple initial discontinuities and dynamic boundary conditions, even when trained specifically on such complex samples. The predicted solutions often appear smoother, resulting in a reduction in total variation and a loss of important physical features. This smoothing behavior is similar to issues discussed by Daganzo (1995), where models that introduce artificial diffusion were shown to miss essential features such as shock waves in hyperbolic systems. These results suggest that current neural operator architectures may introduce unintended regularization effects that limit their ability to capture transport dynamics governed by discontinuities. They also raise concerns about generalizing these methods to traffic applications where shock preservation is essential.         ",
    "url": "https://arxiv.org/abs/2504.18267",
    "authors": [
      "Prajwal Chauhan",
      "Salah Eddine Choutri",
      "Mohamed Ghattassi",
      "Nader Masmoudi",
      "Saif Eddin Jabari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.19820",
    "title": "Hierarchical Uncertainty-Aware Graph Neural Network",
    "abstract": "           Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. This work introduces a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on semi-supervised classification tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability.         ",
    "url": "https://arxiv.org/abs/2504.19820",
    "authors": [
      "Yoonhyuk Choi",
      "Jiho Choi",
      "Taewook Ko",
      "Chong-Kwon Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2504.20065",
    "title": "A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts",
    "abstract": "           We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.         ",
    "url": "https://arxiv.org/abs/2504.20065",
    "authors": [
      "Robert Becker",
      "Aron Culotta"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.20682",
    "title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation",
    "abstract": "           Table structure recognition is a key task in document analysis. However, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. To obtain fine-grained spatial coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge response by Gradient Orientation-aware Extractor, combines a Heterogeneous Kernel Cross Fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. Furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named Deformation Wired Table (DWTAL). Experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. The dataset and the source code are open source: this https URL.         ",
    "url": "https://arxiv.org/abs/2504.20682",
    "authors": [
      "Long Liu",
      "Cihui Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.00034",
    "title": "Improving Phishing Email Detection Performance of Small Large Language Models",
    "abstract": "           Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving both accuracy and F1 score on the SpamAssassin and CEAS\\_08 datasets. Furthermore, the fine-tuned models demonstrated strong transferability, achieving robust performance across multiple unseen phishing datasets, outperforming traditional baselines and approaching standard-sized LLMs.         ",
    "url": "https://arxiv.org/abs/2505.00034",
    "authors": [
      "Zijie Lin",
      "Zikang Liu",
      "Hanbo Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.00316",
    "title": "Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture",
    "abstract": "           The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \\textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.         ",
    "url": "https://arxiv.org/abs/2505.00316",
    "authors": [
      "Tien Comlekoglu",
      "J. Quetzalc\u00f3atl Toledo-Mar\u00edn",
      "Tina Comlekoglu",
      "Douglas W. DeSimone",
      "Shayn M. Peirce",
      "Geoffrey Fox",
      "James A. Glazier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2012.10186",
    "title": "Graph and wreath products of cellular automata",
    "abstract": "           We prove that the set of subgroups of the automorphism group of a two-sided full shift is closed under countable graph products. We introduce the notion of a group action without $A$-cancellation (for an abelian group $A$), and show that when $A$ is a finite abelian group and $G$ is a group of cellular automata whose action does not have $A$-cancellation, the wreath product $A \\wr G$ embeds in the automorphism group of a full shift. We show that all free abelian groups and free groups admit such cellular automata actions. In the one-sided case, we prove variants of these results with reasonable alphabet blow-ups.         ",
    "url": "https://arxiv.org/abs/2012.10186",
    "authors": [
      "Ville Salo"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2203.12961",
    "title": "Bayesian Deep Learning with Multilevel Trace-class Neural Networks",
    "abstract": "           In this article we consider Bayesian inference associated to deep neural networks (DNNs) and in particular, trace-class neural network (TNN) priors which can be preferable to traditional DNNs as (a) they are identifiable and (b) they possess desirable convergence properties. TNN priors are defined on functions with infinitely many hidden units, and have strongly convergent Karhunen-Loeve-type approximations with finitely many hidden units. A practical hurdle is that the Bayesian solution is computationally demanding, requiring simulation methods, so approaches to drive down the complexity are needed. In this paper, we leverage the strong convergence of TNN in order to apply Multilevel Monte Carlo (MLMC) to these models. In particular, an MLMC method that was introduced is used to approximate posterior expectations of Bayesian TNN models with optimal computational complexity, and this is mathematically proved. The results are verified with several numerical experiments on model problems arising in machine learning, including regression, classification, and reinforcement learning.         ",
    "url": "https://arxiv.org/abs/2203.12961",
    "authors": [
      "Neil K. Chada",
      "Ajay Jasra",
      "Kody J. H. Law",
      "Sumeetpal S. Singh"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2206.06526",
    "title": "Overparametrized linear dimensionality reductions: From projection pursuit to two-layer neural networks",
    "abstract": "           Given a cloud of $n$ data points in $\\mathbb{R}^d$, consider all projections onto $m$-dimensional subspaces of $\\mathbb{R}^d$ and, for each such projection, the empirical distribution of the projected points. What does this collection of probability distributions look like when $n,d$ grow large? We consider this question under the null model in which the points are i.i.d. standard Gaussian vectors, focusing on the asymptotic regime in which $n,d\\to\\infty$, with $n/d\\to\\alpha\\in (0,\\infty)$, while $m$ is fixed. Denoting by $\\mathscr{F}_{m, \\alpha}$ the set of probability distributions in $\\mathbb{R}^m$ that arise as low-dimensional projections in this limit, we establish new inner and outer bounds on $\\mathscr{F}_{m, \\alpha}$. In particular, we characterize the Wasserstein radius of $\\mathscr{F}_{m,\\alpha}$ up to constant multiplicative factors, and determine it exactly for $m=1$. We also prove sharp bounds in terms of Kullback-Leibler divergence and R\u00e9nyi information dimension. The previous question has application to unsupervised learning methods, such as projection pursuit and independent component analysis. We introduce a version of the same problem that is relevant for supervised learning, and prove a sharp Wasserstein radius bound. As an application, we establish an upper bound on the interpolation threshold of two-layers neural networks with $m$ hidden neurons.         ",
    "url": "https://arxiv.org/abs/2206.06526",
    "authors": [
      "Andrea Montanari",
      "Kangjie Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2206.13269",
    "title": "Wasserstein Distributionally Robust Estimation in High Dimensions: Performance Analysis and Optimal Hyperparameter Tuning",
    "abstract": "           Distributionally robust optimization (DRO) has become a powerful framework for estimation under uncertainty, offering strong out-of-sample performance and principled regularization. In this paper, we propose a DRO-based method for linear regression and address a central question: how to optimally choose the robustness radius, which controls the trade-off between robustness and accuracy. Focusing on high-dimensional settings where the dimension and the number of samples are both large and comparable in size, we employ tools from high-dimensional asymptotic statistics to precisely characterize the estimation error of the resulting estimator. Remarkably, this error can be recovered by solving a simple convex-concave optimization problem involving only four scalar variables. This characterization enables efficient selection of the radius that minimizes the estimation error. In doing so, it achieves the same effect as cross-validation, but at a fraction of the computational cost. Numerical experiments confirm that our theoretical predictions closely match empirical performance and that the optimal radius selected through our method aligns with that chosen by cross-validation, highlighting both the accuracy and the practical benefits of our approach.         ",
    "url": "https://arxiv.org/abs/2206.13269",
    "authors": [
      "Liviu Aolaritei",
      "Soroosh Shafiee",
      "Florian D\u00f6rfler"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2310.04606",
    "title": "Robust Transfer Learning with Unreliable Source Data",
    "abstract": "           This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We also provide simple approaches to bound the excess misclassification error without the need for specialized knowledge in transfer learning.         ",
    "url": "https://arxiv.org/abs/2310.04606",
    "authors": [
      "Jianqing Fan",
      "Cheng Gao",
      "Jason M. Klusowski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2402.14264",
    "title": "Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation",
    "abstract": "           Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.         ",
    "url": "https://arxiv.org/abs/2402.14264",
    "authors": [
      "Jikai Jin",
      "Vasilis Syrgkanis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2406.16705",
    "title": "Cycles in graphs and in hypergraphs: towards homology theory",
    "abstract": "           In this expository paper we present some ideas of algebraic topology (more precisely, of homology theory) in a language accessible to non-specialists in the area. A $1$-cycle in a graph is a set $C$ of edges such that every vertex is contained in an even number of edges from $C$. It is easy to check that the sum (modulo $2$) of $1$-cycles is a $1$-cycle. We start from the following problems: to find $\\bullet$ the number of all $1$-cycles in a given graph; $\\bullet$ a small number of $1$-cycles in a given graph such that any $1$-cycle is the sum of some of them. We consider generalizations (of these problems) to graphs with symmetry, to $2$-cycles in $2$-dimensional hypergraphs, and to certain configuration spaces of graphs (namely, to the square and the deleted square).         ",
    "url": "https://arxiv.org/abs/2406.16705",
    "authors": [
      "A. Miroshnikov",
      "O. Nikitenko",
      "A. Skopenkov"
    ],
    "subjectives": [
      "History and Overview (math.HO)",
      "Discrete Mathematics (cs.DM)",
      "Algebraic Topology (math.AT)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2410.19027",
    "title": "A Spectral-based Physics-informed Finite Operator Learning for Prediction of Mechanical Behavior of Microstructures",
    "abstract": "           A novel physics-informed operator learning technique based on spectral methods is introduced to model the complex behavior of heterogeneous materials. The Lippmann-Schwinger operator in Fourier space is employed to construct physical constraints with minimal computational overhead, effectively eliminating the need for automatic differentiation. The introduced methodology accelerates the training process by enabling gradient construction on a fixed, finite discretization in Fourier space. Later, the spectral physics-informed finite operator learning (SPiFOL) framework is built based on this discretization and trained to map the arbitrary shape of microstructures to their mechanical responses (strain fields) without relying on labeled data. The training is done by minimizing equilibrium in Fourier space concerning the macroscopic loading condition, which also guarantees the periodicity. SPiFOL, as a physics-informed operator learning method, enables rapid predictions through forward inference after training. To ensure accuracy, we incorporate physical constraints and diversify the training data. However, performance may still degrade for out-of-distribution microstructures. SPiFOL is further enhanced by integrating a Fourier Neural Operator (FNO). Compared to the standard data-driven FNO, SPiFOL shows higher accuracy in predicting stress fields and provides nearly resolution-independent results. Additionally, its zero-shot super-resolution capabilities are explored in heterogeneous domains. Finally, SPiFOL is extended to handle 3D problems and further adapted to finite elasticity, demonstrating the robustness of the framework in handling nonlinear mechanical behavior. The framework shows great potential for efficient and scalable prediction of mechanical responses in complex material systems while also reducing the training time required for training physics-informed neural operators.         ",
    "url": "https://arxiv.org/abs/2410.19027",
    "authors": [
      "Ali Harandi",
      "Hooman Danesh",
      "Kevin Linka",
      "Stefanie Reese",
      "Shahed Rezaei"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2411.12013",
    "title": "Neural and Time-Series Approaches for Pricing Weather Derivatives: Performance and Regime Adaptation Using Satellite Data",
    "abstract": "           This paper studies pricing of weather-derivative (WD) contracts on temperature and precipitation. For temperature-linked strangles in Toronto and Chicago, we benchmark a harmonic-regression/ARMA model against a feed-forward neural network (NN), finding that the NN reduces out-of-sample mean-squared error (MSE) and materially shifts December fair values relative to both the time-series model and the industry-standard Historic Burn Approach (HBA). For precipitation, we employ a compound Poisson--Gamma framework: shape and scale parameters are estimated via maximum likelihood estimation (MLE) and via a convolutional neural network (CNN) trained on 30-day rainfall sequences spanning multiple seasons. The CNN adaptively learns season-specific $(\\alpha,\\beta)$ mappings, thereby capturing heterogeneity across regimes that static i.i.d.\\ fits miss. At valuation, we assume days are i.i.d.\\ $\\Gamma(\\hat{\\alpha},\\hat{\\beta})$ within each regime and apply a mean-count approximation (replacing the Poisson count by its mean ($n\\hat{\\lambda}$) to derive closed-form strangle prices. Exploratory analysis of 1981--2023 NASA POWER data confirms pronounced seasonal heterogeneity in $(\\alpha,\\beta)$ between summer and winter, demonstrating that static global fits are inadequate. Back-testing on Toronto and Chicago grids shows that our regime-adaptive CNN yields competitive valuations and underscores how model choice can shift strangle prices. Payoffs are evaluated analytically when possible and by simulation elsewhere, enabling a like-for-like comparison of forecasting and valuation methods.         ",
    "url": "https://arxiv.org/abs/2411.12013",
    "authors": [
      "Marco Hening Tallarico",
      "Pablo Olivares"
    ],
    "subjectives": [
      "Mathematical Finance (q-fin.MF)",
      "Machine Learning (cs.LG)",
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.20500",
    "title": "Deep Neural OFDM Receivers: Two Novel Architectures for BER-BLER Optimization and Comparison with State-of-the-Art Architectures",
    "abstract": "           Neural receivers have recently become a popular topic, where the received signals can be directly decoded by data driven mechanisms such as machine learning and deep learning. In this paper, we propose two novel neural network based orthogonal frequency division multiplexing (OFDM) receivers performing channel estimation and equalization tasks and directly predicting log likelihood ratios (LLRs) from the received in phase and quadrature phase (IQ) signals. The first network, the Dual Attention Transformer (DAT), employs a state of the art (SOTA) transformer architecture with an attention mechanism. The second network, the Residual Dual Non Local Attention Network (RDNLA), utilizes a parallel residual architecture with a non local attention block. The bit error rate (BER) and block error rate (BLER) performance of various SOTA neural receiver architectures is compared with our proposed methods across different signal to noise ratio (SNR) levels. The simulation results show that DAT and RDNLA outperform both traditional communication systems and existing neural receiver models. The computational efficiency of the proposed neural receivers supports their feasibility for next generation communication systems.         ",
    "url": "https://arxiv.org/abs/2503.20500",
    "authors": [
      "Erhan Karakoca",
      "H\u00fcseyin \u00c7evik",
      "\u0130brahim H\u00f6kelek",
      "Ali G\u00f6r\u00e7in"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.13902",
    "title": "A Graph Theoretic Approach for Exploring the Relationship between EV Adoption and Charging Infrastructure Growth",
    "abstract": "           The increasing global demand for conventional energy has led to significant challenges, particularly due to rising CO2 emissions and the depletion of natural resources. In the U.S., light-duty vehicles contribute significantly to transportation sector emissions, prompting a global shift toward electrified vehicles (EVs). Among the challenges that thwart the widespread adoption of EVs is the insufficient charging infrastructure (CI). This study focuses on exploring the complex relationship between EV adoption and CI growth. Employing a graph theoretic approach, we propose a graph model to analyze correlations between EV adoption and CI growth across 137 counties in six states. We examine how different time granularities impact these correlations in two distinct scenarios: Early Adoption and Late Adoption. Further, we conduct causality tests to assess the directional relationship between EV adoption and CI growth in both scenarios. Our main findings reveal that analysis using lower levels of time granularity result in more homogeneous clusters, with notable differences between clusters in EV adoption and those in CI growth. Additionally, we identify causal relationships between EV adoption and CI growth in 137 counties, and show that causality is observed more frequently in Early Adoption scenarios than in Late Adoption ones. However, the causal effects in Early Adoption are slower than those in Late Adoption.         ",
    "url": "https://arxiv.org/abs/2504.13902",
    "authors": [
      "Fahad S. Alrasheedi",
      "Hesham H. Ali"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2504.21795",
    "title": "Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model",
    "abstract": "           The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on XX-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance.         ",
    "url": "https://arxiv.org/abs/2504.21795",
    "authors": [
      "Yuankang Zhao",
      "Matthew Engelhard"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ]
  }
]