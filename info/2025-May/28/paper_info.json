[
  {
    "id": "arXiv:2505.20300",
    "title": "FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design",
    "abstract": "           We propose FMEnets, a physics-informed machine learning framework for the design and analysis of non-ideal plug flow reactors. FMEnets integrates the fundamental governing equations (Navier-Stokes for fluid flow, material balance for reactive species transport, and energy balance for temperature distribution) into a unified multi-scale network model. The framework is composed of three interconnected sub-networks with independent optimizers that enable both forward and inverse problem-solving. In the forward mode, FMEnets predicts velocity, pressure, species concentrations, and temperature profiles using only inlet and outlet information. In the inverse mode, FMEnets utilizes sparse multi-residence-time measurements to simultaneously infer unknown kinetic parameters and states. FMEnets can be implemented either as FME-PINNs, which employ conventional multilayer perceptrons, or as FME-KANs, based on Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the critical role of the FMEnets architecture in achieving accurate predictions. Specifically, FME-KANs are more robust to noise than FME-PINNs, although both representations are comparable in accuracy and speed in noise-free conditions. The proposed framework is applied to three different sets of reaction scenarios and is compared with finite element simulations. FMEnets effectively captures the complex interactions, achieving relative errors less than 2.5% for the unknown kinetic parameters. The new network framework not only provides a computationally efficient alternative for reactor design and optimization, but also opens new avenues for integrating empirical correlations, limited and noisy experimental data, and fundamental physical equations to guide reactor design.         ",
    "url": "https://arxiv.org/abs/2505.20300",
    "authors": [
      "Chenxi Wu",
      "Juan Diego Toscano",
      "Khemraj Shukla",
      "Yingjie Chen",
      "Ali Shahmohammadi",
      "Edward Raymond",
      "Thomas Toupy",
      "Neda Nazemifard",
      "Charles Papageorgiou",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ]
  },
  {
    "id": "arXiv:2505.20302",
    "title": "VeriThoughts: Enabling Automated Verilog Code Generation using Reasoning and Formal Verification",
    "abstract": "           This paper introduces VeriThoughts, a novel dataset designed for reasoning-based Verilog code generation. We establish a new benchmark framework grounded in formal verification methods to evaluate the quality and correctness of generated hardware descriptions. Additionally, we present a suite of specialized small-scale models optimized specifically for Verilog generation. Our work addresses the growing need for automated hardware design tools that can produce verifiably correct implementations from high-level specifications, potentially accelerating the hardware development process while maintaining rigorous correctness guarantees. Our code and data are available at \\href{this https URL}{this URL}.         ",
    "url": "https://arxiv.org/abs/2505.20302",
    "authors": [
      "Patrick Yubeaton",
      "Andre Nakkab",
      "Weihua Xiao",
      "Luca Collini",
      "Ramesh Karri",
      "Chinmay Hegde",
      "Siddharth Garg"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2505.20303",
    "title": "Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software",
    "abstract": "           As artificial intelligence becomes increasingly integrated into software development processes, the prevalence and sophistication of AI-generated code continue to expand rapidly. This study addresses the critical need for transparency and safety in AI generated code by examining the current landscape, identifying potential risks, and exploring future implications. We analyze market opportunities for detecting AI-generated code, discuss the challenges associated with managing increasing complexity, and propose solutions to enhance transparency and functionality analysis. Furthermore, this study investigates the longterm implications of AI generated code, including its potential role in the development of artificial general intelligence and its impact on human AI interaction. In conclusion, we emphasize the importance of proactive measures for ensuring the responsible development and deployment of AI in software engineering.         ",
    "url": "https://arxiv.org/abs/2505.20303",
    "authors": [
      "David Hanson"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20306",
    "title": "Multi-Modal Artificial Intelligence of Embryo Grading and Pregnancy Prediction in Assisted Reproductive Technology: A Review",
    "abstract": "           As a global disease, infertility has always affected human beings. The development of assisted reproductive technology can effectively solve this disease. However, the traditional in vitro fertilization-embryo transfer technology still faces many challenges in improving the success rate of pregnancy, such as the subjectivity of embryo grading and the inefficiency of integrating multi-modal data. Therefore, the introduction of artificial intelligence-based technologies is particularly crucial. This article reviews the application progress of multi-modal artificial intelligence in embryo grading and pregnancy prediction based on different data modalities (including static images, time-lapse videos and structured table data) from a new perspective, and discusses the main challenges in current research, such as the complexity of multi-modal information fusion and data scarcity.         ",
    "url": "https://arxiv.org/abs/2505.20306",
    "authors": [
      "Xueqiang Ouyang",
      "Jia Wei"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2505.20308",
    "title": "Large Language Model-Powered Decision Support for a Metal Additive Manufacturing Knowledge Graph",
    "abstract": "           Metal additive manufacturing (AM) involves complex interdependencies among processes, materials, feedstock, and post-processing steps. However, the underlying relationships and domain knowledge remain fragmented across literature and static databases that often demand expert-level queries, limiting their applicability in design and planning. To address these gaps, we develop a novel and queryable knowledge graph (KG) in Neo4j, encoding 53 distinct metals and alloys across seven material families, nine AM processes, four feedstock types, and associated post-processing requirements. A large language model (LLM) interface, guided by a few-shot prompting strategy, enables natural language querying without the need for formal query syntax. The system supports a range of tasks, including compatibility checks, multi-constraint filtering, and design for AM (DfAM) guidance. User natural language queries are normalized, translated into Cypher, and executed over the KG, with results reformatted into structured responses. This work presents the first real-time, interactive system that integrates a domain-specific metal AM KG with an LLM interface, offering accessible, explainable decision support for engineers and advancing human-centric tools in manufacturing intelligence.         ",
    "url": "https://arxiv.org/abs/2505.20308",
    "authors": [
      "Muhammad Tayyab Khan",
      "Lequn Chen",
      "Wenhe Feng",
      "Seung Ki Moon"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20322",
    "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
    "abstract": "           Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.         ",
    "url": "https://arxiv.org/abs/2505.20322",
    "authors": [
      "Mengru Wang",
      "Ziwen Xu",
      "Shengyu Mao",
      "Shumin Deng",
      "Zhaopeng Tu",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20324",
    "title": "Evaluating the Energy-Efficiency of the Code Generated by LLMs",
    "abstract": "           As the quality of code generated by Large Language Models (LLMs) improves, their adoption in the software industry for automated code generation continues to grow. Researchers primarily focus on enhancing the functional correctness of the generated code while commonly overlooking its energy efficiency and environmental impact. This paper investigates the energy efficiency of the code generated by 20 popular LLMs for 878 programming problems of varying difficulty levels and diverse algorithmic categories selected from the LeetCode platform by comparing them against canonical human-written solutions. Although LLMs can produce functionally correct results in most cases, our findings show that the performance and energy efficiency of LLM-produced solutions are often far below those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and GPT-4o generate the most energy-efficient code, whereas Grok-2 and Gemini-1.5-Pro are among the least energy-efficient models. On average, human-generated canonical solutions are approximately 1.17 times more energy efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific algorithmic groups such as dynamic programming, backtracking, and bit manipulation, LLM-generated code can consume up to 450 times more energy than human-generated canonical solutions.         ",
    "url": "https://arxiv.org/abs/2505.20324",
    "authors": [
      "Md Arman Islam",
      "Devi Varaprasad Jonnala",
      "Ritika Rekhi",
      "Pratik Pokharel",
      "Siddharth Cilamkoti",
      "Asif Imran",
      "Tevfik Kosar",
      "Bekir Turkkan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20356",
    "title": "LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability",
    "abstract": "           Large language models (LLMs) have the potential to revolutionize how we design and implement compilers and code translation tools. However, existing LLMs struggle to handle long and complex programs. We introduce LEGO-Compiler, a novel neural compilation system that leverages LLMs to translate high-level languages into assembly code. Our approach centers on three key innovations: LEGO translation, which decomposes the input program into manageable blocks; breaking down the complex compilation process into smaller, simpler verifiable steps by organizing it as a verifiable LLM workflow by external tests; and a feedback mechanism for self-correction. Supported by formal proofs of translation composability, LEGO-Compiler demonstrates high accuracy on multiple datasets, including over 99% on ExeBench and 97.9% on industrial-grade AnsiBench. Additionally, LEGO-Compiler has also acheived near one order-of-magnitude improvement on compilable code size scalability. This work opens new avenues for applying LLMs to system-level tasks, complementing traditional compiler technologies.         ",
    "url": "https://arxiv.org/abs/2505.20356",
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Chunwei Xia",
      "Zheng Wang",
      "Yunji Chen",
      "Xiaobing Feng",
      "Huimin Cui"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.20378",
    "title": "Hyperbolic embedding of multilayer networks",
    "abstract": "           Multilayer networks offer a powerful framework for modeling complex systems across diverse domains, effectively capturing multiple types of connections and interdependent subsystems commonly found in real world scenarios. To analyze these networks, embedding techniques that project nodes into a lower-dimensional geometric space are essential. This paper introduces a novel hyperbolic embedding framework that advances the state of the art in multilayer network analysis. Our method, which supports heterogeneous node sets across networks and inter-layer connections, generates layer-specific hyperbolic embeddings, enabling detailed intra-layer analysis and inter-layer comparisons, while simultaneously preserving the global multilayer structure within hyperbolic space, a capability that sets it apart from existing approaches, which typically rely on independent embedding of layers. Through experiments on synthetic multilayer stochastic block models, we demonstrate that our approach effectively preserves community structure, even when layers consist of different node sets. When applied to real brain networks, the method successfully clusters disease-related brain regions from different patients, outperforming layer-independent approaches and highlighting its relevance for comparative analysis. Overall, this work provides a robust tool for multilayer network analysis, enhancing interpretability and offering new insights into the structure and function of complex systems.         ",
    "url": "https://arxiv.org/abs/2505.20378",
    "authors": [
      "Martin Guillemaud",
      "Vera Dinkelacker",
      "Mario Chavez"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2505.20380",
    "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
    "abstract": "           The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora. Existing domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks. This paper introduces Group Robust Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously. GRAPE dynamically adjusts sampling weights across source domains (domain weights) while concurrently modulating task weights that quantify the relative importance of each individual target task. This adaptive process prioritizes tasks based on their learning difficulty throughout training. We formulate this interleaved reweighting mechanism as a minimax optimization problem: The inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; The outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning performance across 6 benchmarks. Furthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.         ",
    "url": "https://arxiv.org/abs/2505.20380",
    "authors": [
      "Simin Fan",
      "Maria Ios Glarou",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20404",
    "title": "Co-Design of Soft Gripper with Neural Physics",
    "abstract": "           For robot manipulation, both the controller and end-effector design are crucial. Soft grippers are generalizable by deforming to different geometries, but designing such a gripper and finding its grasp pose remains challenging. In this paper, we propose a co-design framework that generates an optimized soft gripper's block-wise stiffness distribution and its grasping pose, using a neural physics model trained in simulation. We derived a uniform-pressure tendon model for a flexure-based soft finger, then generated a diverse dataset by randomizing both gripper pose and design parameters. A neural network is trained to approximate this forward simulation, yielding a fast, differentiable surrogate. We embed that surrogate in an end-to-end optimization loop to optimize the ideal stiffness configuration and best grasp pose. Finally, we 3D-print the optimized grippers of various stiffness by changing the structural parameters. We demonstrate that our co-designed grippers significantly outperform baseline designs in both simulation and hardware experiments.         ",
    "url": "https://arxiv.org/abs/2505.20404",
    "authors": [
      "Sha Yi",
      "Xueqian Bai",
      "Adabhav Singh",
      "Jianglong Ye",
      "Michael T Tolley",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.20421",
    "title": "Precise Gradient Discontinuities in Neural Fields for Subspace Physics",
    "abstract": "           Discontinuities in spatial derivatives appear in a wide range of physical systems, from creased thin sheets to materials with sharp stiffness transitions. Accurately modeling these features is essential for simulation but remains challenging for traditional mesh-based methods, which require discontinuity-aligned remeshing -- entangling geometry with simulation and hindering generalization across shape families. Neural fields offer an appealing alternative by encoding basis functions as smooth, continuous functions over space, enabling simulation across varying shapes. However, their smoothness makes them poorly suited for representing gradient discontinuities. Prior work addresses discontinuities in function values, but capturing sharp changes in spatial derivatives while maintaining function continuity has received little attention. We introduce a neural field construction that captures gradient discontinuities without baking their location into the network weights. By augmenting input coordinates with a smoothly clamped distance function in a lifting framework, we enable encoding of gradient jumps at evolving interfaces. This design supports discretization-agnostic simulation of parametrized shape families with heterogeneous materials and evolving creases, enabling new reduced-order capabilities such as shape morphing, interactive crease editing, and simulation of soft-rigid hybrid structures. We further demonstrate that our method can be combined with previous lifting techniques to jointly capture both gradient and value discontinuities, supporting simultaneous cuts and creases within a unified model.         ",
    "url": "https://arxiv.org/abs/2505.20421",
    "authors": [
      "Mengfei Liu",
      "Yue Chang",
      "Zhecheng Wang",
      "Peter Yichen Chen",
      "Eitan Grinspun"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2505.20422",
    "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model",
    "abstract": "           Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling zero-shot reasoning over unseen graphs by learning transferable patterns. However, most existing KGFMs rely solely on graph structure, overlooking the rich semantic signals encoded in textual attributes. We introduce SEMMA, a dual-module KGFM that systematically integrates transferable textual semantics alongside structure. SEMMA leverages Large Language Models (LLMs) to enrich relation identifiers, generating semantic embeddings that subsequently form a textual relation graph, which is fused with the structural component. Across 54 diverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully inductive link prediction. Crucially, we show that in more challenging generalization settings, where the test-time relation vocabulary is entirely unseen, structural methods collapse while SEMMA is 2x more effective. Our findings demonstrate that textual semantics are critical for generalization in settings where structure alone fails, highlighting the need for foundation models that unify structural and linguistic signals in knowledge reasoning.         ",
    "url": "https://arxiv.org/abs/2505.20422",
    "authors": [
      "Arvindh Arun",
      "Sumit Kumar",
      "Mojtaba Nayyeri",
      "Bo Xiong",
      "Ponnurangam Kumaraguru",
      "Antonio Vergari",
      "Steffen Staab"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20423",
    "title": "Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments",
    "abstract": "           Landing safely in crowded urban environments remains an essential yet challenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in emergency situations. In this work, we propose a risk-aware approach that harnesses semantic segmentation to continuously evaluate potential hazards in the drone's field of view. By using a specialized deep neural network to assign pixel-level risk values and applying an algorithm based on risk maps, our method adaptively identifies a stable Safe Landing Zone (SLZ) despite moving critical obstacles such as vehicles, people, etc., and other visual challenges like shifting illumination. A control system then guides the UAV toward this low-risk region, employing altitude-dependent safety thresholds and temporal landing point stabilization to ensure robust descent trajectories. Experimental validation in diverse urban environments demonstrates the effectiveness of our approach, achieving over 90% landing success rates in very challenging real scenarios, showing significant improvements in various risk metrics. Our findings suggest that risk-oriented vision methods can effectively help reduce the risk of accidents in emergency landing situations, particularly in complex, unstructured, urban scenarios, densely populated with moving risky obstacles, while potentiating the true capabilities of UAVs in complex urban operations.         ",
    "url": "https://arxiv.org/abs/2505.20423",
    "authors": [
      "Julio de la Torre-Vanegas",
      "Miguel Soriano-Garcia",
      "Israel Becerra",
      "Diego Mercado-Ravell"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20426",
    "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
    "abstract": "           Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: this https URL ",
    "url": "https://arxiv.org/abs/2505.20426",
    "authors": [
      "Yunlong Tang",
      "Pinxin Liu",
      "Mingqian Feng",
      "Zhangyun Tan",
      "Rui Mao",
      "Chao Huang",
      "Jing Bi",
      "Yunzhong Xiao",
      "Susan Liang",
      "Hang Hua",
      "Ali Vosoughi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20435",
    "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence",
    "abstract": "           Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.         ",
    "url": "https://arxiv.org/abs/2505.20435",
    "authors": [
      "Aideen Fay",
      "In\u00e9s Garc\u00eda-Redondo",
      "Qiquan Wang",
      "Haim Dubossarsky",
      "Anthea Monod"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)"
    ]
  },
  {
    "id": "arXiv:2505.20444",
    "title": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models",
    "abstract": "           Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20444",
    "authors": [
      "Haoran Li",
      "Yingjie Qin",
      "Baoyuan Ou",
      "Lai Xu",
      "Ruiwen Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20451",
    "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries",
    "abstract": "           Today, large language models are widely used as judges to evaluate responses from other language models. Hence, it is imperative to benchmark and improve these LLM-judges on real-world language model usage: a typical human-assistant conversation is lengthy, and shows significant diversity in topics, intents, and requirements across turns, e.g. social interactions, task requests, feedback. We present Amulet, a framework that leverages pertinent linguistic concepts of dialog-acts and maxims to improve the accuracy of LLM-judges on preference data with complex, multi-turn conversational context. Amulet presents valuable insights about (a) the communicative structures and intents present in the conversation (dialog acts), and (b) the satisfaction of conversational principles (maxims) by the preference responses, and uses them to make judgments. On four challenging datasets, Amulet shows that (a) humans frequently (60 to 70 percent of the time) change their intents from one turn of the conversation to the next, and (b) in 75 percent of instances, the preference responses can be differentiated via dialog acts and/or maxims, reiterating the latter's significance in judging such data. Amulet can be used either as a judge by applying the framework to a single LLM, or integrated into a jury with different LLM judges; our judges and juries show strong improvements on relevant baselines for all four datasets.         ",
    "url": "https://arxiv.org/abs/2505.20451",
    "authors": [
      "Sahana Ramnath",
      "Anurag Mudgil",
      "Brihi Joshi",
      "Skyler Hallinan",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20452",
    "title": "Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes",
    "abstract": "           Multiple change point (MCP) detection in non-stationary time series is challenging due to the variety of underlying patterns. To address these challenges, we propose a novel algorithm that integrates Active Learning (AL) with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method leverages spectral analysis to identify potential changes and employs AL to strategically select new sampling points for improved efficiency. By incorporating the modeling flexibility of DGPs with the change-identification capabilities of spectral methods, our approach adapts to diverse spectral change behaviors and effectively localizes multiple change points. Experiments on both simulated and real-world data demonstrate that our method outperforms existing techniques in terms of detection accuracy and sampling efficiency for non-stationary time series.         ",
    "url": "https://arxiv.org/abs/2505.20452",
    "authors": [
      "Hao Zhao",
      "Rong Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20454",
    "title": "BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction",
    "abstract": "           Accurate prediction of blast pressure fields is essential for applications in structural safety, defense planning, and hazard mitigation. Traditional methods such as empirical models and computational fluid dynamics (CFD) simulations offer limited trade offs between speed and accuracy; empirical models fail to capture complex interactions in cluttered environments, while CFD simulations are computationally expensive and time consuming. In this work, we introduce BlastOFormer, a novel Transformer based surrogate model for full field maximum pressure prediction from arbitrary obstacle and charge configurations. BlastOFormer leverages a signed distance function (SDF) encoding and a grid to grid attention based architecture inspired by OFormer and Vision Transformer (ViT) frameworks. Trained on a dataset generated using the open source blastFoam CFD solver, our model outperforms convolutional neural networks (CNNs) and Fourier Neural Operators (FNOs) across both log transformed and unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score (0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for inference, more than 600,000 times faster than CFD simulations. Qualitative visualizations and error analyses further confirm BlastOFormer's superior spatial coherence and generalization capabilities. These results highlight its potential as a real time alternative to conventional CFD approaches for blast pressure estimation in complex environments.         ",
    "url": "https://arxiv.org/abs/2505.20454",
    "authors": [
      "Reid Graves",
      "Anthony Zhou",
      "Amir Barati Farimani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20473",
    "title": "Stochastic Preconditioning for Neural Field Optimization",
    "abstract": "           Neural fields are a highly effective representation across visual computing. This work observes that fitting these fields is greatly improved by incorporating spatial stochasticity during training, and that this simple technique can replace or even outperform custom-designed hierarchies and frequency space constructions. The approach is formalized as implicitly operating on a blurred version of the field, evaluated in-expectation by sampling with Gaussian-distributed offsets. Querying the blurred field during optimization greatly improves convergence and robustness, akin to the role of preconditioners in numerical linear algebra. This implicit, sampling-based perspective fits naturally into the neural field paradigm, comes at no additional cost, and is extremely simple to implement. We describe the basic theory of this technique, including details such as handling boundary conditions, and extending to a spatially-varying blur. Experiments demonstrate this approach on representations including coordinate MLPs, neural hashgrids, triplanes, and more, across tasks including surface reconstruction and radiance fields. In settings where custom-designed hierarchies have already been developed, stochastic preconditioning nearly matches or improves their performance with a simple and unified approach; in settings without existing hierarchies it provides an immediate boost to quality and robustness.         ",
    "url": "https://arxiv.org/abs/2505.20473",
    "authors": [
      "Selena Ling",
      "Merlin Nimier-David",
      "Alec Jacobson",
      "Nicholas Sharp"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20498",
    "title": "ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image",
    "abstract": "           Vision-based tactile sensing has been widely used in perception, reconstruction, and robotic manipulation. However, collecting large-scale tactile data remains costly due to the localized nature of sensor-object interactions and inconsistencies across sensor instances. Existing approaches to scaling tactile data, such as simulation and free-form tactile generation, often suffer from unrealistic output and poor transferability to downstream this http URL address this, we propose ControlTac, a two-stage controllable framework that generates realistic tactile images conditioned on a single reference tactile image, contact force, and contact position. With those physical priors as control input, ControlTac generates physically plausible and varied tactile images that can be used for effective data augmentation. Through experiments on three downstream tasks, we demonstrate that ControlTac can effectively augment tactile datasets and lead to consistent gains. Our three real-world experiments further validate the practical utility of our approach. Project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20498",
    "authors": [
      "Dongyu Luo",
      "Kelin Yu",
      "Amir-Hossein Shahidzadeh",
      "Cornelia Ferm\u00fcller",
      "Yiannis Aloimonos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.20515",
    "title": "Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints",
    "abstract": "           Despite the promise of scientific machine learning (SciML) in combining data-driven techniques with mechanistic modeling, existing approaches for incorporating hard constraints in neural differential equations (NDEs) face significant limitations. Scalability issues and poor numerical properties prevent these neural models from being used for modeling physical systems with complicated conservation laws. We propose Manifold-Projected Neural ODEs (PNODEs), a method that explicitly enforces algebraic constraints by projecting each ODE step onto the constraint manifold. This framework arises naturally from semi-explicit differential-algebraic equations (DAEs), and includes both a robust iterative variant and a fast approximation requiring a single Jacobian factorization. We further demonstrate that prior works on relaxation methods are special cases of our approach. PNODEs consistently outperform baselines across six benchmark problems achieving a mean constraint violation error below $10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to other methods for a given level of error tolerance. These results show that constraint projection offers a simple strategy for learning physically consistent long-horizon dynamics.         ",
    "url": "https://arxiv.org/abs/2505.20515",
    "authors": [
      "Avik Pal",
      "Alan Edelman",
      "Christopher Rackauckas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.20532",
    "title": "One-shot Robust Federated Learning of Independent Component Analysis",
    "abstract": "           This paper investigates a general robust one-shot aggregation framework for distributed and federated Independent Component Analysis (ICA) problem. We propose a geometric median-based aggregation algorithm that leverages $k$-means clustering to resolve the permutation ambiguity in local client estimations. Our method first performs k-means to partition client-provided estimators into clusters and then aggregates estimators within each cluster using the geometric median. This approach provably remains effective even in highly heterogeneous scenarios where at most half of the clients can observe only a minimal number of samples. The key theoretical contribution lies in the combined analysis of the geometric median's error bound-aided by sample quantiles-and the maximum misclustering rates of the aforementioned solution of $k$-means. The effectiveness of the proposed approach is further supported by simulation studies conducted under various heterogeneous settings.         ",
    "url": "https://arxiv.org/abs/2505.20532",
    "authors": [
      "Dian Jin",
      "Xin Bing",
      "Yuqian Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.20538",
    "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy",
    "abstract": "           Large Language Models (LLMs) are being explored for applications in scientific research, including their capabilities to synthesize literature, answer research questions, generate research ideas, and even conduct computational experiments. Ultimately, our goal is for these to help scientists derive novel scientific insights. In many areas of science, such insights often arise from processing and visualizing data to understand its patterns. However, evaluating whether an LLM-mediated scientific workflow produces outputs conveying the correct scientific insights is challenging to evaluate and has not been addressed in past work. We introduce AstroVisBench, the first benchmark for both scientific computing and visualization in the astronomy domain. AstroVisBench judges a language model's ability to both (1) create astronomy-specific workflows to process and analyze data and (2) visualize the results of these workflows through complex plots. Our evaluation of visualizations uses a novel LLM-as-a-judge workflow, which is validated against annotation by five professional astronomers. Using AstroVisBench we present an evaluation of state-of-the-art language models, showing a significant gap in their ability to engage in astronomy research as useful assistants. This evaluation provides a strong end-to-end evaluation for AI scientists that offers a path forward for the development of visualization-based workflows, which are central to a broad range of domains from physics to biology.         ",
    "url": "https://arxiv.org/abs/2505.20538",
    "authors": [
      "Sebastian Antony Joseph",
      "Syed Murtaza Husain",
      "Stella S. R. Offner",
      "St\u00e9phanie Juneau",
      "Paul Torrey",
      "Adam S. Bolton",
      "Juan P. Farias",
      "Niall Gaffney",
      "Greg Durrett",
      "Junyi Jessy Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20540",
    "title": "Causality and \"In-the-Wild\" Video-Based Person Re-ID: A Survey",
    "abstract": "           Video-based person re-identification (Re-ID) remains brittle in real-world deployments despite impressive benchmark performance. Most existing models rely on superficial correlations such as clothing, background, or lighting that fail to generalize across domains, viewpoints, and temporal variations. This survey examines the emerging role of causal reasoning as a principled alternative to traditional correlation-based approaches in video-based Re-ID. We provide a structured and critical analysis of methods that leverage structural causal models, interventions, and counterfactual reasoning to isolate identity-specific features from confounding factors. The survey is organized around a novel taxonomy of causal Re-ID methods that spans generative disentanglement, domain-invariant modeling, and causal transformers. We review current evaluation metrics and introduce causal-specific robustness measures. In addition, we assess practical challenges of scalability, fairness, interpretability, and privacy that must be addressed for real-world adoption. Finally, we identify open problems and outline future research directions that integrate causal modeling with efficient architectures and self-supervised learning. This survey aims to establish a coherent foundation for causal video-based person Re-ID and to catalyze the next phase of research in this rapidly evolving domain.         ",
    "url": "https://arxiv.org/abs/2505.20540",
    "authors": [
      "Md Rashidunnabi",
      "Kailash Hambarde",
      "Hugo Proen\u00e7a"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20567",
    "title": "Byzantine-Resilient Distributed P2P Energy Trading via Spatial-Temporal Anomaly Detection",
    "abstract": "           Distributed peer-to-peer (P2P) energy trading mandates an escalating coupling between the physical power network and communication network, necessitating high-frequency sharing of real-time data among prosumers. However, this data-sharing scheme renders the system vulnerable to various malicious behaviors, as Byzantine agents can initiate cyberattacks by injecting sophisticated false data. To better investigate the impacts of malicious Byzantine faults, this paper develops a fully distributed P2P energy trading model by accounting for the high-fidelity physical network constraints. To further detect Byzantine faults and mitigate their impacts on distributed P2P energy trading problem, we propose an online spatial-temporal anomaly detection approach by leveraging the tensor learning method, which is informed by the domain knowledge to enable awesome detection performance. Moreover, to enhance its computational efficiency, we further develop closed-form solutions for the proposed detection approach. Subsequently, we derive theoretical conditions for guaranteeing optimality and convergence of the distributed P2P energy trading problem with anomaly detection mechanisms. Results from numerical simulations validate the effectiveness, optimality, and scalability of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2505.20567",
    "authors": [
      "Junhong Liu",
      "Qinfei Long",
      "Rong-Peng Liu",
      "Wenjie Liu",
      "Yunhe Hou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2505.20584",
    "title": "A Dashboard Approach to Monitoring Mpox-Related Discourse and Misinformation on Social Media",
    "abstract": "           Mpox (formerly monkeypox) is a zoonotic disease caused by an orthopoxvirus closely related to variola and remains a significant global public health concern. During outbreaks, social media platforms like X (formerly Twitter) can both inform and misinform the public, complicating efforts to convey accurate health information. To support local response efforts, we developed a researcher-focused dashboard for use by public health stakeholders and the public that enables searching and visualizing mpox-related tweets through an interactive interface. Following the CDC's designation of mpox as an emerging virus in August 2024, our dashboard recorded a marked increase in tweet volume compared to 2023, illustrating the rapid spread of health discourse across digital platforms. These findings underscore the continued need for real-time social media monitoring tools to support public health communication and track evolving sentiment and misinformation trends at the local level.         ",
    "url": "https://arxiv.org/abs/2505.20584",
    "authors": [
      "Linfeng",
      "Zhao",
      "Rishul Bhuvanagiri",
      "Blake Gonzales",
      "Kellen Sharp",
      "Dhiraj Murthy"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2505.20589",
    "title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction",
    "abstract": "           The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at this https URL .         ",
    "url": "https://arxiv.org/abs/2505.20589",
    "authors": [
      "Mahdi Pourmirzaei",
      "Farzaneh Esmaili",
      "Salhuldin Alqarghuli",
      "Mohammadreza Pourmirzaei",
      "Ye Han",
      "Kai Chen",
      "Mohsen Rezaei",
      "Duolin Wang",
      "Dong Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2505.20606",
    "title": "Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation",
    "abstract": "           Whisper's robust performance in automatic speech recognition (ASR) is often attributed to its massive 680k-hour training set, an impractical scale for most researchers. In this work, we examine how linguistic and acoustic diversity in training data affect the robustness of the ASR model and reveal that transcription generalization is primarily driven by acoustic variation rather than linguistic richness. We find that targeted acoustic augmentation methods could significantly improve the generalization ability of ASR models, reducing word-error rates by up to 19.24 percent on unseen datasets when training on the 960-hour Librispeech dataset. These findings highlight strategic acoustically focused data augmentation as a promising alternative to massive datasets for building robust ASR models, offering a potential solution to future foundation ASR models when massive human speech data is lacking.         ",
    "url": "https://arxiv.org/abs/2505.20606",
    "authors": [
      "Dancheng Liu",
      "Amir Nassereldine",
      "Chenhui Xu",
      "Jinjun Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2505.20612",
    "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models",
    "abstract": "           Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Our code and dataset are available at this https URL and this https URL ",
    "url": "https://arxiv.org/abs/2505.20612",
    "authors": [
      "Peter Robicheaux",
      "Matvei Popov",
      "Anish Madan",
      "Isaac Robinson",
      "Joseph Nelson",
      "Deva Ramanan",
      "Neehar Peri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20615",
    "title": "Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea",
    "abstract": "           Obstructive sleep apnea (OSA) is a significant risk factor for hypertension, primarily due to intermittent hypoxia and sleep fragmentation. Predicting whether individuals with OSA will develop hypertension within five years remains a complex challenge. This study introduces a novel deep learning approach that integrates Discrete Cosine Transform (DCT)-based transfer learning to enhance prediction accuracy. We are the first to incorporate all polysomnography signals together for hypertension prediction, leveraging their collective information to improve model performance. Features were extracted from these signals and transformed into a 2D representation to utilize pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet variants. To further improve feature learning, we introduced a DCT layer, which transforms input features into a frequency-based representation, preserving essential spectral information, decorrelating features, and enhancing robustness to noise. This frequency-domain approach, coupled with transfer learning, is especially beneficial for limited medical datasets, as it leverages rich representations from pre-trained networks to improve generalization. By strategically placing the DCT layer at deeper truncation depths within EfficientNet, our model achieved a best area under the curve (AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature extraction and transfer learning in predicting hypertension risk in OSA patients over a five-year period.         ",
    "url": "https://arxiv.org/abs/2505.20615",
    "authors": [
      "Omid Halimi Milani",
      "Ahmet Enis Cetin",
      "Bharati Prasad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20617",
    "title": "OccLE: Label-Efficient 3D Semantic Occupancy Prediction",
    "abstract": "           3D semantic occupancy prediction offers an intuitive and efficient scene understanding and has attracted significant interest in autonomous driving perception. Existing approaches either rely on full supervision, which demands costly voxel-level annotations, or on self-supervision, which provides limited guidance and yields suboptimal performance. To address these challenges, we propose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes images and LiDAR as inputs and maintains high performance with limited voxel annotations. Our intuition is to decouple the semantic and geometric learning tasks and then fuse the learned feature grids from both tasks for the final semantic occupancy prediction. Therefore, the semantic branch distills 2D foundation model to provide aligned pseudo labels for 2D and 3D semantic learning. The geometric branch integrates image and LiDAR inputs in cross-plane synergy based on their inherency, employing semi-supervision to enhance geometry learning. We fuse semantic-geometric feature grids through Dual Mamba and incorporate a scatter-accumulated projection to supervise unannotated prediction with aligned pseudo labels. Experiments show that OccLE achieves competitive performance with only 10% of voxel annotations, reaching a mIoU of 16.59% on the SemanticKITTI validation set.         ",
    "url": "https://arxiv.org/abs/2505.20617",
    "authors": [
      "Naiyu Fang",
      "Zheyuan Zhou",
      "Fayao Liu",
      "Xulei Yang",
      "Jiacheng Wei",
      "Lemiao Qiu",
      "Guosheng Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20621",
    "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
    "abstract": "           Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly improving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.         ",
    "url": "https://arxiv.org/abs/2505.20621",
    "authors": [
      "Shijie Liu",
      "Andrew C. Cullen",
      "Paul Montague",
      "Sarah Erfani",
      "Benjamin I. P. Rubinstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20623",
    "title": "Institutionalizing Folk Theories of Algorithms: How Multi-Channel Networks (MCNs) Govern Algorithmic Labor in Chinese Live-Streaming Industry",
    "abstract": "           As algorithmic systems increasingly structure platform labor, workers often rely on informal \"folk theories\", experience-based beliefs about how algorithms work, to navigate opaque and unstable algorithmic environments. Prior research has largely treated these theories as bottom-up, peer-driven strategies for coping with algorithmic opacity and uncertainty. In this study, we shift analytical attention to intermediary organizations and examine how folk theories of algorithms can be institutionally constructed and operationalized by those organizations as tools of labor management. Drawing on nine months of ethnographic fieldwork and 37 interviews with live-streamers and staff at Multi-Channel Networks (MCNs) in China, we show that MCNs develop and circulate dual algorithmic theories: internally, they acknowledge the volatility of platform systems and adopt probabilistic strategies to manage risk; externally, they promote simplified, prescriptive theories portraying the algorithm as transparent, fair, and responsive to individual effort. They have further operationalize those folk theories for labor management, encouraging streamers to self-discipline and invest in equipment, training, and routines, while absolving MCNs of accountability. We contribute to CSCW and platform labor literature by demonstrating how informal algorithmic knowledge, once institutionalized, can become infrastructures of soft control -- shaping not only how workers interpret platform algorithms, but also how their labor is structured, moralized and governed.         ",
    "url": "https://arxiv.org/abs/2505.20623",
    "authors": [
      "Qing Xiao",
      "Rongyi Chen",
      "Jingjia Xiao",
      "Tianyang Fu",
      "Alice Qian Zhang",
      "Xianzhe Fan",
      "Bingbing Zhang",
      "Zhicong Lu",
      "Hong Shen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2505.20630",
    "title": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis",
    "abstract": "           As Large Language Models (LLMs) evolve in understanding and generating code, accurately evaluating their reliability in analyzing source code vulnerabilities becomes increasingly vital. While studies have examined LLM capabilities in tasks like vulnerability detection and repair, they often overlook the importance of both structure and semantic reasoning crucial for trustworthy vulnerability analysis. To address this gap, we introduce SV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for vulnerability analysis of code written in the C programming language through two key dimensions: structure reasoning - assessing how models identify relationships between code elements under varying data and control flow complexities; and semantic reasoning - examining their logical consistency in scenarios where code is structurally and semantically perturbed. Our results show that current LLMs are far from satisfactory in understanding complex code relationships and that their vulnerability analyses rely more on pattern matching than on robust logical reasoning. These findings underscore the effectiveness of the SV-TrustEval-C benchmark and highlight critical areas for enhancing the reasoning capabilities and trustworthiness of LLMs in real-world vulnerability analysis tasks. Our initial benchmark dataset is publicly available.         ",
    "url": "https://arxiv.org/abs/2505.20630",
    "authors": [
      "Yansong Li",
      "Paula Branco",
      "Alexander M. Hoole",
      "Manish Marwah",
      "Hari Manassery Koduvely",
      "Guy-Vincent Jourdan",
      "Stephan Jou"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20639",
    "title": "Open-Det: An Efficient Learning Framework for Open-Ended Detection",
    "abstract": "           Open-Ended object Detection (OED) is a novel and challenging task that detects objects and generates their category names in a free-form manner, without requiring additional vocabularies during inference. However, the existing OED models, such as GenerateU, require large-scale datasets for training, suffer from slow convergence, and exhibit limited performance. To address these issues, we present a novel and efficient Open-Det framework, consisting of four collaborative parts. Specifically, Open-Det accelerates model training in both the bounding box and object name generation process by reconstructing the Object Detector and the Object Name Generator. To bridge the semantic gap between Vision and Language modalities, we propose a Vision-Language Aligner with V-to-L and L-to-V alignment mechanisms, incorporating with the Prompts Distiller to transfer knowledge from the VLM into VL-prompts, enabling accurate object name generation for the LLM. In addition, we design a Masked Alignment Loss to eliminate contradictory supervision and introduce a Joint Loss to enhance classification, resulting in more efficient training. Compared to GenerateU, Open-Det, using only 1.5% of the training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs. 149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher performance (+1.0% in APr). The source codes are available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20639",
    "authors": [
      "Guiping Cao",
      "Tao Wang",
      "Wenjian Huang",
      "Xiangyuan Lan",
      "Jianguo Zhang",
      "Dongmei Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20641",
    "title": "See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction",
    "abstract": "           Occupancy prediction aims to estimate the 3D spatial distribution of occupied regions along with their corresponding semantic labels. Existing vision-based methods perform well on daytime benchmarks but struggle in nighttime scenarios due to limited visibility and challenging lighting conditions. To address these challenges, we propose \\textbf{LIAR}, a novel framework that learns illumination-affined representations. LIAR first introduces Selective Low-light Image Enhancement (SLLIE), which leverages the illumination priors from daytime scenes to adaptively determine whether a nighttime image is genuinely dark or sufficiently well-lit, enabling more targeted global enhancement. Building on the illumination maps generated by SLLIE, LIAR further incorporates two illumination-aware components: 2D Illumination-guided Sampling (2D-IGS) and 3D Illumination-driven Projection (3D-IDP), to respectively tackle local underexposure and overexposure. Specifically, 2D-IGS modulates feature sampling positions according to illumination maps, assigning larger offsets to darker regions and smaller ones to brighter regions, thereby alleviating feature degradation in underexposed areas. Subsequently, 3D-IDP enhances semantic understanding in overexposed regions by constructing illumination intensity fields and supplying refined residual queries to the BEV context refinement process. Extensive experiments on both real and synthetic datasets demonstrate the superior performance of LIAR under challenging nighttime scenarios. The source code and pretrained models are available \\href{this https URL}{here}.         ",
    "url": "https://arxiv.org/abs/2505.20641",
    "authors": [
      "Yuan Wu",
      "Zhiqiang Yan",
      "Yigong Zhang",
      "Xiang Li",
      "ian Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20646",
    "title": "Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory",
    "abstract": "           Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.         ",
    "url": "https://arxiv.org/abs/2505.20646",
    "authors": [
      "Eduardo Y. Sakabe",
      "Felipe S. Abrah\u00e3o",
      "Alexandre Sim\u00f5es",
      "Esther Colombini",
      "Paula Costa",
      "Ricardo Gudwin",
      "Hector Zenil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2505.20649",
    "title": "Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design",
    "abstract": "           In AI-empowered poster design, content-aware layout generation is crucial for the on-image arrangement of visual-textual elements, e.g., logo, text, and underlay. To perceive the background images, existing work demanded a high parameter count that far exceeds the size of available training data, which has impeded the model's real-time performance and generalization ability. To address these challenges, we proposed a patch-level data summarization and augmentation approach, vividly named Scan-and-Print. Specifically, the scan procedure selects only the patches suitable for placing element vertices to perform fine-grained perception efficiently. Then, the print procedure mixes up the patches and vertices across two image-layout pairs to synthesize over 100% new samples in each epoch while preserving their plausibility. Besides, to facilitate the vertex-level operations, a vertex-based layout representation is introduced. Extensive experimental results on widely used benchmarks demonstrated that Scan-and-Print can generate visually appealing layouts with state-of-the-art quality while dramatically reducing computational bottleneck by 95.2%.         ",
    "url": "https://arxiv.org/abs/2505.20649",
    "authors": [
      "HsiaoYuan Hsu",
      "Yuxin Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20653",
    "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment",
    "abstract": "           Recent advancements in domain generalization for deepfake detection have attracted significant attention, with previous methods often incorporating additional modules to prevent overfitting to domain-specific patterns. However, such regularization can hinder the optimization of the empirical risk minimization (ERM) objective, ultimately degrading model performance. In this paper, we propose a novel learning objective that aligns generalization gradient updates with ERM gradient updates. The key innovation is the application of perturbations to model parameters, aligning the ascending points across domains, which specifically enhances the robustness of deepfake detection models to domain shifts. This approach effectively preserves domain-invariant features while managing domain-specific characteristics, without introducing additional regularization. Experimental results on multiple challenging deepfake detection datasets demonstrate that our gradient alignment strategy outperforms state-of-the-art domain generalization techniques, confirming the efficacy of our method. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20653",
    "authors": [
      "Lingyu Qiu",
      "Ke Jiang",
      "Xiaoyang Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20660",
    "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism",
    "abstract": "           Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent's performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.         ",
    "url": "https://arxiv.org/abs/2505.20660",
    "authors": [
      "Qinzhuo Wu",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20675",
    "title": "Contrastive Desensitization Learning for Cross Domain Face Forgery Detection",
    "abstract": "           In this paper, we propose a new cross-domain face forgery detection method that is insensitive to different and possibly unseen forgery methods while ensuring an acceptable low false positive rate. Although existing face forgery detection methods are applicable to multiple domains to some degree, they often come with a high false positive rate, which can greatly disrupt the usability of the system. To address this issue, we propose an Contrastive Desensitization Network (CDN) based on a robust desensitization algorithm, which captures the essential domain characteristics through learning them from domain transformation over pairs of genuine face images. One advantage of CDN lies in that the learnt face representation is theoretical justified with regard to the its robustness against the domain changes. Extensive experiments over large-scale benchmark datasets demonstrate that our method achieves a much lower false alarm rate with improved detection accuracy compared to several state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2505.20675",
    "authors": [
      "Lingyu Qiu",
      "Ke Jiang",
      "Xiaoyang Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20679",
    "title": "SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations",
    "abstract": "           Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at this https URL .         ",
    "url": "https://arxiv.org/abs/2505.20679",
    "authors": [
      "Danush Khanna",
      "Pratinav Seth",
      "Sidhaarth Sredharan Murali",
      "Aditya Kumar Guru",
      "Siddharth Shukla",
      "Tanuj Tyagi",
      "Sandeep Chaurasia",
      "Kripabandhu Ghosh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20687",
    "title": "VisAlgae 2023: A Dataset and Challenge for Algae Detection in Microscopy Images",
    "abstract": "           Microalgae, vital for ecological balance and economic sectors, present challenges in detection due to their diverse sizes and conditions. This paper summarizes the second \"Vision Meets Algae\" (VisAlgae 2023) Challenge, aiming to enhance high-throughput microalgae cell detection. The challenge, which attracted 369 participating teams, includes a dataset of 1000 images across six classes, featuring microalgae of varying sizes and distinct features. Participants faced tasks such as detecting small targets, handling motion blur, and complex backgrounds. The top 10 methods, outlined here, offer insights into overcoming these challenges and maximizing detection accuracy. This intersection of algae research and computer vision offers promise for ecological understanding and technological advancement. The dataset can be accessed at: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20687",
    "authors": [
      "Mingxuan Sun",
      "Juntao Jiang",
      "Zhiqiang Yang",
      "Shenao Kong",
      "Jiamin Qi",
      "Jianru Shang",
      "Shuangling Luo",
      "Wanfa Sun",
      "Tianyi Wang",
      "Yanqi Wang",
      "Qixuan Wang",
      "Tingjian Dai",
      "Tianxiang Chen",
      "Jinming Zhang",
      "Xuerui Zhang",
      "Yuepeng He",
      "Pengcheng Fu",
      "Qiu Guan",
      "Shizheng Zhou",
      "Yanbo Yu",
      "Qigui Jiang",
      "Teng Zhou",
      "Liuyong Shi",
      "Hong Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20692",
    "title": "Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions",
    "abstract": "           Recent advances in generative AI have enabled visual content creation through text-to-image (T2I) generation. However, despite their creative potential, T2I models often replicate and amplify societal stereotypes -- particularly those related to gender, race, and culture -- raising important ethical concerns. This paper proposes a theory-driven bias detection rubric and a Social Stereotype Index (SSI) to systematically evaluate social biases in T2I outputs. We audited three major T2I model outputs -- DALL-E-3, Midjourney-6.1, and Stability AI Core -- using 100 queries across three categories -- geocultural, occupational, and adjectival. Our analysis reveals that initial outputs are prone to include stereotypical visual cues, including gendered professions, cultural markers, and western beauty norms. To address this, we adopted our rubric to conduct targeted prompt refinement using LLMs, which significantly reduced bias -- SSI dropped by 61% for geocultural, 69% for occupational, and 51% for adjectival queries. We complemented our quantitative analysis through a user study examining perceptions, awareness, and preferences around AI-generated biased imagery. Our findings reveal a key tension -- although prompt refinement can mitigate stereotypes, it can limit contextual alignment. Interestingly, users often perceived stereotypical images to be more aligned with their expectations. We discuss the need to balance ethical debiasing with contextual relevance and call for T2I systems that support global diversity and inclusivity while not compromising the reflection of real-world social complexity.         ",
    "url": "https://arxiv.org/abs/2505.20692",
    "authors": [
      "Saharsh Barve",
      "Andy Mao",
      "Jiayue Melissa Shi",
      "Prerna Juneja",
      "Koustuv Saha"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20697",
    "title": "Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series",
    "abstract": "           The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.         ",
    "url": "https://arxiv.org/abs/2505.20697",
    "authors": [
      "Zachary C. Brown",
      "David Carlson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.20698",
    "title": "Sparsified State-Space Models are Efficient Highway Networks",
    "abstract": "           State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20698",
    "authors": [
      "Woomin Song",
      "Jihoon Tack",
      "Sangwoo Mo",
      "Seunghyuk Oh",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20705",
    "title": "Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures",
    "abstract": "           This paper addresses the challenges of fault prediction and delayed response in distributed systems by proposing an intelligent prediction method based on temporal feature learning. The method takes multi-dimensional performance metric sequences as input. We use a Gated Recurrent Unit (GRU) to model the evolution of system states over time. An attention mechanism is then applied to enhance key temporal segments, improving the model's ability to identify potential faults. On this basis, a feedforward neural network is designed to perform the final classification, enabling early warning of system failures. To validate the effectiveness of the proposed approach, comparative experiments and ablation analyses were conducted using data from a large-scale real-world cloud system. The experimental results show that the model outperforms various mainstream time-series models in terms of Accuracy, F1-Score, and AUC. This demonstrates strong prediction capability and stability. Furthermore, the loss function curve confirms the convergence and reliability of the training process. It indicates that the proposed method effectively learns system behavior patterns and achieves efficient fault detection.         ",
    "url": "https://arxiv.org/abs/2505.20705",
    "authors": [
      "Yang Wang",
      "Wenxuan Zhu",
      "Xuehui Quan",
      "Heyi Wang",
      "Chang Liu",
      "Qiyuan Wu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20720",
    "title": "Hybrid Wave-wind System Power Optimisation Using Effective Ensemble Covariance Matrix Adaptation Evolutionary Algorithm",
    "abstract": "           Floating hybrid wind-wave systems combine offshore wind platforms with wave energy converters (WECs) to create cost-effective and reliable energy solutions. Adequately designed and tuned WECs are essential to avoid unwanted loads disrupting turbine motion while efficiently harvesting wave energy. These systems diversify energy sources, enhancing energy security and reducing supply risks while providing a more consistent power output by smoothing energy production variability. However, optimising such systems is complex due to the physical and hydrodynamic interactions between components, resulting in a challenging optimisation space. This study uses a 5-MW OC4-DeepCwind semi-submersible platform with three spherical WECs to explore these synergies. To address these challenges, we propose an effective ensemble optimisation (EEA) technique that combines covariance matrix adaptation, novelty search, and discretisation techniques. To evaluate the EEA performance, we used four sea sites located along Australia's southern coast. In this framework, geometry and power take-off parameters are simultaneously optimised to maximise the average power output of the hybrid wind-wave system. Ensemble optimisation methods enhance performance, flexibility, and robustness by identifying the best algorithm or combination of algorithms for a given problem, addressing issues like premature convergence, stagnation, and poor search space exploration. The EEA was benchmarked against 14 advanced optimisation methods, demonstrating superior solution quality and convergence rates. EEA improved total power output by 111%, 95%, and 52% compared to WOA, EO, and AHA, respectively. Additionally, in comparisons with advanced methods, LSHADE, SaNSDE, and SLPSO, EEA achieved absorbed power enhancements of 498%, 638%, and 349% at the Sydney sea site, showcasing its effectiveness in optimising hybrid energy systems.         ",
    "url": "https://arxiv.org/abs/2505.20720",
    "authors": [
      "Mehdi Neshat",
      "Nataliia Y. Sergiienko",
      "Leandro S.P. da Silva",
      "Seyedali Mirjalili",
      "Amir H. Gandomi",
      "Ossama Abdelkhalik",
      "John Boland"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2505.20721",
    "title": "Recurrent Neural Operators: Stable Long-Term PDE Prediction",
    "abstract": "           Neural operators have emerged as powerful tools for learning solution operators of partial differential equations. However, in time-dependent problems, standard training strategies such as teacher forcing introduce a mismatch between training and inference, leading to compounding errors in long-term autoregressive predictions. To address this issue, we propose Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent training into neural operator architectures. Instead of conditioning each training step on ground-truth inputs, RNOs recursively apply the operator to their own predictions over a temporal window, effectively simulating inference-time dynamics during training. This alignment mitigates exposure bias and enhances robustness to error accumulation. Theoretically, we show that recurrent training can reduce the worst-case exponential error growth typical of teacher forcing to linear growth. Empirically, we demonstrate that recurrently trained Multigrid Neural Operators significantly outperform their teacher-forced counterparts in long-term accuracy and stability on standard benchmarks. Our results underscore the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning.         ",
    "url": "https://arxiv.org/abs/2505.20721",
    "authors": [
      "Zaijun Ye",
      "Chen-Song Zhang",
      "Wansheng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.20734",
    "title": "Adversarial bandit optimization for approximately linear functions",
    "abstract": "           We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.         ",
    "url": "https://arxiv.org/abs/2505.20734",
    "authors": [
      "Zhuoyu Cheng",
      "Kohei Hatano",
      "Eiji Takimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20741",
    "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
    "abstract": "           Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.         ",
    "url": "https://arxiv.org/abs/2505.20741",
    "authors": [
      "Jiatong Shi",
      "Hye-Jin Shim",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.20743",
    "title": "UQLegalAI@COLIEE2025: Advancing Legal Case Retrieval with Large Language Models and Graph Neural Networks",
    "abstract": "           Legal case retrieval plays a pivotal role in the legal domain by facilitating the efficient identification of relevant cases, supporting legal professionals and researchers to propose legal arguments and make informed decision-making. To improve retrieval accuracy, the Competition on Legal Information Extraction and Entailment (COLIEE) is held annually, offering updated benchmark datasets for evaluation. This paper presents a detailed description of CaseLink, the method employed by UQLegalAI, the second highest team in Task 1 of COLIEE 2025. The CaseLink model utilises inductive graph learning and Global Case Graphs to capture the intrinsic case connectivity to improve the accuracy of legal case retrieval. Specifically, a large language model specialized in text embedding is employed to transform legal texts into embeddings, which serve as the feature representations of the nodes in the constructed case graph. A new contrastive objective, incorporating a regularization on the degree of case nodes, is proposed to leverage the information within the case reference relationship for model optimization. The main codebase used in our method is based on an open-sourced repo of CaseLink: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20743",
    "authors": [
      "Yanran Tang",
      "Ruihong Qiu",
      "Zi Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2505.20765",
    "title": "Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies",
    "abstract": "           Unsupervised anomaly detection in time series has been a pivotal research area for decades. Current mainstream approaches focus on learning normality, on the assumption that all or most of the samples in the training set are normal. However, anomalies in the training set (i.e., anomaly contamination) can be misleading. Recent studies employ data augmentation to generate pseudo-anomalies and learn the boundary separating the training samples from the augmented samples. Although this approach mitigates anomaly contamination if augmented samples mimic unseen real anomalies, it suffers from several limitations. (1) Covering a wide range of time series anomalies is challenging. (2) It disregards augmented samples that resemble normal samples (i.e., false anomalies). (3) It places too much trust in the labels of training and augmented samples. In response, we propose RedLamp, which employs diverse data augmentations to generate multiclass pseudo-anomalies and learns the multiclass boundary. Such multiclass pseudo-anomalies cover a wide variety of time series anomalies. We conduct multiclass classification using soft labels, which prevents the model from being overconfident and ensures its robustness against contaminated/false anomalies. The learned latent space is inherently explainable as it is trained to separate pseudo-anomalies into multiclasses. Extensive experiments demonstrate the effectiveness of RedLamp in anomaly detection and its robustness against anomaly contamination.         ",
    "url": "https://arxiv.org/abs/2505.20765",
    "authors": [
      "Kohei Obata",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20769",
    "title": "Physics-Informed Neural Network for Cross-Domain Predictive Control of Tapered Amplifier Thermal Stabilization",
    "abstract": "           Thermally induced laser noise poses a critical limitation to the sensitivity of quantum sensor arrays employing ultra-stable amplified lasers, primarily stemming from nonlinear gain-temperature coupling effects in tapered amplifiers (TAs). To address this challenge, we present a robust intelligent control strategy that synergistically integrates an encoder-decoder physics-informed gated recurrent unit (PI-GRU) network with a model predictive control (MPC) framework. Our methodology incorporates physical soft constraints into the neural network architecture, yielding a predictive model with enhanced physical consistency that demonstrates robust extrapolation capabilities beyond the training data distribution. Leveraging the PI-GRU model's accurate multi-step predictive performance, we implement a hierarchical parallel MPC architecture capable of real-time thermal instability compensation. This hybrid approach achieves cross-domain consistent thermal stabilization in TAs under diverse laser power operations. Remarkably, while trained exclusively on low-power operational data, our system demonstrates exceptional generalization, improving prediction accuracy by 58.2% and temperature stability by 69.1% in previously unseen high-power operating regimes, as experimentally validated. The novel synchronization of physics-informed neural networks with advanced MPC frameworks presented in this work establishes a groundbreaking paradigm for addressing robustness challenges in cross-domain predictive control applications, overcoming conventional modeling limitations.         ",
    "url": "https://arxiv.org/abs/2505.20769",
    "authors": [
      "Yanpei Shi",
      "Bo Feng",
      "Yuxin Zhong",
      "Haochen Guo",
      "Bangcheng Han",
      "Rui Feng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.20782",
    "title": "Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks",
    "abstract": "           We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for generating adversarial examples that mislead image classifiers toward any target class, including those not seen during training. Traditional targeted attacks are limited to one class per model, requiring expensive retraining for each target. Multi-targeted attacks address this by introducing a perturbation generator with a conditional input to specify the target class. However, existing methods are constrained to classes observed during training and require access to the black-box model's training data--introducing a form of data leakage that undermines realistic evaluation in practical black-box scenarios. We identify overreliance on class embeddings as a key limitation, leading to overfitting and poor generalization to unseen classes. To address this, CD-MTA replaces class-level supervision with an image-based conditional input and introduces class-agnostic losses that align the perturbed and target images in the feature space. This design removes dependence on class semantics, thereby enabling generalization to unseen classes across datasets. Experiments on ImageNet and seven other datasets show that CD-MTA outperforms prior multi-targeted attacks in both standard and cross-domain settings--without accessing the black-box model's training data.         ",
    "url": "https://arxiv.org/abs/2505.20782",
    "authors": [
      "Ta\u00efga Gon\u00e7alves",
      "Tomo Miyazaki",
      "Shinichiro Omachi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20784",
    "title": "Colouring Probe $H$-Free Graphs",
    "abstract": "           The NP-complete problems Colouring and k-Colouring $(k\\geq 3$) are well studied on $H$-free graphs, i.e., graphs that do not contain some fixed graph $H$ as an induced subgraph. We research to what extent the known polynomial-time algorithms for $H$-free graphs can be generalized if we only know some of the edges of the input graph. We do this by considering the classical probe graph model introduced in the early nineties. For a graph $H$, a partitioned probe $H$-free graph $(G,P,N)$ consists of a graph $G=(V,E)$, together with a set $P\\subseteq V$ of probes and an independent set $N=V\\setminus P$ of non-probes, such that $G+F$ is $H$-free for some edge set $F\\subseteq \\binom{N}{2}$. We first fully classify the complexity of Colouring on partitioned probe $H$-free graphs and show that this dichotomy is different from the known dichotomy of Colouring for $H$-free graphs. Our main result is a dichotomy of $3$-Colouring for partitioned probe $P_t$-free graphs: we prove that the problem is polynomial-time solvable if $t\\leq 5$ but NP-complete if $t\\geq 6$. In contrast, $3$-Colouring on $P_t$-free graphs is known to be polynomial-time solvable if $t\\leq 7$ and quasi polynomial-time solvable for $t\\geq 8$.         ",
    "url": "https://arxiv.org/abs/2505.20784",
    "authors": [
      "Dani\u00ebl Paulusma",
      "Johannes Rauch",
      "Erik Jan van Leeuwen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2505.20788",
    "title": "Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset",
    "abstract": "           Wearable human activity recognition has been shown to benefit from the inclusion of acoustic data, as the sounds around a person often contain valuable context. However, due to privacy concerns, it is usually not ethically feasible to record and save microphone data from the device, since the audio could, for instance, also contain private conversations. Rather, the data should be processed locally, which in turn requires processing power and consumes energy on the wearable device. One special use case of contextual information that can be utilized to augment special tasks in human activity recognition is water flow detection, which can, e.g., be used to aid wearable hand washing detection. We created a new label called tap water for the recently released HD-Epic data set, creating 717 hand-labeled annotations of tap water flow, based on existing annotations of the water class. We analyzed the relation of tap water and water in the dataset and additionally trained and evaluated two lightweight classifiers to evaluate the newly added label class, showing that the new class can be learned more easily.         ",
    "url": "https://arxiv.org/abs/2505.20788",
    "authors": [
      "Robin Burchard",
      "Kristof Van Laerhoven"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20796",
    "title": "Describe Me Something You Do Not Remember - Challenges and Risks of Exposure Design Using Generative Artificial Intelligence for Therapy of Complex Post-traumatic Disorder",
    "abstract": "           Post-traumatic stress disorder (PTSD) is associated with sudden, uncontrollable, and intense flashbacks of traumatic memories. Trauma exposure psychotherapy has proven effective in reducing the severity of trauma-related symptoms. It involves controlled recall of traumatic memories to train coping mechanisms for flashbacks and enable autobiographical integration of distressing experiences. In particular, exposure to visualizations of these memories supports successful recall. Although this approach is effective for various trauma types, it remains available for only a few. This is due to the lack of cost-efficient solutions for creating individualized exposure visualizations. This issue is particularly relevant for the treatment of Complex PTSD (CPTSD), where traumatic memories are highly individual and generic visualizations do not meet therapeutic needs. Generative Artificial Intelligence (GAI) offers a flexible and cost-effective alternative. GAI enables the creation of individualized exposure visualizations during therapy and, for the first time, allows patients to actively participate in the visualization process. While GAI opens new therapeutic perspectives and may improve access to trauma therapy, especially for CPTSD, it also introduces significant challenges and risks. The extreme uncertainty and lack of control that define both CPTSD and GAI raise concerns about feasibility and safety. To support safe and effective three-way communication, it is essential to understand the roles of patient, system, and therapist in exposure visualization and how each can contribute to safety. This paper outlines perspectives, challenges, and risks associated with the use of GAI in trauma therapy, with a focus on CPTSD.         ",
    "url": "https://arxiv.org/abs/2505.20796",
    "authors": [
      "Annalisa Degenhard",
      "Stefan Tsch\u00f6ke",
      "Michael Rietzler",
      "Enrico Rukzio"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2505.20807",
    "title": "Simple yet Effective Graph Distillation via Clustering",
    "abstract": "           Despite plentiful successes achieved by graph representation learning in various domains, the training of graph neural networks (GNNs) still remains tenaciously challenging due to the tremendous computational overhead needed for sizable graphs in practice. Recently, graph data distillation (GDD), which seeks to distill large graphs into compact and informative ones, has emerged as a promising technique to enable efficient GNN training. However, most existing GDD works rely on heuristics that align model gradients or representation distributions on condensed and original graphs, leading to compromised result quality, expensive training for distilling large graphs, or both. Motivated by this, this paper presents an efficient and effective GDD approach, ClustGDD. Under the hood, ClustGDD resorts to synthesizing the condensed graph and node attributes through fast and theoretically-grounded clustering that minimizes the within-cluster sum of squares and maximizes the homophily on the original graph. The fundamental idea is inspired by our empirical and theoretical findings unveiling the connection between clustering and empirical condensation quality using Fr\u00e9chet Inception Distance, a well-known quality metric for synthetic images. Furthermore, to mitigate the adverse effects caused by the homophily-based clustering, ClustGDD refines the nodal attributes of the condensed graph with a small augmentation learned via class-aware graph sampling and consistency loss. Our extensive experiments exhibit that GNNs trained over condensed graphs output by ClustGDD consistently achieve superior or comparable performance to state-of-the-art GDD methods in terms of node classification on five benchmark datasets, while being orders of magnitude faster.         ",
    "url": "https://arxiv.org/abs/2505.20807",
    "authors": [
      "Yurui Lai",
      "Taiyan Zhang",
      "Renchi Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20808",
    "title": "Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis",
    "abstract": "           Diffusion models have shown strong capabilities in high-fidelity image generation but often falter when synthesizing rare concepts, i.e., prompts that are infrequently observed in the training distribution. In this paper, we introduce RAP, a principled framework that treats rare concept generation as navigating a latent causal path: a progressive, model-aligned trajectory through the generative space from frequent concepts to rare targets. Rather than relying on heuristic prompt alternation, we theoretically justify that rare prompt guidance can be approximated by semantically related frequent prompts. We then formulate prompt switching as a dynamic process based on score similarity, enabling adaptive stage transitions. Furthermore, we reinterpret prompt alternation as a second-order denoising mechanism, promoting smooth semantic progression and coherent visual synthesis. Through this causal lens, we align input scheduling with the model's internal generative dynamics. Experiments across diverse diffusion backbones demonstrate that RAP consistently enhances rare concept generation, outperforming strong baselines in both automated evaluations and human studies.         ",
    "url": "https://arxiv.org/abs/2505.20808",
    "authors": [
      "Bo-Kai Ruan",
      "Zi-Xiang Ni",
      "Bo-Lun Huang",
      "Teng-Fang Hsiao",
      "Hong-Han Shuai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20809",
    "title": "Improved Representation Steering for Language Models",
    "abstract": "           Steering methods for language models (LMs) seek to provide fine-grained and interpretable control over model generations by variously changing model inputs, weights, or representations to adjust behavior. Recent work has shown that adjusting weights or representations is often less effective than steering by prompting, for instance when wanting to introduce or suppress a particular concept. We demonstrate how to improve representation steering via our new Reference-free Preference Steering (RePS), a bidirectional preference-optimization objective that jointly does concept steering and suppression. We train three parameterizations of RePS and evaluate them on AxBench, a large-scale model steering benchmark. On Gemma models with sizes ranging from 2B to 27B, RePS outperforms all existing steering methods trained with a language modeling objective and substantially narrows the gap with prompting -- while promoting interpretability and minimizing parameter count. In suppression, RePS matches the language-modeling objective on Gemma-2 and outperforms it on the larger Gemma-3 variants while remaining resilient to prompt-based jailbreaking attacks that defeat prompting. Overall, our results suggest that RePS provides an interpretable and robust alternative to prompting for both steering and suppression.         ",
    "url": "https://arxiv.org/abs/2505.20809",
    "authors": [
      "Zhengxuan Wu",
      "Qinan Yu",
      "Aryaman Arora",
      "Christopher D. Manning",
      "Christopher Potts"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20813",
    "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph",
    "abstract": "           In knowledge graph embedding, leveraging relation-specific entity-transformation has markedly enhanced performance. However, the consistency of embedding differences before and after transformation remains unaddressed, risking the loss of valuable inductive bias inherent in the embeddings. This inconsistency stems from two problems. First, transformation representations are specified for relations in a disconnected manner, allowing dissimilar transformations and corresponding entity-embeddings for similar relations. Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on Relations) disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. In this paper, we introduce a plug-in KGE method, Relation-Semantics Consistent Filter (RSCF), containing more consistent entity-transformation characterized by three features: 1) shared affine transformation of relation embeddings across all relations, 2) rooted entity-transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. To amplify the advantages of consistency that preserve semantics on embeddings, RSCF adds relation transformation and prediction modules for enhancing the semantics. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF significantly outperforms state-of-the-art KGE methods, showing robustness across all relations and their frequencies.         ",
    "url": "https://arxiv.org/abs/2505.20813",
    "authors": [
      "Junsik Kim",
      "Jinwook Park",
      "Kangil Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20815",
    "title": "Interpretable Credit Default Prediction with Ensemble Learning and SHAP",
    "abstract": "           This study focuses on the problem of credit default prediction, builds a modeling framework based on machine learning, and conducts comparative experiments on a variety of mainstream classification algorithms. Through preprocessing, feature engineering, and model training of the Home Credit dataset, the performance of multiple models including logistic regression, random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and recall is evaluated. The results show that the ensemble learning method has obvious advantages in predictive performance, especially in dealing with complex nonlinear relationships between features and data imbalance problems. It shows strong robustness. At the same time, the SHAP method is used to analyze the importance and dependency of features, and it is found that the external credit score variable plays a dominant role in model decision making, which helps to improve the model's interpretability and practical application value. The research results provide effective reference and technical support for the intelligent development of credit risk control systems.         ",
    "url": "https://arxiv.org/abs/2505.20815",
    "authors": [
      "Shiqi Yang",
      "Ziyi Huang",
      "Wengran Xiao",
      "Xinyu Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20818",
    "title": "Domain Decomposition Subspace Neural Network Method for Solving Linear and Nonlinear Partial Differential Equations",
    "abstract": "           This paper proposes a domain decomposition subspace neural network method for efficiently solving linear and nonlinear partial differential equations. By combining the principles of domain decomposition and subspace neural networks, the method constructs basis functions using neural networks to approximate PDE solutions. It imposes $C^k$ continuity conditions at the interface of subdomains, ensuring smoothness across the global solution. Nonlinear PDEs are solved using Picard and Newton iterations, analogous to classical methods. Numerical experiments demonstrate that our method achieves exceptionally high accuracy, with errors reaching up to $10^{-13}$, while significantly reducing computational costs compared to existing approaches, including PINNs, DGM, DRM. The results highlight the method's superior accuracy and training efficiency.         ",
    "url": "https://arxiv.org/abs/2505.20818",
    "authors": [
      "Zhenxing Fu",
      "Hongliang Liu",
      "Zhiqiang Sheng",
      "Baixue Xing"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.20827",
    "title": "Frame-Level Captions for Long Video Generation with Complex Multi Scenes",
    "abstract": "           Generating long videos that can show complex stories, like movie scenes from scripts, has great promise and offers much more than short clips. However, current methods that use autoregression with diffusion models often struggle because their step-by-step process naturally leads to a serious error accumulation (drift). Also, many existing ways to make long videos focus on single, continuous scenes, making them less useful for stories with many events and changes. This paper introduces a new approach to solve these problems. First, we propose a novel way to annotate datasets at the frame-level, providing detailed text guidance needed for making complex, multi-scene long videos. This detailed guidance works with a Frame-Level Attention Mechanism to make sure text and video match precisely. A key feature is that each part (frame) within these windows can be guided by its own distinct text prompt. Our training uses Diffusion Forcing to provide the model with the ability to handle time flexibly. We tested our approach on difficult VBench 2.0 benchmarks (\"Complex Plots\" and \"Complex Landscapes\") based on the WanX2.1-T2V-1.3B model. The results show our method is better at following instructions in complex, changing scenes and creates high-quality long videos. We plan to share our dataset annotation methods and trained models with the research community. Project page: this https URL .         ",
    "url": "https://arxiv.org/abs/2505.20827",
    "authors": [
      "Guangcong Zheng",
      "Jianlong Yuan",
      "Bo Wang",
      "Haoyang Huang",
      "Guoqing Ma",
      "Nan Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20834",
    "title": "Fully Spiking Neural Networks for Unified Frame-Event Object Tracking",
    "abstract": "           The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. The code will be released.         ",
    "url": "https://arxiv.org/abs/2505.20834",
    "authors": [
      "Jingjun Yang",
      "Liangwei Fan",
      "Jinpu Zhang",
      "Xiangkai Lian",
      "Hui Shen",
      "Dewen Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2505.20835",
    "title": "ECC-SNN: Cost-Effective Edge-Cloud Collaboration for Spiking Neural Networks",
    "abstract": "           Most edge-cloud collaboration frameworks rely on the substantial computational and storage capabilities of cloud-based artificial neural networks (ANNs). However, this reliance results in significant communication overhead between edge devices and the cloud and high computational energy consumption, especially when applied to resource-constrained edge devices. To address these challenges, we propose ECC-SNN, a novel edge-cloud collaboration framework incorporating energy-efficient spiking neural networks (SNNs) to offload more computational workload from the cloud to the edge, thereby improving cost-effectiveness and reducing reliance on the cloud. ECC-SNN employs a joint training approach that integrates ANN and SNN models, enabling edge devices to leverage knowledge from cloud models for enhanced performance while reducing energy consumption and processing latency. Furthermore, ECC-SNN features an on-device incremental learning algorithm that enables edge models to continuously adapt to dynamic environments, reducing the communication overhead and resource consumption associated with frequent cloud update requests. Extensive experimental results on four datasets demonstrate that ECC-SNN improves accuracy by 4.15%, reduces average energy consumption by 79.4%, and lowers average processing latency by 39.1%.         ",
    "url": "https://arxiv.org/abs/2505.20835",
    "authors": [
      "Di Yu",
      "Changze Lv",
      "Xin Du",
      "Linshan Jiang",
      "Wentao Tong",
      "Zhenyu Liao",
      "Xiaoqing Zheng",
      "Shuiguang Deng"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.20853",
    "title": "Cooperation of Experts: Fusing Heterogeneous Information with Large Margin",
    "abstract": "           Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20853",
    "authors": [
      "Shuo Wang",
      "Shunyang Huang",
      "Jinghui Yuan",
      "Zhixiang Shen",
      "Zhao Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20866",
    "title": "Respond to Change with Constancy: Instruction-tuning with LLM for Non-I.I.D. Network Traffic Classification",
    "abstract": "           Encrypted traffic classification is highly challenging in network security due to the need for extracting robust features from content-agnostic traffic data. Existing approaches face critical issues: (i) Distribution drift, caused by reliance on the closedworld assumption, limits adaptability to realworld, shifting patterns; (ii) Dependence on labeled data restricts applicability where such data is scarce or unavailable. Large language models (LLMs) have demonstrated remarkable potential in offering generalizable solutions across a wide range of tasks, achieving notable success in various specialized fields. However, their effectiveness in traffic analysis remains constrained by challenges in adapting to the unique requirements of the traffic domain. In this paper, we introduce a novel traffic representation model named Encrypted Traffic Out-of-Distribution Instruction Tuning with LLM (ETooL), which integrates LLMs with knowledge of traffic structures through a self-supervised instruction tuning paradigm. This framework establishes connections between textual information and traffic interactions. ETooL demonstrates more robust classification performance and superior generalization in both supervised and zero-shot traffic classification tasks. Notably, it achieves significant improvements in F1 scores: APP53 (I.I.D.) to 93.19%(6.62%) and 92.11%(4.19%), APP53 (O.O.D.) to 74.88%(18.17%) and 72.13%(15.15%), and ISCX-Botnet (O.O.D.) to 95.03%(9.16%) and 81.95%(12.08%). Additionally, we construct NETD, a traffic dataset designed to support dynamic distributional shifts, and use it to validate ETooL's effectiveness under varying distributional conditions. Furthermore, we evaluate the efficiency gains achieved through ETooL's instruction tuning approach.         ",
    "url": "https://arxiv.org/abs/2505.20866",
    "authors": [
      "Xinjie Lin",
      "Gang Xiong",
      "Gaopeng Gou",
      "Wenqi Dong",
      "Jing Yu",
      "Zhen Li",
      "Wei Xia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2505.20875",
    "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties",
    "abstract": "           Large Language Models (LLMs) are predominantly evaluated on Standard American English (SAE), often overlooking the diversity of global English varieties. This narrow focus may raise fairness concerns as degraded performance on non-standard varieties can lead to unequal benefits for users worldwide. Therefore, it is critical to extensively evaluate the linguistic robustness of LLMs on multiple non-standard English varieties. We introduce Trans-EnV, a framework that automatically transforms SAE datasets into multiple English varieties to evaluate the linguistic robustness. Our framework combines (1) linguistics expert knowledge to curate variety-specific features and transformation guidelines from linguistic literature and corpora, and (2) LLM-based transformations to ensure both linguistic validity and scalability. Using Trans-EnV, we transform six benchmark datasets into 38 English varieties and evaluate seven state-of-the-art LLMs. Our results reveal significant performance disparities, with accuracy decreasing by up to 46.3% on non-standard varieties. These findings highlight the importance of comprehensive linguistic robustness evaluation across diverse English varieties. Each construction of Trans-EnV was validated through rigorous statistical testing and consultation with a researcher in the field of second language acquisition, ensuring its linguistic validity. Our \\href{this https URL}{code} and \\href{this https URL}{datasets} are publicly available.         ",
    "url": "https://arxiv.org/abs/2505.20875",
    "authors": [
      "Jiyoung Lee",
      "Seungho Kim",
      "Jieun Han",
      "Jun-Min Lee",
      "Kitaek Kim",
      "Alice Oh",
      "Edward Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20880",
    "title": "MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection",
    "abstract": "           This paper describes our submission for SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The task involves detecting hallucinated spans in text generated by instruction-tuned Large Language Models (LLMs) across multiple languages. Our approach combines task-specific prompt engineering with an LLM ensemble verification mechanism, where a primary model extracts hallucination spans and three independent LLMs adjudicate their validity through probability-based voting. This framework simulates the human annotation workflow used in the shared task validation and test data. Additionally, fuzzy matching refines span alignment. Our system ranked 1st in Arabic and Basque, 2nd in German, Swedish, and Finnish, and 3rd in Czech, Farsi, and French.         ",
    "url": "https://arxiv.org/abs/2505.20880",
    "authors": [
      "Baraa Hikal",
      "Ahmed Nasreldin",
      "Ali Hamdi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20882",
    "title": "Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning",
    "abstract": "           Decentralized machine learning - where each client keeps its own data locally and uses its own computational resources to collaboratively train a model by exchanging peer-to-peer messages - is increasingly popular, as it enables better scalability and control over the data. A major challenge in this setting is that learning dynamics depend on the topology of the communication graph, which motivates the use of real graph datasets for benchmarking decentralized algorithms. Unfortunately, existing graph datasets are largely limited to for-profit social networks crawled at a fixed point in time and often collected at the user scale, where links are heavily influenced by the platform and its recommendation algorithms. The Fediverse, which includes several free and open-source decentralized social media platforms such as Mastodon, Misskey, and Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a new dataset of 182 graphs, covering seven social networks from the Fediverse, crawled weekly over 14 weeks. We release the dataset along with a Python package to facilitate its use, and illustrate its utility on several tasks, including a new defederation task, which captures a process of link deletion observed on these networks.         ",
    "url": "https://arxiv.org/abs/2505.20882",
    "authors": [
      "Marc Damie",
      "Edwige Cyffers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.20884",
    "title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation",
    "abstract": "           Fire detection in dynamic environments faces continuous challenges, including the interference of illumination changes, many false detections or missed detections, and it is difficult to achieve both efficiency and accuracy. To address the problem of feature extraction limitation and information loss in the existing YOLO-based models, this study propose You Only Look Once for Fire Detection with Attention-guided Inverted Residual and Dual-pooling Downscale Fusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted Residual Block (AIR) integrates hybrid channel-spatial attention with inverted residuals to adaptively enhance fire features and suppress environmental noise; (2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns through learnable fusion of max-average pooling outputs, mitigating small-fire detection failures. Extensive evaluation on two public datasets shows the efficient performance of our model. Our proposed model keeps the sum amount of parameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n), and mAP75 is higher than the mainstream real-time object detection models YOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants 1.3-5.5%.         ",
    "url": "https://arxiv.org/abs/2505.20884",
    "authors": [
      "Weichao Pan",
      "Bohan Xu",
      "Xu Wang",
      "Chengze Lv",
      "Shuoyang Wang",
      "Zhenke Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20887",
    "title": "Dynamical ON-OFF Control with Trajectory Prediction for Multi-RIS Wireless Networks",
    "abstract": "           Reconfigurable intelligent surfaces (RISs) have demonstrated an unparalleled ability to reconfigure wireless environments by dynamically controlling the phase, amplitude, and polarization of impinging waves. However, as nearly passive reflective metasurfaces, RISs may not distinguish between desired and interference signals, which can lead to severe spectrum pollution and even affect performance negatively. In particular, in large-scale networks, the signal-to-interference-plus-noise ratio (SINR) at the receiving node can be degraded due to excessive interference reflected from the RIS. To overcome this fundamental limitation, we propose in this paper a trajectory prediction-based dynamical control algorithm (TPC) for anticipating RIS ON-OFF states sequence, integrating a long-short-term-memory (LSTM) scheme to predict user trajectories. In particular, through a codebook-based algorithm, the RIS controller adaptively coordinates the configuration of the RIS elements to maximize the received SINR. Our simulation results demonstrate the superiority of the proposed TPC method over various system settings.         ",
    "url": "https://arxiv.org/abs/2505.20887",
    "authors": [
      "Kaining Wang",
      "Bo Yang",
      "Yusheng Lei",
      "Zhiwen Yu",
      "Xuelin Cao",
      "George C. Alexandropoulos",
      "Marco Di Renzo",
      "Chau Yuen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.20890",
    "title": "Frequency Composition for Compressed and Domain-Adaptive Neural Networks",
    "abstract": "           Modern on-device neural network applications must operate under resource constraints while adapting to unpredictable domain shifts. However, this combined challenge-model compression and domain adaptation-remains largely unaddressed, as prior work has tackled each issue in isolation: compressed networks prioritize efficiency within a fixed domain, whereas large, capable models focus on handling domain shifts. In this work, we propose CoDA, a frequency composition-based framework that unifies compression and domain adaptation. During training, CoDA employs quantization-aware training (QAT) with low-frequency components, enabling a compressed model to selectively learn robust, generalizable features. At test time, it refines the compact model in a source-free manner (i.e., test-time adaptation, TTA), leveraging the full-frequency information from incoming data to adapt to target domains while treating high-frequency components as domain-specific cues. LFC are aligned with the trained distribution, while HFC unique to the target distribution are solely utilized for batch normalization. CoDA can be integrated synergistically into existing QAT and TTA methods. CoDA is evaluated on widely used domain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various model architectures. With significant compression, it achieves accuracy improvements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the full-precision TTA baseline.         ",
    "url": "https://arxiv.org/abs/2505.20890",
    "authors": [
      "Yoojin Kwon",
      "Hongjun Suh",
      "Wooseok Lee",
      "Taesik Gong",
      "Songyi Han",
      "Hyung-Sin Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20901",
    "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models",
    "abstract": "           As large vision language models(LVLMs) rapidly advance, concerns about their potential to learn and generate social biases and stereotypes are increasing. Previous studies on LVLM's stereotypes face two primary limitations: metrics that overlooked the importance of content words, and datasets that overlooked the effect of color. To address these limitations, this study introduces new evaluation metrics based on the Stereotype Content Model (SCM). We also propose BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes. As a result, we found three findings. (1) The SCM-based evaluation is effective in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output along with gender and race ones. (3) Interaction between model architecture and parameter sizes seems to affect stereotypes. We release BASIC publicly on [anonymized for review].         ",
    "url": "https://arxiv.org/abs/2505.20901",
    "authors": [
      "Junhyuk Choi",
      "Minju Kim",
      "Yeseon Hong",
      "Bugeun Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20904",
    "title": "HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion",
    "abstract": "           Transparent and reflective objects pose significant challenges for depth sensors, resulting in incomplete depth information that adversely affects downstream robotic perception and manipulation tasks. To address this issue, we propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba architectures. The encoder is constructed based on a dual-branch Transformer-CNN framework, while the multi-scale fusion module leverages a Transformer-Mamba architecture, which also serves as the foundation for the decoder design. We introduce a novel multimodal fusion module grounded in self-attention mechanisms and state space models, marking the first application of the Mamba architecture in the field of transparent object depth completion and revealing its promising potential. Additionally, we design an innovative multi-scale fusion module for the decoder that combines channel attention, spatial attention, and multi-scale feature extraction techniques to effectively integrate multi-scale features through a down-fusion strategy. Extensive evaluations on multiple public datasets demonstrate that our model achieves state-of-the-art(SOTA) performance, validating the effectiveness of our approach.         ",
    "url": "https://arxiv.org/abs/2505.20904",
    "authors": [
      "Guanghu Xie",
      "Yonglong Zhang",
      "Zhiduo Jiang",
      "Yang Liu",
      "Zongwu Xie",
      "Baoshi Cao",
      "Hong Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20910",
    "title": "Automated Privacy Information Annotation in Large Language Model Interactions",
    "abstract": "           Users interacting with large language models (LLMs) under their real identifiers often unknowingly risk disclosing private information. Automatically notifying users whether their queries leak privacy and which phrases leak what private information has therefore become a practical need. Existing privacy detection methods, however, were designed for different objectives and application scenarios, typically tagging personally identifiable information (PII) in anonymous content. In this work, to support the development and evaluation of privacy detection models for LLM interactions that are deployable on local user devices, we construct a large-scale multilingual dataset with 249K user queries and 154K annotated privacy phrases. In particular, we build an automated privacy annotation pipeline with cloud-based strong LLMs to automatically extract privacy phrases from dialogue datasets and annotate leaked information. We also design evaluation metrics at the levels of privacy leakage, extracted privacy phrase, and privacy information. We further establish baseline methods using light-weight LLMs with both tuning-free and tuning-based methods, and report a comprehensive evaluation of their performance. Evaluation results reveal a gap between current performance and the requirements of real-world LLM applications, motivating future research into more effective local privacy detection methods grounded in our dataset.         ",
    "url": "https://arxiv.org/abs/2505.20910",
    "authors": [
      "Hang Zeng",
      "Xiangyu Liu",
      "Yong Hu",
      "Chaoyue Niu",
      "Fan Wu",
      "Shaojie Tang",
      "Guihai Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.20916",
    "title": "Imago Obscura: An Image Privacy AI Co-pilot to Enable Identification and Mitigation of Risks",
    "abstract": "           Users often struggle to navigate the privacy / publicity boundary in sharing images online: they may lack awareness of image privacy risks and/or the ability to apply effective mitigation strategies. To address this challenge, we introduce and evaluate Imago Obscura, an AI-powered, image-editing copilot that enables users to identify and mitigate privacy risks with images they intend to share. Driven by design requirements from a formative user study with 7 image-editing experts, Imago Obscura enables users to articulate their image-sharing intent and privacy concerns. The system uses these inputs to surface contextually pertinent privacy risks, and then recommends and facilitates application of a suite of obfuscation techniques found to be effective in prior literature -- e.g., inpainting, blurring, and generative content replacement. We evaluated Imago Obscura with 15 end-users in a lab study and found that it greatly improved users' awareness of image privacy risks and their ability to address those risks, allowing them to make more informed sharing decisions.         ",
    "url": "https://arxiv.org/abs/2505.20916",
    "authors": [
      "Kyzyl Monteiro",
      "Yuchen Wu",
      "Sauvik Das"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2505.20934",
    "title": "NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion",
    "abstract": "           Adversarial samples exploit irregularities in the manifold ``learned'' by deep learning models to cause misclassifications. The study of these adversarial samples provides insight into the features a model uses to classify inputs, which can be leveraged to improve robustness against future attacks. However, much of the existing literature focuses on constrained adversarial samples, which do not accurately reflect test-time errors encountered in real-world settings. To address this, we propose `NatADiff', an adversarial sampling scheme that leverages denoising diffusion to generate natural adversarial samples. Our approach is based on the observation that natural adversarial samples frequently contain structural elements from the adversarial class. Deep learning models can exploit these structural elements to shortcut the classification process, rather than learning to genuinely distinguish between classes. To leverage this behavior, we guide the diffusion trajectory towards the intersection of the true and adversarial classes, combining time-travel sampling with augmented classifier guidance to enhance attack transferability while preserving image fidelity. Our method achieves comparable attack success rates to current state-of-the-art techniques, while exhibiting significantly higher transferability across model architectures and better alignment with natural test-time errors as measured by FID. These results demonstrate that NatADiff produces adversarial samples that not only transfer more effectively across models, but more faithfully resemble naturally occurring test-time errors.         ",
    "url": "https://arxiv.org/abs/2505.20934",
    "authors": [
      "Max Collins",
      "Jordan Vice",
      "Tim French",
      "Ajmal Mian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20948",
    "title": "Controllable Logical Hypothesis Generation for Abductive Reasoning in Knowledge Graphs",
    "abstract": "           Abductive reasoning in knowledge graphs aims to generate plausible logical hypotheses from observed entities, with broad applications in areas such as clinical diagnosis and scientific discovery. However, due to a lack of controllability, a single observation may yield numerous plausible but redundant or irrelevant hypotheses on large-scale knowledge graphs. To address this limitation, we introduce the task of controllable hypothesis generation to improve the practical utility of abductive reasoning. This task faces two key challenges when controlling for generating long and complex logical hypotheses: hypothesis space collapse and hypothesis oversensitivity. To address these challenges, we propose CtrlHGen, a Controllable logcial Hypothesis Generation framework for abductive reasoning over knowledge graphs, trained in a two-stage paradigm including supervised learning and subsequent reinforcement learning. To mitigate hypothesis space collapse, we design a dataset augmentation strategy based on sub-logical decomposition, enabling the model to learn complex logical structures by leveraging semantic patterns in simpler components. To address hypothesis oversensitivity, we incorporate smoothed semantic rewards including Dice and Overlap scores, and introduce a condition-adherence reward to guide the generation toward user-specified control constraints. Extensive experiments on three benchmark datasets demonstrate that our model not only better adheres to control conditions but also achieves superior semantic similarity performance compared to baselines.         ",
    "url": "https://arxiv.org/abs/2505.20948",
    "authors": [
      "Yisen Gao",
      "Jiaxin Bai",
      "Tianshi Zheng",
      "Qingyun Sun",
      "Ziwei Zhang",
      "Jianxin Li",
      "Yangqiu Song",
      "Xingcheng Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20949",
    "title": "Streamlining Knowledge Graph Creation with PyRML",
    "abstract": "           Knowledge Graphs (KGs) are increasingly adopted as a foundational technology for integrating heterogeneous data in domains such as climate science, cultural heritage, and the life sciences. Declarative mapping languages like R2RML and RML have played a central role in enabling scalable and reusable KG construction, offering a transparent means of transforming structured and semi-structured data into RDF. In this paper, we present PyRML, a lightweight, Python-native library for building Knowledge Graphs through declarative mappings. PyRML supports core RML constructs and provides a programmable interface for authoring, executing, and testing mappings directly within Python environments. It integrates with popular data and semantic web libraries (e.g., Pandas and RDFlib), enabling transparent and modular workflows. By lowering the barrier to entry for KG creation and fostering reproducible, ontology-aligned data integration, PyRML bridges the gap between declarative semantics and practical KG engineering.         ",
    "url": "https://arxiv.org/abs/2505.20949",
    "authors": [
      "Andrea Giovanni Nuzzolese"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20950",
    "title": "Scattering Networks on Noncommutative Finite Groups",
    "abstract": "           Scattering Networks were initially designed to elucidate the behavior of early layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and are grounded in wavelets. In this work, we introduce a scattering transform on an arbitrary finite group (not necessarily abelian) within the context of group-equivariant convolutional neural networks (G-CNNs). We present wavelets on finite groups and analyze their similarity to classical wavelets. We demonstrate that, under certain conditions in the wavelet coefficients, the scattering transform is non-expansive, stable under deformations, preserves energy, equivariant with respect to left and right group translations, and, as depth increases, the scattering coefficients are less sensitive to group translations of the signal, all desirable properties of convolutional neural networks. Furthermore, we provide examples illustrating the application of the scattering transform to classify data with domains involving abelian and nonabelian groups.         ",
    "url": "https://arxiv.org/abs/2505.20950",
    "authors": [
      "Maria Teresa Arias",
      "Davide Barbieri",
      "Eugenio Hern\u00e1ndez"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.20951",
    "title": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction",
    "abstract": "           Camera-based 3D semantic occupancy prediction offers an efficient and cost-effective solution for perceiving surrounding scenes in autonomous driving. However, existing works rely on explicit occupancy state inference, leading to numerous incorrect feature assignments, and insufficient samples restrict the learning of occupancy class inference. To address these challenges, we propose leveraging Depth awareness and Semantic aid to boost camera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform occupancy state and occupancy class inference, where soft occupancy confidence is calculated through non-learning method and multiplied with image features to make the voxel representation aware of depth, enabling adaptive implicit occupancy state inference. Rather than focusing on improving feature learning, we directly utilize well-trained image semantic segmentation and fuse multiple frames with their occupancy probabilities to aid occupancy class inference, thereby enhancing robustness. Experimental results demonstrate that DSOcc achieves state-of-the-art performance on the SemanticKITTI dataset among camera-based methods.         ",
    "url": "https://arxiv.org/abs/2505.20951",
    "authors": [
      "Naiyu Fang",
      "Zheyuan Zhou",
      "Kang Wang",
      "Ruibo Li",
      "Lemiao Qiu",
      "Shuyou Zhang",
      "Zhe Wang",
      "Guosheng Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.20955",
    "title": "Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models",
    "abstract": "           Diffusion models have achieved tremendous success in image generation, but they also raise significant concerns regarding privacy and copyright issues. Membership Inference Attacks (MIAs) are designed to ascertain whether specific data were utilized during a model's training phase. As current MIAs for diffusion models typically exploit the model's image prediction ability, we formalize them into a unified general paradigm which computes the membership score for membership identification. Under this paradigm, we empirically find that existing attacks overlook the inherent deficiency in how diffusion models process high-frequency information. Consequently, this deficiency leads to member data with more high-frequency content being misclassified as hold-out data, and hold-out data with less high-frequency content tend to be misclassified as member data. Moreover, we theoretically demonstrate that this deficiency reduces the membership advantage of attacks, thereby interfering with the effective discrimination of member data and hold-out data. Based on this insight, we propose a plug-and-play high-frequency filter module to mitigate the adverse effects of the deficiency, which can be seamlessly integrated into any attacks within this general paradigm without additional time costs. Extensive experiments corroborate that this module significantly improves the performance of baseline attacks across different datasets and models.         ",
    "url": "https://arxiv.org/abs/2505.20955",
    "authors": [
      "Puwei Lian",
      "Yujun Cai",
      "Songze Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20956",
    "title": "Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection",
    "abstract": "           Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.         ",
    "url": "https://arxiv.org/abs/2505.20956",
    "authors": [
      "Shiqi Zhang",
      "Tuomas Virtanen"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.20959",
    "title": "Research Community Perspectives on \"Intelligence\" and Large Language Models",
    "abstract": "           Despite the widespread use of ''artificial intelligence'' (AI) framing in Natural Language Processing (NLP) research, it is not clear what researchers mean by ''intelligence''. To that end, we present the results of a survey on the notion of ''intelligence'' among researchers and its role in the research agenda. The survey elicited complete responses from 303 researchers from a variety of fields including NLP, Machine Learning (ML), Cognitive Science, Linguistics, and Neuroscience. We identify 3 criteria of intelligence that the community agrees on the most: generalization, adaptability, & reasoning. Our results suggests that the perception of the current NLP systems as ''intelligent'' is a minority position (29%). Furthermore, only 16.2% of the respondents see developing intelligent systems as a research goal, and these respondents are more likely to consider the current systems intelligent.         ",
    "url": "https://arxiv.org/abs/2505.20959",
    "authors": [
      "Bertram H\u00f8jer",
      "Terne Sasha Thorn Jakobsen",
      "Anna Rogers",
      "Stefan Heinrich"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2505.20970",
    "title": "Understanding the behavior of representation forgetting in continual learning",
    "abstract": "           In continual learning scenarios, catastrophic forgetting of previously learned tasks is a critical issue, making it essential to effectively measure such forgetting. Recently, there has been growing interest in focusing on representation forgetting, the forgetting measured at the hidden layer. In this paper, we provide the first theoretical analysis of representation forgetting and use this analysis to better understand the behavior of continual learning. First, we introduce a new metric called representation discrepancy, which measures the difference between representation spaces constructed by two snapshots of a model trained through continual learning. We demonstrate that our proposed metric serves as an effective surrogate for the representation forgetting while remaining analytically tractable. Second, through mathematical analysis of our metric, we derive several key findings about the dynamics of representation forgetting: the forgetting occurs more rapidly to a higher degree as the layer index increases, while increasing the width of the network slows down the forgetting process. Third, we support our theoretical findings through experiments on real image datasets, including Split-CIFAR100 and ImageNet1K.         ",
    "url": "https://arxiv.org/abs/2505.20970",
    "authors": [
      "Joonkyu Kim",
      "Yejin Kim",
      "Jy-yong Sohn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20971",
    "title": "Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA",
    "abstract": "           LLMs have demonstrated remarkable capabilities in complex reasoning tasks, yet they often suffer from hallucinations and lack reliable factual grounding. Meanwhile, knowledge graphs (KGs) provide structured factual knowledge but lack the flexible reasoning abilities of LLMs. In this paper, we present Reason-Align-Respond (RAR), a novel framework that systematically integrates LLM reasoning with knowledge graphs for KGQA. Our approach consists of three key components: a Reasoner that generates human-like reasoning chains, an Aligner that maps these chains to valid KG paths, and a Responser that synthesizes the final answer. We formulate this process as a probabilistic model and optimize it using the Expectation-Maximization algorithm, which iteratively refines the reasoning chains and knowledge paths. Extensive experiments on multiple benchmarks demonstrate the effectiveness of RAR, achieving state-of-the-art performance with Hit@1 scores of 93.3% and 91.0% on WebQSP and CWQ respectively. Human evaluation confirms that RAR generates high-quality, interpretable reasoning chains well-aligned with KG paths. Furthermore, RAR exhibits strong zero-shot generalization capabilities and maintains computational efficiency during inference.         ",
    "url": "https://arxiv.org/abs/2505.20971",
    "authors": [
      "Xiangqing Shen",
      "Fanfan Wang",
      "Rui Xia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20972",
    "title": "Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs",
    "abstract": "           Along with AI computing shining in scientific discovery, its potential in the combinatorial optimization (CO) domain has also emerged in recent years. Yet, existing unsupervised neural network solvers struggle to solve $k$-grouping problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs, due to limited computational frameworks. In this work, we propose Deep $k$-grouping, an unsupervised learning-based CO framework. Specifically, we contribute: Novel one-hot encoded polynomial unconstrained binary optimization (OH-PUBO), a formulation for modeling k-grouping problems on graphs and hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs the relaxation of large-scale OH-PUBO objectives as differentiable loss functions and trains to optimize them in an unsupervised manner. To ensure scalability, it leverages GPU-accelerated algorithms to unify the training pipeline; A Gini coefficient-based continuous relaxation annealing strategy to enforce discreteness of solutions while preventing convergence to local optima. Experimental results demonstrate that Deep $k$-grouping outperforms existing neural network solvers and classical heuristics such as SCIP and Tabu.         ",
    "url": "https://arxiv.org/abs/2505.20972",
    "authors": [
      "Sen Bai",
      "Chunqi Yang",
      "Xin Bai",
      "Xin Zhang",
      "Zhengang Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20979",
    "title": "MelodySim: Measuring Melody-aware Music Similarity for Plagiarism Detection",
    "abstract": "           We propose MelodySim, a melody-aware music similarity model and dataset for plagiarism detection. First, we introduce a novel method to construct a dataset with focus on melodic similarity. By augmenting Slakh2100; an existing MIDI dataset, we generate variations of each piece while preserving the melody through modifications such as note splitting, arpeggiation, minor track dropout (excluding bass), and re-instrumentation. A user study confirms that positive pairs indeed contain similar melodies, with other musical tracks significantly changed. Second, we develop a segment-wise melodic-similarity detection model that uses a MERT encoder and applies a triplet neural network to capture melodic similarity. The resultant decision matrix highlights where plagiarism might occur. Our model achieves high accuracy on the MelodySim test set.         ",
    "url": "https://arxiv.org/abs/2505.20979",
    "authors": [
      "Tongyu Lu",
      "Charlotta-Marlena Geist",
      "Jan Melechovsky",
      "Abhinaba Roy",
      "Dorien Herremans"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.20980",
    "title": "Identifying Super Spreaders in Multilayer Networks",
    "abstract": "           Identifying super-spreaders can be framed as a subtask of the influence maximisation problem. It seeks to pinpoint agents within a network that, if selected as single diffusion seeds, disseminate information most effectively. Multilayer networks, a specific class of heterogeneous graphs, can capture diverse types of interactions (e.g., physical-virtual or professional-social), and thus offer a more accurate representation of complex relational structures. In this work, we introduce a novel approach to identifying super-spreaders in such networks by leveraging graph neural networks. To this end, we construct a dataset by simulating information diffusion across hundreds of networks - to the best of our knowledge, the first of its kind tailored specifically to multilayer networks. We further formulate the task as a variation of the ranking prediction problem based on a four-dimensional vector that quantifies each agent's spreading potential: (i) the number of activations; (ii) the duration of the diffusion process; (iii) the peak number of activations; and (iv) the simulation step at which this peak occurs. Our model, TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom aggregation layer. This design enables generalisation to previously unseen data and adapts to varying graph sizes. In an extensive evaluation, we compare our model against classic centrality-based heuristics and competitive deep learning methods. The results, obtained across a broad spectrum of real-world and synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves superior performance in identifying high-impact nodes, while also offering improved interpretability through its structured output.         ",
    "url": "https://arxiv.org/abs/2505.20980",
    "authors": [
      "Micha\u0142 Czuba",
      "Mateusz Stolarski",
      "Adam Pir\u00f3g",
      "Piotr Bielak",
      "Piotr Br\u00f3dka"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20981",
    "title": "RefAV: Towards Planning-Centric Scenario Mining",
    "abstract": "           Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal data localized to HD maps during normal fleet testing. However, identifying interesting and safety-critical scenarios from uncurated driving logs remains a significant challenge. Traditional scenario mining techniques are error-prone and prohibitively time-consuming, often relying on hand-crafted structured queries. In this work, we revisit spatio-temporal scenario mining through the lens of recent vision-language models (VLMs) to detect whether a described scenario occurs in a driving log and, if so, precisely localize it in both time and space. To address this problem, we introduce RefAV, a large-scale dataset of 10,000 diverse natural language queries that describe complex multi-agent interactions relevant to motion planning derived from 1000 driving logs in the Argoverse 2 Sensor dataset. We evaluate several referential multi-object trackers and present an empirical analysis of our baselines. Notably, we find that naively repurposing off-the-shelf VLMs yields poor performance, suggesting that scenario mining presents unique challenges. Our code and dataset are available at this https URL and this https URL ",
    "url": "https://arxiv.org/abs/2505.20981",
    "authors": [
      "Cainan Davidson",
      "Deva Ramanan",
      "Neehar Peri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.20992",
    "title": "Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation",
    "abstract": "           Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.         ",
    "url": "https://arxiv.org/abs/2505.20992",
    "authors": [
      "Meng Qin",
      "Jiahong Liu",
      "Irwin King"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.20997",
    "title": "BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks",
    "abstract": "           Binary (0-1) integer programming (BIP) is pivotal in scientific domains requiring discrete decision-making. As the advance of AI computing, recent works explore neural network-based solvers for integer linear programming (ILP) problems. Yet, they lack scalability for tackling nonlinear challenges. To handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear relaxations, leading to exponential growth in auxiliary variables and severe computation limitations. To overcome these limitations, we propose BIPNN (Binary Integer Programming Neural Network), an unsupervised learning framework to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN). Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear (sin, log, exp) optimization problems-into unconstrained, differentiable, and polynomial loss functions. The reformulation stems from the observation of a precise one-to-one mapping between polynomial BIP objectives and hypergraph structures, enabling the unsupervised training of HyperGNN to optimize BIP problems in an end-to-end manner. On this basis, we propose a GPU-accelerated and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel via straightforward gradient descent, thus significantly reducing the training cost while ensuring the generation of discrete, high-quality solutions. Extensive experiments on synthetic and real-world datasets highlight the superiority of our approach.         ",
    "url": "https://arxiv.org/abs/2505.20997",
    "authors": [
      "Sen Bai",
      "Chunqi Yang",
      "Xin Bai",
      "Xin Zhang",
      "Zhengang Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21018",
    "title": "Interference Detection in Spectrum-Blind Multi-User Optical Spectrum as a Service",
    "abstract": "           With the growing demand for high-bandwidth, low-latency applications, Optical Spectrum as a Service (OSaaS) is of interest for flexible bandwidth allocation within Elastic Optical Networks (EONs) and Open Line Systems (OLS). While OSaaS facilitates transparent connectivity and resource sharing among users, it raises concerns over potential network vulnerabilities due to shared fiber access and inter-channel interference, such as fiber non-linearity and amplifier based crosstalk. These challenges are exacerbated in multi-user environments, complicating the identification and localization of service interferences. To reduce system disruptions and system repair costs, it is beneficial to detect and identify such interferences timely. Addressing these challenges, this paper introduces a Machine Learning (ML) based architecture for network operators to detect and attribute interferences to specific OSaaS users while blind to the users' internal spectrum details. Our methodology leverages available coarse power measurements and operator channel performance data, bypassing the need for internal user information of wide-band shared spectra. Experimental studies conducted on a 190 km optical line system in the Open Ireland testbed, with three OSaaS users demonstrate the model's capability to accurately classify the source of interferences, achieving a classification accuracy of 90.3%.         ",
    "url": "https://arxiv.org/abs/2505.21018",
    "authors": [
      "Agastya Raj",
      "Daniel C. Kilper",
      "Marco Ruffini"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2505.21020",
    "title": "NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation",
    "abstract": "           Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically important for marine research, yet remains challenging due to its substantial thermal inertia and extended time delay. Machine learning (ML)-based models have demonstrated significant advancements in simulation accuracy and computational efficiency compared to traditional numerical methods. Nevertheless, a significant limitation of current ML models for S2S ocean simulation is their inadequate incorporation of physical consistency and the slow-changing properties of the ocean system. In this work, we propose a neural ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive graph neural network to emulate diverse physical phenomena associated with ocean systems effectively. Specifically, we propose a multi-stage framework tailored to model the ocean's slowly changing nature. Additionally, we introduce a multi-scale interactive messaging module to capture complex dynamical behaviors, such as gradient changes and multiplicative coupling relationships inherent in ocean dynamics. Extensive experimental evaluations confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S and extreme event simulation. The codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21020",
    "authors": [
      "Yuan Gao",
      "Ruiqi Shu",
      "Hao Wu",
      "Fan Xu",
      "Yanfei Xiang",
      "Ruijian Gou",
      "Qingsong Wen",
      "Xian Wu",
      "Xiaomeng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2505.21027",
    "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data",
    "abstract": "           Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks have been extensively studied in unstructured data like images, their application to tabular data presents new challenges. These challenges arise from the inherent heterogeneity and complex feature interdependencies in tabular data, which differ significantly from those in image data. To address these differences, it is crucial to consider imperceptibility as a key criterion specific to tabular data. Most current research focuses primarily on achieving effective adversarial attacks, often overlooking the importance of maintaining imperceptibility. To address this gap, we propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. In this study, we assess the effectiveness and imperceptibility of five adversarial attacks across four models using eleven tabular datasets, including both mixed and numerical-only datasets. Our analysis explores how these factors interact and influence the overall performance of the attacks. We also compare the results across different dataset types to understand the broader implications of these findings. The findings from this benchmark provide valuable insights for improving the design of adversarial attack algorithms, thereby advancing the field of adversarial machine learning on tabular data.         ",
    "url": "https://arxiv.org/abs/2505.21027",
    "authors": [
      "Zhipeng He",
      "Chun Ouyang",
      "Lijie Wen",
      "Cong Liu",
      "Catarina Moreira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21034",
    "title": "LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms",
    "abstract": "           Bayesian optimization (BO) is a powerful class of algorithms for optimizing expensive black-box functions, but designing effective BO algorithms remains a manual, expertise-driven task. Recent advancements in Large Language Models (LLMs) have opened new avenues for automating scientific discovery, including the automatic design of optimization algorithms. While prior work has used LLMs within optimization loops or to generate non-BO algorithms, we tackle a new challenge: Using LLMs to automatically generate full BO algorithm code. Our framework uses an evolution strategy to guide an LLM in generating Python code that preserves the key components of BO algorithms: An initial design, a surrogate model, and an acquisition function. The LLM is prompted to produce multiple candidate algorithms, which are evaluated on the established Black-Box Optimization Benchmarking (BBOB) test suite from the COmparing Continuous Optimizers (COCO) platform. Based on their performance, top candidates are selected, combined, and mutated via controlled prompt variations, enabling iterative refinement. Despite no additional fine-tuning, the LLM-generated algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB functions in dimension 5 and generalize well to higher dimensions, and different tasks (from the Bayesmark framework). This work demonstrates that LLMs can serve as algorithmic co-designers, offering a new paradigm for automating BO development and accelerating the discovery of novel algorithmic combinations. The source code is provided at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21034",
    "authors": [
      "Wenhu Li",
      "Niki van Stein",
      "Thomas B\u00e4ck",
      "Elena Raponi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2505.21039",
    "title": "Scalable and adaptive prediction bands with kernel sum-of-squares",
    "abstract": "           Conformal Prediction (CP) is a popular framework for constructing prediction bands with valid coverage in finite samples, while being free of any distributional assumption. A well-known limitation of conformal prediction is the lack of adaptivity, although several works introduced practically efficient alternate procedures. In this work, we build upon recent ideas that rely on recasting the CP problem as a statistical learning problem, directly targeting coverage and adaptivity. This statistical learning problem is based on reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS) methods. First, we extend previous results with a general representer theorem and exhibit the dual formulation of the learning problem. Crucially, such dual formulation can be solved efficiently by accelerated gradient methods with several hundreds or thousands of samples, unlike previous strategies based on off-the-shelf semidefinite programming algorithms. Second, we introduce a new hyperparameter tuning strategy tailored specifically to target adaptivity through bounds on test-conditional coverage. This strategy, based on the Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune kernel lengthscales in our framework, but has broader applicability since it could be used in any CP algorithm where the score function is learned. Finally, extensive experiments are conducted to show how our method compares to related work. All figures can be reproduced with the accompanying code.         ",
    "url": "https://arxiv.org/abs/2505.21039",
    "authors": [
      "Louis Allain",
      "S\u00e9bastien da Veiga",
      "Brian Staber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2505.21046",
    "title": "A domain adaptation neural network for digital twin-supported fault diagnosis",
    "abstract": "           Digital twins offer a promising solution to the lack of sufficient labeled data in deep learning-based fault diagnosis by generating simulated data for model training. However, discrepancies between simulation and real-world systems can lead to a significant drop in performance when models are applied in real scenarios. To address this issue, we propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN), which enables knowledge transfer from simulated (source domain) to real-world (target domain) data. We evaluate the proposed framework using a publicly available robotics fault diagnosis dataset, which includes 3,600 sequences generated by a digital twin model and 90 real sequences collected from physical systems. The DANN method is compared with commonly used lightweight deep learning models such as CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating domain adaptation significantly improves the diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data, demonstrating the effectiveness of domain adaptation in bridging the sim-to-real gap.         ",
    "url": "https://arxiv.org/abs/2505.21046",
    "authors": [
      "Zhenling Chen",
      "Haiwei Fu",
      "Zhiguo Zeng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.21049",
    "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing",
    "abstract": "           Road potholes pose a serious threat to driving safety and comfort, making their detection and assessment a critical task in fields such as autonomous driving. When driving vehicles, the operators usually avoid large potholes and approach smaller ones at reduced speeds to ensure safety. Therefore, accurately estimating pothole area is of vital importance. Most existing vision-based methods rely on distance priors to construct geometric models. However, their performance is susceptible to variations in camera angles and typically relies on the assumption of a flat road surface, potentially leading to significant errors in complex real-world environments. To address these problems, a robust pothole area estimation framework that integrates object detection and monocular depth estimation in a video stream is proposed in this paper. First, to enhance pothole feature extraction and improve the detection of small potholes, ACSH-YOLOv8 is proposed with ACmix module and the small object detection head. Then, the BoT-SORT algorithm is utilized for pothole tracking, while DepthAnything V2 generates depth maps for each frame. With the obtained depth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel (MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter based on Confidence and Distance (CDKF) is developed to maintain consistency of estimation results across consecutive frames. The results show that ACSH-YOLOv8 model achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8. Through CDKF optimization across consecutive frames, pothole predictions become more robust, thereby enhancing the method's practical applicability.         ",
    "url": "https://arxiv.org/abs/2505.21049",
    "authors": [
      "Dehao Wang",
      "Haohang Zhu",
      "Yiwen Xu",
      "Kaiqi Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21051",
    "title": "SHE-LoRA: Selective Homomorphic Encryption for Federated Tuning with Heterogeneous LoRA",
    "abstract": "           Federated fine-tuning of large language models (LLMs) is critical for improving their performance in handling domain-specific tasks. However, prior work has shown that clients' private data can actually be recovered via gradient inversion attacks. Existing privacy preservation techniques against such attacks typically entail performance degradation and high costs, making them ill-suited for clients with heterogeneous data distributions and device capabilities. In this paper, we propose SHE-LoRA, which integrates selective homomorphic encryption (HE) and low-rank adaptation (LoRA) to enable efficient and privacy-preserving federated tuning of LLMs in cross-device environment. Heterogeneous clients adaptively select partial model parameters for homomorphic encryption based on parameter sensitivity assessment, with the encryption subset obtained via negotiation. To ensure accurate model aggregation, we design a column-aware secure aggregation method and customized reparameterization techniques to align the aggregation results with the heterogeneous device capabilities of clients. Extensive experiments demonstrate that SHE-LoRA maintains performance comparable to non-private baselines, achieves strong resistance to the state-of-the-art attacks, and significantly reduces communication overhead by 94.901\\% and encryption computation overhead by 99.829\\%, compared to baseline. Our code is accessible at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21051",
    "authors": [
      "Jianmin Liu",
      "Li Yan",
      "Borui Li",
      "Lei Yu",
      "Chao Shen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.21077",
    "title": "Efficient Large Language Model Inference with Neural Block Linearization",
    "abstract": "           The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.         ",
    "url": "https://arxiv.org/abs/2505.21077",
    "authors": [
      "Mete Erdogan",
      "Francesco Tonin",
      "Volkan Cevher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21106",
    "title": "Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation",
    "abstract": "           Large Vision Language Models (LVLMs) have achieved remarkable progress in multimodal tasks, yet they also exhibit notable social biases. These biases often manifest as unintended associations between neutral concepts and sensitive human attributes, leading to disparate model behaviors across demographic groups. While existing studies primarily focus on detecting and quantifying such biases, they offer limited insight into the underlying mechanisms within the models. To address this gap, we propose an explanatory framework that combines information flow analysis with multi-round dialogue evaluation, aiming to understand the origin of social bias from the perspective of imbalanced internal information utilization. Specifically, we first identify high-contribution image tokens involved in the model's reasoning process for neutral questions via information flow analysis. Then, we design a multi-turn dialogue mechanism to evaluate the extent to which these key tokens encode sensitive information. Extensive experiments reveal that LVLMs exhibit systematic disparities in information usage when processing images of different demographic groups, suggesting that social bias is deeply rooted in the model's internal reasoning dynamics. Furthermore, we complement our findings from a textual modality perspective, showing that the model's semantic representations already display biased proximity patterns, thereby offering a cross-modal explanation of bias formation.         ",
    "url": "https://arxiv.org/abs/2505.21106",
    "authors": [
      "Zhengyang Ji",
      "Yifan Jia",
      "Shang Gao",
      "Yutao Yue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21133",
    "title": "Robust and Computation-Aware Gaussian Processes",
    "abstract": "           Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks.         ",
    "url": "https://arxiv.org/abs/2505.21133",
    "authors": [
      "Marshal Arijona Sinaga",
      "Julien Martinelli",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.21138",
    "title": "Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis",
    "abstract": "           Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese accents and dialects remain a challenge for most ASR models. Recent advancements in self-supervised learning have shown that self-supervised pre- training, combined with large language models (LLM), can effectively enhance ASR performance in low-resource scenarios. We aim to investigate the effectiveness of this paradigm for Chinese dialects. Specifically, we pre-train a Data2vec2 model on 300,000 hours of unlabeled dialect and accented speech data and do alignment training on a supervised dataset of 40,000 hours. Then, we systematically examine the impact of various projectors and LLMs on Mandarin, dialect, and accented speech recognition performance under this paradigm. Our method achieved SOTA results on multiple dialect datasets, including Kespeech. We will open-source our work to promote reproducible research         ",
    "url": "https://arxiv.org/abs/2505.21138",
    "authors": [
      "Tianyi Xu",
      "Hongjie Chen",
      "Wang Qing",
      "Lv Hang",
      "Jian Kang",
      "Li Jie",
      "Zhennan Lin",
      "Yongxiang Li",
      "Xie Lei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.21140",
    "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs",
    "abstract": "           Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.         ",
    "url": "https://arxiv.org/abs/2505.21140",
    "authors": [
      "Honglin Gao",
      "Xiang Li",
      "Lan Zhao",
      "Gaoxi Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21147",
    "title": "Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score",
    "abstract": "           Conformal prediction (CP) is a powerful framework for uncertainty quantification, providing prediction sets with coverage guarantees when calibrated on sufficient labeled data. However, in real-world applications where labeled data is often limited, standard CP can lead to coverage deviation and output overly large prediction sets. In this paper, we extend CP to the semi-supervised setting and propose SemiCP, leveraging both labeled data and unlabeled data for calibration. Specifically, we introduce a novel nonconformity score function, NNM, designed for unlabeled data. This function selects labeled data with similar pseudo-label scores to estimate nonconformity scores, integrating them into the calibration process to overcome sample size limitations. We theoretically demonstrate that, under mild assumptions, SemiCP provide asymptotically coverage guarantee for prediction sets. Extensive experiments further validate that our approach effectively reduces instability and inefficiency under limited calibration data, can be adapted to conditional coverage settings, and integrates seamlessly with existing CP methods.         ",
    "url": "https://arxiv.org/abs/2505.21147",
    "authors": [
      "Xuanning Zhou",
      "Hao Zeng",
      "Xiaobo Xia",
      "Bingyi Jing",
      "Hongxin Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.21152",
    "title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images",
    "abstract": "           Robust unsupervised anomaly detection (AD) in real-world scenarios is an important task. Current methods exhibit severe performance degradation on the MVTec AD 2 benchmark due to its complex real-world challenges. To solve this problem, we propose a robust framework RoBiS, which consists of three core modules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to preserve the information of small anomalies through overlapping window cropping. (2) The data augmentation of noise addition and lighting simulation is carried out on the training data to improve the robustness of AD model. We use INP-Former as our baseline, which could generate better results on the various sub-images. (3) The traditional statistical-based binarization strategy (mean+3std) is combined with our previous work, MEBin (published in CVPR2025), for joint adaptive binarization. Then, SAM is further employed to refine the segmentation results. Compared with some methods reported by the MVTec AD 2, our RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on Test_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on Test_private_mixed. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21152",
    "authors": [
      "Xurui Li",
      "Zhonesheng Jiang",
      "Tingxuan Ai",
      "Yu Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21162",
    "title": "Leveraging GANs for citation intent classification and its impact on citation network analysis",
    "abstract": "           Citations play a fundamental role in the scientific ecosystem, serving as a foundation for tracking the flow of knowledge, acknowledging prior work, and assessing scholarly influence. In scientometrics, they are also central to the construction of quantitative indicators. Not all citations, however, serve the same function: some provide background, others introduce methods, or compare results. Therefore, understanding citation intent allows for a more nuanced interpretation of scientific impact. In this paper, we adopted a GAN-based method to classify citation intents. Our results revealed that the proposed method achieves competitive classification performance, closely matching state-of-the-art results with substantially fewer parameters. This demonstrates the effectiveness and efficiency of leveraging GAN architectures combined with contextual embeddings in intent classification task. We also investigated whether filtering citation intents affects the centrality of papers in citation networks. Analyzing the network constructed from the unArXiv dataset, we found that paper rankings can be significantly influenced by citation intent. All four centrality metrics examined- degree, PageRank, closeness, and betweenness - were sensitive to the filtering of citation types. The betweenness centrality displayed the greatest sensitivity, showing substantial changes in ranking when specific citation intents were removed.         ",
    "url": "https://arxiv.org/abs/2505.21162",
    "authors": [
      "Davi A. Bezerra",
      "Filipi N. Silva",
      "Diego R. Amancio"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.21180",
    "title": "Latent label distribution grid representation for modeling uncertainty",
    "abstract": "           Although \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning (LDL) has promising representation capabilities for characterizing the polysemy of an instance, the complexity and high cost of the label distribution annotation lead to inexact in the construction of the label space. The existence of a large number of inexact labels generates a label space with uncertainty, which misleads the LDL algorithm to yield incorrect decisions. To alleviate this problem, we model the uncertainty of label distributions by constructing a \\textbf{L}atent \\textbf{L}abel \\textbf{D}istribution \\textbf{G}rid (LLDG) to form a low-noise representation space. Specifically, we first construct a label correlation matrix based on the differences between labels, and then expand each value of the matrix into a vector that obeys a Gaussian distribution, thus building a LLDG to model the uncertainty of the label space. Finally, the LLDG is reconstructed by the LLDG-Mixer to generate an accurate label distribution. Note that we enforce a customized low-rank scheme on this grid, which assumes that the label relations may be noisy and it needs to perform noise-reduction with the help of a Tucker reconstruction technique. Furthermore, we attempt to evaluate the effectiveness of the LLDG by considering its generation as an upstream task to achieve the classification of the objects. Extensive experimental results show that our approach performs competitively on several benchmarks.         ",
    "url": "https://arxiv.org/abs/2505.21180",
    "authors": [
      "ShuNing Sun",
      "YinSong Xiong",
      "Yu Zhang",
      "Zhuoran Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21181",
    "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion",
    "abstract": "           Adversarial attacks have become a significant challenge in the security of machine learning models, particularly in the context of black-box defense strategies. Existing methods for enhancing adversarial transferability primarily focus on the spatial domain. This paper presents Frequency-Space Attack (FSA), a new adversarial attack framework that effectively integrates frequency-domain and spatial-domain transformations. FSA combines two key techniques: (1) High-Frequency Augmentation, which applies Fourier transform with frequency-selective amplification to diversify inputs and emphasize the critical role of high-frequency components in adversarial attacks, and (2) Hierarchical-Gradient Fusion, which merges multi-scale gradient decomposition and fusion to capture both global structures and fine-grained details, resulting in smoother perturbations. Our experiment demonstrates that FSA consistently outperforms state-of-the-art methods across various black-box models. Notably, our proposed FSA achieves an average attack success rate increase of 23.6% compared with BSR (CVPR 2024) on eight black-box defense models.         ",
    "url": "https://arxiv.org/abs/2505.21181",
    "authors": [
      "Yayin Zheng",
      "Chen Wan",
      "Zihong Guo",
      "Hailing Kuang",
      "Xiaohai Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.21185",
    "title": "Constructive community race: full-density spiking neural network model drives neuromorphic computing",
    "abstract": "           The local circuitry of the mammalian brain is a focus of the search for generic computational principles because it is largely conserved across species and modalities. In 2014 a model was proposed representing all neurons and synapses of the stereotypical cortical microcircuit below $1\\,\\text{mm}^2$ of brain surface. The model reproduces fundamental features of brain activity but its impact remained limited because of its computational demands. For theory and simulation, however, the model was a breakthrough because it removes uncertainties of downscaling, and larger models are less densely connected. This sparked a race in the neuromorphic computing community and the model became a de facto standard benchmark. Within a few years real-time performance was reached and surpassed at significantly reduced energy consumption. We review how the computational challenge was tackled by different simulation technologies and derive guidelines for the next generation of benchmarks and other domains of science.         ",
    "url": "https://arxiv.org/abs/2505.21185",
    "authors": [
      "Johanna Senk",
      "Anno Kurth",
      "Steve Furber",
      "Tobias Gemmeke",
      "Bruno Golosio",
      "Arne Heittmann",
      "James C. Knight",
      "Eric M\u00fcller",
      "Tobias Noll",
      "Thomas Nowotny",
      "Gorka Peraza Coppola",
      "Luca Peres",
      "Oliver Rhodes",
      "Andrew Rowley",
      "Johannes Schemmel",
      "Tim Stadtmann",
      "Tom Tetzlaff",
      "Gianmarco Tiddia",
      "Sacha J. van Albada",
      "Jos\u00e9 Villamar",
      "Markus Diesmann"
    ],
    "subjectives": [
      "Performance (cs.PF)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.21204",
    "title": "Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics",
    "abstract": "           Hematotoxicity, drug-induced damage to the blood-forming system, is a frequent side effect of cytotoxic chemotherapy and poses a significant challenge in clinical practice due to its high inter-patient variability and limited predictability. Current mechanistic models often struggle to accurately forecast outcomes for patients with irregular or atypical trajectories. In this study, we develop and compare hybrid mechanistic and data-driven approaches for individualized time series modeling of platelet counts during chemotherapy. We consider hybrid models that combine mechanistic models with neural networks, known as universal differential equations. As a purely data-driven alternative, we utilize a nonlinear autoregressive exogenous model using gated recurrent units as the underlying architecture. These models are evaluated across a range of real patient scenarios, varying in data availability and sparsity, to assess predictive performance. Our findings demonstrate that data-driven methods, when provided with sufficient data, significantly improve prediction accuracy, particularly for high-risk patients with irregular platelet dynamics. This highlights the potential of data-driven approaches in enhancing clinical decision-making. In contrast, hybrid and mechanistic models are superior in scenarios with limited or sparse data. The proposed modeling and comparison framework is generalizable and could be extended to predict other treatment-related toxicities, offering broad applicability in personalized medicine.         ",
    "url": "https://arxiv.org/abs/2505.21204",
    "authors": [
      "Marie Steinacker",
      "Yuri Kheifetz",
      "Markus Scholz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2505.21224",
    "title": "A Representation Level Analysis of NMT Model Robustness to Grammatical Errors",
    "abstract": "           Understanding robustness is essential for building reliable NLP systems. Unfortunately, in the context of machine translation, previous work mainly focused on documenting robustness failures or improving robustness. In contrast, we study robustness from a model representation perspective by looking at internal model representations of ungrammatical inputs and how they evolve through model layers. For this purpose, we perform Grammatical Error Detection (GED) probing and representational similarity analysis. Our findings indicate that the encoder first detects the grammatical error, then corrects it by moving its representation toward the correct form. To understand what contributes to this process, we turn to the attention mechanism where we identify what we term Robustness Heads. We find that Robustness Heads attend to interpretable linguistic units when responding to grammatical errors, and that when we fine-tune models for robustness, they tend to rely more on Robustness Heads for updating the ungrammatical word representation.         ",
    "url": "https://arxiv.org/abs/2505.21224",
    "authors": [
      "Abderrahmane Issam",
      "Yusuf Can Semerci",
      "Jan Scholtes",
      "Gerasimos Spanakis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.21236",
    "title": "Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies",
    "abstract": "           Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21236",
    "authors": [
      "Felix Chalumeau",
      "Daniel Rajaonarivonivelomanantsoa",
      "Ruan de Kock",
      "Claude Formanek",
      "Sasha Abramowitz",
      "Oumayma Mahjoub",
      "Wiem Khlifi",
      "Simon Du Toit",
      "Louay Ben Nessir",
      "Refiloe Shabe",
      "Arnol Fokam",
      "Siddarth Singh",
      "Ulrich Mbou Sob",
      "Arnu Pretorius"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2505.21251",
    "title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework",
    "abstract": "           We introduce copresheaf topological neural networks (CTNNs), a powerful and unifying framework that encapsulates a wide spectrum of deep learning architectures, designed to operate on structured data: including images, point clouds, graphs, meshes, and topological manifolds. While deep learning has profoundly impacted domains ranging from digital assistants to autonomous systems, the principled design of neural architectures tailored to specific tasks and data types remains one of the field's most persistent open challenges. CTNNs address this gap by grounding model design in the language of copresheaves, a concept from algebraic topology that generalizes and subsumes most practical deep learning models in use today. This abstract yet constructive formulation yields a rich design space from which theoretically sound and practically effective solutions can be derived to tackle core challenges in representation learning: long-range dependencies, oversmoothing, heterophily, and non-Euclidean domains. Our empirical results on structured data benchmarks demonstrate that CTNNs consistently outperform conventional baselines, particularly in tasks requiring hierarchical or localized sensitivity. These results underscore CTNNs as a principled, multi-scale foundation for the next generation of deep learning architectures.         ",
    "url": "https://arxiv.org/abs/2505.21251",
    "authors": [
      "Mustafa Hajij",
      "Lennart Bastian",
      "Sarah Osentoski",
      "Hardik Kabaria",
      "John L. Davenport",
      "Sheik Dawood",
      "Balaji Cherukuri",
      "Joseph G. Kocheemoolayil",
      "Nastaran Shahmansouri",
      "Adrian Lew",
      "Theodore Papamarkou",
      "Tolga Birdal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.21258",
    "title": "Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation",
    "abstract": "           We present Plenodium (plenoptic medium), an effective and efficient 3D representation framework capable of jointly modeling both objects and participating media. In contrast to existing medium representations that rely solely on view-dependent modeling, our novel plenoptic medium representation incorporates both directional and positional information through spherical harmonics encoding, enabling highly accurate underwater scene reconstruction. To address the initialization challenge in degraded underwater environments, we propose the pseudo-depth Gaussian complementation to augment COLMAP-derived point clouds with robust depth priors. In addition, a depth ranking regularized loss is developed to optimize the geometry of the scene and improve the ordinal consistency of the depth maps. Extensive experiments on real-world underwater datasets demonstrate that our method achieves significant improvements in 3D reconstruction. Furthermore, we conduct a simulated dataset with ground truth and the controllable scattering medium to demonstrate the restoration capability of our method in underwater scenarios. Our code and dataset are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21258",
    "authors": [
      "Changguanng Wu",
      "Jiangxin Dong",
      "Chengjian Li",
      "Jinhui Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21269",
    "title": "Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands",
    "abstract": "           Accurate wetland land-cover classification is essential for environmental monitoring, biodiversity assessment, and sustainable ecosystem management. However, the scarcity of annotated data, especially for high-resolution satellite imagery, poses a significant challenge for supervised learning approaches. To tackle this issue, this study presents a methodology for wetland land-cover segmentation and classification that adopts both supervised and self-supervised learning (SSL). We train a U-Net model from scratch on Sentinel-2 imagery across six wetland regions in the Netherlands, achieving a baseline model accuracy of 85.26%. Addressing the limited availability of labeled data, the results show that SSL pretraining with an autoencoder can improve accuracy, especially for the high-resolution imagery where it is more difficult to obtain labeled data, reaching an accuracy of 88.23%. Furthermore, we introduce a framework to scale manually annotated high-resolution labels to medium-resolution inputs. While the quantitative performance between resolutions is comparable, high-resolution imagery provides significantly sharper segmentation boundaries and finer spatial detail. As part of this work, we also contribute a curated Sentinel-2 dataset with Dynamic World labels, tailored for wetland classification tasks and made publicly available.         ",
    "url": "https://arxiv.org/abs/2505.21269",
    "authors": [
      "Eva Gmelich Meijling",
      "Roberto Del Prete",
      "Arnoud Visser"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2505.21277",
    "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
    "abstract": "           Large Language Models (LLMs), despite advanced general capabilities, still suffer from numerous safety risks, especially jailbreak attacks that bypass safety protocols. Understanding these vulnerabilities through black-box jailbreak attacks, which better reflect real-world scenarios, offers critical insights into model robustness. While existing methods have shown improvements through various prompt engineering techniques, their success remains limited against safety-aligned models, overlooking a more fundamental problem: the effectiveness is inherently bounded by the predefined strategy spaces. However, expanding this space presents significant challenges in both systematically capturing essential attack patterns and efficiently navigating the increased complexity. To better explore the potential of expanding the strategy space, we address these challenges through a novel framework that decomposes jailbreak strategies into essential components based on the Elaboration Likelihood Model (ELM) theory and develops genetic-based optimization with intention evaluation mechanisms. To be striking, our experiments reveal unprecedented jailbreak capabilities by expanding the strategy space: we achieve over 90% success rate on Claude-3.5 where prior methods completely fail, while demonstrating strong cross-model transferability and surpassing specialized safeguard models in evaluation accuracy. The code is open-sourced at: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21277",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Shouwei Ruan",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.21281",
    "title": "RLJP: Legal Judgment Prediction via First-Order Logic Rule-enhanced with Large Language Models",
    "abstract": "           Legal Judgment Prediction (LJP) is a pivotal task in legal AI. Existing semantic-enhanced LJP models integrate judicial precedents and legal knowledge for high performance. But they neglect legal reasoning logic, a critical component of legal judgments requiring rigorous logical analysis. Although some approaches utilize legal reasoning logic for high-quality predictions, their logic rigidity hinders adaptation to case-specific logical frameworks, particularly in complex cases that are lengthy and detailed. This paper proposes a rule-enhanced legal judgment prediction framework based on first-order logic (FOL) formalism and comparative learning (CL) to develop an adaptive adjustment mechanism for legal judgment logic and further enhance performance in LJP. Inspired by the process of human exam preparation, our method follows a three-stage approach: first, we initialize judgment rules using the FOL formalism to capture complex reasoning logic accurately; next, we propose a Confusion-aware Contrastive Learning (CACL) to dynamically optimize the judgment rules through a quiz consisting of confusable cases; finally, we utilize the optimized judgment rules to predict legal judgments. Experimental results on two public datasets show superior performance across all metrics. The code is publicly available{this https URL}.         ",
    "url": "https://arxiv.org/abs/2505.21281",
    "authors": [
      "Yue Zhang",
      "Zhiliang Tian",
      "Shicheng Zhou",
      "Haiyang Wang",
      "Wenqing Hou",
      "Yuying Liu",
      "Xuechen Zhao",
      "Minlie Huang",
      "Ye Wang",
      "Bin Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21285",
    "title": "Learnable Kernel Density Estimation for Graphs",
    "abstract": "           This work proposes a framework LGKDE that learns kernel density estimation for graphs. The key challenge in graph density estimation lies in effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees. Combining graph kernels and kernel density estimation (KDE) is a standard approach to graph density estimation, but has unsatisfactory performance due to the handcrafted and fixed features of kernels. Our method LGKDE leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE, where all parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts. The perturbations are conducted on both node features and graph spectra, which helps better characterize the boundary of normal density regions. Theoretically, we establish consistency and convergence guarantees for LGKDE, including bounds on the mean integrated squared error, robustness, and complexity. We validate LGKDE by demonstrating its effectiveness in recovering the underlying density of synthetic graph distributions and applying it to graph anomaly detection across diverse benchmark datasets. Extensive empirical evaluation shows that LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2505.21285",
    "authors": [
      "Xudong Wang",
      "Ziheng Sun",
      "Chris Ding",
      "Jicong Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.21288",
    "title": "GSAT: Graph Structure Attention Networks",
    "abstract": "           Graph Neural Networks (GNNs) have emerged as a powerful tool for processing data represented in graph structures, achieving remarkable success across a wide range of applications. However, to further improve the performance on graph classification benchmarks, structural representation of each node that encodes rich local topological information in the neighbourhood of nodes is an important type of feature that is often overlooked in the modeling. The consequence of neglecting the structural information has resulted high number of layers to connect messages from distant nodes which by itself produces other problems such as oversmoothing. In the present paper, we leverage these structural information that are modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT) which is a generalization of graph attention network(GAT) to integrate the original attribute and the structural representation to enforce the model to automatically find patterns for attending to different edges in the node neighbourhood to enrich graph representation. Our experiments show GSAT slightly improves SOTA on some graph classification benchmarks.         ",
    "url": "https://arxiv.org/abs/2505.21288",
    "authors": [
      "Farshad Noravesh",
      "Reza Haffari",
      "Layki Soon",
      "Arghya Pal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21291",
    "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large Language Model-Enhanced Framework",
    "abstract": "           In this paper, we present a novel diagnostic framework that integrates Knowledge Graphs (KGs) and Large Language Models (LLMs) to support system diagnostics in high-reliability systems such as nuclear power plants. Traditional diagnostic modeling struggles when systems become too complex, making functional modeling a more attractive approach. Our approach introduces a diagnostic framework grounded in the functional modeling principles of the Dynamic Master Logic (DML) model. It incorporates two coordinated LLM components, including an LLM-based workflow for automated construction of DML logic from system documentation and an LLM agent that facilitates interactive diagnostics. The generated logic is encoded into a structured KG, referred to as KG-DML, which supports hierarchical fault reasoning. Expert knowledge or operational data can also be incorporated to refine the model's precision and diagnostic depth. In the interaction phase, users submit natural language queries, which are interpreted by the LLM agent. The agent selects appropriate tools for structured reasoning, including upward and downward propagation across the KG-DML. Rather than embedding KG content into every prompt, the LLM agent distinguishes between diagnostic and interpretive tasks. For diagnostics, the agent selects and executes external tools that perform structured KG reasoning. For general queries, a Graph-based Retrieval-Augmented Generation (Graph-RAG) approach is used, retrieving relevant KG segments and embedding them into the prompt to generate natural explanations. A case study on an auxiliary feedwater system demonstrated the framework's effectiveness, with over 90% accuracy in key elements and consistent tool and argument extraction, supporting its use in safety-critical diagnostics.         ",
    "url": "https://arxiv.org/abs/2505.21291",
    "authors": [
      "Saman Marandi",
      "Yu-Shu Hu",
      "Mohammad Modarres"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21297",
    "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset",
    "abstract": "           Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21297",
    "authors": [
      "Yifei Liu",
      "Li Lyna Zhang",
      "Yi Zhu",
      "Bingcheng Dong",
      "Xudong Zhou",
      "Ning Shang",
      "Fan Yang",
      "Mao Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.21309",
    "title": "Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation",
    "abstract": "           Transformer-based 3D human pose estimation methods suffer from high computational costs due to the quadratic complexity of self-attention with respect to sequence length. Additionally, pose sequences often contain significant redundancy between frames. However, recent methods typically fail to improve model capacity while effectively eliminating sequence redundancy. In this work, we introduce the Spectral Compression Transformer (SCT) to reduce sequence length and accelerate computation. The SCT encoder treats hidden features between blocks as Temporal Feature Signals (TFS) and applies the Discrete Cosine Transform, a Fourier transform-based technique, to determine the spectral components to be retained. By filtering out certain high-frequency noise components, SCT compresses the sequence length and reduces redundancy. To further enrich the input sequence with prior structural information, we propose the Line Pose Graph (LPG) based on line graph theory. The LPG generates skeletal position information that complements the input 2D joint positions, thereby improving the model's performance. Finally, we design a dual-stream network architecture to effectively model spatial joint relationships and the compressed motion trajectory within the pose sequence. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our model achieves state-of-the-art performance with improved computational efficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE of 37.7mm while maintaining a low computational cost. Furthermore, we perform ablation studies on each module to assess its effectiveness. The code and models will be released.         ",
    "url": "https://arxiv.org/abs/2505.21309",
    "authors": [
      "Zenghao Zheng",
      "Lianping Yang",
      "Hegui Zhu",
      "Mingrui Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21317",
    "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features",
    "abstract": "           Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.         ",
    "url": "https://arxiv.org/abs/2505.21317",
    "authors": [
      "Ihab Bendidi",
      "Yassir El Mesbahi",
      "Alisandra K. Denton",
      "Karush Suri",
      "Kian Kenyon-Dean",
      "Auguste Genovesio",
      "Emmanuel Noutahi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21319",
    "title": "efunc: An Efficient Function Representation without Neural Networks",
    "abstract": "           Function fitting/approximation plays a fundamental role in computer graphics and other engineering applications. While recent advances have explored neural networks to address this task, these methods often rely on architectures with many parameters, limiting their practical applicability. In contrast, we pursue high-quality function approximation using parameter-efficient representations that eliminate the dependency on neural networks entirely. We first propose a novel framework for continuous function modeling. Most existing works can be formulated using this framework. We then introduce a compact function representation, which is based on polynomials interpolated using radial basis functions, bypassing both neural networks and complex/hierarchical data structures. We also develop memory-efficient CUDA-optimized algorithms that reduce computational time and memory consumption to less than 10% compared to conventional automatic differentiation frameworks. Finally, we validate our representation and optimization pipeline through extensive experiments on 3D signed distance functions (SDFs). The proposed representation achieves comparable or superior performance to state-of-the-art techniques (e.g., octree/hash-grid techniques) with significantly fewer parameters.         ",
    "url": "https://arxiv.org/abs/2505.21319",
    "authors": [
      "Biao Zhang",
      "Peter Wonka"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21324",
    "title": "Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts",
    "abstract": "           Despite rapid advances in large language models (LLMs), their integration with traditional supervised machine learning (ML) techniques that have proven applicability to medical data remains underexplored. This is particularly true for psychiatric applications, where narrative data often exhibit nuanced linguistic and contextual complexity, and can benefit from the combination of multiple models with differing characteristics. In this study, we introduce an ensemble framework for automatically classifying Attention-Deficit/Hyperactivity Disorder (ADHD) diagnosis (binary) using narrative transcripts. Our approach integrates three complementary models: LLaMA3, an open-source LLM that captures long-range semantic structure; RoBERTa, a pre-trained transformer model fine-tuned on labeled clinical narratives; and a Support Vector Machine (SVM) classifier trained using TF-IDF-based lexical features. These models are aggregated through a majority voting mechanism to enhance predictive robustness. The dataset includes 441 instances, including 352 for training and 89 for validation. Empirical results show that the ensemble outperforms individual models, achieving an F$_1$ score of 0.71 (95\\% CI: [0.60-0.80]). Compared to the best-performing individual model (SVM), the ensemble improved recall while maintaining competitive precision. This indicates the strong sensitivity of the ensemble in identifying ADHD-related linguistic cues. These findings demonstrate the promise of hybrid architectures that leverage the semantic richness of LLMs alongside the interpretability and pattern recognition capabilities of traditional supervised ML, offering a new direction for robust and generalizable psychiatric text classification.         ",
    "url": "https://arxiv.org/abs/2505.21324",
    "authors": [
      "Yuxin Zhu",
      "Yuting Guo",
      "Noah Marchuck",
      "Abeed Sarker",
      "Yun Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.21339",
    "title": "An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction",
    "abstract": "           Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting a single, most likely suffix. However, if the future course of a process is exposed to uncertainty or has high variability, the expressiveness of a single suffix prediction can be limited. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report provides a detailed evaluation of the U-ED-LSTM's predictive performance and assesses its calibration on four real-life event logs with three different hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable predictive performance across various datasets, ii) aggregating probabilistic suffix predictions into mean values can outperform most likely predictions, particularly for rare prefixes or longer suffixes, and iii) the approach effectively captures uncertainties present in event logs.         ",
    "url": "https://arxiv.org/abs/2505.21339",
    "authors": [
      "Henryk Mustroph",
      "Michel Kunkler",
      "Stefanie Rinderle-Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21356",
    "title": "Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning",
    "abstract": "           Objective: Perceptual voice quality assessment plays a critical role in diagnosing and monitoring voice disorders by providing standardized evaluation of vocal function. Traditionally, this process relies on expert raters utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are inherently subjective and susceptible to inter-rater variability, motivating the need for automated and objective assessment methods. Methods: We propose Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to capture high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we present VOQANet+, which integrates handcrafted acoustic features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings. Results: Sentence-based input yields stronger performance than vowel-based input, especially at the patient level. VOQANet consistently outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even better and maintains robustness under noisy conditions. Conclusion: Combining SFM embeddings with domain-informed acoustic features improves interpretability and resilience. Significance: VOQANet+ shows strong potential for deployment in real-world and telehealth settings, addressing the limitations of subjective perceptual assessments with an interpretable and noise-resilient solution.         ",
    "url": "https://arxiv.org/abs/2505.21356",
    "authors": [
      "Whenty Ariyanti",
      "Kuan-Yu Chen",
      "Sabato Marco Siniscalchi",
      "Hsin-Min Wang",
      "Yu Tsao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2505.21360",
    "title": "CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models",
    "abstract": "           Competing risks are crucial considerations in survival modelling, particularly in healthcare domains where patients may experience multiple distinct event types. We propose CRISP-NAM (Competing Risks Interpretable Survival Prediction with Neural Additive Models), an interpretable neural additive model for competing risks survival analysis which extends the neural additive architecture to model cause-specific hazards while preserving feature-level interpretability. Each feature contributes independently to risk estimation through dedicated neural networks, allowing for visualization of complex non-linear relationships between covariates and each competing risk. We demonstrate competitive performance on multiple datasets compared to existing approaches.         ",
    "url": "https://arxiv.org/abs/2505.21360",
    "authors": [
      "Dhanesh Ramachandram"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.21363",
    "title": "Subgroups Matter for Robust Bias Mitigation",
    "abstract": "           Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and suggest it as a alternative lever for improving the robustness and fairness of machine learning models.         ",
    "url": "https://arxiv.org/abs/2505.21363",
    "authors": [
      "Anissa Alloula",
      "Charles Jones",
      "Ben Glocker",
      "Bart\u0142omiej W. Papie\u017c"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21366",
    "title": "PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment",
    "abstract": "           Network alignment (NA) aims to identify node correspondence across different networks and serves as a critical cornerstone behind various downstream multi-network learning tasks. Despite growing research in NA, there lacks a comprehensive library that facilitates the systematic development and benchmarking of NA methods. In this work, we introduce PLANETALIGN, a comprehensive Python library for network alignment that features a rich collection of built-in datasets, methods, and evaluation pipelines with easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA methods with extensible APIs for easy use and development of NA methods. Our standardized evaluation pipeline encompasses a wide range of metrics, enabling a systematic assessment of the effectiveness, scalability, and robustness of NA methods. Through extensive comparative studies, we reveal practical insights into the strengths and limitations of existing NA methods. We hope that PLANETALIGN can foster a deeper understanding of the NA problem and facilitate the development and benchmarking of more effective, scalable, and robust methods in the future. The source code of PLANETALIGN is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21366",
    "authors": [
      "Qi Yu",
      "Zhichen Zeng",
      "Yuchen Yan",
      "Zhining Liu",
      "Baoyu Jing",
      "Ruizhong Qiu",
      "Ariful Azad",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.21370",
    "title": "YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration",
    "abstract": "           Object detection in remote sensing imagery remains a challenging task due to extreme scale variation, dense object distributions, and cluttered backgrounds. While recent detectors such as YOLOv8 have shown promising results, their backbone architectures lack explicit mechanisms to guide multi-scale feature refinement, limiting performance on high-resolution aerial data. In this work, we propose YOLO-SPCI, an attention-enhanced detection framework that introduces a lightweight Selective-Perspective-Class Integration (SPCI) module to improve feature representation. The SPCI module integrates three components: a Selective Stream Gate (SSG) for adaptive regulation of global feature flow, a Perspective Fusion Module (PFM) for context-aware multi-scale integration, and a Class Discrimination Module (CDM) to enhance inter-class separability. We embed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone, enabling effective refinement while preserving compatibility with the original neck and head. Experiments on the NWPU VHR-10 dataset demonstrate that YOLO-SPCI achieves superior performance compared to state-of-the-art detectors.         ",
    "url": "https://arxiv.org/abs/2505.21370",
    "authors": [
      "Xinyuan Wang",
      "Lian Peng",
      "Xiangcheng Li",
      "Yilin He",
      "KinTak U"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21374",
    "title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?",
    "abstract": "           Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21374",
    "authors": [
      "Junhao Cheng",
      "Yuying Ge",
      "Teng Wang",
      "Yixiao Ge",
      "Jing Liao",
      "Ying Shan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21380",
    "title": "PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense",
    "abstract": "           As malicious users increasingly employ phonetic substitution to evade hate speech detection, researchers have investigated such strategies. However, two key challenges remain. First, existing studies have overlooked the Korean language, despite its vulnerability to phonetic perturbations due to its phonographic nature. Second, prior work has primarily focused on constructing datasets rather than developing architectural defenses. To address these challenges, we propose (1) PHonetic-Informed Substitution for Hangul (PHISH) that exploits the phonological characteristics of the Korean writing system, and (2) Mixed Encoding of Semantic-pHonetic features (MESH) that enhances the detector's robustness by incorporating phonetic information at the architectural level. Our experimental results demonstrate the effectiveness of our proposed methods on both perturbed and unperturbed datasets, suggesting that they not only improve detection performance but also reflect realistic adversarial behaviors employed by malicious users.         ",
    "url": "https://arxiv.org/abs/2505.21380",
    "authors": [
      "Byungjun Kim",
      "Minju Kim",
      "Hyeonchu Park",
      "Bugeun Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.21387",
    "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios",
    "abstract": "           Leveraging the powerful representation learning capabilities, deep multi-view clustering methods have demonstrated reliable performance by effectively integrating multi-source information from diverse views in recent years. Most existing methods rely on the assumption of clean views. However, noise is pervasive in real-world scenarios, leading to a significant degradation in performance. To tackle this problem, we propose a novel multi-view clustering framework for the automatic identification and rectification of noisy data, termed AIRMVC. Specifically, we reformulate noisy identification as an anomaly identification problem using GMM. We then design a hybrid rectification strategy to mitigate the adverse effects of noisy data based on the identification results. Furthermore, we introduce a noise-robust contrastive mechanism to generate reliable representations. Additionally, we provide a theoretical proof demonstrating that these representations can discard noisy information, thereby improving the performance of downstream tasks. Extensive experiments on six benchmark datasets demonstrate that AIRMVC outperforms state-of-the-art algorithms in terms of robustness in noisy scenarios. The code of AIRMVC are available at this https URL on Github.         ",
    "url": "https://arxiv.org/abs/2505.21387",
    "authors": [
      "Xihong Yang",
      "Siwei Wang",
      "Fangdi Wang",
      "Jiaqi Jin",
      "Suyuan Liu",
      "Yue Liu",
      "En Zhu",
      "Xinwang Liu",
      "Yueming Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21388",
    "title": "DeSocial: Blockchain-based Decentralized Social Networks",
    "abstract": "           Web 2.0 social platforms are inherently centralized, with user data and algorithmic decisions controlled by the platform. However, users can only passively receive social predictions without being able to choose the underlying algorithm, which limits personalization. Fortunately, with the emergence of blockchain, users are allowed to choose algorithms that are tailored to their local situation, improving prediction results in a personalized way. In a blockchain environment, each user possesses its own model to perform the social prediction, capturing different perspectives on social interactions. In our work, we propose DeSocial, a decentralized social network learning framework deployed on an Ethereum (ETH) local development chain that integrates distributed data storage, node-level consensus, and user-driven model selection through Ganache. In the first stage, each user leverages DeSocial to evaluate multiple backbone models on their local subgraph. DeSocial coordinates the execution and returns model-wise prediction results, enabling the user to select the most suitable backbone for personalized social prediction. Then, DeSocial uniformly selects several validation nodes that possess the algorithm specified by each user, and aggregates the prediction results by majority voting, to prevent errors caused by any single model's misjudgment. Extensive experiments show that DeSocial has an evident improvement compared to the five classical centralized social network learning models, promoting user empowerment in blockchain-based decentralized social networks, showing the importance of multi-node validation and personalized algorithm selection based on blockchain. Our implementation is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21388",
    "authors": [
      "Jingyuan Huang",
      "Xi Zhu",
      "Minghao Guo",
      "Yongfeng Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.21395",
    "title": "Square$\u03c7$PO: Differentially Private and Robust $\u03c7^2$-Preference Optimization in Offline Direct Alignment",
    "abstract": "           In this paper, we theoretically study the offline alignment of language models with human preference feedback, under both preference label corruption and privacy protections. To this end, we propose Square$\\chi$PO, a simple one-line change to $\\chi$PO where the standard log-loss is replaced by a new square loss over probability. Thanks to the inherent properties of this new loss, we have advanced the state-of-the-art of differentially private and robust offline direct alignment. Specifically, for the local model of label privacy, Square$\\chi$PO is the first algorithm that attains an optimal rate based on single-policy concentrability even with general function approximations. It also gives the first result under the central model of privacy protection over both prompts (responses) and labels. On the robustness side against Huber label corruption, Square$\\chi$PO is the first alignment method that has a meaningful theoretical guarantee under general function approximations. More importantly, Square$\\chi$PO can address privacy protection and corruption simultaneously, where an interesting separation is observed, implying that the order of privacy and corruption matters. Furthermore, we show that Square$\\chi$PO can also be easily extended to handle the scenario of the general preference model with state-of-the-art guarantees under corruption and privacy. Last but not least, all of our theoretical guarantees enjoy a unified analysis, building upon a new result on the generalization error bounds of least-square regression under corruption and privacy constraints, which we believe is of independent interest to the community.         ",
    "url": "https://arxiv.org/abs/2505.21395",
    "authors": [
      "Xingyu Zhou",
      "Yulian Wu",
      "Wenqian Weng",
      "Francesco Orabona"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.21396",
    "title": "Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science",
    "abstract": "           Recent advancements in large language models (LLMs) have shown promise in generating novel research ideas. However, these ideas often face challenges related to feasibility and expected effectiveness. This paper explores how augmenting LLMs with relevant data during the idea generation process can enhance the quality of generated ideas. We introduce two ways of incorporating data: (1) providing metadata during the idea generation stage to guide LLMs toward feasible directions, and (2) adding automatic validation during the idea selection stage to assess the empirical plausibility of hypotheses within ideas. We conduct experiments in the social science domain, specifically with climate negotiation topics, and find that metadata improves the feasibility of generated ideas by 20%, while automatic validation improves the overall quality of selected ideas by 7%. A human study shows that LLM-generated ideas, along with their related data and validation processes, inspire researchers to propose research ideas with higher quality. Our work highlights the potential of data-driven research idea generation, and underscores the practical utility of LLM-assisted ideation in real-world academic settings.         ",
    "url": "https://arxiv.org/abs/2505.21396",
    "authors": [
      "Xiao Liu",
      "Xinyi Dong",
      "Xinyang Gao",
      "Yansong Feng",
      "Xun Pang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2505.21404",
    "title": "Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks",
    "abstract": "           Natural-gradient methods markedly accelerate the training of Physics-Informed Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is the number of network trainable weights. We show that exactly the same step can instead be formulated in a generally smaller residual space of size $m = \\sum_{\\gamma} N_{\\gamma} d_{\\gamma}$, where each residual class $\\gamma$ (e.g. PDE interior, boundary, initial data) contributes $N_{\\gamma}$ collocation points of output dimension $d_{\\gamma}$. Building on this insight, we introduce \\textit{Dual Natural Gradient Descent} (D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it with a geodesic-acceleration correction at negligible extra cost, and provides both a dense direct solver for modest $m$ and a Nystrom-preconditioned conjugate-gradient solver for larger $m$. Experimentally, D-NGD scales second-order PINN optimization to networks with up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton methods, and -- crucially -- enables natural-gradient training of PINNs at this scale on a single GPU.         ",
    "url": "https://arxiv.org/abs/2505.21404",
    "authors": [
      "Anas Jnini",
      "Flavio Vella"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2505.21406",
    "title": "Enhancing JavaScript Malware Detection through Weighted Behavioral DFAs",
    "abstract": "           This work addresses JavaScript malware detection to enhance client-side web application security with a behavior-based system. The ability to detect malicious JavaScript execution sequences is a critical problem in modern web security as attack techniques become more sophisticated. This study introduces a new system for detecting JavaScript malware using a Deterministic Finite Automaton (DFA) along with a weighted-behavior system, which we call behavior DFA. This system captures malicious patterns and provides a dynamic mechanism to classify new sequences that exhibit partial similarity to known attacks, differentiating them between benign, partially malicious, and fully malicious behaviors. Experimental evaluation on a dataset of 1,058 sequences captured in a real-world environment demonstrates the capability of the system to detect and classify threats effectively, with the behavior DFA successfully identifying exact matches and partial similarities to known malicious behaviors. The results highlight the adaptability of the system in detecting emerging threats while maintaining transparency in decision making.         ",
    "url": "https://arxiv.org/abs/2505.21406",
    "authors": [
      "Pedro Pereira",
      "Jos\u00e9 Gon\u00e7alves",
      "Jo\u00e3o Vitorino",
      "Eva Maia",
      "Isabel Pra\u00e7a"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.21414",
    "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment",
    "abstract": "           This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.         ",
    "url": "https://arxiv.org/abs/2505.21414",
    "authors": [
      "Brett Bissey",
      "Kyle Gatesman",
      "Walker Dimon",
      "Mohammad Alam",
      "Luis Robaina",
      "Joseph Weissman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2505.21420",
    "title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning",
    "abstract": "           Multimodal feature reconstruction is a promising approach for 3D anomaly detection, leveraging the complementary information from dual modalities. We further advance this paradigm by utilizing multi-modal mentor learning, which fuses intermediate features to further distinguish normal from feature differences. To address these challenges, we propose a novel method called Mentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared features of different modalities, Mentor3AD can extract more effective features and guide feature reconstruction, ultimately improving detection performance. Specifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges features extracted from RGB and 3D modalities to create a mentor feature. Additionally, we have designed a Mentor of Guidance Module (MGM) to facilitate cross-modal reconstruction, supported by the mentor feature. Lastly, we introduce a Voting Module (VM) to more accurately generate the final anomaly score. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies have verified the effectiveness of the proposed method.         ",
    "url": "https://arxiv.org/abs/2505.21420",
    "authors": [
      "Jinbao Wang",
      "Hanzhe Liang",
      "Can Gao",
      "Chenxi Hu",
      "Jie Zhou",
      "Yunkang Cao",
      "Linlin Shen",
      "Weiming Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21425",
    "title": "GUARD:Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural Code Generation",
    "abstract": "           With the widespread application of large language models in code generation, recent studies demonstrate that employing additional Chain-of-Thought generation models can significantly enhance code generation performance by providing explicit reasoning steps. However, as external components, CoT models are particularly vulnerable to backdoor attacks, which existing defense mechanisms often fail to detect effectively. To address this challenge, we propose GUARD, a novel dual-agent defense framework specifically designed to counter CoT backdoor attacks in neural code generation. GUARD integrates two core components: GUARD-Judge, which identifies suspicious CoT steps and potential triggers through comprehensive analysis, and GUARD-Repair, which employs a retrieval-augmented generation approach to regenerate secure CoT steps for identified anomalies. Experimental results show that GUARD effectively mitigates attacks while maintaining generation quality, advancing secure code generation systems.         ",
    "url": "https://arxiv.org/abs/2505.21425",
    "authors": [
      "Naizhu Jin",
      "Zhong Li",
      "Tian Zhang",
      "Qingkai Zeng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2505.21426",
    "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks",
    "abstract": "           Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.         ",
    "url": "https://arxiv.org/abs/2505.21426",
    "authors": [
      "Francesco Cozzi",
      "Marco Pangallo",
      "Alan Perotti",
      "Andr\u00e9 Panisson",
      "Corrado Monti"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Econometrics (econ.EM)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2505.21433",
    "title": "Optimal Approximations for the Requirement Cut Problem on Sparse Graph Classes",
    "abstract": "           We study the Requirement Cut problem, a generalization of numerous classical graph partitioning problems including Multicut, Multiway Cut, $k$-Cut, and Steiner Multicut among others. Given a graph with edge costs, terminal groups $(S_1, ..., S_g)$ and integer requirements $(r_1,... , r_g)$; the goal is to compute a minimum-cost edge cut that separates each group $S_i$ into at least $r_i$ connected components. Despite many efforts, the best known approximation for Requirement Cut yields a double-logarithmic $O(log(g).\\log(n))$ approximation ratio as it relies on embedding general graphs into trees and solving the tree instance. In this paper, we explore two largely unstudied structural parameters in order to obtain single-logarithmic approximation ratios: (1) the number of minimal Steiner trees in the instance, which in particular is upper-bounded by the number of spanning trees of the graphs multiplied by $g$, and (2) the depth of series-parallel graphs. Specifically, we show that if the number of minimal Steiner trees is polynomial in $n$, then a simple LP-rounding algorithm yields an $O(log n)$-approximation, and if the graph is series-parallel with a constant depth then a refined analysis of a known probabilistic embedding yields a $O(this http URL(g))$-approximation on series-parallel graphs of bounded depth. Both results extend the known class of graphs that have a single-logarithmic approximation ratio.         ",
    "url": "https://arxiv.org/abs/2505.21433",
    "authors": [
      "Nadym Mallek",
      "Kirill Simonov"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2505.21451",
    "title": "Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication",
    "abstract": "           Conversational breakdowns in close relationships are deeply shaped by personal histories and emotional context, yet most NLP research treats conflict detection as a general task, overlooking the relational dynamics that influence how messages are perceived. In this work, we leverage nonviolent communication (NVC) theory to evaluate LLMs in detecting conversational breakdowns and assessing how relationship backstory influences both human and model perception of conflicts. Given the sensitivity and scarcity of real-world datasets featuring conflict between familiar social partners with rich personal backstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772 naturalistic simulated dialogues spanning diverse conflict scenarios between friends, family members, and romantic partners. Through a controlled human study, we annotate a subset of dialogues and obtain fine-grained labels of communication breakdown types on individual turns, and assess the impact of backstory on human and model perception of conflict in conversation. We find that the polarity of relationship backstories significantly shifted human perception of communication breakdowns and impressions of the social partners, yet models struggle to meaningfully leverage those backstories in the detection task. Additionally, we find that models consistently overestimate how positively a message will make a listener feel. Our findings underscore the critical role of personalization to relationship contexts in enabling LLMs to serve as effective mediators in human communication for authentic connection.         ",
    "url": "https://arxiv.org/abs/2505.21451",
    "authors": [
      "Jocelyn Shen",
      "Akhila Yerukola",
      "Xuhui Zhou",
      "Cynthia Breazeal",
      "Maarten Sap",
      "Hae Won Park"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.21462",
    "title": "M3S-UPD: Efficient Multi-Stage Self-Supervised Learning for Fine-Grained Encrypted Traffic Classification with Unknown Pattern Discovery",
    "abstract": "           The growing complexity of encrypted network traffic presents dual challenges for modern network management: accurate multiclass classification of known applications and reliable detection of unknown traffic patterns. Although deep learning models show promise in controlled environments, their real-world deployment is hindered by data scarcity, concept drift, and operational constraints. This paper proposes M3S-UPD, a novel Multi-Stage Self-Supervised Unknown-aware Packet Detection framework that synergistically integrates semi-supervised learning with representation analysis. Our approach eliminates artificial segregation between classification and detection tasks through a four-phase iterative process: 1) probabilistic embedding generation, 2) clustering-based structure discovery, 3) distribution-aligned outlier identification, and 4) confidence-aware model updating. Key innovations include a self-supervised unknown detection mechanism that requires neither synthetic samples nor prior knowledge, and a continuous learning architecture that is resistant to performance degradation. Experimental results show that M3S-UPD not only outperforms existing methods on the few-shot encrypted traffic classification task, but also simultaneously achieves competitive performance on the zero-shot unknown traffic discovery task.         ",
    "url": "https://arxiv.org/abs/2505.21462",
    "authors": [
      "Yali Yuan",
      "Yu Huang",
      "Xingjian Zeng",
      "Hantao Mei",
      "Guang Cheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2505.21468",
    "title": "Causal Posterior Estimation",
    "abstract": "           We present Causal Posterior Estimation (CPE), a novel method for Bayesian inference in simulator models, i.e., models where the evaluation of the likelihood function is intractable or too computationally expensive, but where one can simulate model outputs given parameter values. CPE utilizes a normalizing flow-based (NF) approximation to the posterior distribution which carefully incorporates the conditional dependence structure induced by the graphical representation of the model into the neural network. Thereby it is possible to improve the accuracy of the approximation. We introduce both discrete and continuous NF architectures for CPE and propose a constant-time sampling procedure for the continuous case which reduces the computational complexity of drawing samples to O(1) as for discrete NFs. We show, through an extensive experimental evaluation, that by incorporating the conditional dependencies induced by the graphical model directly into the neural network, rather than learning them from data, CPE is able to conduct highly accurate posterior inference either outperforming or matching the state of the art in the field.         ",
    "url": "https://arxiv.org/abs/2505.21468",
    "authors": [
      "Simon Dirmeier",
      "Antonietta Mira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.21473",
    "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction",
    "abstract": "           This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.         ",
    "url": "https://arxiv.org/abs/2505.21473",
    "authors": [
      "Yiheng Liu",
      "Liao Qu",
      "Huichao Zhang",
      "Xu Wang",
      "Yi Jiang",
      "Yiming Gao",
      "Hu Ye",
      "Xian Li",
      "Shuai Wang",
      "Daniel K. Du",
      "Shu Cheng",
      "Zehuan Yuan",
      "Xinglong Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21486",
    "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming",
    "abstract": "           Automating robust hypothesis generation in open environments is pivotal for AI cognition. We introduce a novel framework integrating a multi-agent system, powered by Large Language Models (LLMs), with Inductive Logic Programming (ILP). Our system's LLM agents autonomously define a structured symbolic vocabulary (predicates) and relational templates , i.e., \\emph{language bias} directly from raw textual data. This automated symbolic grounding (the construction of the language bias), traditionally an expert-driven bottleneck for ILP, then guides the transformation of text into facts for an ILP solver, which inductively learns interpretable rules. This approach overcomes traditional ILP's reliance on predefined symbolic structures and the noise-sensitivity of pure LLM methods. Extensive experiments in diverse, challenging scenarios validate superior performance, paving a new path for automated, explainable, and verifiable hypothesis generation.         ",
    "url": "https://arxiv.org/abs/2505.21486",
    "authors": [
      "Yang Yang",
      "Jiemin Wu",
      "Yutao Yue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.21494",
    "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
    "abstract": "           Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21494",
    "authors": [
      "Xiaojun Jia",
      "Sensen Gao",
      "Simeng Qin",
      "Tianyu Pang",
      "Chao Du",
      "Yihao Huang",
      "Xinfeng Li",
      "Yiming Li",
      "Bo Li",
      "Yang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21499",
    "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
    "abstract": "           Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.21499",
    "authors": [
      "Haowei Wang",
      "Junjie Wang",
      "Xiaojun Jia",
      "Rupeng Zhang",
      "Mingyang Li",
      "Zhe Liu",
      "Yang Liu",
      "Qing Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20299",
    "title": "MetamatBench: Integrating Heterogeneous Data, Computational Tools, and Visual Interface for Metamaterial Discovery",
    "abstract": "           Metamaterials, engineered materials with architected structures across multiple length scales, offer unprecedented and tunable mechanical properties that surpass those of conventional materials. However, leveraging advanced machine learning (ML) for metamaterial discovery is hindered by three fundamental challenges: (C1) Data Heterogeneity Challenge arises from heterogeneous data sources, heterogeneous composition scales, and heterogeneous structure categories; (C2) Model Complexity Challenge stems from the intricate geometric constraints of ML models, which complicate their adaptation to metamaterial structures; and (C3) Human-AI Collaboration Challenge comes from the \"dual black-box'' nature of sophisticated ML models and the need for intuitive user interfaces. To tackle these challenges, we introduce a unified framework, named MetamatBench, that operates on three levels. (1) At the data level, we integrate and standardize 5 heterogeneous, multi-modal metamaterial datasets. (2) The ML level provides a comprehensive toolkit that adapts 17 state-of-the-art ML methods for metamaterial discovery. It also includes a comprehensive evaluation suite with 12 novel performance metrics with finite element-based assessments to ensure accurate and reliable model validation. (3) The user level features a visual-interactive interface that bridges the gap between complex ML techniques and non-ML researchers, advancing property prediction and inverse design of metamaterials for research and applications. MetamatBench offers a unified platform deployed at this http URL that enables machine learning researchers and practitioners to develop and evaluate new methodologies in metamaterial discovery. For accessibility and reproducibility, we open-source our benchmark and the codebase at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.20299",
    "authors": [
      "Jianpeng Chen",
      "Wangzhi Zhan",
      "Haohui Wang",
      "Zian Jia",
      "Jingru Gan",
      "Junkai Zhang",
      "Jingyuan Qi",
      "Tingwei Chen",
      "Lifu Huang",
      "Muhao Chen",
      "Ling Li",
      "Wei Wang",
      "Dawei Zhou"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20301",
    "title": "Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering",
    "abstract": "           A pivotal area of research in antibody engineering is to find effective modifications that enhance antibody-antigen binding affinity. Traditional wet-lab experiments assess mutants in a costly and time-consuming manner. Emerging deep learning solutions offer an alternative by modeling antibody structures to predict binding affinity changes. However, they heavily depend on high-quality complex structures, which are frequently unavailable in practice. Therefore, we propose ProtAttBA, a deep learning model that predicts binding affinity changes based solely on the sequence information of antibody-antigen complexes. ProtAttBA employs a pre-training phase to learn protein sequence patterns, following a supervised training phase using labeled antibody-antigen complex data to train a cross-attention-based regressor for predicting binding affinity changes. We evaluated ProtAttBA on three open benchmarks under different conditions. Compared to both sequence- and structure-based prediction methods, our approach achieves competitive performance, demonstrating notable robustness, especially with uncertain complex structures. Notably, our method possesses interpretability from the attention mechanism. We show that the learned attention scores can identify critical residues with impacts on binding affinity. This work introduces a rapid and cost-effective computational tool for antibody engineering, with the potential to accelerate the development of novel therapeutic antibodies.         ",
    "url": "https://arxiv.org/abs/2505.20301",
    "authors": [
      "Chen Liu",
      "Mingchen Li",
      "Yang Tan",
      "Wenrui Gou",
      "Guisheng Fan",
      "Bingxin Zhou"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20332",
    "title": "An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images",
    "abstract": "           Accurate identification of breast cancer types plays a critical role in guiding treatment decisions and improving patient outcomes. This paper presents an artificial intelligence enabled tool designed to aid in the identification of breast cancer types using histopathological biopsy images. Traditionally additional tests have to be done on women who are detected with breast cancer to find out the types of cancer it is to give the necessary cure. Those tests are not only invasive but also delay the initiation of treatment and increase patient burden. The proposed model utilizes a convolutional neural network (CNN) architecture to distinguish between benign and malignant tissues as well as accurate subclassification of breast cancer types. By preprocessing the images to reduce noise and enhance features, the model achieves reliable levels of classification performance. Experimental results on such datasets demonstrate the model's effectiveness, outperforming several existing solutions in terms of accuracy, precision, recall, and F1-score. The study emphasizes the potential of deep learning techniques in clinical diagnostics and offers a promising tool to assist pathologists in breast cancer classification.         ",
    "url": "https://arxiv.org/abs/2505.20332",
    "authors": [
      "Neil Chaudhary",
      "Zaynah Dhunny"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20361",
    "title": "Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks",
    "abstract": "           Despite the remarkable progress of physics-informed neural networks (PINNs) in scientific computing, they continue to face challenges when solving hydrodynamic problems with multiple discontinuities. In this work, we propose Separation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such problems. By sequentially resolving discontinuities from strong to weak and leveraging transfer learning during training, ST-PINNs significantly reduce the problem complexity and enhance solution accuracy. To the best of our knowledge, this is the first study to apply a PINNs-based approach to the two-dimensional unsteady planar shock refraction problem, offering new insights into the application of PINNs to complex shock-interface interactions. Numerical experiments demonstrate that ST-PINNs more accurately capture sharp discontinuities and substantially reduce solution errors in hydrodynamic problems involving multiple discontinuities.         ",
    "url": "https://arxiv.org/abs/2505.20361",
    "authors": [
      "Chuanxing Wang",
      "Hui Luo",
      "Kai Wang",
      "Guohuai Zhu",
      "Mingxing Luo"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20477",
    "title": "Robust fine-tuning of speech recognition models via model merging: application to disordered speech",
    "abstract": "           Automatic Speech Recognition (ASR) has advanced with Speech Foundation Models (SFMs), yet performance degrades on dysarthric speech due to variability and limited data. This study as part of the submission to the Speech Accessibility challenge, explored model merging to improve ASR generalization using Whisper as the base SFM. We compared fine-tuning with single-trajectory merging, combining models from one fine-tuning path, and multi-run merging, merging independently trained models. Our best multi-run merging approach achieved a 12% relative decrease of WER over classic fine-tuning, and a 16.2% relative decrease on long-form audios, a major loss contributor in dysarthric ASR. Merging more and more models led to continuous gains, remained effective in low-data regimes, and generalized across model architectures. These results highlight model merging as an easily replicable adaptation method that consistently improves ASR without additional inference cost or hyperparameter tuning.         ",
    "url": "https://arxiv.org/abs/2505.20477",
    "authors": [
      "Alexandre Ducorroy",
      "Rachid Riad"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2505.20480",
    "title": "BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics",
    "abstract": "           Decoding speech directly from neural activity is a central goal in brain-computer interface (BCI) research. In recent years, exciting advances have been made through the growing use of intracranial field potential recordings, such as stereo-ElectroEncephaloGraphy (sEEG) and ElectroCorticoGraphy (ECoG). These neural signals capture rich population-level activity but present key challenges: (i) task-relevant neural signals are sparsely distributed across sEEG electrodes, and (ii) they are often entangled with task-irrelevant neural signals in both sEEG and ECoG. To address these challenges, we introduce a unified Coarse-to-Fine neural disentanglement framework, BrainStratify, which includes (i) identifying functional groups through spatial-context-guided temporal-spatial modeling, and (ii) disentangling distinct neural dynamics within the target functional group using Decoupled Product Quantization (DPQ). We evaluate BrainStratify on two open-source sEEG datasets and one (epidural) ECoG dataset, spanning tasks like vocal production and speech perception. Extensive experiments show that BrainStratify, as a unified framework for decoding speech from intracranial neural signals, significantly outperforms previous decoding methods. Overall, by combining data-driven stratification with neuroscience-inspired modularity, BrainStratify offers a robust and interpretable solution for speech decoding from intracranial recordings.         ",
    "url": "https://arxiv.org/abs/2505.20480",
    "authors": [
      "Hui Zheng",
      "Hai-Teng Wang",
      "Yi-Tao Jing",
      "Pei-Yang Lin",
      "Han-Qing Zhao",
      "Wei Chen",
      "Peng-Hu Wei",
      "Yong-Zhi Shan",
      "Guo-Guang Zhao",
      "Yun-Zhe Liu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computation and Language (cs.CL)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2505.20536",
    "title": "Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models",
    "abstract": "           This paper studies the task of estimating heterogeneous treatment effects in causal panel data models, in the presence of covariate effects. We propose a novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models, that employs flexible model structures and powerful neural network architectures to cohesively deal with the underlying heterogeneity and nonlinearity of both panel units and covariate effects. The proposed CoDEAL integrates nonlinear covariate effect components (parameterized by a feed-forward neural network) with nonlinear factor structures (modeled by a multi-output autoencoder) to form a heterogeneous causal panel model. The nonlinear covariate component offers a flexible framework for capturing the complex influences of covariates on outcomes. The nonlinear factor analysis enables CoDEAL to effectively capture both cross-sectional and temporal dependencies inherent in the data panel. This latent structural information is subsequently integrated into a customized matrix completion algorithm, thereby facilitating more accurate imputation of missing counterfactual outcomes. Moreover, the use of a multi-output autoencoder explicitly accounts for heterogeneity across units and enhances the model interpretability of the latent factors. We establish theoretical guarantees on the convergence of the estimated counterfactuals, and demonstrate the compelling performance of the proposed method using extensive simulation studies and a real data application.         ",
    "url": "https://arxiv.org/abs/2505.20536",
    "authors": [
      "Guanhao Zhou",
      "Yuefeng Han",
      "Xiufan Yu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2505.20635",
    "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction",
    "abstract": "           Audio-visual speaker extraction isolates a target speaker's speech from a mixture speech signal conditioned on a visual cue, typically using the target speaker's face recording. However, in real-world scenarios, other co-occurring faces are often present on-screen, providing valuable speaker activity cues in the scene. In this work, we introduce a plug-and-play inter-speaker attention module to process these flexible numbers of co-occurring faces, allowing for more accurate speaker extraction in complex multi-person environments. We integrate our module into two prominent models: the AV-DPRNN and the state-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets, including the highly overlapped VoxCeleb2 and sparsely overlapped MISP, demonstrate that our approach consistently outperforms baselines. Furthermore, cross-dataset evaluations on LRS2 and LRS3 confirm the robustness and generalizability of our method.         ",
    "url": "https://arxiv.org/abs/2505.20635",
    "authors": [
      "Zexu Pan",
      "Shengkui Zhao",
      "Tingting Wang",
      "Kun Zhou",
      "Yukun Ma",
      "Chong Zhang",
      "Bin Ma"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ]
  },
  {
    "id": "arXiv:2505.20731",
    "title": "Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data",
    "abstract": "           Electronic Health Records (EHR) offer rich real-world data for personalized medicine, providing insights into disease progression, treatment responses, and patient outcomes. However, their sparsity, heterogeneity, and high dimensionality make them difficult to model, while the lack of standardized ground truth further complicates predictive modeling. To address these challenges, we propose SCORE, a semi-supervised representation learning framework that captures multi-domain disease profiles through patient embeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model with pre-trained code embeddings to characterize codified features and extract meaningful patient phenotypes and embeddings. To handle the computational challenges of large-scale data, it introduces a hybrid Expectation-Maximization (EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited labeled data to refine estimates on a vast pool of unlabeled samples. We theoretically establish the convergence of this hybrid approach, quantify GVA errors, and derive SCORE's error rate under diverging embedding dimensions. Our analysis shows that incorporating unlabeled data enhances accuracy and reduces sensitivity to label scarcity. Extensive simulations confirm SCORE's superior finite-sample performance over existing methods. Finally, we apply SCORE to predict disability status for patients with multiple sclerosis (MS) using partially labeled EHR data, demonstrating that it produces more informative and predictive patient embeddings for multiple MS-related conditions compared to existing approaches.         ",
    "url": "https://arxiv.org/abs/2505.20731",
    "authors": [
      "Linshanshan Wang",
      "Mengyan Li",
      "Zongqi Xia",
      "Molei Liu",
      "Tianxi Cai"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.20810",
    "title": "The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective",
    "abstract": "           Retinal imaging has emerged as a powerful, non-invasive modality for detecting and quantifying biomarkers of systemic diseases-ranging from diabetes and hypertension to Alzheimer's disease and cardiovascular disorders but current insights remain dispersed across platforms and specialties. Recent technological advances in optical coherence tomography (OCT/OCTA) and adaptive optics (AO) now deliver ultra-high-resolution scans (down to 5 {\\mu}m ) with superior contrast and spatial integration, allowing early identification of microvascular abnormalities and neurodegenerative changes. At the same time, AI-driven and machine learning (ML) algorithms have revolutionized the analysis of large-scale retinal datasets, increasing sensitivity and specificity; for example, deep learning models achieve > 90 \\% sensitivity for diabetic retinopathy and AUC = 0.89 for the prediction of cardiovascular risk from fundus photographs. The proliferation of mobile health technologies and telemedicine platforms further extends access, reduces costs, and facilitates community-based screening and longitudinal monitoring. Despite these breakthroughs, translation into routine practice is hindered by heterogeneous imaging protocols, limited external validation of AI models, and integration challenges within clinical workflows. In this review, we systematically synthesize the latest OCT/OCT and AO developments, AI/ML approaches, and mHealth/Tele-ophthalmology initiatives and quantify their diagnostic performance across disease domains. Finally, we propose a roadmap for multicenter protocol standardization, prospective validation trials, and seamless incorporation of retinal screening into primary and specialty care pathways-paving the way for precision prevention, early intervention, and ongoing treatment of life-threatening systemic diseases.         ",
    "url": "https://arxiv.org/abs/2505.20810",
    "authors": [
      "Tariq M Khan",
      "Toufique Ahmed Soomro",
      "Imran Razzak"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.21139",
    "title": "Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach",
    "abstract": "           The COVID-19 pandemic has significantly increased the incidence of post-infection cardiovascular events, particularly myocardial infarction, in individuals over 40. While the underlying mechanisms remain elusive, this study employs a hybrid machine learning approach to analyze epidemiological data in assessing 13 key heart attack risk factors and their susceptibility. Based on a unique dataset that combines demographic, biochemical, ECG, and thallium stress-tests, this study categorizes distinct subpopulations against varying risk profiles and then divides the population into 'at-risk' (AR) and 'not-at-risk' (NAR) groups using clustering algorithms. The study reveals strong association between the likelihood of experiencing a heart attack on the 13 risk factors studied. The aggravated risk for postmenopausal patients indicates compromised individual risk factors due to estrogen depletion that may be, further compromised by extraneous stress impacts, like anxiety and fear, aspects that have traditionally eluded data modeling predictions.         ",
    "url": "https://arxiv.org/abs/2505.21139",
    "authors": [
      "Subhagata Chattopadhyay",
      "Amit K Chattopadhyay"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2505.21208",
    "title": "Input Convex Kolmogorov Arnold Networks",
    "abstract": "           This article presents an input convex neural network architecture using Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the first is based on a low-order, linear-by-part, representation of functions, and a universal approximation theorem is provided. The second is based on cubic splines, for which only numerical results support convergence. We demonstrate on simple tests that these networks perform competitively with classical input convex neural networks (ICNNs). In a second part, we use the networks to solve some optimal transport problems needing a convex approximation of functions and demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs produce results similar to those of classical ICNNs.         ",
    "url": "https://arxiv.org/abs/2505.21208",
    "authors": [
      "Thomas Deschatre",
      "Xavier Warin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2505.21450",
    "title": "Pushing Cops and Robber on Graphs of Maximum Degree 4",
    "abstract": "           \\textsc{Cops and Robber} is a game played on graphs where a set of \\textit{cops} aim to \\textit{capture} the position of a single \\textit{robber}. The main parameter of interest in this game is the \\textit{cop number}, which is the minimum number of cops that are sufficient to guarantee the capture of the robber. In a directed graph $\\overrightarrow{G}$, the \\textit{push} operation on a vertex $v$ reverses the orientation of all arcs incident on $v$. We consider a variation of classical \\textsc{Cops and Robber} on oriented graphs, where in its turn, each cop can either move to an out-neighbor of its current vertex or push some vertex of the graph, whereas, the robber can move to an adjacent vertex in its turn. [Das et al., CALDAM, 2023] introduced this variant and established that if $\\overrightarrow{G}$ is an orientation of a subcubic graph, then one cop with push ability has a winning strategy. We extend these results to establish that if $\\overrightarrow{G}$ is an orientation of a $3$-degenerate graph, or of a graph with maximum degree $4$, then one cop with push ability has a winning strategy.         ",
    "url": "https://arxiv.org/abs/2505.21450",
    "authors": [
      "Harmender Gahlawat"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2304.12244",
    "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
    "abstract": "           Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL ",
    "url": "https://arxiv.org/abs/2304.12244",
    "authors": [
      "Can Xu",
      "Qingfeng Sun",
      "Kai Zheng",
      "Xiubo Geng",
      "Pu Zhao",
      "Jiazhan Feng",
      "Chongyang Tao",
      "Qingwei Lin",
      "Daxin Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.08568",
    "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "abstract": "           Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at this https URL ",
    "url": "https://arxiv.org/abs/2306.08568",
    "authors": [
      "Ziyang Luo",
      "Can Xu",
      "Pu Zhao",
      "Qingfeng Sun",
      "Xiubo Geng",
      "Wenxiang Hu",
      "Chongyang Tao",
      "Jing Ma",
      "Qingwei Lin",
      "Daxin Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.03635",
    "title": "CLEVRER-Humans: Describing Physical and Causal Events the Human Way",
    "abstract": "           Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting the great challenges set forth by our benchmark.         ",
    "url": "https://arxiv.org/abs/2310.03635",
    "authors": [
      "Jiayuan Mao",
      "Xuelin Yang",
      "Xikun Zhang",
      "Noah D. Goodman",
      "Jiajun Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2312.09866",
    "title": "PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment",
    "abstract": "           Neural implicit scene representations have recently shown encouraging results in dense visual SLAM. However, existing methods produce low-quality scene reconstruction and low-accuracy localization performance when scaling up to large indoor scenes and long sequences. These limitations are mainly due to their single, global radiance field with finite capacity, which does not adapt to large scenarios. Their end-to-end pose networks are also not robust enough with the growth of cumulative errors in large scenes. To this end, we introduce PLGSLAM, a neural visual SLAM system capable of high-fidelity surface reconstruction and robust camera tracking in real-time. To handle large-scale indoor scenes, PLGSLAM proposes a progressive scene representation method which dynamically allocates new local scene representation trained with frames within a local sliding window. This allows us to scale up to larger indoor scenes and improves robustness (even under pose drifts). In local scene representation, PLGSLAM utilizes tri-planes for local high-frequency features with multi-layer perceptron (MLP) networks for the low-frequency feature, achieving smoothness and scene completion in unobserved areas. Moreover, we propose local-to-global bundle adjustment method with a global keyframe database to address the increased pose drifts on long sequences. Experimental results demonstrate that PLGSLAM achieves state-of-the-art scene reconstruction results and tracking performance across various datasets and scenarios (both in small and large-scale indoor environments). The code is open-sourced at this https URL.         ",
    "url": "https://arxiv.org/abs/2312.09866",
    "authors": [
      "Tianchen Deng",
      "Guole Shen",
      "Tong Qin",
      "Jianyu Wang",
      "Wentao Zhao",
      "Jingchuan Wang",
      "Danwei Wang",
      "Weidong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.03846",
    "title": "UOD: Unseen Object Detection in 3D Point Cloud",
    "abstract": "           Existing 3D object detectors encounter extreme challenges in localizing unseen 3D objects and recognizing them as unseen, which is a crucial technology in autonomous driving in the wild. To address these challenges, we propose practical methods to enhance the performance of 3D detection and Out-Of-Distribution (OOD) classification for unseen objects. The proposed methods include anomaly sample augmentation, learning of universal objectness, learning of detecting unseen objects, and learning of distinguishing unseen objects. To demonstrate the effectiveness of our approach, we propose the KITTI Misc benchmark and two additional synthetic OOD benchmarks: the Nuscenes OOD benchmark and the SUN-RGBD OOD benchmark. The proposed methods consistently enhance performance by a large margin across all existing methods, giving insight for future work on unseen 3D object detection in the wild.         ",
    "url": "https://arxiv.org/abs/2401.03846",
    "authors": [
      "Hyunjun Choi",
      "Daeho Um",
      "Hawook Jeong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2401.16230",
    "title": "Elementary first-order model checking for sparse graphs",
    "abstract": "           It is known that for subgraph-closed graph classes the first-order model checking problem is fixed-parameter tractable if and only if the class is nowhere dense [Grohe, Kreutzer, Siebertz, STOC 2014]. However, the dependency on the formula size is non-elementary, and in fact, this is unavoidable even for the class of all trees [Frick and Grohe, LICS 2002]. On the other hand, it is known that the dependency is elementary for classes of bounded degree [Frick and Grohe, LICS 2002] as well as for classes of bounded pathwidth [Lampis, ICALP 2023]. In this paper we generalise these results and almost completely characterise subgraph-closed graph classes for which the model checking problem is fixed-parameter tractable with an elementary dependency on the formula size. Those are the graph classes for which there exists a number $d$ such that for every $r$, some tree of depth $d$ and size bounded by an elementary function of $r$ is avoided as an $({\\leq} r)$-subdivision in all graphs in the class. In particular, this implies that if the class in question excludes a fixed tree as a topological minor, then first-order model checking for graphs in the class is fixed-parameter tractable with an elementary dependency on the formula size.         ",
    "url": "https://arxiv.org/abs/2401.16230",
    "authors": [
      "Jakub Gajarsk\u00fd",
      "Micha\u0142 Pilipczuk",
      "Marek Soko\u0142owski",
      "Giannos Stamoulis",
      "Szymon Toru\u0144czyk"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)",
      "Logic (math.LO)"
    ]
  },
  {
    "id": "arXiv:2402.16678",
    "title": "The Complexity of Diameter on H-free graphs",
    "abstract": "           The intensively studied Diameter problem is to find the diameter of a given connected graph. We investigate, for the first time in a structured manner, the complexity of Diameter for H-free graphs, that is, graphs that do not contain a fixed graph H as an induced subgraph. We first show that if H is not a linear forest with small components, then Diameter cannot be solved in subquadratic time for H-free graphs under SETH. For some small linear forests, we do show linear-time algorithms for solving Diameter. For other linear forests H, we make progress towards linear-time algorithms by considering specific diameter values. If H is a linear forest, the maximum value of the diameter of any graph in a connected H-free graph class is some constant dmax dependent only on H. We give linear-time algorithms for deciding if a connected H-free graph has diameter dmax, for several linear forests H. In contrast, for one such linear forest H, Diameter cannot be solved in subquadratic time for H-free graphs under SETH. Moreover, we even show that, for several other linear forests H, one cannot decide in subquadratic time if a connected H-free graph has diameter dmax under SETH.         ",
    "url": "https://arxiv.org/abs/2402.16678",
    "authors": [
      "Jelle J. Oostveen",
      "Dani\u00ebl Paulusma",
      "Erik Jan van Leeuwen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2403.15105",
    "title": "SAGraph: A Large-Scale Social Graph Dataset with Comprehensive Context for Influencer Selection in Marketing",
    "abstract": "           Influencer marketing campaign success heavily depends on identifying key opinion leaders who can effectively leverage their credibility and reach to promote products or services. The selecting influencers process is vital for boosting brand visibility, fostering consumer trust, and driving sales. While traditional research often simplifies complex factors like user attitudes, interaction frequency, and advertising content, into simple numerical values. However, this reductionist approach fails to capture the dynamic nature of influencer marketing effectiveness. To bridge this gap, we present SAGraph, a novel comprehensive dataset from Weibo that captures multi-dimensional marketing campaign data across six product domains. The dataset encompasses 345,039 user profiles with their complete interaction histories, including 1.3M comments and 554K reposts across 44K posts, providing unprecedented granularity in influencer marketing dynamics. SAGraph uniquely integrates user profiles, content features, and temporal interaction patterns, enabling in-depth analysis of influencer marketing mechanisms. Experimental results using both traditional baselines and state-of-the-art large language models (LLMs) demonstrate the crucial role of content analysis in predicting advertising effectiveness. Our findings reveal that LLM-based approaches achieve superior performance in understanding and predicting campaign success, opening new avenues for data-driven influencer marketing strategies. We hope that this dataset will inspire further research this https URL.         ",
    "url": "https://arxiv.org/abs/2403.15105",
    "authors": [
      "Xiaoqing Zhang",
      "Yuhan Liu",
      "Jianzhou Wang",
      "Zhenxing Hu",
      "Xiuying Chen",
      "Rui Yan"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2404.04183",
    "title": "RACS-SADL: Robust and Understandable Randomized Consensus in the Cloud",
    "abstract": "           Widely deployed consensus protocols in the cloud are often leader-based and optimized for low latency under synchronous network conditions. However, cloud networks can experience disruptions such as network partitions, high-loss links, and configuration errors. These disruptions interfere with the operation of leader-based protocols, as their view change mechanisms interrupt the normal case replication and cause the system to stall. We propose RACS, a novel randomized consensus protocol that ensures robustness against adversarial network conditions. RACS achieves optimal one-round trip latency under synchronous network conditions while remaining resilient to adversarial network conditions. RACS follows a simple design inspired by Raft, the most widely used consensus protocol in the cloud, and therefore enables seamless integration with the existing cloud software stack. Experiments with a prototype running on Amazon EC2 show that RACS achieves 28k cmd/sec throughput, ninefold higher than Raft under adversarial cloud network conditions. Under synchronous network conditions, RACS matches the performance of Multi-Paxos and Raft, achieving a throughput of 200k cmd/sec with a median latency of 300ms, confirming that RACS introduces no unnecessary overhead. Finally, SADL-RACS, a throughput-optimized version of RACS, achieves a throughput of 500k cmd/sec, delivering 150% higher throughput than Raft.         ",
    "url": "https://arxiv.org/abs/2404.04183",
    "authors": [
      "Pasindu Tennage",
      "Antoine Desjardins",
      "Lefteris Kokoris-Kogias"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2405.00604",
    "title": "Toward Unified Practices in Trajectory Prediction Research on Bird's-Eye-View Datasets",
    "abstract": "           The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles. This paper highlights the need to standardize the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this. Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualization, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems. The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies. The toolbox is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2405.00604",
    "authors": [
      "Theodor Westny",
      "Bj\u00f6rn Olofsson",
      "Erik Frisk"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2405.07072",
    "title": "Focused digital cohort selection from social media using the metric backbone of biomedical knowledge graphs",
    "abstract": "           Social media data allows researchers to construct large digital cohorts to study the interplay between human behavior and medical this http URL the users most relevant to a specific health problem is, however, a challenge in that social media sites vary in the generality of their discourse. To filter relevant users on any social media, we have developed a general method and tested it on epilepsy discourse. We analyzed the text from posts by users who mention epilepsy drugs at least once in the general-purpose social media sites X and Instagram, the epilepsy-focused Reddit subgroup (r/Epilepsy), and the Epilepsy Foundation of America (EFA) forums. We used a curated medical terminology dictionary to generate a knowledge graph (KG) from each social media site, whereby nodes represent terms, and edge weights denote the strength of association between pairs of terms in the collected text. Our method is based on computing the metric backbone of each KG, which yields the subgraph of edges that participate in shortest paths. By comparing the subset of users who contribute to the backbone to the subset who do not, we show that epilepsy-focused social media users contribute to the KG backbone in much higher proportion than do general-purpose social media users. Furthermore, using human annotation of Instagram posts, we demonstrate that users who do not contribute to the backbone are much more likely to use dictionary terms in a manner inconsistent with their biomedical meaning and are rightly excluded from the cohort of interest.         ",
    "url": "https://arxiv.org/abs/2405.07072",
    "authors": [
      "Ziqi Guo",
      "Jack Felag",
      "Jordan C. Rozum",
      "Rion Brattig Correia",
      "Xuan Wang",
      "Luis M. Rocha"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2405.13268",
    "title": "Stochastic Online Conformal Prediction with Semi-Bandit Feedback",
    "abstract": "           Conformal prediction has emerged as an effective strategy for uncertainty quantification by modifying a model to output sets of labels instead of a single label. These prediction sets come with the guarantee that they contain the true label with high probability. However, conformal prediction typically requires a large calibration dataset of i.i.d. examples. We consider the online learning setting, where examples arrive over time, and the goal is to construct prediction sets dynamically. Departing from existing work, we assume semi-bandit feedback, where we only observe the true label if it is contained in the prediction set. For instance, consider calibrating a document retrieval model to a new domain; in this setting, a user would only be able to provide the true label if the target document is in the prediction set of retrieved documents. We propose a novel conformal prediction algorithm targeted at this setting, and prove that it obtains sublinear regret compared to the optimal conformal predictor. We evaluate our algorithm on a retrieval task, an image classification task, and an auction price-setting task, and demonstrate that it empirically achieves good performance compared to several baselines.         ",
    "url": "https://arxiv.org/abs/2405.13268",
    "authors": [
      "Haosen Ge",
      "Hamsa Bastani",
      "Osbert Bastani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2406.03143",
    "title": "ZeroPur: Succinct Training-Free Adversarial Purification",
    "abstract": "           Adversarial purification is a kind of defense technique that can defend against various unseen adversarial attacks without modifying the victim classifier. Existing methods often depend on external generative models or cooperation between auxiliary functions and victim classifiers. However, retraining generative models, auxiliary functions, or victim classifiers relies on the domain of the fine-tuned dataset and is computation-consuming. In this work, we suppose that adversarial images are outliers of the natural image manifold, and the purification process can be considered as returning them to this manifold. Following this assumption, we present a simple adversarial purification method without further training to purify adversarial images, called ZeroPur. ZeroPur contains two steps: given an adversarial example, Guided Shift obtains the shifted embedding of the adversarial example by the guidance of its blurred counterparts; after that, Adaptive Projection constructs a directional vector by this shifted embedding to provide momentum, projecting adversarial images onto the manifold adaptively. ZeroPur is independent of external models and requires no retraining of victim classifiers or auxiliary functions, relying solely on victim classifiers themselves to achieve purification. Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) using various classifier architectures (ResNet, WideResNet) demonstrate that our method achieves state-of-the-art robust performance. The code will be publicly available.         ",
    "url": "https://arxiv.org/abs/2406.03143",
    "authors": [
      "Erhu Liu",
      "Zonglin Yang",
      "Bo Liu",
      "Bin Xiao",
      "Xiuli Bi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.04461",
    "title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation",
    "abstract": "           Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. While previous diffusion-based graph generation methods have shown promising results, they often struggle to scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive RandOm Walk Diffusion), a novel random walk-based diffusion approach for efficient large-scale graph generation. Our method encompasses two components in an iterative process of random walk sampling and graph pruning. We demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing other baseline methods in terms of both generation time and multiple graph statistics, reflecting the high quality of the generated graphs.         ",
    "url": "https://arxiv.org/abs/2408.04461",
    "authors": [
      "Tobias Bernecker",
      "Ghalia Rehawi",
      "Francesco Paolo Casale",
      "Janine Knauer-Arloth",
      "Annalisa Marsico"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.04520",
    "title": "Advancing Molecular Machine Learning Representations with Stereoelectronics-Infused Molecular Graphs",
    "abstract": "           Molecular representation is a critical element in our understanding of the physical world and the foundation for modern molecular machine learning. Previous molecular machine learning models have employed strings, fingerprints, global features, and simple molecular graphs that are inherently information-sparse representations. However, as the complexity of prediction tasks increases, the molecular representation needs to encode higher fidelity information. This work introduces a novel approach to infusing quantum-chemical-rich information into molecular graphs via stereoelectronic effects, enhancing expressivity and interpretability. Learning to predict the stereoelectronics-infused representation with a tailored double graph neural network workflow enables its application to any downstream molecular machine learning task without expensive quantum chemical calculations. We show that the explicit addition of stereoelectronic information significantly improves the performance of message-passing 2D machine learning models for molecular property prediction. We show that the learned representations trained on small molecules can accurately extrapolate to much larger molecular structures, yielding chemical insight into orbital interactions for previously intractable systems, such as entire proteins, opening new avenues of molecular design. Finally, we have developed a web application (this http URL) where users can rapidly explore stereoelectronic information for their own molecular systems.         ",
    "url": "https://arxiv.org/abs/2408.04520",
    "authors": [
      "Daniil A. Boiko",
      "Thiago Resch\u00fctzegger",
      "Benjamin Sanchez-Lengeling",
      "Samuel M. Blau",
      "Gabe Gomes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ]
  },
  {
    "id": "arXiv:2408.08105",
    "title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Discern Causal Links Across Modalities",
    "abstract": "           Multimodal Large Language Models (MLLMs) have showcased exceptional Chain-of-Thought (CoT) reasoning ability in complex textual inference tasks including causal reasoning. However, will these causalities remain straightforward when crucial hints hide in visual details? If not, what factors might influence cross-modal generalization? Whether we can effectively enhance their capacity for robust causal inference across both text and vision? Motivated by these, we introduce MuCR - a novel Multimodal Causal Reasoning benchmark that leverages synthetic siamese images and text pairs to challenge MLLMs. Additionally, we develop tailored metrics from multiple perspectives, including image-level match, phrase-level understanding, and sentence-level explanation, to comprehensively assess MLLMs' comprehension abilities. Our experiments reveal that current MLLMs fall short in multimodal causal reasoning compared to their performance in purely textual settings. Additionally, we find that identifying visual cues across images is key to effective cross-modal generalization. Finally, we propose a VcCoT strategy that better highlights visual cues, and our results confirm its efficacy in enhancing multimodal causal reasoning. The project is available at: this https URL ",
    "url": "https://arxiv.org/abs/2408.08105",
    "authors": [
      "Zhiyuan Li",
      "Heng Wang",
      "Dongnan Liu",
      "Chaoyi Zhang",
      "Ao Ma",
      "Jieting Long",
      "Weidong Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.04183",
    "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding",
    "abstract": "           Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.         ",
    "url": "https://arxiv.org/abs/2409.04183",
    "authors": [
      "Ziyin Zhang",
      "Hang Yu",
      "Shijie Li",
      "Peng Di",
      "Jianguo Li",
      "Rui Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2409.06084",
    "title": "Symmetry constrained neural networks for detection and localization of damage in metal plates",
    "abstract": "           The present paper is concerned with deep learning techniques applied to detection and localization of damage in a thin aluminum plate. We used data collected on a tabletop apparatus by mounting to the plate four piezoelectric transducers, each of which took turn to generate a Lamb wave that then traversed the region of interest before being received by the remaining three sensors. On training a neural network to analyze time-series data of the material response, which displayed damage-reflective features whenever the plate guided waves interacted with a contact load, we achieved a model that detected with greater than $99\\%$ accuracy in addition to a model that localized with $2.58 \\pm 0.12$ mm mean distance error. For each task, the best-performing model was designed according to the inductive bias that our transducers were both similar and arranged in a square pattern on a nearly uniform plate.         ",
    "url": "https://arxiv.org/abs/2409.06084",
    "authors": [
      "James Amarel",
      "Christopher Rudolf",
      "Athanasios Iliopoulos",
      "John Michopoulos",
      "Leslie N. Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2409.18594",
    "title": "\"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree\": Zero-Shot Decision Tree Induction and Embedding with Large Language Models",
    "abstract": "           Large language models (LLMs) provide powerful means to leverage prior knowledge for predictive modeling when data is limited. In this work, we demonstrate how LLMs can use their compressed world knowledge to generate intrinsically interpretable machine learning models, i.e., decision trees, without any training data. We find that these zero-shot decision trees can even surpass data-driven trees on some small-sized tabular datasets and that embeddings derived from these trees perform better than data-driven tree-based embeddings on average. Our decision tree induction and embedding approaches can therefore serve as new knowledge-driven baselines for data-driven machine learning methods in the low-data regime. Furthermore, they offer ways to harness the rich world knowledge within LLMs for tabular machine learning tasks. Our code and results are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2409.18594",
    "authors": [
      "Ricardo Knauer",
      "Mario Koddenbrock",
      "Raphael Wallsberger",
      "Nicholas M. Brisson",
      "Georg N. Duda",
      "Deborah Falla",
      "David W. Evans",
      "Erik Rodner"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.01635",
    "title": "Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis",
    "abstract": "           In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.         ",
    "url": "https://arxiv.org/abs/2410.01635",
    "authors": [
      "Qunzhong Wang",
      "Xiangguo Sun",
      "Hong Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2410.01651",
    "title": "Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling",
    "abstract": "           Despite the success of Transformers, handling long contexts remains challenging due to the limited length generalization and quadratic complexity of self-attention. Thus Transformers often require post-training with a larger attention window, significantly increasing computational and memory costs. In this paper, we propose a novel attention mechanism based on dynamic context, Grouped Cross Attention (GCA), which can generalize to 1000 times the pre-training context length while maintaining the ability to access distant information with a constant attention window size. For a given input sequence, we split it into chunks and use each chunk to retrieve top-k relevant past chunks for subsequent text generation. Specifically, unlike most previous works that use an off-the-shelf retriever, our key innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner. Such a mechanism accommodates retrieved chunks with a fixed-size attention window to achieve long-range information access, significantly reducing computational and memory costs during training and inference. Experiments show that GCA-based models achieve near-perfect accuracy in passkey retrieval for 16M context lengths, which is 1000 times the training length.         ",
    "url": "https://arxiv.org/abs/2410.01651",
    "authors": [
      "Xiang Hu",
      "Zhihao Teng",
      "Jun Zhao",
      "Wei Wu",
      "Kewei Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2410.09516",
    "title": "Bridging the Gap Between Data-Driven And Theory-Driven Modelling - Leveraging Causal Machine Learning for Integrative Modelling of Dynamical Systems",
    "abstract": "           Classical machine learning techniques often struggle with overfitting and unreliable predictions when exposed to novel conditions. Introducing causality into the modelling process offers a promising way to mitigate these challenges by enhancing predictive robustness. However, constructing an initial causal graph manually using domain knowledge is time-consuming, particularly in complex time series with numerous variables. To address this, causal discovery algorithms can provide a preliminary causal structure that domain experts can refine. This study investigates causal feature selection with domain knowledge using a data center system as an example. We use simulated time-series data to compare different causal feature selection with traditional machine-learning feature selection methods. Our results show that predictions based on causal features are more robust compared to those derived from traditional methods. These findings underscore the potential of combining causal discovery algorithms with human expertise to improve machine learning applications.         ",
    "url": "https://arxiv.org/abs/2410.09516",
    "authors": [
      "David Zapata Gonzalez",
      "Marcel Meyer",
      "Oliver Mueller"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2410.09829",
    "title": "Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles",
    "abstract": "           Cyber-physical systems like autonomous vehicles are tested in simulation before deployment, using domain-specific programs for scenario specification. To aid the testing of autonomous vehicles in simulation, we design a natural language interface, using an instruction-following large language model, to assist a non-coding domain expert in synthesising the desired scenarios and vehicle behaviours. We show that using it to convert utterances to the symbolic program is feasible, despite the very small training dataset. Human experiments show that dialogue is critical to successful simulation generation, leading to a 4.5 times higher success rate than a generation without engaging in extended conversation.         ",
    "url": "https://arxiv.org/abs/2410.09829",
    "authors": [
      "Rimvydas Rubavicius",
      "Antonio Valerio Miceli-Barone",
      "Alex Lascarides",
      "Subramanian Ramamoorthy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2410.12128",
    "title": "Multimodal Fusion with Relational Learning for Molecular Property Prediction",
    "abstract": "           Graph based molecular representation learning is essential for accurately predicting molecular properties in drug discovery and materials science; however, it faces significant challenges due to the intricate relationships among molecules and the limited chemical knowledge utilized during training. While contrastive learning is often employed to handle molecular relationships, its reliance on binary metrics is insufficient for capturing the complexity of these interactions. Multimodal fusion has gained attention for property reasoning, but previous work has explored only a limited range of modalities, and the optimal stages for fusing different modalities in molecular property tasks remain underexplored. In this paper, we introduce MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction), a novel framework designed to overcome these limitations. Our method enhances embedding initialization through multimodal pretraining using relational learning. We also conduct a systematic investigation into the impact of modality fusion at different stages such as early, intermediate, and late, highlighting their advantages and shortcomings. Extensive experiments on MoleculeNet benchmarks demonstrate that MMFRL significantly outperforms existing methods. Furthermore, MMFRL enables task-specific optimizations. Additionally, the explainability of MMFRL provides valuable chemical insights, emphasizing its potential to enhance real-world drug discovery applications.         ",
    "url": "https://arxiv.org/abs/2410.12128",
    "authors": [
      "Zhengyang Zhou",
      "Yunrui Li",
      "Pengyu Hong",
      "Hao Xu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2410.12458",
    "title": "The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph",
    "abstract": "           The performance of large language models (LLMs) is strongly influenced by the quality and diversity of data used during supervised fine-tuning (SFT). However, current data selection methods often prioritize one aspect over the other, resulting in suboptimal training outcomes. To address this, we formulate data selection as a set cover problem and present GraphFilter, a novel approach that balances both quality and diversity in data selection. GraphFilter models the dataset as a bipartite graph connecting sentences to their constituent n-grams, then employs a priority function that combines quality and diversity metrics multiplicatively. GraphFilter iteratively selects sentences with the highest priority, removes covered n-grams from the bipartite graph, and recomputes priorities to reflect the changing data landscape. We validate GraphFilter using three model backbones across six widely-used benchmarks, demonstrating that it outperforms nine existing baselines in both model performance and computational efficiency. Further analysis shows that our design choices lead to more effective subset selection, underscores the value of instruction diversity, and provides insights into how quality and diversity interact with different subset sizes.         ",
    "url": "https://arxiv.org/abs/2410.12458",
    "authors": [
      "Minghao Wu",
      "Thuy-Trang Vu",
      "Lizhen Qu",
      "Gholamreza Haffari"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.12671",
    "title": "New Paradigm of Adversarial Training: Releasing Accuracy-Robustness Trade-Off via Dummy Class",
    "abstract": "           Adversarial Training (AT) is one of the most effective methods to enhance the robustness of Deep Neural Networks (DNNs). However, existing AT methods suffer from an inherent accuracy-robustness trade-off. Previous works have studied this issue under the current AT paradigm, but still face over 10% accuracy reduction without significant robustness improvement over simple baselines such as PGD-AT. This inherent trade-off raises a question: Whether the current AT paradigm, which assumes to learn corresponding benign and adversarial samples as the same class, inappropriately mixes clean and robust objectives that may be essentially inconsistent. In fact, our empirical results show that up to 40% of CIFAR-10 adversarial samples always fail to satisfy such an assumption across various AT methods and robust models, explicitly indicating the room for improvement of the current AT paradigm. To relax from this overstrict assumption and the tension between clean and robust learning, in this work, we propose a new AT paradigm by introducing an additional dummy class for each original class, aiming to accommodate hard adversarial samples with shifted distribution after perturbation. The robustness w.r.t. these adversarial samples can be achieved by runtime recovery from the predicted dummy classes to the corresponding original ones, without conflicting with the clean objective on accuracy of benign samples. Finally, based on our new paradigm, we propose a novel DUmmy Classes-based Adversarial Training (DUCAT) method that concurrently improves accuracy and robustness in a plug-and-play manner only relevant to logits, loss, and a proposed two-hot soft label-based supervised signal. Our method outperforms state-of-the-art (SOTA) benchmarks, effectively releasing the current trade-off. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2410.12671",
    "authors": [
      "Yanyun Wang",
      "Li Liu",
      "Zi Liang",
      "Yi R.",
      "Fung",
      "Qingqing Ye",
      "Haibo Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.14116",
    "title": "Robustness to Model Approximation, Model Learning From Data, and Sample Complexity in Wasserstein Regular MDPs",
    "abstract": "           The paper studies the robustness properties of discrete-time stochastic optimal control under Wasserstein model approximation for both discounted cost and average cost criteria. Specifically, we study the performance loss when applying an optimal policy designed for an approximate model to the true dynamics compared with the optimal cost for the true model under the sup-norm-induced metric, and relate it to the Wasserstein-1 distance between the approximate and true transition kernels. A primary motivation of this analysis is empirical model learning, as well as empirical noise distribution learning, where Wasserstein convergence holds under mild conditions but stronger convergence criteria, such as total variation, may not. We discuss applications of the results to the disturbance estimation problem, where sample complexity bounds are given, and also to a general empirical model learning approach, obtained under either Markov or i.i.d. learning settings.         ",
    "url": "https://arxiv.org/abs/2410.14116",
    "authors": [
      "Yichen Zhou",
      "Yanglei Song",
      "Serdar Y\u00fcksel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2410.17573",
    "title": "Securing Federated Learning against Backdoor Threats with Foundation Model Integration",
    "abstract": "           Federated Learning (FL) enables decentralized model training while preserving privacy. Recently, the integration of Foundation Models (FMs) into FL has enhanced performance but introduced a novel backdoor attack mechanism. Attackers can exploit FM vulnerabilities to embed backdoors into synthetic data generated by FMs. During global model fusion, these backdoors are transferred to the global model through compromised synthetic data, subsequently infecting all client models. Existing FL backdoor defenses are ineffective against this novel attack due to its fundamentally different mechanism compared to classic ones. In this work, we propose a novel data-free defense strategy that addresses both classic and novel backdoor attacks in FL. The shared attack pattern lies in the abnormal activations within the hidden feature space during model aggregation. Hence, we propose to constrain internal activations to remain within reasonable ranges, effectively mitigating attacks while preserving model functionality. The activation constraints are optimized using synthetic data alongside FL training. Extensive experiments demonstrate its effectiveness against both novel and classic backdoor attacks, outperforming existing defenses.         ",
    "url": "https://arxiv.org/abs/2410.17573",
    "authors": [
      "Xiaohuan Bi",
      "Xi Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2410.18122",
    "title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models",
    "abstract": "           This article introduces misinfo-general, a benchmark dataset for evaluating misinformation models' ability to perform out-of-distribution generalization. Misinformation changes rapidly, much more quickly than moderators can annotate at scale, resulting in a shift between the training and inference data distributions. As a result, misinformation detectors need to be able to perform out-of-distribution generalization, an attribute they currently lack. Our benchmark uses distant labelling to enable simulating covariate shifts in misinformation content. We identify time, event, topic, publisher, political bias, misinformation type as important axes for generalization, and we evaluate a common class of baseline models on each. Using article metadata, we show how this model fails desiderata, which is not necessarily obvious from classification metrics. Finally, we analyze properties of the data to ensure limited presence of modelling shortcuts. We make the dataset and accompanying code publicly available: this https URL ",
    "url": "https://arxiv.org/abs/2410.18122",
    "authors": [
      "Ivo Verhoeven",
      "Pushkar Mishra",
      "Ekaterina Shutova"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2410.21952",
    "title": "On the Robustness of Adversarial Training Against Uncertainty Attacks",
    "abstract": "           In learning problems, the noise inherent to the task at hand hinders the possibility to infer without a certain degree of uncertainty. Quantifying this uncertainty, regardless of its wide use, assumes high relevance for security-sensitive applications. Within these scenarios, it becomes fundamental to guarantee good (i.e., trustworthy) uncertainty measures, which downstream modules can securely employ to drive the final decision-making process. However, an attacker may be interested in forcing the system to produce either (i) highly uncertain outputs jeopardizing the system's availability or (ii) low uncertainty estimates, making the system accept uncertain samples that would instead require a careful inspection (e.g., human intervention). Therefore, it becomes fundamental to understand how to obtain robust uncertainty estimates against these kinds of attacks. In this work, we reveal both empirically and theoretically that defending against adversarial examples, i.e., carefully perturbed samples that cause misclassification, additionally guarantees a more secure, trustworthy uncertainty estimate under common attack scenarios without the need for an ad-hoc defense strategy. To support our claims, we evaluate multiple adversarial-robust models from the publicly available benchmark RobustBench on the CIFAR-10 and ImageNet datasets.         ",
    "url": "https://arxiv.org/abs/2410.21952",
    "authors": [
      "Emanuele Ledda",
      "Giovanni Scodeller",
      "Daniele Angioni",
      "Giorgio Piras",
      "Antonio Emanuele Cin\u00e0",
      "Giorgio Fumera",
      "Battista Biggio",
      "Fabio Roli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.14055",
    "title": "DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization",
    "abstract": "           Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, a method that dynamically adjusts the data distribution during training to restore balanced performance across heterogeneous and multi-tasking data. Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. Further analysis demonstrates the robustness of DRPruning towards various domains and distribution shifts. Furthermore, DRPruning can determine optimal reference losses and data ratios automatically, suggesting potential for broader applications. Code and scripts are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2411.14055",
    "authors": [
      "Hexuan Deng",
      "Wenxiang Jiao",
      "Xuebo Liu",
      "Jing Li",
      "Min Zhang",
      "Zhaopeng Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.11927",
    "title": "Transparent and Coherent Procedural Mistake Detection",
    "abstract": "           Procedural mistake detection (PMD) is a challenging problem of classifying whether a human user (observed through egocentric video) has successfully executed a task (specified by a procedural text). Despite significant recent efforts, machine performance in the wild remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we extend PMD to require generating visual self-dialog rationales to inform decisions. Given the impressive, mature image understanding capabilities observed in recent vision-and-language models (VLMs), we curate a suitable benchmark dataset for PMD based on individual frames. As our reformulation enables unprecedented transparency, we leverage a natural language inference (NLI) model to formulate two automated metrics for the coherence of generated rationales. We establish baselines for this reframed task, showing that while VLMs struggle off-the-shelf, their accuracy, coherence, and efficiency can be improved by incorporating these metrics into common inference and fine-tuning methods- though not without tradeoff. Lastly, our multi-faceted metrics visualize common outcomes, highlighting areas for further improvement.         ",
    "url": "https://arxiv.org/abs/2412.11927",
    "authors": [
      "Shane Storks",
      "Itamar Bar-Yossef",
      "Yayuan Li",
      "Zheyuan Zhang",
      "Jason J. Corso",
      "Joyce Chai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.13690",
    "title": "Personalized Clustering via Targeted Representation Learning",
    "abstract": "           Clustering traditionally aims to reveal a natural grouping structure within unlabeled data. However, this structure may not always align with users' preferences. In this paper, we propose a personalized clustering method that explicitly performs targeted representation learning by interacting with users via modicum task information (e.g., $\\textit{must-link}$ or $\\textit{cannot-link}$ pairs) to guide the clustering direction. We query users with the most informative pairs, i.e., those pairs most hard to cluster and those most easy to miscluster, to facilitate the representation learning in terms of the clustering preference. Moreover, by exploiting attention mechanism, the targeted representation is learned and augmented. By leveraging the targeted representation and constrained contrastive loss as well, personalized clustering is obtained. Theoretically, we verify that the risk of personalized clustering is tightly bounded, guaranteeing that active queries to users do mitigate the clustering risk. Experimentally, extensive results show that our method performs well across different clustering tasks and datasets, even when only a limited number of queries are available.         ",
    "url": "https://arxiv.org/abs/2412.13690",
    "authors": [
      "Xiwen Geng",
      "Suyun Zhao",
      "Yixin Yu",
      "Borui Peng",
      "Pan Du",
      "Hong Chen",
      "Cuiping Li",
      "Mengdie Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2412.16216",
    "title": "GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration",
    "abstract": "           The sparse Mixture-of-Experts (MoE) architecture of large language models (LLMs) confronts an inherent issue of load imbalance arising from the simplistic linear router strategy, which ultimately causes the instability and inefficient learning of LLMs. To address this challenge, we introduce a novel MoE graph-based framework $\\textbf{GMoE}$, aimed at enhancing the collaboration among multiple experts. In GMoE, a graph router function is designed to capture the collaboration signals among experts. This enables all experts to dynamically allocate information derived from input data by sharing information with their neighboring experts. Moreover, we put forward two coordination strategies in GMoE: the $\\textit{Poisson distribution-based distinction strategy}$ and the $\\textit{Normal distribution-based balance strategy}$, to further release the capacity of each expert and increase the model stability in the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph MoE architecture. Extensive experiments on four real-world benchmark datasets demonstrate the effectiveness of GMoE, showing the benefits of facilitating collaborations of multiple experts in LLM fine-tuning. The code of experimental implementation is available at this https URL ",
    "url": "https://arxiv.org/abs/2412.16216",
    "authors": [
      "Ting Bai",
      "Yue Yu",
      "Le Huang",
      "Zenan Xu",
      "Zhe Zhao",
      "Chuan Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2412.17629",
    "title": "Learn from Global Correlations: Enhancing Evolutionary Algorithm via Spectral GNN",
    "abstract": "           Evolutionary algorithms (EAs) simulate natural selection but have two main limitations: (1) they rarely update individuals based on global correlations, limiting comprehensive learning; (2) they struggle with balancing exploration and exploitation, where excessive exploitation causes premature convergence, and excessive exploration slows down the search. Moreover, EAs often depend on manual parameter settings, which can disrupt the exploration-exploitation this http URL address these issues, we propose Graph Neural Evolution (GNE), a novel EA framework. GNE represents the population as a graph, where nodes represent individuals, and edges capture their relationships, enabling global information usage. GNE utilizes spectral graph neural networks (GNNs) to decompose evolutionary signals into frequency components, applying a filtering function to fuse these components. High-frequency components capture diverse global information, while low-frequency ones capture more consistent information. This explicit frequency filtering strategy directly controls global-scale features through frequency components, overcoming the limitations of manual parameter settings and making the exploration-exploitation control more interpretable and this http URL tests on nine benchmark functions (e.g., Sphere, Rastrigin, Rosenbrock) show that GNE outperforms classical (GA, DE, CMA-ES) and advanced algorithms (SDAES, RL-SHADE) under various conditions, including noise-corrupted and optimal solution deviation scenarios. GNE achieves solutions several orders of magnitude better (e.g., 3.07e-20 mean on Sphere vs. 1.51e-07).         ",
    "url": "https://arxiv.org/abs/2412.17629",
    "authors": [
      "Kaichen Ouyang",
      "Shengwei Fu",
      "Zong Ke",
      "Renxiang Guan",
      "Ke Liang",
      "Dayu Hu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2412.17920",
    "title": "Causal Composition Diffusion Model for Closed-loop Traffic Generation",
    "abstract": "           Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort.         ",
    "url": "https://arxiv.org/abs/2412.17920",
    "authors": [
      "Haohong Lin",
      "Xin Huang",
      "Tung Phan-Minh",
      "David S. Hayden",
      "Huan Zhang",
      "Ding Zhao",
      "Siddhartha Srinivasa",
      "Eric M. Wolff",
      "Hongge Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2412.20367",
    "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey",
    "abstract": "           Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing large language models (LLMs) in code generation and optimization. This survey systematically reviews RL-driven techniques across the code development lifecycle, from compiler-level optimizations and resource allocation strategies to end-to-end code synthesis frameworks. We first examine classical and modern RL algorithms -- spanning policy gradients, actor-critic methods, human-feedback alignment, and preference-based optimization -- and their adaptations to the unique challenges of code generation, such as sparse and delayed rewards. Next, we analyze key benchmarks, datasets, and evaluation metrics that drive progress in RL-augmented Code LLMs. Finally, we identify open problems, including the need for richer feedback sources, support for low-level and domain-specific languages, and methods to reduce computational overhead. By consolidating current insights and outlining future directions, this work aims to guide researchers and practitioners in leveraging RL to produce more robust, efficient, and human-aligned code generation systems.         ",
    "url": "https://arxiv.org/abs/2412.20367",
    "authors": [
      "Junqiao Wang",
      "Zeng Zhang",
      "Yangfan He",
      "Zihao Zhang",
      "Yuyang Song",
      "Tianyu Shi",
      "Yuchen Li",
      "Hengyuan Xu",
      "Kunyu Wu",
      "Xin Yi",
      "Zhongwei Wan",
      "Xinhang Yuan",
      "Kuan Lu",
      "Menghao Huo",
      "Guangwu Qian",
      "Keqin Li",
      "Qiuwu Chen",
      "Lewei He"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2501.00701",
    "title": "ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals",
    "abstract": "           Analyzing the long-term behavior of high-dimensional nonlinear dynamical systems remains a significant challenge. While the Koopman operator framework provides a powerful global linearization tool, current methods for approximating its spectral components often face theoretical limitations and depend on predefined dictionaries. Residual Dynamic Mode Decomposition (ResDMD) advanced the field by introducing the \\emph{spectral residual} to assess Koopman operator approximation accuracy; however, its approach of only filtering precomputed spectra prevents the discovery of the operator's complete spectral information, a limitation known as the `spectral inclusion' problem. We introduce ResKoopNet (Residual-based Koopman-learning Network), a novel method that directly addresses this by explicitly minimizing the \\emph{spectral residual} to compute Koopman eigenpairs. This enables the identification of a more precise and complete Koopman operator spectrum. Using neural networks, our approach provides theoretical guarantees while maintaining computational adaptability. Experiments on a variety of physical and biological systems show that ResKoopNet achieves more accurate spectral approximations than existing methods, particularly for high-dimensional systems and those with continuous spectra, which demonstrates its effectiveness as a tool for analyzing complex dynamical systems.         ",
    "url": "https://arxiv.org/abs/2501.00701",
    "authors": [
      "Yuanchao Xu",
      "Kaidi Shao",
      "Nikos Logothetis",
      "Zhongwei Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ]
  },
  {
    "id": "arXiv:2501.05359",
    "title": "SC-Pro: Training-Free Framework for Defending Unsafe Image Synthesis Attack",
    "abstract": "           With advances in diffusion models, image generation has shown significant performance improvements. This raises concerns about the potential abuse of image generation, such as the creation of explicit or violent images, commonly referred to as Not Safe For Work (NSFW) content. To address this, the Stable Diffusion model includes several safety checkers to censor initial text prompts and final output images generated from the model. However, recent research has shown that these safety checkers have vulnerabilities against adversarial attacks, allowing them to generate NSFW images. In this paper, we find that these adversarial attacks are not robust to small changes in text prompts or input latents. Based on this, we propose SC-Pro (Spherical or Circular Probing), a training-free framework that easily defends against adversarial attacks generating NSFW images. Moreover, we develop an approach that utilizes one-step diffusion models for efficient NSFW detection (SC-Pro-o), further reducing computational resources. We demonstrate the superiority of our method in terms of performance and applicability.         ",
    "url": "https://arxiv.org/abs/2501.05359",
    "authors": [
      "Junha Park",
      "Jaehui Hwang",
      "Ian Ryu",
      "Hyungkeun Park",
      "Jiyoon Kim",
      "Jong-Seok Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.13229",
    "title": "The Yoneda embedding in simplicial type theory",
    "abstract": "           Riehl and Shulman introduced simplicial type theory (STT), a variant of homotopy type theory which aimed to study not just homotopy theory, but its fusion with category theory: $(\\infty,1)$-category theory. While notoriously technical, manipulating $\\infty$-categories in simplicial type theory is often easier than working with ordinary categories, with the type theory handling infinite stacks of coherences in the background. We capitalize on recent work by Gratzer et al. defining the $(\\infty,1)$-category of $\\infty$-groupoids in STT to define presheaf categories within STT and systematically develop their theory. In particular, we construct the Yoneda embedding, prove the universal property of presheaf categories, refine the theory of adjunctions in STT, introduce the theory of Kan extensions, and prove Quillen's Theorem A.         ",
    "url": "https://arxiv.org/abs/2501.13229",
    "authors": [
      "Daniel Gratzer",
      "Jonathan Weinberger",
      "Ulrik Buchholtz"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Algebraic Topology (math.AT)",
      "Category Theory (math.CT)"
    ]
  },
  {
    "id": "arXiv:2501.18326",
    "title": "Transductions of Graph Classes Admitting Product Structure",
    "abstract": "           In a quest to thoroughly understand the first-order transduction hierarchy of hereditary graph classes, some questions in particular stand out; such as, what properties hold for graph classes that are first-order transductions of planar graphs (and of similar classes)? When addressing this (so-far wide open) question, we turn to the concept of a product structure - being a subgraph of the strong product of a path and a graph of bounded tree-width, introduced by Dujmovic et al. [JACM 2020]. Namely, we prove that any graph class which is a first-order transduction of a class admitting such product structure, up to perturbations also meets a structural description generalizing the concept of a product structure in a dense hereditary way - the latter concept being introduced just recently by Hlineny and Jedelsky under the name of H-clique-width [MFCS 2024]. Using this characterization, we show that the class of the 3D grids, as well as a class of certain modifications of 2D grids, are not first-order transducible from classes admitting a product structure, and in particular not from the class of planar graphs.         ",
    "url": "https://arxiv.org/abs/2501.18326",
    "authors": [
      "Petr Hlin\u011bn\u00fd",
      "Jan Jedelsk\u00fd"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2502.04583",
    "title": "Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan",
    "abstract": "           We address the convergence problem in learning the Optimal Transport (OT) map, where the OT Map refers to a map from one distribution to another while minimizing the transport cost. Semi-dual Neural OT, a widely used approach for learning OT Maps with neural networks, often generates spurious solutions that fail to transfer one distribution to another accurately. We identify a sufficient condition under which the max-min solution of Semi-dual Neural OT recovers the true OT Map. Moreover, to address cases when this sufficient condition is not satisfied, we propose a novel method, OTP, which learns both the OT Map and the Optimal Transport Plan, representing the optimal coupling between two distributions. Under sharp assumptions on the distributions, we prove that our model eliminates the spurious solution issue and correctly solves the OT problem. Our experiments show that the OTP model recovers the optimal transport map where existing methods fail and outperforms current OT-based models in image-to-image translation tasks. Notably, the OTP model can learn stochastic transport maps when deterministic OT Maps do not exist, such as one-to-many tasks like colorization.         ",
    "url": "https://arxiv.org/abs/2502.04583",
    "authors": [
      "Jaemoo Choi",
      "Jaewoong Choi",
      "Dohyun Kwon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.06832",
    "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach",
    "abstract": "           Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their susceptibility to adversarial attacks presents a critical challenge for deployment in robust applications. This paper addresses the critical question of how to incorporate robustness into MoEs while maintaining high natural accuracy. We begin by analyzing the vulnerability of MoE components, finding that expert networks are notably more susceptible to adversarial attacks than the router. Based on this insight, we propose a targeted robust training technique that integrates a novel loss function to enhance the adversarial robustness of MoE, requiring only the robustification of one additional expert without compromising training or inference efficiency. Building on this, we introduce a dual-model strategy that linearly combines a standard MoE model with our robustified MoE model using a smoothing parameter. This approach allows for flexible control over the robustness-accuracy trade-off. We further provide theoretical foundations by deriving certified robustness bounds for both the single MoE and the dual-model. To push the boundaries of robustness and accuracy, we propose a novel joint training strategy JTDMoE for the dual-model. This joint training enhances both robustness and accuracy beyond what is achievable with separate models. Experimental results on CIFAR-10 and TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures demonstrate the effectiveness of our proposed methods. The code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2502.06832",
    "authors": [
      "Xu Zhang",
      "Kaidi Xu",
      "Ziqing Hu",
      "Ren Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.07746",
    "title": "HiPoNet: A Multi-View Simplicial Complex Network for High Dimensional Point-Cloud and Single-Cell Data",
    "abstract": "           In this paper, we propose HiPoNet, an end-to-end differentiable neural network for regression, classification, and representation learning on high-dimensional point clouds. Our work is motivated by single-cell data which can have very high-dimensionality --exceeding the capabilities of existing methods for point clouds which are mostly tailored for 3D data. Moreover, modern single-cell and spatial experiments now yield entire cohorts of datasets (i.e., one data set for every patient), necessitating models that can process large, high-dimensional point-clouds at scale. Most current approaches build a single nearest-neighbor graph, discarding important geometric and topological information. In contrast, HiPoNet models the point-cloud as a set of higher-order simplicial complexes, with each particular complex being created using a reweighting of features. This method thus generates multiple constructs corresponding to different views of high-dimensional data, which in biology offers the possibility of disentangling distinct cellular processes. It then employs simplicial wavelet transforms to extract multiscale features, capturing both local and global topology from each view. We show that geometric and topological information is preserved in this framework both theoretically and empirically. We showcase the utility of HiPoNet on point-cloud level tasks, involving classification and regression of entire point-clouds in data cohorts. Experimentally, we find that HiPoNet outperforms other point-cloud and graph-based models on single-cell data. We also apply HiPoNet to spatial transcriptomics datasets using spatial coordinates as one of the views. Overall, HiPoNet offers a robust and scalable solution for high-dimensional data analysis.         ",
    "url": "https://arxiv.org/abs/2502.07746",
    "authors": [
      "Siddharth Viswanath",
      "Hiren Madhu",
      "Dhananjay Bhaskar",
      "Jake Kovalic",
      "David R Johnson",
      "Christopher Tape",
      "Ian Adelstein",
      "Rex Ying",
      "Michael Perlmutter",
      "Smita Krishnaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Topology (math.AT)"
    ]
  },
  {
    "id": "arXiv:2502.12051",
    "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
    "abstract": "           Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.         ",
    "url": "https://arxiv.org/abs/2502.12051",
    "authors": [
      "Ayan Sengupta",
      "Yash Goel",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.12777",
    "title": "Evaluating link prediction: New perspectives and recommendations",
    "abstract": "           Link prediction (LP) is an important problem in network science and machine learning research. The state-of-the-art LP methods are usually evaluated in a uniform setup, ignoring several factors associated with the data and application specific needs. We identify a number of such factors, such as, network-type, problem-type, geodesic distance between the end nodes and its distribution over the classes, nature and applicability of LP methods, class imbalance and its impact on early retrieval, evaluation metric, etc., and present an experimental setup which allows us to evaluate LP methods in a rigorous and controlled manner. We perform extensive experiments with a variety of LP methods over real network datasets in this controlled setup, and gather valuable insights on the interactions of these factors with the performance of LP through an array of carefully designed hypotheses. Following the insights, we provide recommendations to be followed as best practice for evaluating LP methods.         ",
    "url": "https://arxiv.org/abs/2502.12777",
    "authors": [
      "Bhargavi Kalyani I",
      "A Rama Prasad Mathi",
      "Niladri Sett"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.13010",
    "title": "Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge",
    "abstract": "           Large Language Models (LLMs) have significantly advanced medical question-answering by leveraging extensive clinical data and medical literature. However, the rapid evolution of medical knowledge and the labor-intensive process of manually updating domain-specific resources pose challenges to the reliability of these systems. To address this, we introduce Agentic Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates the construction and continuous updating of medical knowledge graphs, integrates reasoning, and retrieves current external evidence, such as PubMed and WikiSearch. By dynamically linking new findings and complex medical concepts, AMG-RAG not only improves accuracy but also enhances interpretability in medical queries. Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of 66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to 100 times larger. Notably, these improvements are achieved without increasing computational overhead, highlighting the critical role of automated knowledge graph generation and external evidence retrieval in delivering up-to-date, trustworthy medical insights.         ",
    "url": "https://arxiv.org/abs/2502.13010",
    "authors": [
      "Mohammad Reza Rezaei",
      "Reza Saadati Fard",
      "Rahul G. Krishnan",
      "Milad Lankarany"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2502.14132",
    "title": "Can Community Notes Replace Professional Fact-Checkers?",
    "abstract": "           Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. Our results show that successful community moderation relies on professional fact-checking and highlight how citizen and professional fact-checking are deeply intertwined.         ",
    "url": "https://arxiv.org/abs/2502.14132",
    "authors": [
      "Nadav Borenstein",
      "Greta Warren",
      "Desmond Elliott",
      "Isabelle Augenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.14213",
    "title": "Communication-Efficient Solving of Linear Systems via Asynchronous Randomized Block Projections in ROS2 Networks",
    "abstract": "           This paper proposes an event-triggered asynchronous distributed randomized block Kaczmarz projection (ER-AD-RBKP) algorithm for efficiently solving large-scale linear systems in resource-constrained and communication-unstable environments. The algorithm enables each agent to update its local state estimate independently and engage in communication only when specific triggering conditions are satisfied, thereby significantly reducing communication overhead. At each iteration, agents project onto randomized subsets of local data blocks to lower computational cost and enhance scalability. From a theoretical standpoint, we establish exponential convergence conditions for the proposed algorithm. By defining events that ensure strong connectivity in the communication graph, we derive sufficient conditions for global convergence under a probabilistic framework. Our analysis guarantees that the algorithm is guaranteed to converge in expectation under mild probabilistic assumptions, provided that persistent agent isolation is avoided. For inconsistent systems, auxiliary variables are incorporated to transform the problem into an equivalent consistent formulation, and theoretical error bounds are derived. For practical evaluation, we implement the ER-AD-RBKP algorithm in an asynchronous communication environment built on ROS2, a distributed middleware framework for real-time robotic systems. We evaluate the algorithm under various settings, including varying numbers of agents, neighborhood sizes, communication intervals, and failure scenarios such as communication disruptions and processing faults. Experimental results demonstrate robust performance in terms of computational efficiency, communication cost, and system resilience, highlighting its strong potential for practical applicability in real-world distributed systems.         ",
    "url": "https://arxiv.org/abs/2502.14213",
    "authors": [
      "Yanchen Yin",
      "Yongli Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2502.17442",
    "title": "Thinking Before Running! Efficient Code Generation with Thorough Exploration and Optimal Refinement",
    "abstract": "           Code generation is crucial in software engineering for automating the coding process efficiently. While test-time computation methods show promise, they suffer from high latency due to multiple computation rounds. To overcome this, we introduce \\textbf{ThinkCoder}, a framework that combines thorough exploration with optimal refinement. The exploration phase diversifies the solution space by searching for potential solutions, followed by a refinement phase that enhances precision. This approach allows us to select the best solution through careful consideration before taking action, avoiding excessive trial and error. To further minimize test-time computation overhead, we introduce preference-driven optimization with Reinforced Self-Training (ReST), which uses exploration trajectories from ThinkCoder to guide LLM's evolution. This approach enhances LLM's exploration efficiency via preference learning, cutting costs while maintaining accuracy. ThinkCoder boosts the performance with a single LLM, excelling on benchmarks like HumanEval and MBPP. Compared to SOTA models, it improves Pass@1 by 3.0\\% over MapCoder with just 6.4\\% of the computation cost. Against AgentCoder, ThinkCoder achieves a 0.5\\% higher Pass@1 after 2 rounds, outperforming AgentCoder's 5 rounds. Additionally, ReST with success trajectories enhances efficiency, allowing models like LLaMA2-7B to achieve competitive results using only 20\\% of the computational resources. These results highlight the framework's effectiveness and scalability.         ",
    "url": "https://arxiv.org/abs/2502.17442",
    "authors": [
      "Xiaoqing Zhang",
      "Yuhan Liu",
      "Flood Sung",
      "Xiuying Chen",
      "Shuo Shang",
      "Rui Yan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.17817",
    "title": "Predicting Through Generation: Why Generation Is Better for Prediction",
    "abstract": "           This paper argues that generating output tokens is more effective than using pooled representations for prediction tasks because token-level generation retains more mutual information. Since LLMs are trained on massive text corpora using next-token prediction, generation aligns naturally with their learned behavior. Using the Data Processing Inequality (DPI), we provide both theoretical and empirical evidence supporting this claim. However, autoregressive models face two key challenges when used for prediction: (1) exposure bias, where the model sees ground truth tokens during training but relies on its own predictions during inference, leading to errors, and (2) format mismatch, where discrete tokens do not always align with the tasks required output structure. To address these challenges, we introduce PredGen(Predicting Through Generating), an end to end framework that (i) uses scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter to convert the generated tokens into structured outputs. Additionally, we introduce Writer-Director Alignment Loss (WDAL), which ensures consistency between token generation and final task predictions, improving both text coherence and numerical accuracy. We evaluate PredGen on multiple classification and regression benchmarks. Our results show that PredGen consistently outperforms standard baselines, demonstrating its effectiveness in structured prediction tasks.         ",
    "url": "https://arxiv.org/abs/2502.17817",
    "authors": [
      "Md Kowsher",
      "Nusrat Jahan Prottasha",
      "Prakash Bhat",
      "Chun-Nam Yu",
      "Mojtaba Soltanalian",
      "Ivan Garibay",
      "Ozlem Garibay",
      "Chen Chen",
      "Niloofar Yousefi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2502.18874",
    "title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework",
    "abstract": "           Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.         ",
    "url": "https://arxiv.org/abs/2502.18874",
    "authors": [
      "Kaishuai Xu",
      "Tiezheng Yu",
      "Wenjun Hou",
      "Yi Cheng",
      "Liangyou Li",
      "Xin Jiang",
      "Lifeng Shang",
      "Qun Liu",
      "Wenjie Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2502.20411",
    "title": "Backpropagation-free Spiking Neural Networks with the Forward-Forward Algorithm",
    "abstract": "           Spiking Neural Networks (SNNs) offer a biologically inspired computational paradigm that emulates neuronal activity through discrete spike-based processing. Despite their advantages, training SNNs with traditional backpropagation (BP) remains challenging due to computational inefficiencies and a lack of biological plausibility. This study explores the Forward-Forward (FF) algorithm as an alternative learning framework for SNNs. Unlike backpropagation, which relies on forward and backward passes, the FF algorithm employs two forward passes, enabling layer-wise localized learning, enhanced computational efficiency, and improved compatibility with neuromorphic hardware. We introduce an FF-based SNN training framework and evaluate its performance across both non-spiking (MNIST, Fashion-MNIST, Kuzushiji-MNIST) and spiking (Neuro-MNIST, SHD) datasets. Experimental results demonstrate that our model surpasses existing FF-based SNNs on evaluated static datasets with a much lighter architecture while achieving accuracy comparable to state-of-the-art backpropagation-trained SNNs. On more complex spiking tasks such as SHD, our approach outperforms other SNN models and remains competitive with leading backpropagation-trained SNNs. These findings highlight the FF algorithm's potential to advance SNN training methodologies by addressing some key limitations of backpropagation.         ",
    "url": "https://arxiv.org/abs/2502.20411",
    "authors": [
      "Mohammadnavid Ghader",
      "Saeed Reza Kheradpisheh",
      "Bahar Farahani",
      "Mahmood Fazlali"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.20606",
    "title": "Map Space Belief Prediction for Manipulation-Enhanced Mapping",
    "abstract": "           Searching for objects in cluttered environments requires selecting efficient viewpoints and manipulation actions to remove occlusions and reduce uncertainty in object locations, shapes, and categories. In this work, we address the problem of manipulation-enhanced semantic mapping, where a robot has to efficiently identify all objects in a cluttered shelf. Although Partially Observable Markov Decision Processes~(POMDPs) are standard for decision-making under uncertainty, representing unstructured interactive worlds remains challenging in this formalism. To tackle this, we define a POMDP whose belief is summarized by a metric-semantic grid map and propose a novel framework that uses neural networks to perform map-space belief updates to reason efficiently and simultaneously about object geometries, locations, categories, occlusions, and manipulation physics. Further, to enable accurate information gain analysis, the learned belief updates should maintain calibrated estimates of uncertainty. Therefore, we propose Calibrated Neural-Accelerated Belief Updates (CNABUs) to learn a belief propagation model that generalizes to novel scenarios and provides confidence-calibrated predictions for unknown areas. Our experiments show that our novel POMDP planner improves map completeness and accuracy over existing methods in challenging simulations and successfully transfers to real-world cluttered shelves in zero-shot fashion.         ",
    "url": "https://arxiv.org/abs/2502.20606",
    "authors": [
      "Joao Marcos Correia Marques",
      "Nils Dengler",
      "Tobias Zaenker",
      "Jesper Mucke",
      "Shenlong Wang",
      "Maren Bennewitz",
      "Kris Hauser"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.21033",
    "title": "A data augmentation strategy for deep neural networks with application to epidemic modelling",
    "abstract": "           In this work, we integrate the predictive capabilities of compartmental disease dynamics models with machine learning ability to analyze complex, high-dimensional data and uncover patterns that conventional models may overlook. Specifically, we present a proof of concept demonstrating the application of data-driven methods and deep neural networks to a recently introduced Susceptible-Infected-Recovered type model with social features, including a saturated incidence rate, to improve epidemic prediction and forecasting. Our results show that a robust data augmentation strategy trough suitable data-driven models can improve the reliability of Feed-Forward Neural Networks and Nonlinear Autoregressive Networks, providing a complementary strategy to Physics-Informed Neural Networks, particularly in settings where data augmentation from mechanistic models can enhance learning. This approach enhances the ability to handle nonlinear dynamics and offers scalable, data-driven solutions for epidemic forecasting, prioritizing predictive accuracy over the constraints of physics-based models. Numerical simulations of the lockdown and post-lockdown phase of the COVID-19 epidemic in Italy and Spain validate our methodology.         ",
    "url": "https://arxiv.org/abs/2502.21033",
    "authors": [
      "Muhammad Awais",
      "Abu Safyan Ali",
      "Giacomo Dimarco",
      "Federica Ferrarese",
      "Lorenzo Pareschi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)",
      "Populations and Evolution (q-bio.PE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2503.04615",
    "title": "HalluCounter: Reference-free LLM Hallucination Detection in the Wild!",
    "abstract": "           Response consistency-based, reference-free hallucination detection (RFHD) methods do not depend on internal model states, such as generation probabilities or gradients, which Grey-box models typically rely on but are inaccessible in closed-source LLMs. However, their inability to capture query-response alignment patterns often results in lower detection accuracy. Additionally, the lack of large-scale benchmark datasets spanning diverse domains remains a challenge, as most existing datasets are limited in size and scope. To this end, we propose HalluCounter, a novel reference-free hallucination detection method that utilizes both response-response and query-response consistency and alignment patterns. This enables the training of a classifier that detects hallucinations and provides a confidence score and an optimal response for user queries. Furthermore, we introduce HalluCounterEval, a benchmark dataset comprising both synthetically generated and human-curated samples across multiple domains. Our method outperforms state-of-the-art approaches by a significant margin, achieving over 90\\% average confidence in hallucination detection across datasets.         ",
    "url": "https://arxiv.org/abs/2503.04615",
    "authors": [
      "Ashok Urlana",
      "Gopichand Kanumolu",
      "Charaka Vinayak Kumar",
      "Bala Mallikarjunarao Garlapati",
      "Rahul Mishra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.05203",
    "title": "Path Pooling: Training-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation",
    "abstract": "           Although Large Language Models achieve strong success in many tasks, they still suffer from hallucinations and knowledge deficiencies in real-world applications. Many knowledge graph-based retrieval-augmented generation (KG-RAG) methods enhance the quality and credibility of LLMs by leveraging structure and semantic information in KGs as external knowledge bases. However, these methods struggle to effectively incorporate structure information, either incurring high computational costs or underutilizing available knowledge. Inspired by smoothing operations in graph representation learning, we propose path pooling, a simple, training-free strategy that introduces structure information through a novel path-centric pooling operation. It seamlessly integrates into existing KG-RAG methods in a plug-and-play manner, enabling richer structure information utilization. Extensive experiments demonstrate that incorporating the path pooling into the state-of-the-art KG-RAG method consistently improves performance across various settings while introducing negligible additional cost.         ",
    "url": "https://arxiv.org/abs/2503.05203",
    "authors": [
      "Hairu Wang",
      "Yuan Feng",
      "Xike Xie",
      "S Kevin Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2503.09128",
    "title": "FlexiReg: Flexible Urban Region Representation Learning",
    "abstract": "           The increasing availability of urban data offers new opportunities for learning region representations, which can be used as input to machine learning models for downstream tasks such as check-in or crime prediction. While existing solutions have produced promising results, an issue is their fixed formation of regions and fixed input region features, which may not suit the needs of different downstream tasks. To address this limitation, we propose a model named FlexiReg for urban region representation learning that is flexible with both the formation of urban regions and the input region features. FlexiReg is based on a spatial grid partitioning over the spatial area of interest. It learns representations for the grid cells, leveraging publicly accessible data, including POI, land use, satellite imagery, and street view imagery. We propose adaptive aggregation to fuse the cell representations and prompt learning techniques to tailor the representations towards different tasks, addressing the needs of varying formations of urban regions and downstream tasks. Extensive experiments on five real-world datasets demonstrate that FlexiReg outperforms state-of-the-art models by up to 202% in term of the accuracy of four diverse downstream tasks using the produced urban region representations.         ",
    "url": "https://arxiv.org/abs/2503.09128",
    "authors": [
      "Fengze Sun",
      "Yanchuan Chang",
      "Egemen Tanin",
      "Shanika Karunasekera",
      "Jianzhong Qi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2503.10042",
    "title": "How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game",
    "abstract": "           The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities.         ",
    "url": "https://arxiv.org/abs/2503.10042",
    "authors": [
      "Ziyue Wang",
      "Yurui Dong",
      "Fuwen Luo",
      "Minyuan Ruan",
      "Zhili Cheng",
      "Chi Chen",
      "Peng Li",
      "Yang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2503.13753",
    "title": "Compact routing schemes in undirected and directed graphs",
    "abstract": "           In this paper, we study the problem of compact routing schemes in weighted undirected and directed graphs. \\textit{For weighted undirected graphs}, more than a decade ago, Chechik [PODC'13] presented a $\\approx3.68k$-stretch compact routing scheme that uses $\\tilde{O}(n^{1/k}\\log{D})$ local storage, where $D$ is the normalized diameter, for every $k>1$. We present a $\\approx 2.64k$-stretch compact routing scheme that uses $\\tilde{O}(n^{1/k})$ local storage \\textit{on average} in each vertex. This is the first compact routing scheme that uses total local storage of $\\tilde{O}(n^{1+1/k})$ while achieving a $c \\cdot k$ stretch, for a constant $c < 3$. In real-world network protocols, messages are usually transformed as part of a communication session between two parties. Therefore, more than two decades ago, Thorup and Zwick [SPAA'01] considered compact routing schemes that establish a communication session using a handshake. In their handshake-based compact routing scheme, the handshake is routed along a $(4k-5)$-stretch path, and the rest of the communication session is routed along an optimal $(2k-1)$-stretch path. It is straightforward to improve the $(4k-5)$-stretch of the handshake to $\\approx3.68k$-stretch using the compact routing scheme of Chechik [PODC'13]. We improve the handshake stretch to the optimal $(2k-1)$, by borrowing the concept of roundtrip routing from directed graphs to \\textit{undirected} graphs. \\textit{For weighted directed graphs}, more than two decades ago, Roditty, Thorup, and Zwick [SODA'02 and TALG'08] presented a $(4k+\\eps)$-stretch compact roundtrip routing scheme that uses $\\tilde{O}(n^{1/k})$ local storage for every $k\\ge 3$. For $k=3$, this gives a $(12+\\eps)$-roundtrip stretch using $\\tilde{O}(n^{1/3})$ local storage. We improve the stretch by developing a $7$-roundtrip stretch routing scheme with $\\tilde{O}(n^{1/3})$ local storage.         ",
    "url": "https://arxiv.org/abs/2503.13753",
    "authors": [
      "Avi Kadria",
      "Liam Roditty"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2503.14228",
    "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images",
    "abstract": "           Person detection methods are used widely in applications including visual surveillance, pedestrian detection, and robotics. However, accurate detection of persons from overhead fisheye images remains an open challenge because of factors including person rotation and small-sized persons. To address the person rotation problem, we convert the fisheye images into panoramic images. For smaller people, we focused on the geometry of the panoramas. Conventional detection methods tend to focus on larger people because these larger people yield large significant areas for feature maps. In equirectangular panoramic images, we find that a person's height decreases linearly near the top of the images. Using this finding, we leverage the significance values and aggregate tokens that are sorted based on these values to balance the significant areas. In this leveraging process, we introduce panoramic distortion-aware tokenization. This tokenization procedure divides a panoramic image using self-similarity figures that enable determination of optimal divisions without gaps, and we leverage the maximum significant values in each tile of token groups to preserve the significant areas of smaller people. To achieve higher detection accuracy, we propose a person detection and localization method that combines panoramic-image remapping and the tokenization procedure. Extensive experiments demonstrated that our method outperforms conventional methods when applied to large-scale datasets.         ",
    "url": "https://arxiv.org/abs/2503.14228",
    "authors": [
      "Nobuhiko Wakai",
      "Satoshi Sato",
      "Yasunori Ishii",
      "Takayoshi Yamashita"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2503.14434",
    "title": "LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers",
    "abstract": "           Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.         ",
    "url": "https://arxiv.org/abs/2503.14434",
    "authors": [
      "Nikhil Abhyankar",
      "Parshin Shojaee",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2503.19666",
    "title": "Towards Efficient Training of Graph Neural Networks: A Multiscale Approach",
    "abstract": "           Graph Neural Networks (GNNs) have become powerful tools for learning from graph-structured data, finding applications across diverse domains. However, as graph sizes and connectivity increase, standard GNN training methods face significant computational and memory challenges, limiting their scalability and efficiency. In this paper, we present a novel framework for efficient multiscale training of GNNs. Our approach leverages hierarchical graph representations and subgraphs, enabling the integration of information across multiple scales and resolutions. By utilizing coarser graph abstractions and subgraphs, each with fewer nodes and edges, we significantly reduce computational overhead during training. Building on this framework, we propose a suite of scalable training strategies, including coarse-to-fine learning, subgraph-to-full-graph transfer, and multiscale gradient computation. We also provide some theoretical analysis of our methods and demonstrate their effectiveness across various datasets and learning tasks. Our results show that multiscale training can substantially accelerate GNN training for large scale problems while maintaining, or even improving, predictive performance.         ",
    "url": "https://arxiv.org/abs/2503.19666",
    "authors": [
      "Eshed Gal",
      "Moshe Eliasof",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Ivan I. Kyrchei",
      "Eldad Haber",
      "Eran Treister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2504.12898",
    "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models",
    "abstract": "           Despite significant progress, recent studies indicate that current large language models (LLMs) may still capture dataset biases and utilize them during inference, leading to the poor generalizability of LLMs. However, due to the diversity of dataset biases and the insufficient nature of bias suppression based on in-context learning, the effectiveness of previous prior knowledge-based debiasing methods and in-context learning based automatic debiasing methods is limited. To address these challenges, we explore the combination of causal mechanisms with information theory and propose an information gain-guided causal intervention debiasing (ICD) framework. To eliminate biases within the instruction-tuning dataset, it is essential to ensure that these biases do not provide any additional information to predict the answers, i.e., the information gain of these biases for predicting the answers needs to be 0. Under this guidance, this framework utilizes a causal intervention-based data rewriting method to automatically and autonomously balance the distribution of instruction-tuning dataset for reducing the information gain. Subsequently, it employs a standard supervised fine-tuning process to train LLMs on the debiased dataset. Experimental results show that ICD can effectively debias LLM to improve its generalizability across different tasks.         ",
    "url": "https://arxiv.org/abs/2504.12898",
    "authors": [
      "Zhouhao Sun",
      "Xiao Ding",
      "Li Du",
      "Yunpeng Xu",
      "Yixuan Ma",
      "Yang Zhao",
      "Bing Qin",
      "Ting Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2504.13055",
    "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
    "abstract": "           Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to better scale test-time compute remains largely underexplored. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective data augmentation method that mixes trajectories from both clean and moderately distorted images during RL training. By injecting targeted diversity in visual perception and the resulting reasoning patterns, NoisyRollout promotes better policy exploration through vision-oriented inductive biases, ultimately leading to more robust reasoning behaviors. We further adopt a noise annealing schedule that gradually reduces distortion strength over training, leveraging noisy signals early on while ensuring training stability in later stages. Crucially, our method is easy-to-adopt--requiring no additional training cost and no modifications to the RL objective. Extensive experiments on $2$ distinct training datasets demonstrate that NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models across $5$ out-of-domain reasoning and perception benchmarks. Furthermore, we validate the effectiveness of NoisyRollout across model sizes ($7$B and $32$B) and data scales (from $1$K to $6$K), highlighting its generalizability and scalability.         ",
    "url": "https://arxiv.org/abs/2504.13055",
    "authors": [
      "Xiangyan Liu",
      "Jinjie Ni",
      "Zijian Wu",
      "Chao Du",
      "Longxu Dou",
      "Haonan Wang",
      "Tianyu Pang",
      "Michael Qizhe Shieh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.00707",
    "title": "A high-order combined interpolation/finite element technique for evolutionary coupled groundwater-surface water problem",
    "abstract": "           A high-order combined interpolation/finite element technique is developed for solving the coupled groundwater-surface water system that governs flows in karst aquifers. In the proposed high-order scheme we approximate the time derivative with piecewise polynomial interpolation of second-order and use the finite element discretization of piecewise polynomials of degree $d$ and $d+1$, where $d \\geq 2$ is an integer, to approximate the space derivatives. The stability together with the error estimates of the constructed technique are established in $L^{\\infty}(0,T;\\text{\\,}L^{2})$-norm. The analysis suggests that the developed computational technique is unconditionally stable, temporal second-order accurate and convergence in space of order $d+1$. Furthermore, the new approach is faster and more efficient than a broad range of numerical methods discussed in the literature for the given initial-boundary value problem. Some examples are carried out to confirm the theoretical results.         ",
    "url": "https://arxiv.org/abs/2505.00707",
    "authors": [
      "Eric Ngondiep",
      "Areej A. Binsultan",
      "Ibtisam M. Aldawish"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2505.01675",
    "title": "Enhanced Prediction Model for Time Series Characterized by GARCH via Interval Type-2 Fuzzy Inference System",
    "abstract": "           GARCH-type time series (characterized by Generalized Autoregressive Conditional Heteroskedasticity) exhibit pronounced volatility, autocorrelation, and heteroskedasticity. To address these challenges and enhance predictive accuracy, this study introduces a hybrid forecasting framework that integrates the Interval Type-2 Fuzzy Inference System (IT2FIS) with the GARCH model. Leveraging the interval-based uncertainty representation of IT2FIS and the volatility-capturing capability of GARCH, the proposed model effectively mitigates the adverse impact of heteroskedasticity on prediction reliability. Specifically, the GARCH component estimates conditional variance, which is subsequently incorporated into the Gaussian membership functions of IT2FIS. This integration transforms IT2FIS into an adaptive variable-parameter system, dynamically aligning with the time-varying volatility of the target series. Through systematic parameter optimization, the framework not only captures intricate volatility patterns but also accounts for heteroskedasticity and epistemic uncertainties during modeling, thereby improving both prediction precision and model robustness. Experimental validation employs diverse datasets, including air quality concentration, urban traffic flow, and energy consumption. Comparative analyses are conducted against models: the GARCH-Takagi-Sugeno-Kang (GARCH-TSK) model, fixed-variance time series models, the GARCH-Gated Recurrent Unit (GARCH-GRU), and Long Short-Term Memory (LSTM) networks. The results indicate that the proposed model achieves superior predictive performance across the majority of test scenarios in error metrics. These findings underscore the effectiveness of hybrid approaches in forecasting uncertainty for GARCH-type time series, highlighting their practical utility in real-world time series forecasting applications.         ",
    "url": "https://arxiv.org/abs/2505.01675",
    "authors": [
      "Hongpei Shao",
      "Da-Qing Zhang",
      "Feilong Lu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2505.10222",
    "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention",
    "abstract": "           Transformer models rely on self-attention to capture token dependencies but face challenges in effectively integrating positional information while allowing multi-head attention (MHA) flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity. This paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA. CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. ComplexFormer incorporates two key improvements: (1) a per-head Euler transformation, converting real-valued query/key projections into polar-form complex vectors for head-specific complex subspace operation; and (2) a per-head adaptive differential rotation mechanism, exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct strategies for integrating semantic angle differences (ASmn,i) with relative positional encodings (Delta(Pmn),i). Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity , and improved long-context coherence compared to strong baselines like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.         ",
    "url": "https://arxiv.org/abs/2505.10222",
    "authors": [
      "Jintian Shao",
      "Hongyi Huang",
      "Jiayi Wu",
      "Beiwen Zhang",
      "ZhiYu Wu",
      "You Shan",
      "MingKai Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.11015",
    "title": "WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?",
    "abstract": "           The rapid advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced capabilities in Document Understanding. However, prevailing benchmarks like DocVQA and ChartQA predominantly comprise \\textit{scanned or digital} documents, inadequately reflecting the intricate challenges posed by diverse real-world scenarios, such as variable illumination and physical distortions. This paper introduces WildDoc, the inaugural benchmark designed specifically for assessing document understanding in natural environments. WildDoc incorporates a diverse set of manually captured document images reflecting real-world conditions and leverages document sources from established benchmarks to facilitate comprehensive comparisons with digital or scanned documents. Further, to rigorously evaluate model robustness, each document is captured four times under different conditions. Evaluations of state-of-the-art MLLMs on WildDoc expose substantial performance declines and underscore the models' inadequate robustness compared to traditional benchmarks, highlighting the unique challenges posed by real-world document understanding. Our project homepage is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2505.11015",
    "authors": [
      "An-Lan Wang",
      "Jingqun Tang",
      "Liao Lei",
      "Hao Feng",
      "Qi Liu",
      "Xiang Fei",
      "Jinghui Lu",
      "Han Wang",
      "Weiwei Liu",
      "Hao Liu",
      "Yuliang Liu",
      "Xiang Bai",
      "Can Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.11642",
    "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning",
    "abstract": "           Multi-agent systems leverage advanced AI models as autonomous agents that interact, cooperate, or compete to complete complex tasks across applications such as robotics and traffic management. Despite their growing importance, safety in multi-agent systems remains largely underexplored, with most research focusing on single AI models rather than interacting agents. This work investigates backdoor vulnerabilities in multi-agent systems and proposes a defense mechanism based on agent interactions. By leveraging reasoning abilities, each agent evaluates responses from others to detect illogical reasoning processes, which indicate poisoned agents. Experiments on LLM-based multi-agent systems, including ChatGPT series and Llama 3, demonstrate the effectiveness of the proposed method, achieving high accuracy in identifying poisoned agents while minimizing false positives on clean agents. We believe this work provides insights into multi-agent system safety and contributes to the development of robust, trustworthy AI interactions.         ",
    "url": "https://arxiv.org/abs/2505.11642",
    "authors": [
      "Falong Fan",
      "Xi Li"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.12155",
    "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds",
    "abstract": "           Segmentation evaluation metrics traditionally rely on binary decision logic: predictions are either correct or incorrect, based on rigid IoU thresholds. Detection--based metrics such as F1 and mAP determine correctness at the object level using fixed overlap cutoffs, while overlap--based metrics like Intersection over Union (IoU) and Dice operate at the pixel level, often overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify detection and segmentation assessment, but it remains dependent on hard-threshold matching--treating predictions below the threshold as entirely incorrect. This binary framing obscures important distinctions between qualitatively different errors and fails to reward gradual model improvements. We propose SoftPQ, a flexible and interpretable instance segmentation metric that redefines evaluation as a graded continuum rather than a binary classification. SoftPQ introduces tunable upper and lower IoU thresholds to define a partial matching region and applies a sublinear penalty function to ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit smoother score behavior, greater robustness to structural segmentation errors, and more informative feedback for model development and evaluation. Through controlled perturbation experiments, we show that SoftPQ captures meaningful differences in segmentation quality that existing metrics overlook, making it a practical and principled alternative for both benchmarking and iterative model refinement.         ",
    "url": "https://arxiv.org/abs/2505.12155",
    "authors": [
      "Ranit Karmakar",
      "Simon F. N\u00f8rrelykke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.13831",
    "title": "TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning",
    "abstract": "           The selection of base station sites is a critical challenge in 5G network planning, which requires efficient optimization of coverage, cost, user satisfaction, and practical constraints. Traditional manual methods, reliant on human expertise, suffer from inefficiencies and are limited to an unsatisfied planning-construction consistency. Existing AI tools, despite improving efficiency in certain aspects, still struggle to meet the dynamic network conditions and multi-objective needs of telecom operators' networks. To address these challenges, we propose TelePlanNet, an AI-driven framework tailored for the selection of base station sites, integrating a three-layer architecture for efficient planning and large-scale automation. By leveraging large language models (LLMs) for real-time user input processing and intent alignment with base station planning, combined with training the planning model using the improved group relative policy optimization (GRPO) reinforcement learning, the proposed TelePlanNet can effectively address multi-objective optimization, evaluates candidate sites, and delivers practical solutions. Experiments results show that the proposed TelePlanNet can improve the consistency to 78%, which is superior to the manual methods, providing telecom operators with an efficient and scalable tool that significantly advances cellular network planning.         ",
    "url": "https://arxiv.org/abs/2505.13831",
    "authors": [
      "Zongyuan Deng",
      "Yujie Cai",
      "Qing Liu",
      "Shiyao Mu",
      "Bin Lyu",
      "Zhen Yang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.14832",
    "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs",
    "abstract": "           Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial. We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.         ",
    "url": "https://arxiv.org/abs/2505.14832",
    "authors": [
      "Wonje Jeung",
      "Sangyeon Yoon",
      "Albert No"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.15536",
    "title": "DeepCEE: Efficient Cross-Region Model Distributed Training System under Heterogeneous GPUs and Networks",
    "abstract": "           Most existing training systems focus on a single region. In contrast, we envision that cross-region training offers more flexible GPU resource allocation and yields significant potential. However, the hierarchical cluster topology and unstable networks in the cloud-edge-end (CEE) environment, a typical cross-region scenario, pose substantial challenges to building an efficient and autonomous model training system. We propose DeepCEE, a geo-distributed model training system tailored for heterogeneous GPUs and networks in CEE environments. DeepCEE adopts a communication-centric design philosophy to tackle challenges arising from slow and unstable inter-region networks. It begins with a heterogeneous device profiler that identifies and groups devices based on both network and compute characteristics. Leveraging device groups, DeepCEE implements compact, zero-bubble pipeline parallelism, automatically deriving optimal parallel strategies. To further adapt to runtime variability, DeepCEE integrates a dynamic environment adapter that reacts to network fluctuations. Extensive evaluations demonstrate that DeepCEE achieves 1.3-2.8x higher training throughput compared to widely used and SOTA training systems.         ",
    "url": "https://arxiv.org/abs/2505.15536",
    "authors": [
      "Jinquan Wang",
      "Xiaojian Liao",
      "Xuzhao Liu",
      "Jiashun Suo",
      "Zhisheng Huo",
      "Chenhao Zhang",
      "Xiangrong Xu",
      "Runnan Shen",
      "Xilong Xie",
      "Limin Xiao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2505.16522",
    "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing",
    "abstract": "           Despite significant progress, recent studies have indicated that current large language models (LLMs) may still utilize bias during inference, leading to the poor generalizability of LLMs. Some benchmarks are proposed to investigate the generalizability of LLMs, with each piece of data typically containing one type of controlled bias. However, a single piece of data may contain multiple types of biases in practical applications. To bridge this gap, we propose a multi-bias benchmark where each piece of data contains five types of biases. The evaluations conducted on this benchmark reveal that the performance of existing LLMs and debiasing methods is unsatisfying, highlighting the challenge of eliminating multiple types of biases simultaneously. To overcome this challenge, we propose a causal effect estimation-guided multi-bias elimination method (CMBE). This method first estimates the causal effect of multiple types of biases simultaneously. Subsequently, we eliminate the causal effect of biases from the total causal effect exerted by both the semantic information and biases during inference. Experimental results show that CMBE can effectively eliminate multiple types of bias simultaneously to enhance the generalizability of LLMs.         ",
    "url": "https://arxiv.org/abs/2505.16522",
    "authors": [
      "Zhouhao Sun",
      "Zhiyuan Kan",
      "Xiao Ding",
      "Li Du",
      "Yang Zhao",
      "Bing Qin",
      "Ting Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.16670",
    "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models",
    "abstract": "           Large language models (LLMs) have shown impressive capabilities across a wide range of applications, but their ever-increasing size and resource demands make them vulnerable to inference cost attacks, where attackers induce victim LLMs to generate the longest possible output content. In this paper, we revisit existing inference cost attacks and reveal that these methods can hardly produce large-scale malicious effects since they are self-targeting, where attackers are also the users and therefore have to execute attacks solely through the inputs, whose generated content will be charged by LLMs and can only directly influence themselves. Motivated by these findings, this paper introduces a new type of inference cost attacks (dubbed 'bit-flip inference cost attack') that target the victim model itself rather than its inputs. Specifically, we design a simple yet effective method (dubbed 'BitHydra') to effectively flip critical bits of model parameters. This process is guided by a loss function designed to suppress <EOS> token's probability with an efficient critical bit search algorithm, thus explicitly defining the attack objective and enabling effective optimization. We evaluate our method on 11 LLMs ranging from 1.5B to 14B parameters under both int8 and float16 settings. Experimental results demonstrate that with just 4 search samples and as few as 3 bit flips, BitHydra can force 100% of test prompts to reach the maximum generation length (e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its efficiency, scalability, and strong transferability across unseen inputs.         ",
    "url": "https://arxiv.org/abs/2505.16670",
    "authors": [
      "Xiaobei Yan",
      "Yiming Li",
      "Zhaoxin Fan",
      "Han Qiu",
      "Tianwei Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.17473",
    "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics",
    "abstract": "           Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce OrionBench, a benchmark designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 26,250 real and 78,750 synthetic infographics, with over 6.9 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of OrionBench through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.         ",
    "url": "https://arxiv.org/abs/2505.17473",
    "authors": [
      "Jiangning Zhu",
      "Yuxing Zhou",
      "Zheng Wang",
      "Juntao Yao",
      "Yima Gu",
      "Yuhui Yuan",
      "Shixia Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.17708",
    "title": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations",
    "abstract": "           Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.         ",
    "url": "https://arxiv.org/abs/2505.17708",
    "authors": [
      "Dingling Yao",
      "Shimeng Huang",
      "Riccardo Cadei",
      "Kun Zhang",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.17854",
    "title": "Out of the Shadows: Exploring a Latent Space for Neural Network Verification",
    "abstract": "           Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a \"shadow\" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are \"shadows\" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24).         ",
    "url": "https://arxiv.org/abs/2505.17854",
    "authors": [
      "Lukas Koller",
      "Tobias Ladner",
      "Matthias Althoff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.18034",
    "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks",
    "abstract": "           Despite remarkable advances in the field, LLMs remain unreliable in distinguishing causation from correlation. Recent results from the Corr2Cause dataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score: 29.08) -- only marginally outperform random baselines (Random Uniform, F1 score: 20.38), indicating limited capacity of generalization. To tackle this limitation, we propose a novel structured approach: rather than directly answering causal queries, we provide the model with the capability to structure its thinking by guiding the model to build a structured knowledge graph, systematically encoding the provided correlational premises, to answer the causal queries. This intermediate representation significantly enhances the model's causal capabilities. Experiments on the test subset of the Corr2Cause dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains over standard direct prompting methods, improving F1 scores from 32.71 to 48.26 (over 47.5% relative increase), along with notable improvements in precision and recall. These results underscore the effectiveness of providing the model with the capability to structure its thinking and highlight its promising potential for broader generalization across diverse causal inference tasks.         ",
    "url": "https://arxiv.org/abs/2505.18034",
    "authors": [
      "Wentao Sun",
      "Jo\u00e3o Paulo Nogueira",
      "Alonso Silva"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2505.18596",
    "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models",
    "abstract": "           The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards robust and interpretable misinformation detection. The code will be open-sourced in a future release.         ",
    "url": "https://arxiv.org/abs/2505.18596",
    "authors": [
      "Chen Han",
      "Wenzhen Zheng",
      "Xijin Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.18781",
    "title": "Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains",
    "abstract": "           The very challenging task of learning solution operators of PDEs on arbitrary domains accurately and efficiently is of vital importance to engineering and industrial simulations. Despite the existence of many operator learning algorithms to approximate such PDEs, we find that accurate models are not necessarily computationally efficient and vice versa. We address this issue by proposing a geometry aware operator transformer (GAOT) for learning PDEs on arbitrary domains. GAOT combines novel multiscale attentional graph neural operator encoders and decoders, together with geometry embeddings and (vision) transformer processors to accurately map information about the domain and the inputs into a robust approximation of the PDE solution. Multiple innovations in the implementation of GAOT also ensure computational efficiency and scalability. We demonstrate this significant gain in both accuracy and efficiency of GAOT over several baselines on a large number of learning tasks from a diverse set of PDEs, including achieving state of the art performance on a large scale three-dimensional industrial CFD dataset.         ",
    "url": "https://arxiv.org/abs/2505.18781",
    "authors": [
      "Shizheng Wen",
      "Arsh Kumbhat",
      "Levi Lingsch",
      "Sepehr Mousavi",
      "Yizhou Zhao",
      "Praveen Chandrashekar",
      "Siddhartha Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.18942",
    "title": "Language Models Surface the Unwritten Code of Science and Society",
    "abstract": "           This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's \"unwritten code\" - such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 computer science conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs' normative priors about the internal characteristics of good science extracted from their self-talk, e.g. theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures. This shift reveals the primacy of scientific myths about intrinsic properties driving scientific excellence rather than extrinsic contextualization and storytelling that influence conceptions of relevance and significance. Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14), despite giving implicit reward to them with positive scores. We discuss the broad applicability of the framework, leveraging LLMs as diagnostic tools to surface the tacit codes underlying human society, enabling more precisely targeted responsible AI.         ",
    "url": "https://arxiv.org/abs/2505.18942",
    "authors": [
      "Honglin Bao",
      "Siyang Wu",
      "Jiwoong Choi",
      "Yingrong Mao",
      "James A. Evans"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)"
    ]
  },
  {
    "id": "arXiv:2505.19030",
    "title": "RECAST: Strengthening LLMs' Complex Instruction Following with Constraint-Verifiable Data",
    "abstract": "           Large language models (LLMs) are increasingly expected to tackle complex tasks, driven by their expanding applications and users' growing proficiency in crafting sophisticated prompts. However, as the number of explicitly stated requirements increases (particularly more than 10 constraints), LLMs often struggle to accurately follow such complex instructions. To address this challenge, we propose RECAST, a novel framework for synthesizing datasets where each example incorporates far more constraints than those in existing benchmarks. These constraints are extracted from real-world prompt-response pairs to ensure practical relevance. RECAST enables automatic verification of constraint satisfaction via rule-based validators for quantitative constraints and LLM-based validators for qualitative ones. Using this framework, we construct RECAST-30K, a large-scale, high-quality dataset comprising 30k instances spanning 15 constraint types. Experimental results demonstrate that models fine-tuned on RECAST-30K show substantial improvements in following complex instructions. Moreover, the verifiability provided by RECAST enables the design of reward functions for reinforcement learning, which further boosts model performance on complex and challenging tasks.         ",
    "url": "https://arxiv.org/abs/2505.19030",
    "authors": [
      "Wenhao Liu",
      "Zhengkang Guo",
      "Mingchen Xie",
      "Jingwen Xu",
      "Zisu Huang",
      "Muzhao Tian",
      "Jianhan Xu",
      "Muling Wu",
      "Xiaohua Wang",
      "Changze Lv",
      "He-Da Wang",
      "Hu Yao",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.19081",
    "title": "Towards Generalized Proactive Defense against Face Swapping with Contour-Hybrid Watermark",
    "abstract": "           Face swapping, recognized as a privacy and security concern, has prompted considerable defensive research. With the advancements in AI-generated content, the discrepancies between the real and swapped faces have become nuanced. Considering the difficulty of forged traces detection, we shift the focus to the face swapping purpose and proactively embed elaborate watermarks against unknown face swapping techniques. Given that the constant purpose is to swap the original face identity while preserving the background, we concentrate on the regions surrounding the face to ensure robust watermark generation, while embedding the contour texture and face identity information to achieve progressive image determination. The watermark is located in the facial contour and contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our approach generalizes face swapping detection without requiring any swapping techniques during training and the storage of large-scale messages in advance. Experiments conducted across 8 face swapping techniques demonstrate the superiority of our approach compared with state-of-the-art passive and proactive detectors while achieving a favorable balance between the image quality and watermark robustness.         ",
    "url": "https://arxiv.org/abs/2505.19081",
    "authors": [
      "Ruiyang Xia",
      "Dawei Zhou",
      "Decheng Liu",
      "Lin Yuan",
      "Jie Li",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.19286",
    "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models",
    "abstract": "           Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.         ",
    "url": "https://arxiv.org/abs/2505.19286",
    "authors": [
      "Utkarsh Sahu",
      "Zhisheng Qi",
      "Yongjia Lei",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Nesreen K. Ahmed",
      "Mahantesh M Halappanavar",
      "Yao Ma",
      "Yu Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2505.19528",
    "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection",
    "abstract": "           Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness.         ",
    "url": "https://arxiv.org/abs/2505.19528",
    "authors": [
      "Yejin Lee",
      "Joonghyuk Hahn",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2505.19589",
    "title": "Model Agnostic Differentially Private Causal Inference",
    "abstract": "           Estimating causal effects from observational data is essential in fields such as medicine, economics and social sciences, where privacy concerns are paramount. We propose a general, model-agnostic framework for differentially private estimation of average treatment effects (ATE) that avoids strong structural assumptions on the data-generating process or the models used to estimate propensity scores and conditional outcomes. In contrast to prior work, which enforces differential privacy by directly privatizing these nuisance components and results in a privacy cost that scales with model complexity, our approach decouples nuisance estimation from privacy protection. This separation allows the use of flexible, state-of-the-art black-box models, while differential privacy is achieved by perturbing only predictions and aggregation steps within a fold-splitting scheme with ensemble techniques. We instantiate the framework for three classical estimators -- the G-formula, inverse propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal utility and privacy guarantees. Empirical results show that our methods maintain competitive performance under realistic privacy budgets. We further extend our framework to support meta-analysis of multiple private ATE estimates. Our results bridge a critical gap between causal inference and privacy-preserving data analysis.         ",
    "url": "https://arxiv.org/abs/2505.19589",
    "authors": [
      "Christian Lebeda",
      "Mathieu Even",
      "Aur\u00e9lien Bellet",
      "Julie Josse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.19646",
    "title": "Energy-based generator matching: A neural sampler for general state space",
    "abstract": "           We propose Energy-based generator matching (EGM), a modality-agnostic approach to train generative models from energy functions in the absence of data. Extending the recently proposed generator matching, EGM enables training of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump, and can generate data from continuous, discrete, and a mixture of two modalities. To this end, we propose estimating the generator matching loss using self-normalized importance sampling with an additional bootstrapping trick to reduce variance in the importance weight. We validate EGM on both discrete and multimodal tasks up to 100 and 20 dimensions, respectively.         ",
    "url": "https://arxiv.org/abs/2505.19646",
    "authors": [
      "Dongyeop Woo",
      "Minsu Kim",
      "Minkyu Kim",
      "Kiyoung Seong",
      "Sungsoo Ahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.19751",
    "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model",
    "abstract": "           Intrinsic image decomposition aims at separating an image into its underlying albedo and shading components, isolating the base color from lighting effects to enable downstream applications such as virtual relighting and scene editing. Despite the rise and success of learning-based approaches, intrinsic image decomposition from real-world images remains a significant challenging task due to the scarcity of labeled ground-truth data. Most existing solutions rely on synthetic data as supervised setups, limiting their ability to generalize to real-world scenes. Self-supervised methods, on the other hand, often produce albedo maps that contain reflections and lack consistency under different lighting conditions. To address this, we propose SAIL, an approach designed to estimate albedo-like representations from single-view real-world images. We repurpose the prior knowledge of a latent diffusion model for unconditioned scene relighting as a surrogate objective for albedo estimation. To extract the albedo, we introduce a novel intrinsic image decomposition fully formulated in the latent space. To guide the training of our latent diffusion model, we introduce regularization terms that constrain both the lighting-dependent and independent components of our latent image decomposition. SAIL predicts stable albedo under varying lighting conditions and generalizes to multiple scenes, using only unlabeled multi-illumination data available online.         ",
    "url": "https://arxiv.org/abs/2505.19751",
    "authors": [
      "Hala Djeghim",
      "Nathan Piasco",
      "Luis Rold\u00e3o",
      "Moussab Bennehar",
      "Dzmitry Tsishkou",
      "C\u00e9line Loscos",
      "D\u00e9sir\u00e9 Sidib\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2505.19809",
    "title": "Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees",
    "abstract": "           In many real-world applications of regression, conditional probability estimation, and uncertainty quantification, exploiting symmetries rooted in physics or geometry can dramatically improve generalization and sample efficiency. While geometric deep learning has made significant empirical advances by incorporating group-theoretic structure, less attention has been given to statistical learning guarantees. In this paper, we introduce an equivariant representation learning framework that simultaneously addresses regression, conditional probability estimation, and uncertainty quantification while providing first-of-its-kind non-asymptotic statistical learning guarantees. Grounded in operator and group representation theory, our framework approximates the spectral decomposition of the conditional expectation operator, building representations that are both equivariant and disentangled along independent symmetry subgroups. Empirical evaluations on synthetic datasets and real-world robotics applications confirm the potential of our approach, matching or outperforming existing equivariant baselines in regression while additionally providing well-calibrated parametric uncertainty estimates.         ",
    "url": "https://arxiv.org/abs/2505.19809",
    "authors": [
      "Daniel Ordo\u00f1ez-Apraez",
      "Vladimir Kosti\u0107",
      "Alek Fr\u00f6hlich",
      "Vivien Brandt",
      "Karim Lounici",
      "Massimiliano Pontil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2505.20089",
    "title": "Homophily Enhanced Graph Domain Adaptation",
    "abstract": "           Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs, addressing the challenge of label scarcity. In this paper, we highlight the significance of graph homophily, a pivotal factor for graph domain alignment, which, however, has long been overlooked in existing approaches. Specifically, our analysis first reveals that homophily discrepancies exist in benchmarks. Moreover, we also show that homophily discrepancies degrade GDA performance from both empirical and theoretical aspects, which further underscores the importance of homophily alignment in GDA. Inspired by this finding, we propose a novel homophily alignment algorithm that employs mixed filters to smooth graph signals, thereby effectively capturing and mitigating homophily discrepancies between graphs. Experimental results on a variety of benchmarks verify the effectiveness of our method.         ",
    "url": "https://arxiv.org/abs/2505.20089",
    "authors": [
      "Ruiyi Fang",
      "Bingheng Li",
      "Jingyu Zhao",
      "Ruizhi Pu",
      "Qiuhao Zeng",
      "Gezheng Xu",
      "Charles Ling",
      "Boyu Wang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2505.20118",
    "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent",
    "abstract": "           As large language models (LLMs) become integrated into sensitive workflows, concerns grow over their potential to leak confidential information. We propose TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to embed sensitive context information into natural-looking outputs via linguistic steganography, without requiring explicit control over inference inputs. We introduce a taxonomy outlining risk factors for compromised LLMs, and use it to evaluate the risk profile of the threat. To implement TrojanStego, we propose a practical encoding scheme based on vocabulary partitioning learnable by LLMs via fine-tuning. Experimental results show that compromised models reliably transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over 97% accuracy using majority voting across three generations. Further, they maintain high utility, can evade human detection, and preserve coherence. These results highlight a new class of LLM data exfiltration attacks that are passive, covert, practical, and dangerous.         ",
    "url": "https://arxiv.org/abs/2505.20118",
    "authors": [
      "Dominik Meier",
      "Jan Philip Wahle",
      "Paul R\u00f6ttger",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:1803.01931",
    "title": "Structure and generation of crossing-critical graphs",
    "abstract": "           We study c-crossing-critical graphs, which are the minimal graphs that require at least c edge-crossings when drawn in the plane. For c=1 there are only two such graphs without degree-2 vertices, K_5 and K_3,3, but for any fixed c>1 there exist infinitely many c-crossing-critical graphs. It has been previously shown that c-crossing-critical graphs have bounded path-width and contain only a bounded number of internally disjoint paths between any two vertices. We expand on these results, providing a more detailed description of the structure of crossing-critical graphs. On the way towards this description, we prove a new structural characterisation of plane graphs of bounded path-width. Then we show that every c-crossing-critical graph can be obtained from a c-crossing-critical graph of bounded size by replicating bounded-size parts that already appear in narrow \"bands\" or \"fans\" in the graph. This also gives an algorithm to generate all the c-crossing-critical graphs of at most given order n in polynomial time per each generated graph.         ",
    "url": "https://arxiv.org/abs/1803.01931",
    "authors": [
      "Zden\u011bk Dvo\u0159\u00e1k",
      "Petr Hlin\u011bn\u00fd",
      "Bojan Mohar"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2402.06033",
    "title": "An Inexact Halpern Iteration with Application to Distributionally Robust Optimization",
    "abstract": "           The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods and for general nonlinear convex-concave loss functions, which are competitive in the literature.         ",
    "url": "https://arxiv.org/abs/2402.06033",
    "authors": [
      "Ling Liang",
      "Zusen Xu",
      "Kim-Chuan Toh",
      "Jia-Jie Zhu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2403.03071",
    "title": "On a Neural Implementation of Brenier's Polar Factorization",
    "abstract": "           In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.         ",
    "url": "https://arxiv.org/abs/2403.03071",
    "authors": [
      "Nina Vesseron",
      "Marco Cuturi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2410.07267",
    "title": "Scintillation pulse characterization with spectrum-inspired temporal neural networks: case studies on particle detector signals",
    "abstract": "           Particle detectors based on scintillators are widely used in high-energy physics and astroparticle physics experiments, nuclear medicine imaging, industrial and environmental detection, etc. Precisely extracting scintillation signal characteristics at the event level is important for these applications, not only in respect of understanding the scintillator itself, but also kinds and physical property of incident particles. Recent researches demonstrate data-driven neural networks surpass traditional statistical methods, especially when the analytical form of signals is hard to obtain, or noise is significant. However, most densely connected or convolution-based networks fail to fully exploit the spectral and temporal structure of scintillation signals, leaving large space for performance improvement. In this paper, we propose a network architecture specially tailored for scintillation pulse characterization based on previous works on time series analysis. The core insight is that, by directly applying Fast Fourier Transform on original signals and utilizing different frequency components, the proposed network architecture can serve as a lightweight and enhanced representation learning backbone. We prove our idea in two case studies: (a) simulation data generated with the setting of the LUX dark matter detector, and (b) experimental electrical signals with fast electronics to emulate scintillation variations for the NICA/MPD calorimeter. The proposed model achieves significantly better results than the reference model in literature and densely connected models and demonstrates higher cost-efficiency than conventional machine learning methods.         ",
    "url": "https://arxiv.org/abs/2410.07267",
    "authors": [
      "Pengcheng Ai",
      "Xiangming Sun",
      "Zhi Deng",
      "Xinchi Ran"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2411.02138",
    "title": "Generalizable and Robust Spectral Method for Multi-view Representation Learning",
    "abstract": "           Multi-view representation learning (MvRL) has garnered substantial attention in recent years, driven by the increasing demand for applications that can effectively process and analyze data from multiple sources. In this context, graph Laplacian-based MvRL methods have demonstrated remarkable success in representing multi-view data. However, these methods often struggle with generalization to new data and face challenges with scalability. Moreover, in many practical scenarios, multi-view data is contaminated by noise or outliers. In such cases, modern deep-learning-based MvRL approaches that rely on alignment or contrastive objectives present degraded performance in downstream tasks, as they may impose incorrect consistency between clear and corrupted data sources. We introduce $\\textit{SpecRaGE}$, a novel fusion-based framework that integrates the strengths of graph Laplacian methods with the power of deep learning to overcome these challenges. SpecRage uses neural networks to learn parametric mapping that approximates a joint diagonalization of graph Laplacians. This solution bypasses the need for alignment while enabling generalizable and scalable learning of informative and meaningful representations. Moreover, it incorporates a meta-learning fusion module that dynamically adapts to data quality, ensuring robustness against outliers and noisy views. Our extensive experiments demonstrate that SpecRaGE outperforms state-of-the-art methods, particularly in scenarios with data contamination, paving the way for more reliable and efficient multi-view learning.         ",
    "url": "https://arxiv.org/abs/2411.02138",
    "authors": [
      "Amitai Yacobi",
      "Ofir Lindenbaum",
      "Uri Shaham"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2411.02549",
    "title": "Distributionally Robust Optimization",
    "abstract": "           Distributionally robust optimization (DRO) studies decision problems under uncertainty where the probability distribution governing the uncertain problem parameters is itself uncertain. A key component of any DRO model is its ambiguity set, that is, a family of probability distributions consistent with any available structural or statistical information. DRO seeks decisions that perform best under the worst distribution in the ambiguity set. This worst case criterion is supported by findings in psychology and neuroscience, which indicate that many decision-makers have a low tolerance for distributional ambiguity. DRO is rooted in statistics, operations research and control theory, and recent research has uncovered its deep connections to regularization techniques and adversarial training in machine learning. This survey presents the key findings of the field in a unified and self-contained manner.         ",
    "url": "https://arxiv.org/abs/2411.02549",
    "authors": [
      "Daniel Kuhn",
      "Soroosh Shafiee",
      "Wolfram Wiesemann"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2411.06308",
    "title": "Exploring Out-of-distribution Detection for Sparse-view Computed Tomography with Diffusion Models",
    "abstract": "           Recent works demonstrate the effectiveness of diffusion models as unsupervised solvers for inverse imaging problems. Sparse-view computed tomography (CT) has greatly benefited from these advancements, achieving improved generalization without reliance on measurement parameters. However, this comes at the cost of potential hallucinations, especially when handling out-of-distribution (OOD) data. To ensure reliability, it is essential to study OOD detection for CT reconstruction across both clinical and industrial applications. This need further extends to enabling the OOD detector to function effectively as an anomaly inspection tool. In this paper, we explore the use of a diffusion model, trained to capture the target distribution for CT reconstruction, as an in-distribution prior. Building on recent research, we employ the model to reconstruct partially diffused input images and assess OOD-ness through multiple reconstruction errors. Adapting this approach for sparse-view CT requires redefining the notions of ``input'' and ``reconstruction error''. Here, we use filtered backprojection (FBP) reconstructions as input and investigate various definitions of reconstruction error. Our proof-of-concept experiments on the MNIST dataset highlight both successes and failures, demonstrating the potential and limitations of integrating such an OOD detector into a CT reconstruction system. Our findings suggest that effective OOD detection can be achieved by comparing measurements with forward-projected reconstructions, provided that reconstructions from noisy FBP inputs are conditioned on the measurements. However, conditioning can sometimes lead the OOD detector to inadvertently reconstruct OOD images well. To counter this, we introduce a weighting approach that improves robustness against highly informative OOD measurements, albeit with a trade-off in performance in certain cases.         ",
    "url": "https://arxiv.org/abs/2411.06308",
    "authors": [
      "Ezgi Demircan-Tureyen",
      "Felix Lucka",
      "Tristan van Leeuwen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2412.02059",
    "title": "Lean classical-quantum hybrid neural network model for image classification",
    "abstract": "           The integration of algorithms from quantum information with neural networks has enabled unprecedented advancements in various domains. Nonetheless, the application of quantum machine learning algorithms for image classification predominantly relies on traditional architectures such as variational quantum circuits. The performance of these models is closely tied to the scale of their parameters, with the substantial demand for parameters potentially leading to limitations in computational resources and a significant increase in computation time. In this paper, we introduce a Lean Classical-Quantum Hybrid Neural Network (LCQHNN), which achieves efficient classification performance with only four layers of variational circuits, thereby substantially reducing computational costs. Our experiments demonstrate that LCQHNN achieves 100\\%, 99.02\\%, and 85.55\\% classification accuracy on MNIST, FashionMNIST, and CIFAR-10 datasets. Under the same parameter conditions, the convergence speed of this method is also faster than that of traditional models. Furthermore, through visualization studies, it is found that the model effectively captures key data features during training and establishes a clear association between these features and their corresponding categories. This study confirms that the employment of quantum algorithms enhances the model's ability to handle complex classification problems.         ",
    "url": "https://arxiv.org/abs/2412.02059",
    "authors": [
      "Ao Liu",
      "Cuihong Wen",
      "Jieci Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2501.18470",
    "title": "Resampling Filter Design for Multirate Neural Audio Effect Processing",
    "abstract": "           Neural networks have become ubiquitous in audio effects modelling, especially for guitar amplifiers and distortion pedals. One limitation of such models is that the sample rate of the training data is implicitly encoded in the model weights and therefore not readily adjustable at inference. Recent work explored modifications to recurrent neural network architecture to approximate a sample rate independent system, enabling audio processing at a rate that differs from the original training rate. This method works well for integer oversampling and can reduce aliasing caused by nonlinear activation functions. For small fractional changes in sample rate, fractional delay filters can be used to approximate sample rate independence, but in some cases this method fails entirely. Here, we explore the use of real-time signal resampling at the input and output of the neural network as an alternative solution. We investigate several resampling filter designs and show that a two-stage design consisting of a half-band IIR filter cascaded with a Kaiser window FIR filter can give similar or better results to the previously proposed model adjustment method with many fewer filtering operations per sample and less than one millisecond of latency at typical audio rates. Furthermore, we investigate interpolation and decimation filters for the task of integer oversampling and show that cascaded half-band IIR and FIR designs can be used in conjunction with the model adjustment method to reduce aliasing in a range of distortion effect models.         ",
    "url": "https://arxiv.org/abs/2501.18470",
    "authors": [
      "Alistair Carson",
      "Vesa V\u00e4lim\u00e4ki",
      "Alec Wright",
      "Stefan Bilbao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2502.10328",
    "title": "Accelerated Parallel Tempering via Neural Transports",
    "abstract": "           Markov Chain Monte Carlo (MCMC) algorithms are essential tools in computational statistics for sampling from unnormalised probability distributions, but can be fragile when targeting high-dimensional, multimodal, or complex target distributions. Parallel Tempering (PT) enhances MCMC's sample efficiency through annealing and parallel computation, propagating samples from tractable reference distributions to intractable targets via state swapping across interpolating distributions. The effectiveness of PT is limited by the often minimal overlap between adjacent distributions in challenging problems, which requires increasing the computational resources to compensate. We introduce a framework that accelerates PT by leveraging neural samplers-including normalising flows, diffusion models, and controlled diffusions-to reduce the required overlap. Our approach utilises neural samplers in parallel, circumventing the computational burden of neural samplers while preserving the asymptotic consistency of classical PT. We demonstrate theoretically and empirically on a variety of multimodal sampling problems that our method improves sample quality, reduces the computational cost compared to classical PT, and enables efficient free energies/normalising constants estimation.         ",
    "url": "https://arxiv.org/abs/2502.10328",
    "authors": [
      "Leo Zhang",
      "Peter Potaptchik",
      "Jiajun He",
      "Yuanqi Du",
      "Arnaud Doucet",
      "Francisco Vargas",
      "Hai-Dang Dau",
      "Saifuddin Syed"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2502.12696",
    "title": "Radar Network for Gait Monitoring: Technology and Validation",
    "abstract": "           In recent years, radar-based devices have emerged as an alternative approach for gait monitoring. However, the radar configuration and the algorithms used to extract the gait parameters often differ between contributions, lacking a systematic evaluation of the most appropriate setup. Additionally, radar-based studies often exclude motorically impaired subjects, leaving it unclear whether the existing algorithms are applicable to such populations. In this paper, a radar network is developed and validated by monitoring the gait of five healthy individuals and three patients with Parkinson's disease. Six configurations and four algorithms were compared using Vicon as ground-truth to determine the most appropriate solution for gait monitoring. The best results were obtained using only three nodes: two oriented towards the feet and one towards the torso. The most accurate stride velocity and distance in the state of the art were obtained with this configuration. Moreover, we show that analyzing the feet velocity increases the reliability of the temporal parameters, especially with aged or motorically impaired subjects. The contribution is significant for the implementation of radar networks in clinical and domestic environments, as it addresses critical aspects concerning the radar network configuration and algorithms.         ",
    "url": "https://arxiv.org/abs/2502.12696",
    "authors": [
      "Ignacio E. L\u00f3pez-Delgado",
      "V\u00edctor Navarro-L\u00f3pez",
      "Francisco Grandas-P\u00e9rez",
      "Juan I. Godino-Llorente",
      "Jes\u00fas Grajal"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2504.10139",
    "title": "Conditional Distribution Compression via the Kernel Conditional Mean Embedding",
    "abstract": "           Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.         ",
    "url": "https://arxiv.org/abs/2504.10139",
    "authors": [
      "Dominic Broadbent",
      "Nick Whiteley",
      "Robert Allison",
      "Tom Lovett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2504.18017",
    "title": "Non-identifiability distinguishes Neural Networks among Parametric Models",
    "abstract": "           One of the enduring problems surrounding neural networks is to identify the factors that differentiate them from traditional statistical models. We prove a pair of results which distinguish feedforward neural networks among parametric models at the population level, for regression tasks. Firstly, we prove that for any pair of random variables $(X,Y)$, neural networks always learn a nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove that for reasonable smooth parametric models, under local and global identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which the parametric model learns the constant predictor $\\mathbb{E}[Y]$. Together, our results suggest that a lack of identifiability distinguishes neural networks among the class of smooth parametric models.         ",
    "url": "https://arxiv.org/abs/2504.18017",
    "authors": [
      "Sourav Chatterjee",
      "Timothy Sudijono"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2505.04937",
    "title": "Generalization Analysis for Contrastive Representation Learning under Non-IID Settings",
    "abstract": "           Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL has remained limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to each class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.         ",
    "url": "https://arxiv.org/abs/2505.04937",
    "authors": [
      "Nong Minh Hieu",
      "Antoine Ledent"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2505.07719",
    "title": "Training neural control variates using correlated configurations",
    "abstract": "           Neural control variates (NCVs) have emerged as a powerful tool for variance reduction in Monte Carlo (MC) simulations, particularly in high-dimensional problems where traditional control variates are difficult to construct analytically. By training neural networks to learn auxiliary functions correlated with the target observable, NCVs can significantly reduce estimator variance while preserving unbiasedness. However, a critical but often overlooked aspect of NCV training is the role of autocorrelated samples generated by Markov Chain Monte Carlo (MCMC). While such samples are typically discarded for error estimation due to their statistical redundancy, they may contain useful information about the structure of the underlying probability distribution that can benefit the training process. In this work, we systematically examine the effect of using correlated configurations in training neural control variates. We demonstrate, both conceptually and numerically, that training on correlated data can improve control variate performance, especially in settings with limited computational resources. Our analysis includes empirical results from $U(1)$ gauge theory and scalar field theory, illustrating when and how autocorrelated samples enhance NCV construction. These findings provide practical guidance for the efficient use of MCMC data in training neural networks.         ",
    "url": "https://arxiv.org/abs/2505.07719",
    "authors": [
      "Hyunwoo Oh"
    ],
    "subjectives": [
      "High Energy Physics - Lattice (hep-lat)",
      "Machine Learning (cs.LG)",
      "Nuclear Theory (nucl-th)"
    ]
  },
  {
    "id": "arXiv:2505.18361",
    "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
    "abstract": "           Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy. For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.         ",
    "url": "https://arxiv.org/abs/2505.18361",
    "authors": [
      "Trinity Chung",
      "Yuchen Shen",
      "Nathan C. L. Kong",
      "Aran Nayebi"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  }
]