[
  {
    "id": "arXiv:2112.09161",
    "title": "Constraint-based graph network simulator",
    "abstract": "In the rapidly advancing area of learned physical simulators, nearly all methods train forward models that directly predict future states from input states. However, many traditional simulation engines use a constraint-based approach instead of direct prediction. Here we present a framework for constraint-based learned simulation, where a scalar constraint function is implemented as a neural network, and future predictions are computed as the solutions to optimization problems under these learned constraints. We implement our method using a graph neural network as the constraint function and gradient descent as the constraint solver. The architecture can be trained by standard backpropagation. We test the model on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids. Our model achieves better or comparable performance to top learned simulators. A key advantage of our model is the ability to generalize to more solver iterations at test time to improve the simulation accuracy. We also show how hand-designed constraints can be added at test time to satisfy objectives which were not present in the training data, which is not possible with forward approaches. Our constraint-based framework is applicable to any setting where forward learned simulators are used, and demonstrates how learned simulators can leverage additional inductive biases as well as the techniques from the field of numerical methods. ",
    "url": "https://arxiv.org/abs/2112.09161",
    "authors": [
      "Yulia Rubanova",
      "Alvaro Sanchez-Gonzalez",
      "Tobias Pfaff",
      "Peter Battaglia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.09175",
    "title": "Effective prevention of semantic drift as angular distance in  memory-less continual deep neural networks",
    "abstract": "Lifelong machine learning or continual learning models attempt to learn incrementally by accumulating knowledge across a sequence of tasks. Therefore, these models learn better and faster. They are used in various intelligent systems that have to interact with humans or any dynamic environment e.g., chatbots and self-driving cars. Memory-less approach is more often used with deep neural networks that accommodates incoming information from tasks within its architecture. It allows them to perform well on all the seen tasks. These models suffer from semantic drift or the plasticity-stability dilemma. The existing models use Minkowski distance measures to decide which nodes to freeze, update or duplicate. These distance metrics do not provide better separation of nodes as they are susceptible to high dimensional sparse vectors. In our proposed approach, we use angular distance to evaluate the semantic drift in individual nodes that provide better separation of nodes and thus better balancing between stability and plasticity. The proposed approach outperforms state-of-the art models by maintaining higher accuracy on standard datasets. ",
    "url": "https://arxiv.org/abs/2112.09175",
    "authors": [
      "Khouloud Saadi",
      "Muhammad Taimoor Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.09181",
    "title": "Approximation of functions with one-bit neural networks",
    "abstract": "This paper examines the approximation capabilities of coarsely quantized neural networks -- those whose parameters are selected from a small set of allowable values. We show that any smooth multivariate function can be arbitrarily well approximated by an appropriate coarsely quantized neural network and provide a quantitative approximation rate. For the quadratic activation, this can be done with only a one-bit alphabet; for the ReLU activation, we use a three-bit alphabet. The main theorems rely on important properties of Bernstein polynomials. We prove new results on approximation of functions with Bernstein polynomials, noise-shaping quantization on the Bernstein basis, and implementation of the Bernstein polynomials by coarsely quantized neural networks. ",
    "url": "https://arxiv.org/abs/2112.09181",
    "authors": [
      "C. Sinan G\u00fcnt\u00fcrk",
      "Weilin Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2112.09260",
    "title": "How to augment your ViTs? Consistency loss and StyleAug, a random style  transfer augmentation",
    "abstract": "The Vision Transformer (ViT) architecture has recently achieved competitive performance across a variety of computer vision tasks. One of the motivations behind ViTs is weaker inductive biases, when compared to convolutional neural networks (CNNs). However this also makes ViTs more difficult to train. They require very large training datasets, heavy regularization, and strong data augmentations. The data augmentation strategies used to train ViTs have largely been inherited from CNN training, despite the significant differences between the two architectures. In this work, we empirical evaluated how different data augmentation strategies performed on CNN (e.g., ResNet) versus ViT architectures for image classification. We introduced a style transfer data augmentation, termed StyleAug, which worked best for training ViTs, while RandAugment and Augmix typically worked best for training CNNs. We also found that, in addition to a classification loss, using a consistency loss between multiple augmentations of the same image was especially helpful when training ViTs. ",
    "url": "https://arxiv.org/abs/2112.09260",
    "authors": [
      "Akash Umakantha",
      "Joao D. Semedo",
      "S. Alireza Golestaneh",
      "Wan-Yi S. Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2112.09425",
    "title": "Knowledge graph enhanced recommender system",
    "abstract": "Knowledge Graphs (KGs) have shown great success in recommendation. This is attributed to the rich attribute information contained in KG to improve item and user representations as side information. However, existing knowledge-aware methods leverage attribute information at a coarse-grained level both in item and user side. In this paper, we proposed a novel attentive knowledge graph attribute network(AKGAN) to learn item attributes and user interests via attribute information in KG. Technically, AKGAN adopts a heterogeneous graph neural network framework, which has a different design between the first layer and the latter layer. With one attribute placed in the corresponding range of element-wise positions, AKGAN employs a novel interest-aware attention network, which releases the limitation that the sum of attention weight is 1, to model the complexity and personality of user interests towards attributes. Experimental results on three benchmark datasets show the effectiveness and explainability of AKGAN. ",
    "url": "https://arxiv.org/abs/2112.09425",
    "authors": [
      "Zepeng Huai",
      "Jianhua Tao",
      "Feihu Che",
      "Guohua Yang",
      "Dawei Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2112.09624",
    "title": "Reciprocity, community detection, and link prediction in dynamic  networks",
    "abstract": "Many complex systems change their structure over time, in these cases dynamic networks can provide a richer representation of such phenomena. As a consequence, many inference methods have been generalized to the dynamic case with the aim to model dynamic interactions. Particular interest has been devoted to extend the stochastic block model and its variant, to capture community structure as the network changes in time. While these models assume that edge formation depends only on the community memberships, recent work for static networks show the importance to include additional parameters capturing structural properties, as reciprocity for instance. Remarkably, these models are capable of generating more realistic network representations than those that only consider community membership. To this aim, we present a probabilistic generative model with hidden variables that integrates reciprocity and communities as structural information of networks that evolve in time. The model assumes a fundamental order in observing reciprocal data, that is an edge is observed, conditional on its reciprocated edge in the past. We deploy a Markovian approach to construct the network's transition matrix between time steps and parameters' inference is performed with an Expectation-Maximization algorithm that leads to high computational efficiency because it exploits the sparsity of the dataset. We test the performance of the model on synthetic dynamical networks, as well as on real networks of citations and email datasets. We show that our model captures the reciprocity of real networks better than standard models with only community structure, while performing well at link prediction tasks. ",
    "url": "https://arxiv.org/abs/2112.09624",
    "authors": [
      "Hadiseh Safdari",
      "Martina Contisciani",
      "Caterina De Bacco"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2112.09646",
    "title": "Generation of data on discontinuous manifolds via continuous stochastic  non-invertible networks",
    "abstract": "The generation of discontinuous distributions is a difficult task for most known frameworks such as generative autoencoders and generative adversarial networks. Generative non-invertible models are unable to accurately generate such distributions, require long training and often are subject to mode collapse. Variational autoencoders (VAEs), which are based on the idea of keeping the latent space to be Gaussian for the sake of a simple sampling, allow an accurate reconstruction, while they experience significant limitations at generation task. In this work, instead of trying to keep the latent space to be Gaussian, we use a pre-trained contrastive encoder to obtain a clustered latent space. Then, for each cluster, representing a unimodal submanifold, we train a dedicated low complexity network to generate this submanifold from the Gaussian distribution. The proposed framework is based on the information-theoretic formulation of mutual information maximization between the input data and latent space representation. We derive a link between the cost functions and the information-theoretic formulation. We apply our approach to synthetic 2D distributions to demonstrate both reconstruction and generation of discontinuous distributions using continuous stochastic networks. ",
    "url": "https://arxiv.org/abs/2112.09646",
    "authors": [
      "Mariia Drozdova",
      "Vitaliy Kinakh",
      "Guillaume Qu\u00e9tant",
      "Tobias Golling",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.09648",
    "title": "Improving neural implicit surfaces geometry with patch warping",
    "abstract": "Neural implicit surfaces have become an important technique for multi-view 3D reconstruction but their accuracy remains limited. In this paper, we argue that this comes from the difficulty to learn and render high frequency textures with neural networks. We thus propose to add to the standard neural rendering optimization a direct photo-consistency term across the different views. Intuitively, we optimize the implicit geometry so that it warps views on each other in a consistent way. We demonstrate that two elements are key to the success of such an approach: (i) warping entire patches, using the predicted occupancy and normals of the 3D points along each ray, and measuring their similarity with a robust structural similarity (SSIM); (ii) handling visibility and occlusion in such a way that incorrect warps are not given too much importance while encouraging a reconstruction as complete as possible. We evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL benchmarks and show it outperforms state of the art unsupervised implicit surfaces reconstructions by over 20% on both datasets. ",
    "url": "https://arxiv.org/abs/2112.09648",
    "authors": [
      "Fran\u00e7ois Darmon",
      "B\u00e9n\u00e9dicte Bascle",
      "Jean-Cl\u00e9ment Devaux",
      "Pascal Monasse",
      "Mathieu Aubry"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2112.09574",
    "title": "Super-resolution reconstruction of cytoskeleton image based on A-net  deep learning network",
    "abstract": "To date, live-cell imaging at the nanometer scale remains challenging. Even though super-resolution microscopy methods have enabled visualization of subcellular structures below the optical resolution limit, the spatial resolution is still far from enough for the structural reconstruction of biomolecules in vivo (i.e. ~24 nm thickness of microtubule fiber). In this study, we proposed an A-net network and showed that the resolution of cytoskeleton images captured by a confocal microscope can be significantly improved by combining the A-net deep learning network with the DWDC algorithm based on degradation model. Utilizing the DWDC algorithm to construct new datasets and taking advantage of A-net neural network's features (i.e., considerably fewer layers), we successfully removed the noise and flocculent structures, which originally interfere with the cellular structure in the raw image, and improved the spatial resolution by 10 times using relatively small dataset. We, therefore, conclude that the proposed algorithm that combines A-net neural network with the DWDC method is a suitable and universal approach for exacting structural details of biomolecules, cells and organs from low-resolution images. ",
    "url": "https://arxiv.org/abs/2112.09574",
    "authors": [
      "Qian Chen",
      "Haoxin Bai",
      "Bingchen Che",
      "Tianyun Zhao",
      "Ce Zhang",
      "Kaige Wang",
      "Jintao Bai",
      "Wei Zhao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.09684",
    "title": "On the existence of global minima and convergence analyses for gradient  descent methods in the training of deep neural networks",
    "abstract": "In this article we study fully-connected feedforward deep ReLU ANNs with an arbitrarily large number of hidden layers and we prove convergence of the risk of the GD optimization method with random initializations in the training of such ANNs under the assumption that the unnormalized probability density function of the probability distribution of the input data of the considered supervised learning problem is piecewise polynomial, under the assumption that the target function (describing the relationship between input data and the output data) is piecewise polynomial, and under the assumption that the risk function of the considered supervised learning problem admits at least one regular global minimum. In addition, in the special situation of shallow ANNs with just one hidden layer and one-dimensional input we also verify this assumption by proving in the training of such shallow ANNs that for every Lipschitz continuous target function there exists a global minimum in the risk landscape. Finally, in the training of deep ANNs with ReLU activation we also study solutions of gradient flow (GF) differential equations and we prove that every non-divergent GF trajectory converges with a polynomial rate of convergence to a critical point (in the sense of limiting Fr\\'echet subdifferentiability). Our mathematical convergence analysis builds up on tools from real algebraic geometry such as the concept of semi-algebraic functions and generalized Kurdyka-Lojasiewicz inequalities, on tools from functional analysis such as the Arzel\\`a-Ascoli theorem, on tools from nonsmooth analysis such as the concept of limiting Fr\\'echet subgradients, as well as on the fact that the set of realization functions of shallow ReLU ANNs with fixed architecture forms a closed subset of the set of continuous functions revealed by Petersen et al. ",
    "url": "https://arxiv.org/abs/2112.09684",
    "authors": [
      "Arnulf Jentzen",
      "Adrian Riekert"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2008.05801",
    "title": "An explicit construction of graphs of bounded degree that are far from  being Hamiltonian",
    "abstract": " Comments: 16 pages, 4 figures ",
    "url": "https://arxiv.org/abs/2008.05801",
    "authors": [
      "Isolde Adler",
      "Noleen K\u00f6hler"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2010.08423",
    "title": "Restless reachability problems in temporal graphs",
    "abstract": " Comments: The paper is updated with more illustrations for improving readability and directed towards broader audience (non-expert group) ",
    "url": "https://arxiv.org/abs/2010.08423",
    "authors": [
      "Suhas Thejaswi",
      "Juho Lauri",
      "Aristides Gionis"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2102.04984",
    "title": "Approximately counting independent sets of a given size in  bounded-degree graphs",
    "abstract": " Comments: New results on approximating coefficients of the partition function of the anti-ferromagnetic Ising model (Theorem 3) ",
    "url": "https://arxiv.org/abs/2102.04984",
    "authors": [
      "Ewan Davies",
      "Will Perkins"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2103.00151",
    "title": "Affine equivalences of surfaces of translation and minimal surfaces, and  applications to symmetry detection and design",
    "abstract": " Comments: 24 pages ",
    "url": "https://arxiv.org/abs/2103.00151",
    "authors": [
      "Juan Gerardo Alc\u00e1zar",
      "Georg Muntingh"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Computational Geometry (cs.CG)",
      "Symbolic Computation (cs.SC)"
    ]
  },
  {
    "id": "arXiv:2103.10922",
    "title": "Landscape analysis for shallow neural networks: complete classification  of critical points for affine target functions",
    "abstract": " Title: Landscape analysis for shallow neural networks: complete classification  of critical points for affine target functions ",
    "url": "https://arxiv.org/abs/2103.10922",
    "authors": [
      "Patrick Cheridito",
      "Arnulf Jentzen",
      "Florian Rossmannek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2103.17171",
    "title": "Spectral decoupling allows training transferable neural networks in  medical imaging",
    "abstract": " Comments: 10 pages, 7 figures and 2 tables ",
    "url": "https://arxiv.org/abs/2103.17171",
    "authors": [
      "Joona Pohjonen",
      "Carolin St\u00fcrenberg",
      "Antti Rannikko",
      "Tuomas Mirtti",
      "Esa Pitk\u00e4nen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2108.02883",
    "title": "Interpolation can hurt robust generalization even when there is no noise",
    "abstract": " Title: Interpolation can hurt robust generalization even when there is no noise ",
    "url": "https://arxiv.org/abs/2108.02883",
    "authors": [
      "Konstantin Donhauser",
      "Alexandru \u0162ifrea",
      "Michael Aerni",
      "Reinhard Heckel",
      "Fanny Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2109.06786",
    "title": "Multiple shooting for training neural differential equations on time  series",
    "abstract": " Comments: in IEEE Control Systems Letters ",
    "url": "https://arxiv.org/abs/2109.06786",
    "authors": [
      "Evren Mert Turan",
      "Johannes J\u00e4schke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2111.13854",
    "title": "A novel knowledge graph development for industry design: A case study on  indirect coal liquefaction process",
    "abstract": " Title: A novel knowledge graph development for industry design: A case study on  indirect coal liquefaction process ",
    "url": "https://arxiv.org/abs/2111.13854",
    "authors": [
      "Zhenhua Wang",
      "Beike Zhang",
      "Dong Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  }
]