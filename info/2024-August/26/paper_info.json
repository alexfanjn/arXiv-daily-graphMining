[
  {
    "id": "arXiv:2408.12603",
    "title": "Sleeper Social Bots: a new generation of AI disinformation bots are already a political threat",
    "abstract": "           This paper presents a study on the growing threat of \"sleeper social bots,\" AI-driven social bots in the political landscape, created to spread disinformation and manipulate public opinion. We based the name sleeper social bots on their ability to pass as humans on social platforms, where they're embedded like political \"sleeper\" agents, making them harder to detect and more disruptive. To illustrate the threat these bots pose, our research team at the University of Southern California constructed a demonstration using a private Mastodon server, where ChatGPT-driven bots, programmed with distinct personalities and political viewpoints, engaged in discussions with human participants about a fictional electoral proposition. Our preliminary findings suggest these bots can convincingly pass as human users, actively participate in conversations, and effectively disseminate disinformation. Moreover, they can adapt their arguments based on the responses of human interlocutors, showcasing their dynamic and persuasive capabilities. College students participating in initial experiments failed to identify our bots, underscoring the urgent need for increased awareness and education about the dangers of AI-driven disinformation, and in particular, disinformation spread by bots. The implications of our research point to the significant challenges posed by social bots in the upcoming 2024 U.S. presidential election and beyond.         ",
    "url": "https://arxiv.org/abs/2408.12603",
    "authors": [
      "Jaiv Doshi",
      "Ines Novacic",
      "Curtis Fletcher",
      "Mats Borges",
      "Elea Zhong",
      "Mark C. Marino",
      "Jason Gan",
      "Sophia Mager",
      "Dane Sprague",
      "Melinda Xia"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12608",
    "title": "A frugal Spiking Neural Network for unsupervised classification of continuous multivariate temporal data",
    "abstract": "           As neural interfaces become more advanced, there has been an increase in the volume and complexity of neural data recordings. These interfaces capture rich information about neural dynamics that call for efficient, real-time processing algorithms to spontaneously extract and interpret patterns of neural dynamics. Moreover, being able to do so in a fully unsupervised manner is critical as patterns in vast streams of neural data might not be easily identifiable by the human eye. Formal Deep Neural Networks (DNNs) have come a long way in performing pattern recognition tasks for various static and sequential pattern recognition applications. However, these networks usually require large labeled datasets for training and have high power consumption preventing their future embedding in active brain implants. An alternative aimed at addressing these issues are Spiking Neural Networks (SNNs) which are neuromorphic and use more biologically plausible neurons with evolving membrane potentials. In this context, we introduce here a frugal single-layer SNN designed for fully unsupervised identification and classification of multivariate temporal patterns in continuous data with a sequential approach. We show that, with only a handful number of neurons, this strategy is efficient to recognize highly overlapping multivariate temporal patterns, first on simulated data, and then on Mel Cepstral representations of speech sounds and finally on multichannel neural data. This approach relies on several biologically inspired plasticity rules, including Spike-timing-dependent plasticity (STDP), Short-term plasticity (STP) and intrinsic plasticity (IP). These results pave the way towards highly frugal SNNs for fully unsupervised and online-compatible learning of complex multivariate temporal patterns for future embedding in dedicated very-low power hardware.         ",
    "url": "https://arxiv.org/abs/2408.12608",
    "authors": [
      "Sai Deepesh Pokala",
      "Marie Bernert",
      "Takuya Nanami",
      "Takashi Kohno",
      "Timoth\u00e9e L\u00e9vi",
      "Blaise Yvert"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12609",
    "title": "Enhanced Prediction of Multi-Agent Trajectories via Control Inference and State-Space Dynamics",
    "abstract": "           In the field of autonomous systems, accurately predicting the trajectories of nearby vehicles and pedestrians is crucial for ensuring both safety and operational efficiency. This paper introduces a novel methodology for trajectory forecasting based on state-space dynamic system modeling, which endows agents with models that have tangible physical implications. To enhance the precision of state estimations within the dynamic system, the paper also presents a novel modeling technique for control variables. This technique utilizes a newly introduced model, termed \"Mixed Mamba,\" to derive initial control states, thereby improving the predictive accuracy of these variables. Moverover, the proposed approach ingeniously integrates graph neural networks with state-space models, effectively capturing the complexities of multi-agent interactions. This combination provides a robust and scalable framework for forecasting multi-agent trajectories across a range of scenarios. Comprehensive evaluations demonstrate that this model outperforms several established benchmarks across various metrics and datasets, highlighting its significant potential to advance trajectory forecasting in autonomous systems.         ",
    "url": "https://arxiv.org/abs/2408.12609",
    "authors": [
      "Yu Zhang",
      "Yongxiang Zou",
      "Haoyu Zhang",
      "Zeyu Liu",
      "Houcheng Li",
      "Long Cheng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12634",
    "title": "Joint Hypergraph Rewiring and Memory-Augmented Forecasting Techniques in Digital Twin Technology",
    "abstract": "           Digital Twin technology creates virtual replicas of physical objects, processes, or systems by replicating their properties, data, and behaviors. This advanced technology offers a range of intelligent functionalities, such as modeling, simulation, and data-driven decision-making, that facilitate design optimization, performance estimation, and monitoring operations. Forecasting plays a pivotal role in Digital Twin technology, as it enables the prediction of future outcomes, supports informed decision-making, minimizes risks, driving improvements in efficiency, productivity, and cost reduction. Recently, Digital Twin technology has leveraged Graph forecasting techniques in large-scale complex sensor networks to enable accurate forecasting and simulation of diverse scenarios, fostering proactive and data-driven decision making. However, existing Graph forecasting techniques lack scalability for many real-world applications. They have limited ability to adapt to non-stationary environments, retain past knowledge, lack a mechanism to capture the higher order spatio-temporal dynamics, and estimate uncertainty in model predictions. To surmount the challenges, we introduce a hybrid architecture that enhances the hypergraph representation learning backbone by incorporating fast adaptation to new patterns and memory-based retrieval of past knowledge. This balance aims to improve the slowly-learned backbone and achieve better performance in adapting to recent changes. In addition, it models the time-varying uncertainty of multi-horizon forecasts, providing estimates of prediction uncertainty. Our forecasting architecture has been validated through ablation studies and has demonstrated promising results across multiple benchmark datasets, surpassing state-ofthe-art forecasting methods by a significant margin.         ",
    "url": "https://arxiv.org/abs/2408.12634",
    "authors": [
      "Sagar Srinivas Sakhinana",
      "Krishna Sai Sudhir Aripirala",
      "Shivam Gupta",
      "Venkataramana Runkana"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12638",
    "title": "AI-driven Transformer Model for Fault Prediction in Non-Linear Dynamic Automotive System",
    "abstract": "           Fault detection in automotive engine systems is one of the most promising research areas. Several works have been done in the field of model-based fault diagnosis. Many researchers have discovered more advanced statistical methods and algorithms for better fault detection on any automotive dynamic engine system. The gas turbines/diesel engines produce highly complex and huge data which are highly non-linear. So, researchers should come up with an automated system that is more resilient and robust enough to handle this huge, complex data in highly non-linear dynamic automotive systems. Here, I present an AI-based fault classification and prediction model in the diesel engine that can be applied to any highly non-linear dynamic automotive system. The main contribution of this paper is the AI-based Transformer fault classification and prediction model in the diesel engine concerning the worldwide harmonic light vehicle test procedure (WLTP) driving cycle. This model used 27 input dimensions, 64 hidden dimensions with 2 layers, and 9 heads to create a classifier with 12 output heads (one for fault-free data and 11 different fault types). This model was trained on the UTSA Arc High-Performance Compute (HPC) cluster with 5 NVIDIA V100 GPUs, 40-core CPUs, and 384GB RAM and achieved 70.01 % accuracy on a held test set.         ",
    "url": "https://arxiv.org/abs/2408.12638",
    "authors": [
      "Priyanka Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12659",
    "title": "Disentangled Structural and Featural Representation for Task-Agnostic Graph Valuation",
    "abstract": "           With the emergence of data marketplaces, the demand for methods to assess the value of data has increased significantly. While numerous techniques have been proposed for this purpose, none have specifically addressed graphs as the main data modality. Graphs are widely used across various fields, ranging from chemical molecules to social networks. In this study, we break down graphs into two main components: structural and featural, and we focus on evaluating data without relying on specific task-related metrics, making it applicable in practical scenarios where validation requirements may be lacking. We introduce a novel framework called blind message passing, which aligns the seller's and buyer's graphs using a shared node permutation based on graph matching. This allows us to utilize the graph Wasserstein distance to quantify the differences in the structural distribution of graph datasets, called the structural disparities. We then consider featural aspects of buyers' and sellers' graphs for data valuation and capture their statistical similarities and differences, referred to as relevance and diversity, respectively. Our approach ensures that buyers and sellers remain unaware of each other's datasets. Our experiments on real datasets demonstrate the effectiveness of our approach in capturing the relevance, diversity, and structural disparities of seller data for buyers, particularly in graph-based data valuation scenarios.         ",
    "url": "https://arxiv.org/abs/2408.12659",
    "authors": [
      "Ali Falahati",
      "Mohammad Mohammadi Amiri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.12664",
    "title": "Multilevel Interpretability Of Artificial Neural Networks: Leveraging Framework And Methods From Neuroscience",
    "abstract": "           As deep learning systems are scaled up to many billions of parameters, relating their internal structure to external behaviors becomes very challenging. Although daunting, this problem is not new: Neuroscientists and cognitive scientists have accumulated decades of experience analyzing a particularly complex system - the brain. In this work, we argue that interpreting both biological and artificial neural systems requires analyzing those systems at multiple levels of analysis, with different analytic tools for each level. We first lay out a joint grand challenge among scientists who study the brain and who study artificial neural networks: understanding how distributed neural mechanisms give rise to complex cognition and behavior. We then present a series of analytical tools that can be used to analyze biological and artificial neural systems, organizing those tools according to Marr's three levels of analysis: computation/behavior, algorithm/representation, and implementation. Overall, the multilevel interpretability framework provides a principled way to tackle neural system complexity; links structure, computation, and behavior; clarifies assumptions and research priorities at each level; and paves the way toward a unified effort for understanding intelligent systems, may they be biological or artificial.         ",
    "url": "https://arxiv.org/abs/2408.12664",
    "authors": [
      "Zhonghao He",
      "Jascha Achterberg",
      "Katie Collins",
      "Kevin Nejad",
      "Danyal Akarca",
      "Yinzhu Yang",
      "Wes Gurnee",
      "Ilia Sucholutsky",
      "Yuhan Tang",
      "Rebeca Ianov",
      "George Ogden",
      "Chole Li",
      "Kai Sandbrink",
      "Stephen Casper",
      "Anna Ivanova",
      "Grace W. Lindsay"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2408.12665",
    "title": "Fairness-Aware Streaming Feature Selection with Causal Graphs",
    "abstract": "           Its crux lies in the optimization of a tradeoff between accuracy and fairness of resultant models on the selected feature subset. The technical challenge of our setting is twofold: 1) streaming feature inputs, such that an informative feature may become obsolete or redundant for prediction if its information has been covered by other similar features that arrived prior to it, and 2) non-associational feature correlation, such that bias may be leaked from those seemingly admissible, non-protected features. To overcome this, we propose Streaming Feature Selection with Causal Fairness (SFCF) that builds two causal graphs egocentric to prediction label and protected feature, respectively, striving to model the complex correlation structure among streaming features, labels, and protected information. As such, bias can be eradicated from predictive modeling by removing those features being causally correlated with the protected feature yet independent to the labels. We theorize that the originally redundant features for prediction can later become admissible, when the learning accuracy is compromised by the large number of removed features (non-protected but can be used to reconstruct bias information). We benchmark SFCF\\ on five datasets widely used in streaming feature research, and the results substantiate its performance superiority over six rival models in terms of efficiency and sparsity of feature selection and equalized odds of the resultant predictive models.         ",
    "url": "https://arxiv.org/abs/2408.12665",
    "authors": [
      "Leizhen Zhang",
      "Lusi Li",
      "Di Wu",
      "Sheng Chen",
      "Yi He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2408.12669",
    "title": "Bayesian Network Modeling of Causal Influence within Cognitive Domains and Clinical Dementia Severity Ratings for Western and Indian Cohorts",
    "abstract": "           This study investigates the causal relationships between Clinical Dementia Ratings (CDR) and its six domain scores across two distinct aging datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Longitudinal Aging Study of India (LASI). Using Directed Acyclic Graphs (DAGs) derived from Bayesian network models, we analyze the dependencies among domain scores and their influence on the global CDR. Our approach leverages the PC algorithm to estimate the DAG structures for both datasets, revealing notable differences in causal relationships and edge strengths between the Western and Indian populations. The analysis highlights a stronger dependency of CDR scores on memory functions in both datasets, but with significant variations in edge strengths and node degrees. By contrasting these findings, we aim to elucidate population-specific differences and similarities in dementia progression, providing insights that could inform targeted interventions and improve understanding of dementia across diverse demographic contexts.         ",
    "url": "https://arxiv.org/abs/2408.12669",
    "authors": [
      "Wupadrasta Santosh Kumar",
      "Sayali Rajendra Bhutare",
      "Neelam Sinha",
      "Thomas Gregor Issac"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2408.12670",
    "title": "Leveraging Information Consistency in Frequency and Spatial Domain for Adversarial Attacks",
    "abstract": "           Adversarial examples are a key method to exploit deep neural networks. Using gradient information, such examples can be generated in an efficient way without altering the victim model. Recent frequency domain transformation has further enhanced the transferability of such adversarial examples, such as spectrum simulation attack. In this work, we investigate the effectiveness of frequency domain-based attacks, aligning with similar findings in the spatial domain. Furthermore, such consistency between the frequency and spatial domains provides insights into how gradient-based adversarial attacks induce perturbations across different domains, which is yet to be explored. Hence, we propose a simple, effective, and scalable gradient-based adversarial attack algorithm leveraging the information consistency in both frequency and spatial domains. We evaluate the algorithm for its effectiveness against different models. Extensive experiments demonstrate that our algorithm achieves state-of-the-art results compared to other gradient-based algorithms. Our code is available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.12670",
    "authors": [
      "Zhibo Jin",
      "Jiayu Zhang",
      "Zhiyu Zhu",
      "Xinyi Wang",
      "Yiyun Huang",
      "Huaming Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12673",
    "title": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A Comprehensive Framework for Gradient Editing",
    "abstract": "           Transferable adversarial attacks pose significant threats to deep neural networks, particularly in black-box scenarios where internal model information is inaccessible. Studying adversarial attack methods helps advance the performance of defense mechanisms and explore model vulnerabilities. These methods can uncover and exploit weaknesses in models, promoting the development of more robust architectures. However, current methods for transferable attacks often come with substantial computational costs, limiting their deployment and application, especially in edge computing scenarios. Adversarial generative models, such as Generative Adversarial Networks (GANs), are characterized by their ability to generate samples without the need for retraining after an initial training phase. GE-AdvGAN, a recent method for transferable adversarial attacks, is based on this principle. In this paper, we propose a novel general framework for gradient editing-based transferable attacks, named GE-AdvGAN+, which integrates nearly all mainstream attack methods to enhance transferability while significantly reducing computational resource consumption. Our experiments demonstrate the compatibility and effectiveness of our framework. Compared to the baseline AdvGAN, our best-performing method, GE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it surpasses the latest competing algorithm, GE-AdvGAN, with an average ASR increase of 5.9. The framework also exhibits enhanced computational efficiency, achieving 2217.7 FPS, outperforming traditional methods such as BIM and MI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at this https URL ",
    "url": "https://arxiv.org/abs/2408.12673",
    "authors": [
      "Zhibo Jin",
      "Jiayu Zhang",
      "Zhiyu Zhu",
      "Yuchen Zhang",
      "Jiahao Huang",
      "Jianlong Zhou",
      "Fang Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12674",
    "title": "One-shot Video Imitation via Parameterized Symbolic Abstraction Graphs",
    "abstract": "           Learning to manipulate dynamic and deformable objects from a single demonstration video holds great promise in terms of scalability. Previous approaches have predominantly focused on either replaying object relationships or actor trajectories. The former often struggles to generalize across diverse tasks, while the latter suffers from data inefficiency. Moreover, both methodologies encounter challenges in capturing invisible physical attributes, such as forces. In this paper, we propose to interpret video demonstrations through Parameterized Symbolic Abstraction Graphs (PSAG), where nodes represent objects and edges denote relationships between objects. We further ground geometric constraints through simulation to estimate non-geometric, visually imperceptible attributes. The augmented PSAG is then applied in real robot experiments. Our approach has been validated across a range of tasks, such as Cutting Avocado, Cutting Vegetable, Pouring Liquid, Rolling Dough, and Slicing Pizza. We demonstrate successful generalization to novel objects with distinct visual and physical properties.         ",
    "url": "https://arxiv.org/abs/2408.12674",
    "authors": [
      "Jianren Wang",
      "Kangni Liu",
      "Dingkun Guo",
      "Xian Zhou",
      "Christopher G Atkeson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12680",
    "title": "Can LLMs Understand Social Norms in Autonomous Driving Games?",
    "abstract": "           Social norm is defined as a shared standard of acceptable behavior in a society. The emergence of social norms fosters coordination among agents without any hard-coded rules, which is crucial for the large-scale deployment of AVs in an intelligent transportation system. This paper explores the application of LLMs in understanding and modeling social norms in autonomous driving games. We introduce LLMs into autonomous driving games as intelligent agents who make decisions according to text prompts. These agents are referred to as LLM-based agents. Our framework involves LLM-based agents playing Markov games in a multi-agent system (MAS), allowing us to investigate the emergence of social norms among individual agents. We aim to identify social norms by designing prompts and utilizing LLMs on textual information related to the environment setup and the observations of LLM-based agents. Using the OpenAI Chat API powered by GPT-4.0, we conduct experiments to simulate interactions and evaluate the performance of LLM-based agents in two driving scenarios: unsignalized intersection and highway platoon. The results show that LLM-based agents can handle dynamically changing environments in Markov games, and social norms evolve among LLM-based agents in both scenarios. In the intersection game, LLM-based agents tend to adopt a conservative driving policy when facing a potential car crash. The advantage of LLM-based agents in games lies in their strong operability and analyzability, which facilitate experimental design.         ",
    "url": "https://arxiv.org/abs/2408.12680",
    "authors": [
      "Boxuan Wang",
      "Haonan Duan",
      "Yanhao Feng",
      "Xu Chen",
      "Yongjie Fu",
      "Zhaobin Mo",
      "Xuan Di"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12687",
    "title": "Bridging the gap between natural user expression with complex automation programming in smart homes",
    "abstract": "           A long-standing challenge in end-user programming (EUP) is to trade off between natural user expression and the complexity of programming tasks. As large language models (LLMs) are empowered to handle semantic inference and natural language understanding, it remains under-explored how such capabilities can facilitate end-users to configure complex automation more naturally and easily. We propose AwareAuto, an EUP system that standardizes user expression and finishes two-step inference with the LLMs to achieve automation generation. AwareAuto allows contextual, multi-modality, and flexible user expression to configure complex automation tasks (e.g., dynamic parameters, multiple conditional branches, and temporal constraints), which are non-manageable in traditional EUP solutions. By studying realistic, complex rules data, AwareAuto gains 91.7% accuracy in matching user intentions and feasibility. We introduced user interaction to ensure system controllability and usability. We discuss the opportunities and challenges of incorporating LLMs in end-user programming techniques and grounding complex smart home contexts.         ",
    "url": "https://arxiv.org/abs/2408.12687",
    "authors": [
      "Yingtian Shi",
      "Xiaoyi Liu",
      "Chun Yu",
      "Tianao Yang",
      "Cheng Gao",
      "Chen Liang",
      "Yuanchun Shi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2408.12695",
    "title": "Learning Valid Dual Bounds in Constraint Programming: Boosted Lagrangian Decomposition with Self-Supervised Learning",
    "abstract": "           Lagrangian decomposition (LD) is a relaxation method that provides a dual bound for constrained optimization problems by decomposing them into more manageable sub-problems. This bound can be used in branch-and-bound algorithms to prune the search space effectively. In brief, a vector of Lagrangian multipliers is associated with each sub-problem, and an iterative procedure (e.g., a sub-gradient optimization) adjusts these multipliers to find the tightest bound. Initially applied to integer programming, Lagrangian decomposition also had success in constraint programming due to its versatility and the fact that global constraints provide natural sub-problems. However, the non-linear and combinatorial nature of sub-problems in constraint programming makes it computationally intensive to optimize the Lagrangian multipliers with sub-gradient methods at each node of the tree search. This currently limits the practicality of LD as a general bounding mechanism for constraint programming. To address this challenge, we propose a self-supervised learning approach that leverages neural networks to generate multipliers directly, yielding tight bounds. This approach significantly reduces the number of sub-gradient optimization steps required, enhancing the pruning efficiency and reducing the execution time of constraint programming solvers. This contribution is one of the few that leverage learning to enhance bounding mechanisms on the dual side, a critical element in the design of combinatorial solvers. To our knowledge, this work presents the first generic method for learning valid dual bounds in constraint programming.         ",
    "url": "https://arxiv.org/abs/2408.12695",
    "authors": [
      "Swann Bessa",
      "Darius Dabert",
      "Max Bourgeat",
      "Louis-Martin Rousseau",
      "Quentin Cappart"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12702",
    "title": "Ant Backpressure Routing for Wireless Multi-hop Networks with Mixed Traffic Patterns",
    "abstract": "           A mixture of streaming and short-lived traffic presents a common yet challenging scenario for Backpressure routing in wireless multi-hop networks. Although state-of-the-art shortest-path biased backpressure (SP-BP) can significantly improve the latency of backpressure routing while retaining throughput optimality, it still suffers from the last-packet problem due to its inherent per-commodity queue structure and link capacity assignment. To address this challenge, we propose Ant Backpressure (Ant-BP), a fully distributed routing scheme that incorporates the multi-path routing capability of SP-BP into ant colony optimization (ACO) routing, which allows packets of different commodities to share link capacity in a first-in-first-out (FIFO) manner. Numerical evaluations show that Ant-BP can improve the latency and delivery ratio over SP-BP and ACO routing schemes, while achieving the same throughput of SP-BP under low-to-medium traffic loads.         ",
    "url": "https://arxiv.org/abs/2408.12702",
    "authors": [
      "Negar Erfaniantaghvayi",
      "Zhongyuan Zhao",
      "Kevin Chan",
      "Gunjan Verma",
      "Ananthram Swami",
      "Santiago Segarra"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2408.12708",
    "title": "Revisiting Cross-Domain Problem for LiDAR-based 3D Object Detection",
    "abstract": "           Deep learning models such as convolutional neural networks and transformers have been widely applied to solve 3D object detection problems in the domain of autonomous driving. While existing models have achieved outstanding performance on most open benchmarks, the generalization ability of these deep networks is still in doubt. To adapt models to other domains including different cities, countries, and weather, retraining with the target domain data is currently necessary, which hinders the wide application of autonomous driving. In this paper, we deeply analyze the cross-domain performance of the state-of-the-art models. We observe that most models will overfit the training domains and it is challenging to adapt them to other domains directly. Existing domain adaptation methods for 3D object detection problems are actually shifting the models' knowledge domain instead of improving their generalization ability. We then propose additional evaluation metrics -- the side-view and front-view AP -- to better analyze the core issues of the methods' heavy drops in accuracy levels. By using the proposed metrics and further evaluating the cross-domain performance in each dimension, we conclude that the overfitting problem happens more obviously on the front-view surface and the width dimension which usually faces the sensor and has more 3D points surrounding it. Meanwhile, our experiments indicate that the density of the point cloud data also significantly influences the models' cross-domain performance.         ",
    "url": "https://arxiv.org/abs/2408.12708",
    "authors": [
      "Ruixiao Zhang",
      "Juheon Lee",
      "Xiaohao Cai",
      "Adam Prugel-Bennett"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12727",
    "title": "BankTweak: Adversarial Attack against Multi-Object Trackers by Manipulating Feature Banks",
    "abstract": "           Multi-object tracking (MOT) aims to construct moving trajectories for objects, and modern multi-object trackers mainly utilize the tracking-by-detection methodology. Initial approaches to MOT attacks primarily aimed to degrade the detection quality of the frames under attack, thereby reducing accuracy only in those specific frames, highlighting a lack of \\textit{efficiency}. To improve efficiency, recent advancements manipulate object positions to cause persistent identity (ID) switches during the association phase, even after the attack ends within a few frames. However, these position-manipulating attacks have inherent limitations, as they can be easily counteracted by adjusting distance-related parameters in the association phase, revealing a lack of \\textit{robustness}. In this paper, we present \\textsf{BankTweak}, a novel adversarial attack designed for MOT trackers, which features efficiency and robustness. \\textsf{BankTweak} focuses on the feature extractor in the association phase and reveals vulnerability in the Hungarian matching method used by feature-based MOT systems. Exploiting the vulnerability, \\textsf{BankTweak} induces persistent ID switches (addressing \\textit{efficiency}) even after the attack ends by strategically injecting altered features into the feature banks without modifying object positions (addressing \\textit{robustness}). To demonstrate the applicability, we apply \\textsf{BankTweak} to three multi-object trackers (DeepSORT, StrongSORT, and MOTDT) with one-stage, two-stage, anchor-free, and transformer detectors. Extensive experiments on the MOT17 and MOT20 datasets show that our method substantially surpasses existing attacks, exposing the vulnerability of the tracking-by-detection framework to \\textsf{BankTweak}.         ",
    "url": "https://arxiv.org/abs/2408.12727",
    "authors": [
      "Woojin Shin",
      "Donghwa Kang",
      "Daejin Choi",
      "Brent Kang",
      "Jinkyu Lee",
      "Hyeongboo Baek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12747",
    "title": "CatFree3D: Category-agnostic 3D Object Detection with Diffusion",
    "abstract": "           Image-based 3D object detection is widely employed in applications such as autonomous vehicles and robotics, yet current systems struggle with generalisation due to complex problem setup and limited training data. We introduce a novel pipeline that decouples 3D detection from 2D detection and depth prediction, using a diffusion-based approach to improve accuracy and support category-agnostic detection. Additionally, we introduce the Normalised Hungarian Distance (NHD) metric for an accurate evaluation of 3D detection results, addressing the limitations of traditional IoU and GIoU metrics. Experimental results demonstrate that our method achieves state-of-the-art accuracy and strong generalisation across various object categories and datasets.         ",
    "url": "https://arxiv.org/abs/2408.12747",
    "authors": [
      "Wenjing Bian",
      "Zirui Wang",
      "Andrea Vedaldi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12748",
    "title": "SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection",
    "abstract": "           Large language models (LLMs) are highly capable but face latency challenges in real-time applications, such as conducting online hallucination detection. To overcome this issue, we propose a novel framework that leverages a small language model (SLM) classifier for initial detection, followed by a LLM as constrained reasoner to generate detailed explanations for detected hallucinated content. This study optimizes the real-time interpretable hallucination detection by introducing effective prompting techniques that align LLM-generated explanations with SLM decisions. Empirical experiment results demonstrate its effectiveness, thereby enhancing the overall user experience.         ",
    "url": "https://arxiv.org/abs/2408.12748",
    "authors": [
      "Mengya Hu",
      "Rui Xu",
      "Deren Lei",
      "Yaxi Li",
      "Mingyu Wang",
      "Emily Ching",
      "Eslam Kamal",
      "Alex Deng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12753",
    "title": "Contrastive Representation Learning for Dynamic Link Prediction in Temporal Networks",
    "abstract": "           Evolving networks are complex data structures that emerge in a wide range of systems in science and engineering. Learning expressive representations for such networks that encode their structural connectivity and temporal evolution is essential for downstream data analytics and machine learning applications. In this study, we introduce a self-supervised method for learning representations of temporal networks and employ these representations in the dynamic link prediction task. While temporal networks are typically characterized as a sequence of interactions over the continuous time domain, our study focuses on their discrete-time versions. This enables us to balance the trade-off between computational complexity and precise modeling of the interactions. We propose a recurrent message-passing neural network architecture for modeling the information flow over time-respecting paths of temporal networks. The key feature of our method is the contrastive training objective of the model, which is a combination of three loss functions: link prediction, graph reconstruction, and contrastive predictive coding losses. The contrastive predictive coding objective is implemented using infoNCE losses at both local and global scales of the input graphs. We empirically show that the additional self-supervised losses enhance the training and improve the model's performance in the dynamic link prediction task. The proposed method is tested on Enron, COLAB, and Facebook datasets and exhibits superior results compared to existing models.         ",
    "url": "https://arxiv.org/abs/2408.12753",
    "authors": [
      "Amirhossein Nouranizadeh",
      "Fatemeh Tabatabaei Far",
      "Mohammad Rahmati"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2408.12767",
    "title": "When In-memory Computing Meets Spiking Neural Networks -- A Perspective on Device-Circuit-System-and-Algorithm Co-design",
    "abstract": "           This review explores the intersection of bio-plausible artificial intelligence in the form of Spiking Neural Networks (SNNs) with the analog In-Memory Computing (IMC) domain, highlighting their collective potential for low-power edge computing environments. Through detailed investigation at the device, circuit, and system levels, we highlight the pivotal synergies between SNNs and IMC architectures. Additionally, we emphasize the critical need for comprehensive system-level analyses, considering the inter-dependencies between algorithms, devices, circuit & system parameters, crucial for optimal performance. An in-depth analysis leads to identification of key system-level bottlenecks arising from device limitations which can be addressed using SNN-specific algorithm-hardware co-design techniques. This review underscores the imperative for holistic device to system design space co-exploration, highlighting the critical aspects of hardware and algorithm research endeavors for low-power neuromorphic solutions.         ",
    "url": "https://arxiv.org/abs/2408.12767",
    "authors": [
      "Abhishek Moitra",
      "Abhiroop Bhattacharjee",
      "Yuhang Li",
      "Youngeun Kim",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12774",
    "title": "Semi-Supervised Variational Adversarial Active Learning via Learning to Rank and Agreement-Based Pseudo Labeling",
    "abstract": "           Active learning aims to alleviate the amount of labor involved in data labeling by automating the selection of unlabeled samples via an acquisition function. For example, variational adversarial active learning (VAAL) leverages an adversarial network to discriminate unlabeled samples from labeled ones using latent space information. However, VAAL has the following shortcomings: (i) it does not exploit target task information, and (ii) unlabeled data is only used for sample selection rather than model training. To address these limitations, we introduce novel techniques that significantly improve the use of abundant unlabeled data during training and take into account the task information. Concretely, we propose an improved pseudo-labeling algorithm that leverages information from all unlabeled data in a semi-supervised manner, thus allowing a model to explore a richer data space. In addition, we develop a ranking-based loss prediction module that converts predicted relative ranking information into a differentiable ranking loss. This loss can be embedded as a rank variable into the latent space of a variational autoencoder and then trained with a discriminator in an adversarial fashion for sample selection. We demonstrate the superior performance of our approach over the state of the art on various image classification and segmentation benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2408.12774",
    "authors": [
      "Zongyao Lyu",
      "William J. Beksi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12787",
    "title": "LLM-PBE: Assessing Data Privacy in Large Language Models",
    "abstract": "           Large Language Models (LLMs) have become integral to numerous domains, significantly advancing applications in data management, mining, and analysis. Their profound capabilities in processing and interpreting complex language data, however, bring to light pressing concerns regarding data privacy, especially the risk of unintentional training data leakage. Despite the critical nature of this issue, there has been no existing literature to offer a comprehensive assessment of data privacy risks in LLMs. Addressing this gap, our paper introduces LLM-PBE, a toolkit crafted specifically for the systematic evaluation of data privacy risks in LLMs. LLM-PBE is designed to analyze privacy across the entire lifecycle of LLMs, incorporating diverse attack and defense strategies, and handling various data types and metrics. Through detailed experimentation with multiple LLMs, LLM-PBE facilitates an in-depth exploration of data privacy concerns, shedding light on influential factors such as model size, data characteristics, and evolving temporal dimensions. This study not only enriches the understanding of privacy issues in LLMs but also serves as a vital resource for future research in the field. Aimed at enhancing the breadth of knowledge in this area, the findings, resources, and our full technical report are made available at this https URL, providing an open platform for academic and practical advancements in LLM privacy assessment.         ",
    "url": "https://arxiv.org/abs/2408.12787",
    "authors": [
      "Qinbin Li",
      "Junyuan Hong",
      "Chulin Xie",
      "Jeffrey Tan",
      "Rachel Xin",
      "Junyi Hou",
      "Xavier Yin",
      "Zhun Wang",
      "Dan Hendrycks",
      "Zhangyang Wang",
      "Bo Li",
      "Bingsheng He",
      "Dawn Song"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12789",
    "title": "Context-Aware Temporal Embedding of Objects in Video Data",
    "abstract": "           In video analysis, understanding the temporal context is crucial for recognizing object interactions, event patterns, and contextual changes over time. The proposed model leverages adjacency and semantic similarities between objects from neighboring video frames to construct context-aware temporal object embeddings. Unlike traditional methods that rely solely on visual appearance, our temporal embedding model considers the contextual relationships between objects, creating a meaningful embedding space where temporally connected object's vectors are positioned in proximity. Empirical studies demonstrate that our context-aware temporal embeddings can be used in conjunction with conventional visual embeddings to enhance the effectiveness of downstream applications. Moreover, the embeddings can be used to narrate a video using a Large Language Model (LLM). This paper describes the intricate details of the proposed objective function to generate context-aware temporal object embeddings for video data and showcases the potential applications of the generated embeddings in video analysis and object classification tasks.         ",
    "url": "https://arxiv.org/abs/2408.12789",
    "authors": [
      "Ahnaf Farhan",
      "M. Shahriar Hossain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12792",
    "title": "Event Detection via Probability Density Function Regression",
    "abstract": "           In the domain of time series analysis, particularly in event detection tasks, current methodologies predominantly rely on segmentation-based approaches, which predict the class label for each individual timesteps and use the changepoints of these labels to detect events. However, these approaches may not effectively detect the precise onset and offset of events within the data and suffer from class imbalance problems. This study introduces a generalized regression-based approach to reframe the time-interval-defined event detection problem. Inspired by heatmap regression techniques from computer vision, our approach aims to predict probability densities at event locations rather than class labels across the entire time series. The primary aim of this approach is to improve the accuracy of event detection methods, particularly for long-duration events where identifying the onset and offset is more critical than classifying individual event states. We demonstrate that regression-based approaches outperform segmentation-based methods across various state-of-the-art baseline networks and datasets, offering a more effective solution for specific event detection tasks.         ",
    "url": "https://arxiv.org/abs/2408.12792",
    "authors": [
      "Clark Peng",
      "Tolga Din\u00e7er"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.12793",
    "title": "La-SoftMoE CLIP for Unified Physical-Digital Face Attack Detection",
    "abstract": "           Facial recognition systems are susceptible to both physical and digital attacks, posing significant security risks. Traditional approaches often treat these two attack types separately due to their distinct characteristics. Thus, when being combined attacked, almost all methods could not deal. Some studies attempt to combine the sparse data from both types of attacks into a single dataset and try to find a common feature space, which is often impractical due to the space is difficult to be found or even non-existent. To overcome these challenges, we propose a novel approach that uses the sparse model to handle sparse data, utilizing different parameter groups to process distinct regions of the sparse feature space. Specifically, we employ the Mixture of Experts (MoE) framework in our model, expert parameters are matched to tokens with varying weights during training and adaptively activated during testing. However, the traditional MoE struggles with the complex and irregular classification boundaries of this problem. Thus, we introduce a flexible self-adapting weighting mechanism, enabling the model to better fit and adapt. In this paper, we proposed La-SoftMoE CLIP, which allows for more flexible adaptation to the Unified Attack Detection (UAD) task, significantly enhancing the model's capability to handle diversity attacks. Experiment results demonstrate that our proposed method has SOTA performance.         ",
    "url": "https://arxiv.org/abs/2408.12793",
    "authors": [
      "Hang Zou",
      "Chenxi Du",
      "Hui Zhang",
      "Yuan Zhang",
      "Ajian Liu",
      "Jun Wan",
      "Zhen Lei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12795",
    "title": "From Mobilisation to Radicalisation: Probing the Persistence and Radicalisation of Social Movements Using an Agent-Based Model",
    "abstract": "           We are living in an age of protest. Although we have an excellent understanding of the factors that predict participation in protest, we understand little about the conditions that foster a sustained (versus transient) movement. How do interactions between supporters and authorities combine to influence whether and how people engage (i.e., using conventional or radical tactics)? This paper introduces a novel, theoretically-founded and empirically-informed agent-based model (DIMESim) to address these questions. We model the complex interactions between the psychological attributes of the protester (agents), the authority to whom the protests are targeted, and the environment that allows protesters to coordinate with each other -- over time, and at a population scale. Where an authority is responsive and failure is contested, a modest sized conventional movement endured. Where authorities repeatedly and incontrovertibly fail the movement, the population disengaged from action but evidenced an ongoing commitment to radicalism (latent radicalism).         ",
    "url": "https://arxiv.org/abs/2408.12795",
    "authors": [
      "Emma F. Thomas",
      "Mengbin Ye",
      "Simon D. Angus",
      "Tony J. Mathew",
      "Winnifred Louis",
      "Liam Walsh",
      "Silas Ellery",
      "Morgana Lizzio-Wilson",
      "Craig McGarty"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Multiagent Systems (cs.MA)",
      "Physics and Society (physics.soc-ph)"
    ]
  },
  {
    "id": "arXiv:2408.12798",
    "title": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models",
    "abstract": "           Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses. While most backdoor research has focused on vision or text classification tasks, backdoor attacks in text generation have been largely overlooked. In this work, we introduce \\textit{BackdoorLLM}, the first comprehensive benchmark for studying backdoor attacks on LLMs. \\textit{BackdoorLLM} features: 1) a repository of backdoor benchmarks with a standardized training pipeline, 2) diverse attack strategies, including data poisoning, weight poisoning, hidden state attacks, and chain-of-thought attacks, 3) extensive evaluations with over 200 experiments on 8 attacks across 7 scenarios and 6 model architectures, and 4) key insights into the effectiveness and limitations of backdoors in LLMs. We hope \\textit{BackdoorLLM} will raise awareness of backdoor threats and contribute to advancing AI safety. The code is available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2408.12798",
    "authors": [
      "Yige Li",
      "Hanxun Huang",
      "Yunhan Zhao",
      "Xingjun Ma",
      "Jun Sun"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12801",
    "title": "Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy",
    "abstract": "           In contemporary data-driven environments, the generation and processing of multivariate time series data is an omnipresent challenge, often complicated by time delays between different time series. These delays, originating from a multitude of sources like varying data transmission dynamics, sensor interferences, and environmental changes, introduce significant complexities. Traditional Time Delay Estimation methods, which typically assume a fixed constant time delay, may not fully capture these variabilities, compromising the precision of predictive models in diverse settings. To address this issue, we introduce the Time Series Model Bootstrap (TSMB), a versatile framework designed to handle potentially varying or even nondeterministic time delays in time series modeling. Contrary to traditional approaches that hinge on the assumption of a single, consistent time delay, TSMB adopts a nonparametric stance, acknowledging and incorporating time delay uncertainties. TSMB significantly bolsters the performance of models that are trained and make predictions using this framework, making it highly suitable for a wide range of dynamic and interconnected data environments.         ",
    "url": "https://arxiv.org/abs/2408.12801",
    "authors": [
      "Jiajie Wang",
      "Zhiyuan Jerry Lin",
      "Wen Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.12806",
    "title": "Is Generative AI the Next Tactical Cyber Weapon For Threat Actors? Unforeseen Implications of AI Generated Cyber Attacks",
    "abstract": "           In an era where digital threats are increasingly sophisticated, the intersection of Artificial Intelligence and cybersecurity presents both promising defenses and potent dangers. This paper delves into the escalating threat posed by the misuse of AI, specifically through the use of Large Language Models (LLMs). This study details various techniques like the switch method and character play method, which can be exploited by cybercriminals to generate and automate cyber attacks. Through a series of controlled experiments, the paper demonstrates how these models can be manipulated to bypass ethical and privacy safeguards to effectively generate cyber attacks such as social engineering, malicious code, payload generation, and spyware. By testing these AI generated attacks on live systems, the study assesses their effectiveness and the vulnerabilities they exploit, offering a practical perspective on the risks AI poses to critical infrastructure. We also introduce Occupy AI, a customized, finetuned LLM specifically engineered to automate and execute cyberattacks. This specialized AI driven tool is adept at crafting steps and generating executable code for a variety of cyber threats, including phishing, malware injection, and system exploitation. The results underscore the urgency for ethical AI practices, robust cybersecurity measures, and regulatory oversight to mitigate AI related threats. This paper aims to elevate awareness within the cybersecurity community about the evolving digital threat landscape, advocating for proactive defense strategies and responsible AI development to protect against emerging cyber threats.         ",
    "url": "https://arxiv.org/abs/2408.12806",
    "authors": [
      "Yusuf Usman",
      "Aadesh Upadhyay",
      "Prashnna Gyawali",
      "Robin Chataut"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12807",
    "title": "Code Ownership: The Principles, Differences, and Their Associations with Software Quality",
    "abstract": "           Code ownership -- an approximation of the degree of ownership of a software component -- is one of the important software measures used in quality improvement plans. However, prior studies proposed different variants of code ownership approximations. Yet, little is known about the difference in code ownership approximations and their association with software quality. In this paper, we investigate the differences in the commonly used ownership approximations (i.e., commit-based and line-based) in terms of the set of developers, the approximated code ownership values, and the expertise level. Then, we analyze the association of each code ownership approximation with the defect-proneness. Through an empirical study of 25 releases that span real-world open-source software systems, we find that commit-based and line-based ownership approximations produce different sets of developers, different code ownership values, and different sets of major developers. In addition, we find that the commit-based approximation has a stronger association with software quality than the line-based approximation. Based on our analysis, we recommend line-based code ownership be used for accountability purposes (e.g., authorship attribution, intellectual property), while commit-based code ownership should be used for rapid bug-fixing and charting quality improvement plans.         ",
    "url": "https://arxiv.org/abs/2408.12807",
    "authors": [
      "Patanamon Thongtanunam",
      "Chakkrit Tantithamthavorn"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.12829",
    "title": "Uncertainty-Aware Mean Opinion Score Prediction",
    "abstract": "           Mean Opinion Score (MOS) prediction has made significant progress in specific domains. However, the unstable performance of MOS prediction models across diverse samples presents ongoing challenges in the practical application of these systems. In this paper, we point out that the absence of uncertainty modeling is a significant limitation hindering MOS prediction systems from applying to the real and open world. We analyze the sources of uncertainty in the MOS prediction task and propose to establish an uncertainty-aware MOS prediction system that models aleatory uncertainty and epistemic uncertainty by heteroscedastic regression and Monte Carlo dropout separately. The experimental results show that the system captures uncertainty well and is capable of performing selective prediction and out-of-domain detection. Such capabilities significantly enhance the practical utility of MOS systems in diverse real and open-world environments.         ",
    "url": "https://arxiv.org/abs/2408.12829",
    "authors": [
      "Hui Wang",
      "Shiwan Zhao",
      "Jiaming Zhou",
      "Xiguang Zheng",
      "Haoqin Sun",
      "Xuechen Wang",
      "Yong Qin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2408.12831",
    "title": "SIMPNet: Spatial-Informed Motion Planning Network",
    "abstract": "           Current robotic manipulators require fast and efficient motion-planning algorithms to operate in cluttered environments. State-of-the-art sampling-based motion planners struggle to scale to high-dimensional configuration spaces and are inefficient in complex environments. This inefficiency arises because these planners utilize either uniform or hand-crafted sampling heuristics within the configuration space. To address these challenges, we present the Spatial-informed Motion Planning Network (SIMPNet). SIMPNet consists of a stochastic graph neural network (GNN)-based sampling heuristic for informed sampling within the configuration space. The sampling heuristic of SIMPNet encodes the workspace embedding into the configuration space through a cross-attention mechanism. It encodes the manipulator's kinematic structure into a graph, which is used to generate informed samples within the framework of sampling-based motion planning algorithms. We have evaluated the performance of SIMPNet using a UR5e robotic manipulator operating within simple and complex workspaces, comparing it against baseline state-of-the-art motion planners. The evaluation results show the effectiveness and advantages of the proposed planner compared to the baseline planners.         ",
    "url": "https://arxiv.org/abs/2408.12831",
    "authors": [
      "Davood Soleymanzadeh",
      "Xiao Liang",
      "Minghui Zheng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.12832",
    "title": "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction",
    "abstract": "           Human mobility prediction is essential for applications like urban planning and transportation management, yet it remains challenging due to the complex, often implicit, intentions behind human behavior. Existing models predominantly focus on spatiotemporal patterns, paying less attention to the underlying intentions that govern movements. Recent advancements in large language models (LLMs) offer a promising alternative research angle for integrating commonsense reasoning into mobility prediction. However, it is a non-trivial problem because LLMs are not natively built for mobility intention inference, and they also face scalability issues and integration difficulties with spatiotemporal models. To address these challenges, we propose a novel LIMP (LLMs for Intent-ware Mobility Prediction) framework. Specifically, LIMP introduces an \"Analyze-Abstract-Infer\" (A2I) agentic workflow to unleash LLM's commonsense reasoning power for mobility intention inference. Besides, we design an efficient fine-tuning scheme to transfer reasoning power from commercial LLM to smaller-scale, open-source language model, ensuring LIMP's scalability to millions of mobility records. Moreover, we propose a transformer-based intention-aware mobility prediction model to effectively harness the intention inference ability of LLM. Evaluated on two real-world datasets, LIMP significantly outperforms baseline models, demonstrating improved accuracy in next-location prediction and effective intention inference. The interpretability of intention-aware mobility prediction highlights our LIMP framework's potential for real-world applications. Codes and data can be found in this https URL .         ",
    "url": "https://arxiv.org/abs/2408.12832",
    "authors": [
      "Songwei Li",
      "Jie Feng",
      "Jiawei Chi",
      "Xinyuan Hu",
      "Xiaomeng Zhao",
      "Fengli Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.12840",
    "title": "HGNAS: Hardware-Aware Graph Neural Architecture Search for Edge Devices",
    "abstract": "           Graph Neural Networks (GNNs) are becoming increasingly popular for graph-based learning tasks such as point cloud processing due to their state-of-the-art (SOTA) performance. Nevertheless, the research community has primarily focused on improving model expressiveness, lacking consideration of how to design efficient GNN models for edge scenarios with real-time requirements and limited resources. Examining existing GNN models reveals varied execution across platforms and frequent Out-Of-Memory (OOM) problems, highlighting the need for hardware-aware GNN design. To address this challenge, this work proposes a novel hardware-aware graph neural architecture search framework tailored for resource constraint edge devices, namely HGNAS. To achieve hardware awareness, HGNAS integrates an efficient GNN hardware performance predictor that evaluates the latency and peak memory usage of GNNs in milliseconds. Meanwhile, we study GNN memory usage during inference and offer a peak memory estimation method, enhancing the robustness of architecture evaluations when combined with predictor outcomes. Furthermore, HGNAS constructs a fine-grained design space to enable the exploration of extreme performance architectures by decoupling the GNN paradigm. In addition, the multi-stage hierarchical search strategy is leveraged to facilitate the navigation of huge candidates, which can reduce the single search time to a few GPU hours. To the best of our knowledge, HGNAS is the first automated GNN design framework for edge devices, and also the first work to achieve hardware awareness of GNNs across different platforms. Extensive experiments across various applications and edge devices have proven the superiority of HGNAS. It can achieve up to a 10.6x speedup and an 82.5% peak memory reduction with negligible accuracy loss compared to DGCNN on ModelNet40.         ",
    "url": "https://arxiv.org/abs/2408.12840",
    "authors": [
      "Ao Zhou",
      "Jianlei Yang",
      "Yingjie Qi",
      "Tong Qiao",
      "Yumeng Shi",
      "Cenlin Duan",
      "Weisheng Zhao",
      "Chunming Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12841",
    "title": "COVID-19 Probability Prediction Using Machine Learning: An Infectious Approach",
    "abstract": "           The ongoing COVID-19 pandemic continues to pose significant challenges to global public health, despite the widespread availability of vaccines. Early detection of the disease remains paramount in curbing its transmission and mitigating its impact on public health systems. In response, this study delves into the application of advanced machine learning (ML) techniques for predicting COVID-19 infection probability. We conducted a rigorous investigation into the efficacy of various ML models, including XGBoost, LGBM, AdaBoost, Logistic Regression, Decision Tree, RandomForest, CatBoost, KNN, and Deep Neural Networks (DNN). Leveraging a dataset comprising 4000 samples, with 3200 allocated for training and 800 for testing, our experiment offers comprehensive insights into the performance of these models in COVID-19 prediction. Our findings reveal that Deep Neural Networks (DNN) emerge as the top-performing model, exhibiting superior accuracy and recall metrics. With an impressive accuracy rate of 89%, DNN demonstrates remarkable potential in early COVID-19 detection. This underscores the efficacy of deep learning approaches in leveraging complex data patterns to identify COVID-19 infections accurately. This study underscores the critical role of machine learning, particularly deep learning methodologies, in augmenting early detection efforts amidst the ongoing pandemic. The success of DNN in accurately predicting COVID-19 infection probability highlights the importance of continued research and development in leveraging advanced technologies to combat infectious diseases.         ",
    "url": "https://arxiv.org/abs/2408.12841",
    "authors": [
      "Mohsen Asghari Ilani",
      "Saba Moftakhar Tehran",
      "Ashkan Kavei",
      "Arian Radmehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12852",
    "title": "Structural Representation Learning and Disentanglement for Evidential Chinese Patent Approval Prediction",
    "abstract": "           Automatic Chinese patent approval prediction is an emerging and valuable task in patent analysis. However, it involves a rigorous and transparent decision-making process that includes patent comparison and examination to assess its innovation and correctness. This resultant necessity of decision evidentiality, coupled with intricate patent comprehension presents significant challenges and obstacles for the patent analysis community. Consequently, few existing studies are addressing this task. This paper presents the pioneering effort on this task using a retrieval-based classification approach. We propose a novel framework called DiSPat, which focuses on structural representation learning and disentanglement to predict the approval of Chinese patents and offer decision-making evidence. DiSPat comprises three main components: base reference retrieval to retrieve the Top-k most similar patents as a reference base; structural patent representation to exploit the inherent claim hierarchy in patents for learning a structural patent representation; disentangled representation learning to learn disentangled patent representations that enable the establishment of an evidential decision-making process. To ensure a thorough evaluation, we have meticulously constructed three datasets of Chinese patents. Extensive experiments on these datasets unequivocally demonstrate our DiSPat surpasses state-of-the-art baselines on patent approval prediction, while also exhibiting enhanced evidentiality.         ",
    "url": "https://arxiv.org/abs/2408.12852",
    "authors": [
      "Jinzhi Shan",
      "Qi Zhang",
      "Chongyang Shi",
      "Mengting Gui",
      "Shoujin Wang",
      "Usman Naseem"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2408.12855",
    "title": "Efficient Training Approaches for Performance Anomaly Detection Models in Edge Computing Environments",
    "abstract": "           Microservice architectures are increasingly used to modularize IoT applications and deploy them in distributed and heterogeneous edge computing environments. Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory), resource contention, etc., which can negatively impact their Quality of Service and violate their Service Level Agreements. Existing research on performance anomaly detection for edge computing environments focuses on model training approaches that either achieve high accuracy at the expense of a time-consuming and resource-intensive training process or prioritize training efficiency at the cost of lower accuracy. To address this gap, while considering the resource constraints and the large number of devices in modern edge platforms, we propose two clustering-based model training approaches : (1) intra-cluster parameter transfer learning-based model training (ICPTL) and (2) cluster-level model training (CM). These approaches aim to find a trade-off between the training efficiency of anomaly detection models and their accuracy. We compared the models trained under ICPTL and CM to models trained for specific devices (most accurate, least efficient) and a single general model trained for all devices (least accurate, most efficient). Our findings show that the model accuracy of ICPTL is comparable to that of the model per device approach while requiring only 40% of the training time. In addition, CM further improves training efficiency by requiring 23% less training time and reducing the number of trained models by approximately 66% compared to ICPTL, yet achieving a higher accuracy than a single general model.         ",
    "url": "https://arxiv.org/abs/2408.12855",
    "authors": [
      "Duneesha Fernando",
      "Maria A. Rodriguez",
      "Patricia Arroba",
      "Leila Ismail",
      "Rajkumar Buyya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2408.12862",
    "title": "Complete Graph Identification in Population Protocols",
    "abstract": "           We consider the population protocol model where indistinguishable state machines, referred to as agents, communicate in pairs. The communication graph specifies potential interactions (\\ie communication) between agent pairs. This paper addresses the complete graph identification problem, requiring agents to determine if their communication graph is a clique or not. We evaluate various settings based on: (i) the fairness preserved by the adversarial scheduler -- either global fairness or weak fairness, and (ii) the knowledge provided to agents beforehand -- either the exact population size $n$, a common upper bound $P$ on $n$, or no prior information. Positively, we show that $O(n^2)$ states per agent suffice to solve the complete graph identification problem under global fairness without prior knowledge. With prior knowledge of $n$, agents can solve the problem using only $O(n)$ states under weak fairness. Negatively, we prove that complete graph identification remains unsolvable under weak fairness when only a common upper bound $P$ on the population size $n$ is known.         ",
    "url": "https://arxiv.org/abs/2408.12862",
    "authors": [
      "Haruki Kanaya",
      "Yuichi Sudo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2408.12866",
    "title": "Obfuscated Memory Malware Detection",
    "abstract": "           Providing security for information is highly critical in the current era with devices enabled with smart technology, where assuming a day without the internet is highly impossible. Fast internet at a cheaper price, not only made communication easy for legitimate users but also for cybercriminals to induce attacks in various dimensions to breach privacy and security. Cybercriminals gain illegal access and breach the privacy of users to harm them in multiple ways. Malware is one such tool used by hackers to execute their malicious intent. Development in AI technology is utilized by malware developers to cause social harm. In this work, we intend to show how Artificial Intelligence and Machine learning can be used to detect and mitigate these cyber-attacks induced by malware in specific obfuscated malware. We conducted experiments with memory feature engineering on memory analysis of malware samples. Binary classification can identify whether a given sample is malware or not, but identifying the type of malware will only guide what next step to be taken for that malware, to stop it from proceeding with its further action. Hence, we propose a multi-class classification model to detect the three types of obfuscated malware with an accuracy of 89.07% using the Classic Random Forest algorithm. To the best of our knowledge, there is very little amount of work done in classifying multiple obfuscated malware by a single model. We also compared our model with a few state-of-the-art models and found it comparatively better.         ",
    "url": "https://arxiv.org/abs/2408.12866",
    "authors": [
      "Sharmila S P",
      "Aruna Tiwari",
      "Narendra S Chaudhari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12869",
    "title": "A Row-wise Algorithm for Graph Realization",
    "abstract": "           Given a $\\{0,1\\}$-matrix $M$, the graph realization problem for $M$ asks if there exists a spanning forest such that the columns of $M$ are incidence vectors of paths in the forest. The problem is closely related to the recognition of network matrices, which are a large subclass of totally unimodular matrices and have many applications in mixed-integer programming. Previously, Bixby and Wagner have designed an efficient algorithm for graph realization that grows a submatrix in a column-wise fashion whilst maintaining a graphic realization. This paper complements their work by providing an algorithm that works in a row-wise fashion and uses similar data structures. The main challenge in designing efficient algorithms for the graph realization problem is ambiguity as there may exist many graphs realizing $M$. The key insight for designing an efficient row-wise algorithm is that a graphic matrix is uniquely represented by an SPQR tree, a graph decomposition that stores all graphs with the same set of cycles. The developed row-wise algorithm uses data structures that are compatible with the column-wise algorithm and can be combined with the latter to detect maximal graphic submatrices.         ",
    "url": "https://arxiv.org/abs/2408.12869",
    "authors": [
      "Rolf van der Hulst",
      "Matthias Walter"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2408.12875",
    "title": "Disentangling, Amplifying, and Debiasing: Learning Disentangled Representations for Fair Graph Neural Networks",
    "abstract": "           Graph Neural Networks (GNNs) have become essential tools for graph representation learning in various domains, such as social media and healthcare. However, they often suffer from fairness issues due to inherent biases in node attributes and graph structure, leading to unfair predictions. To address these challenges, we propose a novel GNN framework, DAB-GNN, that Disentangles, Amplifies, and deBiases attribute, structure, and potential biases in the GNN mechanism. DAB-GNN employs a disentanglement and amplification module that isolates and amplifies each type of bias through specialized disentanglers, followed by a debiasing module that minimizes the distance between subgroup distributions to ensure fairness. Extensive experiments on five datasets demonstrate that DAB-GNN significantly outperforms ten state-of-the-art competitors in terms of achieving an optimal balance between accuracy and fairness.         ",
    "url": "https://arxiv.org/abs/2408.12875",
    "authors": [
      "Yeon-Chang Lee",
      "Hojung Shin",
      "Sang-Wook Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.12876",
    "title": "The local limit theorem for complex valued sequences: the parabolic case",
    "abstract": "           We give a complete expansion, at any accuracy order, for the iterated convolution of a complex valued integrable sequence in one space dimension. The remainders are estimated sharply with generalized Gaussian bounds. The result applies in probability theory for random walks as well as in numerical analysis for studying the large time behavior of numerical schemes.         ",
    "url": "https://arxiv.org/abs/2408.12876",
    "authors": [
      "Jean-Fran\u00e7ois Coulombel",
      "Gr\u00e9gory Faye"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2408.12879",
    "title": "Frequency-aware Feature Fusion for Dense Image Prediction",
    "abstract": "           Dense image prediction tasks demand features with strong category information and precise spatial boundary details at high resolution. To achieve this, modern hierarchical models often utilize feature fusion, directly adding upsampled coarse features from deep layers and high-resolution features from lower levels. In this paper, we observe rapid variations in fused feature values within objects, resulting in intra-category inconsistency due to disturbed high-frequency features. Additionally, blurred boundaries in fused features lack accurate high frequency, leading to boundary displacement. Building upon these observations, we propose Frequency-Aware Feature Fusion (FreqFusion), integrating an Adaptive Low-Pass Filter (ALPF) generator, an offset generator, and an Adaptive High-Pass Filter (AHPF) generator. The ALPF generator predicts spatially-variant low-pass filters to attenuate high-frequency components within objects, reducing intra-class inconsistency during upsampling. The offset generator refines large inconsistent features and thin boundaries by replacing inconsistent features with more consistent ones through resampling, while the AHPF generator enhances high-frequency detailed boundary information lost during downsampling. Comprehensive visualization and quantitative analysis demonstrate that FreqFusion effectively improves feature consistency and sharpens object boundaries. Extensive experiments across various dense prediction tasks confirm its effectiveness. The code is made publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.12879",
    "authors": [
      "Linwei Chen",
      "Ying Fu",
      "Lin Gu",
      "Chenggang Yan",
      "Tatsuya Harada",
      "Gao Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12882",
    "title": "Spatio-Temporal Road Traffic Prediction using Real-time Regional Knowledge",
    "abstract": "           For traffic prediction in transportation services such as car-sharing and ride-hailing, mid-term road traffic prediction (within a few hours) is considered essential. However, the existing road-level traffic prediction has mainly studied how significantly micro traffic events propagate to the adjacent roads in terms of short-term prediction. On the other hand, recent attempts have been made to incorporate regional knowledge such as POIs, road characteristics, and real-time social events to help traffic prediction. However, these studies lack in understandings of different modalities of road-level and region-level spatio-temporal correlations and how to combine such knowledge. This paper proposes a novel method that embeds real-time region-level knowledge using POIs, satellite images, and real-time LTE access traces via a regional spatio-temporal module that consists of dynamic convolution and temporal attention, and conducts bipartite spatial transform attention to convert into road-level knowledge. Then the model ingests this embedded knowledge into a road-level attention-based prediction model. Experimental results on real-world road traffic prediction show that our model outperforms the baselines.         ",
    "url": "https://arxiv.org/abs/2408.12882",
    "authors": [
      "Sumin Han",
      "Jisun An",
      "Dongman Lee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12890",
    "title": "Multiple Areal Feature Aware Transportation Demand Prediction",
    "abstract": "           A reliable short-term transportation demand prediction supports the authorities in improving the capability of systems by optimizing schedules, adjusting fleet sizes, and generating new transit networks. A handful of research efforts incorporate one or a few areal features while learning spatio-temporal correlation, to capture similar demand patterns between similar areas. However, urban characteristics are polymorphic, and they need to be understood by multiple areal features such as land use, sociodemographics, and place-of-interest (POI) distribution. In this paper, we propose a novel spatio-temporal multi-feature-aware graph convolutional recurrent network (ST-MFGCRN) that fuses multiple areal features during spatio-temproal understanding. Inside ST-MFGCRN, we devise sentinel attention to calculate the areal similarity matrix by allowing each area to take partial attention if the feature is not useful. We evaluate the proposed model on two real-world transportation datasets, one with our constructed BusDJ dataset and one with benchmark TaxiBJ. Results show that our model outperforms the state-of-the-art baselines up to 7\\% on BusDJ and 8\\% on TaxiBJ dataset.         ",
    "url": "https://arxiv.org/abs/2408.12890",
    "authors": [
      "Sumin Han",
      "Jisun An",
      "Youngjun Park",
      "Suji Kim",
      "Kitae Jang",
      "Dongman Lee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12946",
    "title": "Soft Decision Decoding of Recursive Plotkin Constructions Based on Hidden Code Words",
    "abstract": "           The Plotkin construction combines two codes to a code of doubled length. It can be applied recursively. The class of Reed-Muller (RM) codes is a particular example. Also, a special class of generalized concatenated codes (GCC) can be described as recursive Plotkin construction. Exploiting a property of the code words constructed by the recursive Plotkin construction, we present novel soft-decision decoders. These are based on the decoding of hidden code words which are inherent contained in the constructed code words and can be uncovered by adding particular parts of the overall code word. The main idea is to use more than one decoding variant where each variant starts with the decoding of a different hidden code word. The final decoding decision selects the best of the decisions of the used variants. The more variants are used the closer the performance gets to the maximum-likelihood (ML) decoding performance. This is verified by an ML-bound for the cases where the ML performance is not known. The decoding algorithms use only additions, comparisons, and sign operations. Further, due to the recursive structure, only relatively short codes have to be decoded, thus, the decoding complexity is very low. In addition, we introduce two novel classes of half-rate codes based on recursive Plotkin constructions with RM codes.         ",
    "url": "https://arxiv.org/abs/2408.12946",
    "authors": [
      "Martin Bossert"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2408.12948",
    "title": "E-code: Mastering Efficient Code Generation through Pretrained Models and Expert Encoder Group",
    "abstract": "           Context: With the waning of Moore's Law, the software industry is placing increasing importance on finding alternative solutions for continuous performance enhancement. The significance and research results of software performance optimization have been on the rise in recent years, especially with the advancement propelled by Large Language Models(LLMs). However, traditional strategies for rectifying performance flaws have shown significant limitations at the competitive code efficiency optimization level, and research on this topic is surprisingly scarce. Objective: This study aims to address the research gap in this domain, offering practical solutions to the various challenges encountered. Specifically, we have overcome the constraints of traditional performance error rectification strategies and developed a Language Model (LM) tailored for the competitive code efficiency optimization realm. Method: We introduced E-code, an advanced program synthesis LM. Inspired by the recent success of expert LMs, we designed an innovative structure called the Expert Encoder Group. This structure employs multiple expert encoders to extract features tailored for different input types. We assessed the performance of E-code against other leading models on a competitive dataset and conducted in-depth ablation experiments. Results: Upon systematic evaluation, E-code achieved a 54.98% improvement in code efficiency, significantly outperforming other advanced models. In the ablation experiments, we further validated the significance of the expert encoder group and other components within E-code. Conclusion: The research findings indicate that the expert encoder group can effectively handle various inputs in efficiency optimization tasks, significantly enhancing the model's performance.         ",
    "url": "https://arxiv.org/abs/2408.12948",
    "authors": [
      "Yue Pan",
      "Chen Lyu",
      "Zhenyu Yang",
      "Lantian Li",
      "Qi Liu",
      "Xiuting Shao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.12953",
    "title": "State-of-the-Art Fails in the Art of Damage Detection",
    "abstract": "           Accurately detecting and classifying damage in analogue media such as paintings, photographs, textiles, mosaics, and frescoes is essential for cultural heritage preservation. While machine learning models excel in correcting global degradation if the damage operator is known a priori, we show that they fail to predict where the damage is even after supervised training; thus, reliable damage detection remains a challenge. We introduce DamBench, a dataset for damage detection in diverse analogue media, with over 11,000 annotations covering 15 damage types across various subjects and media. We evaluate CNN, Transformer, and text-guided diffusion segmentation models, revealing their limitations in generalising across media types.         ",
    "url": "https://arxiv.org/abs/2408.12953",
    "authors": [
      "Daniela Ivanova",
      "Marco Aversa",
      "Paul Henderson",
      "John Williamson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12960",
    "title": "Measuring Code Efficiency Optimization Capabilities with ACEOB",
    "abstract": "           As Moore's Law gains diminish, software performance and efficiency become increasingly vital. Optimizing code efficiency is challenging, even for professional programmers. However, related research remains relatively scarce, and rigorously assessing models' abilities to optimize code efficiency is fraught with difficulties. In response to this challenge, we first conduct an in-depth analysis of \"code patterns\" in the model training dataset, meticulously exploring human-written code. Secondly, we define a task for optimizing code efficiency and introduce the Automatic Code Efficiency Optimization Benchmark (ACEOB), which consists of 95,359 pairs of efficient-inefficient code aimed at assessing code efficiency optimization capabilities. To our knowledge, ACEOB is the first dataset specifically targeting Python code efficiency optimization. To evaluate models' ability in optimizing code efficiency, we propose two new metrics: the Isomorphic Optimal Comparison CodeBLEU (IOCCB) metric and the Normalized Performance Index (NPI) metric, to assess the efficiency of model-generated code. We also evaluate several advanced code models, such as PolyCoder and CodeT5, after fine-tuning them on ACEOB and demonstrate that the efficiency of each model improves after introducing the NPI filter. However, it was observed that even ChatGPT does not perform optimally in code efficiency optimization tasks.         ",
    "url": "https://arxiv.org/abs/2408.12960",
    "authors": [
      "Yue Pan",
      "Xiuting Shao",
      "Chen Lyu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.12978",
    "title": "Energy-Efficient Spiking Recurrent Neural Network for Gesture Recognition on Embedded GPUs",
    "abstract": "           Implementing AI algorithms on event-based embedded devices enables real-time processing of data, minimizes latency, and enhances power efficiency in edge computing. This research explores the deployment of a spiking recurrent neural network (SRNN) with liquid time constant neurons for gesture recognition. We focus on the energy efficiency and computational efficacy of NVIDIA Jetson Nano embedded GPU platforms. The embedded GPU showcases a 14-fold increase in power efficiency relative to a conventional GPU, making a compelling argument for its use in energy-constrained applications. The study's empirical findings also highlight that batch processing significantly boosts frame rates across various batch sizes while maintaining accuracy levels well above the baseline. These insights validate the SRNN with liquid time constant neurons as a robust model for interpreting temporal-spatial data in gesture recognition, striking a critical balance between processing speed and power frugality.         ",
    "url": "https://arxiv.org/abs/2408.12978",
    "authors": [
      "Marzieh Hassanshahi Varposhti",
      "Mahyar Shahsavari",
      "Marcel van Gerven"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12986",
    "title": "Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection",
    "abstract": "           According to our survey of the machine learning for vulnerability detection (ML4VD) literature published in the top Software Engineering conferences, every paper in the past 5 years defines ML4VD as a binary classification problem: Given a function, does it contain a security flaw? In this paper, we ask whether this decision can really be made without further context and study both vulnerable and non-vulnerable functions in the most popular ML4VD datasets. A function is vulnerable if it was involved in a patch of an actual security flaw and confirmed to cause the vulnerability. It is non-vulnerable otherwise. We find that in almost all cases this decision cannot be made without further context. Vulnerable functions are often vulnerable only because a corresponding vulnerability-inducing calling context exists while non-vulnerable functions would often be vulnerable if a corresponding context existed. But why do ML4VD techniques perform so well even though there is demonstrably not enough information in these samples? Spurious correlations: We find that high accuracy can be achieved even when only word counts are available. This shows that these datasets can be exploited to achieve high accuracy without actually detecting any security vulnerabilities. We conclude that the current problem statement of ML4VD is ill-defined and call into question the internal validity of this growing body of work. Constructively, we call for more effective benchmarking methodologies to evaluate the true capabilities of ML4VD, propose alternative problem statements, and examine broader implications for the evaluation of machine learning and programming analysis research.         ",
    "url": "https://arxiv.org/abs/2408.12986",
    "authors": [
      "Niklas Risse",
      "Marcel B\u00f6hme"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12989",
    "title": "RIFF: Inducing Rules for Fraud Detection from Decision Trees",
    "abstract": "           Financial fraud is the cause of multi-billion dollar losses annually. Traditionally, fraud detection systems rely on rules due to their transparency and interpretability, key features in domains where decisions need to be explained. However, rule systems require significant input from domain experts to create and tune, an issue that rule induction algorithms attempt to mitigate by inferring rules directly from data. We explore the application of these algorithms to fraud detection, where rule systems are constrained to have a low false positive rate (FPR) or alert rate, by proposing RIFF, a rule induction algorithm that distills a low FPR rule set directly from decision trees. Our experiments show that the induced rules are often able to maintain or improve performance of the original models for low FPR tasks, while substantially reducing their complexity and outperforming rules hand-tuned by experts.         ",
    "url": "https://arxiv.org/abs/2408.12989",
    "authors": [
      "Jo\u00e3o Lucas Martins",
      "Jo\u00e3o Bravo",
      "Ana Sofia Gomes",
      "Carlos Soares",
      "Pedro Bizarro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12990",
    "title": "A Survey on Drowsiness Detection -- Modern Applications and Methods",
    "abstract": "           Drowsiness detection holds paramount importance in ensuring safety in workplaces or behind the wheel, enhancing productivity, and healthcare across diverse domains. Therefore accurate and real-time drowsiness detection plays a critical role in preventing accidents, enhancing safety, and ultimately saving lives across various sectors and scenarios. This comprehensive review explores the significance of drowsiness detection in various areas of application, transcending the conventional focus solely on driver drowsiness detection. We delve into the current methodologies, challenges, and technological advancements in drowsiness detection schemes, considering diverse contexts such as public transportation, healthcare, workplace safety, and beyond. By examining the multifaceted implications of drowsiness, this work contributes to a holistic understanding of its impact and the crucial role of accurate and real-time detection techniques in enhancing safety and performance. We identified weaknesses in current algorithms and limitations in existing research such as accurate and real-time detection, stable data transmission, and building bias-free systems. Our survey frames existing works and leads to practical recommendations like mitigating the bias issue by using synthetic data, overcoming the hardware limitations with model compression, and leveraging fusion to boost model performance. This is a pioneering work to survey the topic of drowsiness detection in such an entirely and not only focusing on one single aspect. We consider the topic of drowsiness detection as a dynamic and evolving field, presenting numerous opportunities for further exploration.         ",
    "url": "https://arxiv.org/abs/2408.12990",
    "authors": [
      "Biying Fu",
      "Fadi Boutros",
      "Chin-Teng Lin",
      "Naser Damer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2408.12998",
    "title": "Tight Bounds for Constant-Round Domination on Graphs of High Girth and Low Expansion",
    "abstract": "           A long-standing open question is which graph class is the most general one permitting constant-time constant-factor approximations for dominating sets. The approximation ratio has been bounded by increasingly general parameters such as genus, arboricity, or expansion of the input graph. Amiri and Wiederhake considered $k$-hop domination in graphs of bounded $k$-hop expansion and girth at least $4k+3$; the $k$-hop expansion $f(k)$ of a graph family denotes the maximum ratio of edges to nodes that can be achieved by contracting disjoint subgraphs of radius $k$ and deleting nodes. In this setting, these authors to obtain a simple $O(k)$-round algorithm achieving approximation ratio $\\Theta(kf(k))$. In this work, we study the same setting but derive tight bounds: - A $\\Theta(kf(k))$-approximation is possible in $k$, but not $k-1$ rounds. - In $3k$ rounds an $O(k+f(k)^{k/(k+1)})$-approximation can be achieved. - No constant-round deterministic algorithm can achieve approximation ratio $o(k+f(k)^{k/(k+1)})$. Our upper bounds hold in the port numbering model with small messages, while the lower bounds apply to local algorithms, i.e., with arbitrary message size and unique identifiers. This means that the constant-time approximation ratio can be \\emph{sublinear} in the edge density of the graph, in a graph class which does not allow a constant approximation. This begs the question whether this is an artefact of the restriction to high girth or can be extended to all graphs of $k$-hop expansion $f(k)$.         ",
    "url": "https://arxiv.org/abs/2408.12998",
    "authors": [
      "Christoph Lenzen",
      "Sophie Wenning"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2408.13001",
    "title": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution",
    "abstract": "           Code benchmarks such as HumanEval are widely adopted to evaluate Large Language Models' (LLMs) coding capabilities. However, there is an unignorable programming language bias in existing code benchmarks -- over 95% code generation benchmarks are dominated by Python, leaving the LLMs' capabilities in other programming languages such as Java and C/C++ unknown. Moreover, coding task bias is also crucial. Most benchmarks focus on code generation capability, while benchmarks for code reasoning (given input, reasoning output; and given output, reasoning input), an essential coding capability, are insufficient. Yet, constructing multi-lingual benchmarks can be expensive and labor-intensive, and codes in contest websites such as Leetcode suffer from data contamination during training. To fill this gap, we propose CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 programming languages. It comprises at least 600 subjects for each language, along with 19K content-consistent tests in total. In particular, the construction pipeline of CRUXEVAL-X works in a fully automated and test-guided manner, which iteratively generates and repairs based on execution feedback. Also, to cross language barriers (e.g., dynamic/static type systems in Python/C++), we formulated various transition rules between language pairs to facilitate translation. Our intensive evaluation of 24 representative LLMs reveals the correlation between language pairs. For example, TypeScript and JavaScript show a significant positive correlation, while Racket has less correlation with other languages. More interestingly, even a model trained solely on Python can achieve at most 34.4% Pass@1 in other languages, revealing the cross-language generalization of LLMs.         ",
    "url": "https://arxiv.org/abs/2408.13001",
    "authors": [
      "Ruiyang Xu",
      "Jialun Cao",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Ben He",
      "Shing-Chi Cheung",
      "Le Sun"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.13018",
    "title": "Robust Iterative Value Conversion: Deep Reinforcement Learning for Neurochip-driven Edge Robots",
    "abstract": "           A neurochip is a device that reproduces the signal processing mechanisms of brain neurons and calculates Spiking Neural Networks (SNNs) with low power consumption and at high speed. Thus, neurochips are attracting attention from edge robot applications, which suffer from limited battery capacity. This paper aims to achieve deep reinforcement learning (DRL) that acquires SNN policies suitable for neurochip implementation. Since DRL requires a complex function approximation, we focus on conversion techniques from Floating Point NN (FPNN) because it is one of the most feasible SNN techniques. However, DRL requires conversions to SNNs for every policy update to collect the learning samples for a DRL-learning cycle, which updates the FPNN policy and collects the SNN policy samples. Accumulative conversion errors can significantly degrade the performance of the SNN policies. We propose Robust Iterative Value Conversion (RIVC) as a DRL that incorporates conversion error reduction and robustness to conversion errors. To reduce them, FPNN is optimized with the same number of quantization bits as an SNN. The FPNN output is not significantly changed by quantization. To robustify the conversion error, an FPNN policy that is applied with quantization is updated to increase the gap between the probability of selecting the optimal action and other actions. This step prevents unexpected replacements of the policy's optimal actions. We verified RIVC's effectiveness on a neurochip-driven robot. The results showed that RIVC consumed 1/15 times less power and increased the calculation speed by five times more than an edge CPU (quad-core ARM Cortex-A72). The previous framework with no countermeasures against conversion errors failed to train the policies. Videos from our experiments are available: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.13018",
    "authors": [
      "Yuki Kadokawa",
      "Tomohito Kodera",
      "Yoshihisa Tsurumine",
      "Shinya Nishimura",
      "Takamitsu Matsubara"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.13031",
    "title": "VFM-Det: Towards High-Performance Vehicle Detection via Large Foundation Models",
    "abstract": "           Existing vehicle detectors are usually obtained by training a typical detector (e.g., YOLO, RCNN, DETR series) on vehicle images based on a pre-trained backbone (e.g., ResNet, ViT). Some researchers also exploit and enhance the detection performance using pre-trained large foundation models. However, we think these detectors may only get sub-optimal results because the large models they use are not specifically designed for vehicles. In addition, their results heavily rely on visual features, and seldom of they consider the alignment between the vehicle's semantic information and visual representations. In this work, we propose a new vehicle detection paradigm based on a pre-trained foundation vehicle model (VehicleMAE) and a large language model (T5), termed VFM-Det. It follows the region proposal-based detection framework and the features of each proposal can be enhanced using VehicleMAE. More importantly, we propose a new VAtt2Vec module that predicts the vehicle semantic attributes of these proposals and transforms them into feature vectors to enhance the vision features via contrastive learning. Extensive experiments on three vehicle detection benchmark datasets thoroughly proved the effectiveness of our vehicle detector. Specifically, our model improves the baseline approach by $+5.1\\%$, $+6.2\\%$ on the $AP_{0.5}$, $AP_{0.75}$ metrics, respectively, on the Cityscapes dataset.The source code of this work will be released at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.13031",
    "authors": [
      "Wentao Wu",
      "Fanghua Hong",
      "Xiao Wang",
      "Chenglong Li",
      "Jin Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2408.13038",
    "title": "Improving the Classification Effect of Clinical Images of Diseases for Multi-Source Privacy Protection",
    "abstract": "           Privacy data protection in the medical field poses challenges to data sharing, limiting the ability to integrate data across hospitals for training high-precision auxiliary diagnostic models. Traditional centralized training methods are difficult to apply due to violations of privacy protection principles. Federated learning, as a distributed machine learning framework, helps address this issue, but it requires multiple hospitals to participate in training simultaneously, which is hard to achieve in practice. To address these challenges, we propose a medical privacy data training framework based on data vectors. This framework allows each hospital to fine-tune pre-trained models on private data, calculate data vectors (representing the optimization direction of model parameters in the solution space), and sum them up to generate synthetic weights that integrate model information from multiple hospitals. This approach enhances model performance without exchanging private data or requiring synchronous training. Experimental results demonstrate that this method effectively utilizes dispersed private data resources while protecting patient privacy. The auxiliary diagnostic model trained using this approach significantly outperforms models trained independently by a single hospital, providing a new perspective for resolving the conflict between medical data privacy protection and model training and advancing the development of medical intelligence.         ",
    "url": "https://arxiv.org/abs/2408.13038",
    "authors": [
      "Tian Bowen",
      "Xu Zhengyang",
      "Yin Zhihao",
      "Wang Jingying",
      "Yue Yutao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13082",
    "title": "Multivariate Time-Series Anomaly Detection based on Enhancing Graph Attention Networks with Topological Analysis",
    "abstract": "           Unsupervised anomaly detection in time series is essential in industrial applications, as it significantly reduces the need for manual intervention. Multivariate time series pose a complex challenge due to their feature and temporal dimensions. Traditional methods use Graph Neural Networks (GNNs) or Transformers to analyze spatial while RNNs to model temporal dependencies. These methods focus narrowly on one dimension or engage in coarse-grained feature extraction, which can be inadequate for large datasets characterized by intricate relationships and dynamic changes. This paper introduces a novel temporal model built on an enhanced Graph Attention Network (GAT) for multivariate time series anomaly detection called TopoGDN. Our model analyzes both time and feature dimensions from a fine-grained perspective. First, we introduce a multi-scale temporal convolution module to extract detailed temporal features. Additionally, we present an augmented GAT to manage complex inter-feature dependencies, which incorporates graph topology into node features across multiple scales, a versatile, plug-and-play enhancement that significantly boosts the performance of GAT. Our experimental results confirm that our approach surpasses the baseline models on four datasets, demonstrating its potential for widespread application in fields requiring robust anomaly detection. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.13082",
    "authors": [
      "Zhe Liu",
      "Xiang Huang",
      "Jingyun Zhang",
      "Zhifeng Hao",
      "Li Sun",
      "Hao Peng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.13084",
    "title": "Avatar Visual Similarity for Social HCI: Increasing Self-Awareness",
    "abstract": "           Self-awareness is a critical factor in social human-human interaction and, hence, in social HCI interaction. Increasing self-awareness through mirrors or video recordings is common in face-to-face trainings, since it influences antecedents of self-awareness like explicit identification and implicit affective identification (affinity). However, increasing self-awareness has been scarcely examined in virtual trainings with virtual avatars, which allow for adjusting the similarity, e.g. to avoid negative effects of self-consciousness. Automatic visual similarity in avatars is an open issue related to high costs. It is important to understand which features need to be manipulated and which degree of similarity is necessary for self-awareness to leverage the added value of using avatars for self-awareness. This article examines the relationship between avatar visual similarity and increasing self-awareness in virtual training environments. We define visual similarity based on perceptually important facial features for human-human identification and develop a theory-based methodology to systematically manipulate visual similarity of virtual avatars and support self-awareness. Three personalized versions of virtual avatars with varying degrees of visual similarity to participants were created (weak, medium and strong facial features manipulation). In a within-subject study (N=33), we tested effects of degree of similarity on perceived similarity, explicit identification and implicit affective identification (affinity). Results show significant differences between the weak similarity manipulation, and both the strong manipulation and the random avatar for all three antecedents of self-awareness. An increasing degree of avatar visual similarity influences antecedents of self-awareness in virtual environments.         ",
    "url": "https://arxiv.org/abs/2408.13084",
    "authors": [
      "Bernhard Hilpert",
      "Claudio Alves da Silva",
      "Leon Christidis",
      "Chirag Bhuvaneshwara",
      "Patrick Gebhard",
      "Fabrizio Nunnari",
      "Dimitra Tsovaltzi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.13091",
    "title": "Analysis of child development facts and myths using text mining techniques and classification models",
    "abstract": "           The rapid dissemination of misinformation on the internet complicates the decision-making process for individuals seeking reliable information, particularly parents researching child development topics. This misinformation can lead to adverse consequences, such as inappropriate treatment of children based on myths. While previous research has utilized text-mining techniques to predict child abuse cases, there has been a gap in the analysis of child development myths and facts. This study addresses this gap by applying text mining techniques and classification models to distinguish between myths and facts about child development, leveraging newly gathered data from publicly available websites. The research methodology involved several stages. First, text mining techniques were employed to pre-process the data, ensuring enhanced accuracy. Subsequently, the structured data was analysed using six robust Machine Learning (ML) classifiers and one Deep Learning (DL) model, with two feature extraction techniques applied to assess their performance across three different training-testing splits. To ensure the reliability of the results, cross-validation was performed using both k-fold and leave-one-out methods. Among the classification models tested, Logistic Regression (LR) demonstrated the highest accuracy, achieving a 90% accuracy with the Bag-of-Words (BoW) feature extraction technique. LR stands out for its exceptional speed and efficiency, maintaining low testing time per statement (0.97 microseconds). These findings suggest that LR, when combined with BoW, is effective in accurately classifying child development information, thus providing a valuable tool for combating misinformation and assisting parents in making informed decisions.         ",
    "url": "https://arxiv.org/abs/2408.13091",
    "authors": [
      "Mehedi Tajrian",
      "Azizur Rahman",
      "Muhammad Ashad Kabir",
      "Md Rafiqul Islam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.13092",
    "title": "Diffusion-based Episodes Augmentation for Offline Multi-Agent Reinforcement Learning",
    "abstract": "           Offline multi-agent reinforcement learning (MARL) is increasingly recognized as crucial for effectively deploying RL algorithms in environments where real-time interaction is impractical, risky, or costly. In the offline setting, learning from a static dataset of past interactions allows for the development of robust and safe policies without the need for live data collection, which can be fraught with challenges. Building on this foundational importance, we present EAQ, Episodes Augmentation guided by Q-total loss, a novel approach for offline MARL framework utilizing diffusion models. EAQ integrates the Q-total function directly into the diffusion model as a guidance to maximize the global returns in an episode, eliminating the need for separate training. Our focus primarily lies on cooperative scenarios, where agents are required to act collectively towards achieving a shared goal-essentially, maximizing global returns. Consequently, we demonstrate that our episodes augmentation in a collaborative manner significantly boosts offline MARL algorithm compared to the original dataset, improving the normalized return by +17.3% and +12.9% for medium and poor behavioral policies in SMAC simulator, respectively.         ",
    "url": "https://arxiv.org/abs/2408.13092",
    "authors": [
      "Jihwan Oh",
      "Sungnyun Kim",
      "Gahee Kim",
      "Sunghwan Kim",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.13101",
    "title": "Functional Tensor Decompositions for Physics-Informed Neural Networks",
    "abstract": "           Physics-Informed Neural Networks (PINNs) have shown continuous and increasing promise in approximating partial differential equations (PDEs), although they remain constrained by the curse of dimensionality. In this paper, we propose a generalized PINN version of the classical variable separable method. To do this, we first show that, using the universal approximation theorem, a multivariate function can be approximated by the outer product of neural networks, whose inputs are separated variables. We leverage tensor decomposition forms to separate the variables in a PINN setting. By employing Canonic Polyadic (CP), Tensor-Train (TT), and Tucker decomposition forms within the PINN framework, we create robust architectures for learning multivariate functions from separate neural networks connected by outer products. Our methodology significantly enhances the performance of PINNs, as evidenced by improved results on complex high-dimensional PDEs, including the 3d Helmholtz and 5d Poisson equations, among others. This research underscores the potential of tensor decomposition-based variably separated PINNs to surpass the state-of-the-art, offering a compelling solution to the dimensionality challenge in PDE approximation.         ",
    "url": "https://arxiv.org/abs/2408.13101",
    "authors": [
      "Sai Karthikeya Vemuri",
      "Tim B\u00fcchner",
      "Julia Niebling",
      "Joachim Denzler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.13102",
    "title": "Dynamic Label Adversarial Training for Deep Learning Robustness Against Adversarial Attacks",
    "abstract": "           Adversarial training is one of the most effective methods for enhancing model robustness. Recent approaches incorporate adversarial distillation in adversarial training architectures. However, we notice two scenarios of defense methods that limit their performance: (1) Previous methods primarily use static ground truth for adversarial training, but this often causes robust overfitting; (2) The loss functions are either Mean Squared Error or KL-divergence leading to a sub-optimal performance on clean accuracy. To solve those problems, we propose a dynamic label adversarial training (DYNAT) algorithm that enables the target model to gradually and dynamically gain robustness from the guide model's decisions. Additionally, we found that a budgeted dimension of inner optimization for the target model may contribute to the trade-off between clean accuracy and robust accuracy. Therefore, we propose a novel inner optimization method to be incorporated into the adversarial training. This will enable the target model to adaptively search for adversarial examples based on dynamic labels from the guiding model, contributing to the robustness of the target model. Extensive experiments validate the superior performance of our approach.         ",
    "url": "https://arxiv.org/abs/2408.13102",
    "authors": [
      "Zhenyu Liu",
      "Haoran Duan",
      "Huizhi Liang",
      "Yang Long",
      "Vaclav Snasel",
      "Guiseppe Nicosia",
      "Rajiv Ranjan",
      "Varun Ojha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13106",
    "title": "NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks",
    "abstract": "           Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verification and diarization, etc. However, most of these approaches are computationally intensive due to using transformer encoder and lack of sub-sampling. In this paper, we propose a new self-supervised learning model termed as Neural Encoder for Self-supervised Training (NEST). Specifically, we adopt the FastConformer architecture, which has an 8x sub-sampling rate and is faster than Transformer or Conformer architectures. Instead of clustering-based token generation, we resort to fixed random projection for its simplicity and effectiveness. We also propose a generalized noisy speech augmentation that teaches the model to disentangle the main speaker from noise or other speakers. Experiments show that the proposed NEST model improves over existing self-supervised models on a variety of speech processing tasks. Code and checkpoints will be publicly available via NVIDIA NeMo toolkit.         ",
    "url": "https://arxiv.org/abs/2408.13106",
    "authors": [
      "He Huang",
      "Taejin Park",
      "Kunal Dhawan",
      "Ivan Medennikov",
      "Krishna C. Puvvada",
      "Nithin Rao Koluguri",
      "Weiqing Wang",
      "Jagadeesh Balam",
      "Boris Ginsburg"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2408.13124",
    "title": "Advancements in UWB: Paving the Way for Sovereign Data Networks in Healthcare Facilities",
    "abstract": "           Ultra-Wideband (UWB) technology re-emerges as a groundbreaking ranging technology with its precise micro-location capabilities and robustness. This paper highlights the security dimensions of UWB technology, focusing in particular on the intricacies of device fingerprinting for authentication, examined through the lens of state-of-the-art deep learning techniques. Furthermore, we explore various potential enhancements to the UWB standard that could realize a sovereign UWB data network. We argue that UWB data communication holds significant potential in healthcare and ultra-secure environments, where the use of the common unlicensed 2.4~GHz band-centric wireless technology is limited or prohibited. A sovereign UWB network could serve as an alternative, providing secure localization and short-range data communication in such environments.         ",
    "url": "https://arxiv.org/abs/2408.13124",
    "authors": [
      "Khan Reaz",
      "Thibaud Ardoin",
      "Lea Muth",
      "Marian Margraf",
      "Gerhard Wunder",
      "Mahsa Kholghi",
      "Kai Jansen",
      "Christian Zenger",
      "Julian Schmidt",
      "Enrico K\u00f6ppe",
      "Zoran Utkovski",
      "Igor Bjelakovic",
      "Mathis Schmieder",
      "Olaf Dressel"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.13131",
    "title": "DeTPP: Leveraging Object Detection for Robust Long-Horizon Event Prediction",
    "abstract": "           Forecasting future events over extended periods, known as long-horizon prediction, is a fundamental task in various domains, including retail, finance, healthcare, and social networks. Traditional methods, such as Marked Temporal Point Processes (MTPP), typically use autoregressive models to predict multiple future events. However, these models frequently encounter issues such as converging to constant or repetitive outputs, which significantly limits their effectiveness and applicability. To overcome these limitations, we propose DeTPP (Detection-based Temporal Point Processes), a novel approach inspired by object detection methods from computer vision. DeTPP utilizes a novel matching-based loss function that selectively focuses on reliably predictable events, enhancing both training robustness and inference diversity. Our method sets a new state-of-the-art in long-horizon event prediction, significantly outperforming existing MTPP and next-K approaches. The implementation of DeTPP is publicly available on GitHub.         ",
    "url": "https://arxiv.org/abs/2408.13131",
    "authors": [
      "Ivan Karpukhin",
      "Andrey Savchenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.13135",
    "title": "Deep Learning at the Intersection: Certified Robustness as a Tool for 3D Vision",
    "abstract": "           This paper presents preliminary work on a novel connection between certified robustness in machine learning and the modeling of 3D objects. We highlight an intriguing link between the Maximal Certified Radius (MCR) of a classifier representing a space's occupancy and the space's Signed Distance Function (SDF). Leveraging this relationship, we propose to use the certification method of randomized smoothing (RS) to compute SDFs. Since RS' high computational cost prevents its practical usage as a way to compute SDFs, we propose an algorithm to efficiently run RS in low-dimensional applications, such as 3D space, by expressing RS' fundamental operations as Gaussian smoothing on pre-computed voxel grids. Our approach offers an innovative and practical tool to compute SDFs, validated through proof-of-concept experiments in novel view synthesis. This paper bridges two previously disparate areas of machine learning, opening new avenues for further exploration and potential cross-domain advancements.         ",
    "url": "https://arxiv.org/abs/2408.13135",
    "authors": [
      "Gabriel P\u00e9rez S",
      "Juan C. P\u00e9rez",
      "Motasem Alfarra",
      "Jes\u00fas Zarzar",
      "Sara Rojas",
      "Bernard Ghanem",
      "Pablo Arbel\u00e1ez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.13140",
    "title": "Verification of Geometric Robustness of Neural Networks via Piecewise Linear Approximation and Lipschitz Optimisation",
    "abstract": "           We address the problem of verifying neural networks against geometric transformations of the input image, including rotation, scaling, shearing, and translation. The proposed method computes provably sound piecewise linear constraints for the pixel values by using sampling and linear approximations in combination with branch-and-bound Lipschitz optimisation. A feature of the method is that it obtains tighter over-approximations of the perturbation region than the present state-of-the-art. We report results from experiments on a comprehensive set of benchmarks. We show that our proposed implementation resolves more verification cases than present approaches while being more computationally efficient.         ",
    "url": "https://arxiv.org/abs/2408.13140",
    "authors": [
      "Ben Batten",
      "Yang Zheng",
      "Alessandro De Palma",
      "Panagiotis Kouvaros",
      "Alessio Lomuscio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13152",
    "title": "Long-Term Pre-training for Temporal Action Detection with Transformers",
    "abstract": "           Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.         ",
    "url": "https://arxiv.org/abs/2408.13152",
    "authors": [
      "Jihwan Kim",
      "Miso Lee",
      "Jae-Pil Heo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13155",
    "title": "Causal machine learning for sustainable agroecosystems",
    "abstract": "           In a changing climate, sustainable agriculture is essential for food security and environmental health. However, it is challenging to understand the complex interactions among its biophysical, social, and economic components. Predictive machine learning (ML), with its capacity to learn from data, is leveraged in sustainable agriculture for applications like yield prediction and weather forecasting. Nevertheless, it cannot explain causal mechanisms and remains descriptive rather than prescriptive. To address this gap, we propose causal ML, which merges ML's data processing with causality's ability to reason about change. This facilitates quantifying intervention impacts for evidence-based decision-making and enhances predictive model robustness. We showcase causal ML through eight diverse applications that benefit stakeholders across the agri-food chain, including farmers, policymakers, and researchers.         ",
    "url": "https://arxiv.org/abs/2408.13155",
    "authors": [
      "Vasileios Sitokonstantinou",
      "Emiliano D\u00edaz Salas Porras",
      "Jordi Cerd\u00e0 Bautista",
      "Maria Piles",
      "Ioannis Athanasiadis",
      "Hannah Kerner",
      "Giulia Martini",
      "Lily-belle Sweet",
      "Ilias Tsoumas",
      "Jakob Zscheischler",
      "Gustau Camps-Valls"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2408.13160",
    "title": "KonvLiNA: Integrating Kolmogorov-Arnold Network with Linear Nystr\\\"om Attention for feature fusion in Crop Field Detection",
    "abstract": "           Crop field detection is a critical component of precision agriculture, essential for optimizing resource allocation and enhancing agricultural productivity. This study introduces KonvLiNA, a novel framework that integrates Convolutional Kolmogorov-Arnold Networks (cKAN) with Nystr\u00f6m attention mechanisms for effective crop field detection. Leveraging KAN adaptive activation functions and the efficiency of Nystr\u00f6m attention in handling largescale data, KonvLiNA significantly enhances feature extraction, enabling the model to capture intricate patterns in complex agricultural environments. Experimental results on rice crop dataset demonstrate KonvLiNA superiority over state-of-the-art methods, achieving a 0.415 AP and 0.459 AR with the Swin-L backbone, outperforming traditional YOLOv8 by significant margins. Additionally, evaluation on the COCO dataset showcases competitive performance across small, medium, and large objects, highlighting KonvLiNA efficacy in diverse agricultural settings. This work highlights the potential of hybrid KAN and attention mechanisms for advancing precision agriculture through improved crop field detection and management.         ",
    "url": "https://arxiv.org/abs/2408.13160",
    "authors": [
      "Haruna Yunusa",
      "Qin Shiyin",
      "Adamu Lawan",
      "Abdulrahman Hamman Adama Chukkol"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13172",
    "title": "Towards Weaknesses and Attack Patterns Prediction for IoT Devices",
    "abstract": "           As the adoption of Internet of Things (IoT) devices continues to rise in enterprise environments, the need for effective and efficient security measures becomes increasingly critical. This paper presents a cost-efficient platform to facilitate the pre-deployment security checks of IoT devices by predicting potential weaknesses and associated attack patterns. The platform employs a Bidirectional Long Short-Term Memory (Bi-LSTM) network to analyse device-related textual data and predict weaknesses. At the same time, a Gradient Boosting Machine (GBM) model predicts likely attack patterns that could exploit these weaknesses. When evaluated on a dataset curated from the National Vulnerability Database (NVD) and publicly accessible IoT data sources, the system demonstrates high accuracy and reliability. The dataset created for this solution is publicly accessible.         ",
    "url": "https://arxiv.org/abs/2408.13172",
    "authors": [
      "Carlos A. Rivera A.",
      "Arash Shaghaghi",
      "Gustavo Batista",
      "Salil S. Kanhere"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.13182",
    "title": "Target Detection for OTFS-Aided Cell-Free MIMO ISAC System",
    "abstract": "           This letter focuses on enhancing target detection performance for a multi-user integrated sensing and communication (ISAC) system using orthogonal time frequency space (OTFS)-aided cell-free multiple-input multiple-output (MIMO) technology in high-speed vehicular environments. We propose a sensing-centric (SC) approach for target detection using communication signals with or without sensing signals. Power allocation is optimized to maximize the sensing signal-to-noise ratio (SNR) of the proposed SC scheme while ensuring a required quality-of-service (QoS) for the communication user equipment (UEs), and adhering to each access points (APs) power budget. Numerical results show that the proposed SC scheme vastly outperforms a communication-centric method that minimizes the total power consumed at the APs subject to the same constraints.         ",
    "url": "https://arxiv.org/abs/2408.13182",
    "authors": [
      "Shivani Singh",
      "Amudheesan Nakkeeran",
      "Prem Singh",
      "Ekant Sharma",
      "Jyotsna Bapat"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2408.13194",
    "title": "IFH: a Diffusion Framework for Flexible Design of Graph Generative Models",
    "abstract": "           Graph generative models can be classified into two prominent families: one-shot models, which generate a graph in one go, and sequential models, which generate a graph by successive additions of nodes and edges. Ideally, between these two extreme models lies a continuous range of models that adopt different levels of sequentiality. This paper proposes a graph generative model, called Insert-Fill-Halt (IFH), that supports the specification of a sequentiality degree. IFH is based upon the theory of Denoising Diffusion Probabilistic Models (DDPM), designing a node removal process that gradually destroys a graph. An insertion process learns to reverse this removal process by inserting arcs and nodes according to the specified sequentiality degree. We evaluate the performance of IFH in terms of quality, run time, and memory, depending on different sequentiality degrees. We also show that using DiGress, a diffusion-based one-shot model, as a generative step in IFH leads to improvement to the model itself, and is competitive with the current state-of-the-art.         ",
    "url": "https://arxiv.org/abs/2408.13194",
    "authors": [
      "Samuel Cognolato",
      "Alessandro Sperduti",
      "Luciano Serafini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.13195",
    "title": "NAS-Cap: Deep-Learning Driven 3-D Capacitance Extraction with Neural Architecture Search and Data Augmentation",
    "abstract": "           More accurate capacitance extraction is demanded for designing integrated circuits under advanced process technology. The pattern matching approach and the field solver for capacitance extraction have the drawbacks of inaccuracy and large computational cost, respectively. Recent work \\cite{yang2023cnn} proposes a grid-based data representation and a convolutional neural network (CNN) based capacitance models (called CNN-Cap), which opens the third way for 3-D capacitance extraction to get accurate results with much less time cost than field solver. In this work, the techniques of neural architecture search (NAS) and data augmentation are proposed to train better CNN models for 3-D capacitance extraction. Experimental results on datasets from different designs show that the obtained NAS-Cap models achieve remarkably higher accuracy than CNN-Cap, while consuming less runtime for inference and space for model storage. Meanwhile, the transferability of the NAS is validated, as the once searched architecture brought similar error reduction on coupling/total capacitance for the test cases from different design and/or process technology.         ",
    "url": "https://arxiv.org/abs/2408.13195",
    "authors": [
      "Haoyuan Li",
      "Dingcheng Yang",
      "Chunyan Pei",
      "Wenjian Yu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.13196",
    "title": "Predictability of Performance in Communication Networks Under Markovian Dynamics",
    "abstract": "           With the emergence of time-critical applications in modern communication networks, there is a growing demand for proactive network adaptation and quality of service (QoS) prediction. However, a fundamental question remains largely unexplored: how can we quantify and achieve more predictable communication systems in terms of performance? To address this gap, this paper introduces a theoretical framework for defining and analyzing predictability in communication systems, with a focus on the impact of observations for performance forecasting. We establish a mathematical definition of predictability based on the total variation distance between forecast and marginal performance distributions. A system is deemed unpredictable when the forecast distribution, providing the most comprehensive characterization of future states using all accessible information, is indistinguishable from the marginal distribution, which depicts the system's behavior without any observational input. This framework is applied to multi-hop systems under Markovian conditions, with a detailed analysis of Geo/Geo/1 queuing models in both single-hop and multi-hop scenarios. We derive exact and approximate expressions for predictability in these systems, as well as upper bounds based on spectral analysis of the underlying Markov chains. Our results have implications for the design of efficient monitoring and prediction mechanisms in future communication networks aiming to provide deterministic services.         ",
    "url": "https://arxiv.org/abs/2408.13196",
    "authors": [
      "Samie Mostafavi",
      "Simon Eggar",
      "Gy\u00f6rgy D\u00e1n",
      "James Gross"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2408.13204",
    "title": "DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation",
    "abstract": "           Code benchmarks such as HumanEval are widely adopted to evaluate the capabilities of Large Language Models (LLMs), providing insights into their strengths and weaknesses. However, current benchmarks primarily exercise LLMs' capability on common coding tasks (e.g., bubble sort, greatest common divisor), leaving domain-specific coding tasks (e.g., computation, system, cryptography) unexplored. To fill this gap, we propose a multi-domain code benchmark, DOMAINEVAL, designed to evaluate LLMs' coding capabilities thoroughly. Our pipeline works in a fully automated manner, enabling a push-bottom construction from code repositories into formatted subjects under study. Interesting findings are observed by evaluating 12 representative LLMs against DOMAINEVAL. We notice that LLMs are generally good at computation tasks while falling short on cryptography and system coding tasks. The performance gap can be as much as 68.94% (80.94% - 12.0%) in some LLMs. We also observe that generating more samples can increase the overall performance of LLMs, while the domain bias may even increase. The contributions of this study include a code generation benchmark dataset DOMAINEVAL, encompassing six popular domains, a fully automated pipeline for constructing code benchmarks, and an identification of the limitations of LLMs in code generation tasks based on their performance on DOMAINEVAL, providing directions for future research improvements. The leaderboard is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.13204",
    "authors": [
      "Qiming Zhu",
      "Jialun Cao",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun",
      "Shing-Chi Cheung"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.13217",
    "title": "HBIC: A Biclustering Algorithm for Heterogeneous Datasets",
    "abstract": "           Biclustering is an unsupervised machine-learning approach aiming to cluster rows and columns simultaneously in a data matrix. Several biclustering algorithms have been proposed for handling numeric datasets. However, real-world data mining problems often involve heterogeneous datasets with mixed attributes. To address this challenge, we introduce a biclustering approach called HBIC, capable of discovering meaningful biclusters in complex heterogeneous data, including numeric, binary, and categorical data. The approach comprises two stages: bicluster generation and bicluster model selection. In the initial stage, several candidate biclusters are generated iteratively by adding and removing rows and columns based on the frequency of values in the original matrix. In the second stage, we introduce two approaches for selecting the most suitable biclusters by considering their size and homogeneity. Through a series of experiments, we investigated the suitability of our approach on a synthetic benchmark and in a biomedical application involving clinical data of systemic sclerosis patients. The evaluation comparing our method to existing approaches demonstrates its ability to discover high-quality biclusters from heterogeneous data. Our biclustering approach is a starting point for heterogeneous bicluster discovery, leading to a better understanding of complex underlying data structures.         ",
    "url": "https://arxiv.org/abs/2408.13217",
    "authors": [
      "Ad\u00e1n Jos\u00e9-Garc\u00eda",
      "Julie Jacques",
      "Cl\u00e9ment Chauvet",
      "Vincent Sobanski",
      "Clarisse Dhaenens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.13221",
    "title": "Protecting against simultaneous data poisoning attacks",
    "abstract": "           Current backdoor defense methods are evaluated against a single attack at a time. This is unrealistic, as powerful machine learning systems are trained on large datasets scraped from the internet, which may be attacked multiple times by one or more attackers. We demonstrate that simultaneously executed data poisoning attacks can effectively install multiple backdoors in a single model without substantially degrading clean accuracy. Furthermore, we show that existing backdoor defense methods do not effectively prevent attacks in this setting. Finally, we leverage insights into the nature of backdoor attacks to develop a new defense, BaDLoss, that is effective in the multi-attack setting. With minimal clean accuracy degradation, BaDLoss attains an average attack success rate in the multi-attack setting of 7.98% in CIFAR-10 and 10.29% in GTSRB, compared to the average of other defenses at 64.48% and 84.28% respectively.         ",
    "url": "https://arxiv.org/abs/2408.13221",
    "authors": [
      "Neel Alex",
      "Shoaib Ahmed Siddiqui",
      "Amartya Sanyal",
      "David Krueger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.13222",
    "title": "An Overview on Machine Learning Methods for Partial Differential Equations: from Physics Informed Neural Networks to Deep Operator Learning",
    "abstract": "           The approximation of solutions of partial differential equations (PDEs) with numerical algorithms is a central topic in applied mathematics. For many decades, various types of methods for this purpose have been developed and extensively studied. One class of methods which has received a lot of attention in recent years are machine learning-based methods, which typically involve the training of artificial neural networks (ANNs) by means of stochastic gradient descent type optimization methods. While approximation methods for PDEs using ANNs have first been proposed in the 1990s they have only gained wide popularity in the last decade with the rise of deep learning. This article aims to provide an introduction to some of these methods and the mathematical theory on which they are based. We discuss methods such as physics-informed neural networks (PINNs) and deep BSDE methods and consider several operator learning approaches.         ",
    "url": "https://arxiv.org/abs/2408.13222",
    "authors": [
      "Lukas Gonon",
      "Arnulf Jentzen",
      "Benno Kuckuck",
      "Siyu Liang",
      "Adrian Riekert",
      "Philippe von Wurstemberger"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.13223",
    "title": "Social Welfare Maximization for Federated Learning with Network Effects",
    "abstract": "           A proper mechanism design can help federated learning (FL) to achieve good social welfare by coordinating self-interested clients through the learning process. However, existing mechanisms neglect the network effects of client participation, leading to suboptimal incentives and social welfare. This paper addresses this gap by exploring network effects in FL incentive mechanism design. We establish a theoretical model to analyze FL model performance and quantify the impact of network effects on heterogeneous client participation. Our analysis reveals the non-monotonic nature of FL network effects. To leverage such effects, we propose a model trading and sharing (MTS) framework that allows clients to obtain FL models through participation or purchase. To tackle heterogeneous clients' strategic behaviors, we further design a socially efficient model trading and sharing (SEMTS) mechanism. Our mechanism achieves social welfare maximization solely through customer payments, without additional incentive costs. Experimental results on an FL hardware prototype demonstrate up to 148.86% improvement in social welfare compared to existing mechanisms.         ",
    "url": "https://arxiv.org/abs/2408.13223",
    "authors": [
      "Xiang Li",
      "Yuan Luo",
      "Bing Luo",
      "Jianwei Huang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2408.13224",
    "title": "Quadratic estimation for stochastic systems in the presence of random parameter matrices, time-correlated additive noise and deception attacks",
    "abstract": "           Networked systems usually face different random uncertainties that make the performance of the least-squares (LS) linear filter decline significantly. For this reason, great attention has been paid to the search for other kinds of suboptimal estimators. Among them, the LS quadratic estimation approach has attracted considerable interest in the scientific community for its balance between computational complexity and estimation accuracy. When it comes to stochastic systems subject to different random uncertainties and deception attacks, the quadratic estimator design has not been deeply studied. In this paper, using covariance information, the LS quadratic filtering and fixed-point smoothing problems are addressed under the assumption that the measurements are perturbed by a time-correlated additive noise, as well as affected by random parameter matrices and exposed to random deception attacks. The use of random parameter matrices covers a wide range of common uncertainties and random failures, thus better reflecting the engineering reality. The signal and observation vectors are augmented by stacking the original vectors with their second-order Kronecker powers; then, the linear estimator of the original signal based on the augmented observations provides the required quadratic estimator. A simulation example illustrates the superiority of the proposed quadratic estimators over the conventional linear ones and the effect of the deception attacks on the estimation performance.         ",
    "url": "https://arxiv.org/abs/2408.13224",
    "authors": [
      "Raquel Caballero-\u00c1guila",
      "Josefa Linares-P\u00e9rez"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2408.13226",
    "title": "D&M: Enriching E-commerce Videos with Sound Effects by Key Moment Detection and SFX Matching",
    "abstract": "           Videos showcasing specific products are increasingly important for E-commerce. Key moments naturally exist as the first appearance of a specific product, presentation of its distinctive features, the presence of a buying link, etc. Adding proper sound effects (SFX) to these key moments, or video decoration with SFX (VDSFX), is crucial for enhancing the user engaging experience. Previous studies about adding SFX to videos perform video to SFX matching at a holistic level, lacking the ability of adding SFX to a specific moment. Meanwhile, previous studies on video highlight detection or video moment retrieval consider only moment localization, leaving moment to SFX matching untouched. By contrast, we propose in this paper D&M, a unified method that accomplishes key moment detection and moment to SFX matching simultaneously. Moreover, for the new VDSFX task we build a large-scale dataset SFX-Moment from an E-commerce platform. For a fair comparison, we build competitive baselines by extending a number of current video moment detection methods to the new task. Extensive experiments on SFX-Moment show the superior performance of the proposed method over the baselines. Code and data will be released.         ",
    "url": "https://arxiv.org/abs/2408.13226",
    "authors": [
      "Jingyu Liu",
      "Minquan Wang",
      "Ye Ma",
      "Bo Wang",
      "Aozhu Chen",
      "Quan Chen",
      "Peng Jiang",
      "Xirong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13251",
    "title": "Re-evaluation of Face Anti-spoofing Algorithm in Post COVID-19 Era Using Mask Based Occlusion Attack",
    "abstract": "           Face anti-spoofing algorithms play a pivotal role in the robust deployment of face recognition systems against presentation attacks. Conventionally, full facial images are required by such systems to correctly authenticate individuals, but the widespread requirement of masks due to the current COVID-19 pandemic has introduced new challenges for these biometric authentication systems. Hence, in this work, we investigate the performance of presentation attack detection (PAD) algorithms under synthetic facial occlusions using masks and glasses. We have used five variants of masks to cover the lower part of the face with varying coverage areas (low-coverage, medium-coverage, high-coverage, round coverage), and 3D cues. We have also used different variants of glasses that cover the upper part of the face. We systematically tested the performance of four PAD algorithms under these occlusion attacks using a benchmark dataset. We have specifically looked at four different baseline PAD algorithms that focus on, texture, image quality, frame difference/motion, and abstract features through a convolutional neural network (CNN). Additionally we have introduced a new hybrid model that uses CNN and local binary pattern textures. Our experiment shows that adding the occlusions significantly degrades the performance of all of the PAD algorithms. Our results show the vulnerability of face anti-spoofing algorithms with occlusions, which could be in the usage of such algorithms in the post-pandemic era.         ",
    "url": "https://arxiv.org/abs/2408.13251",
    "authors": [
      "Vaibhav Sundharam",
      "Abhijit Sarkar",
      "A. Lynn Abbott"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12602",
    "title": "Fiber neural networks for the intelligent optical fiber communications",
    "abstract": "           Optical neural networks have long cast attention nowadays. Like other optical structured neural networks, fiber neural networks which utilize the mechanism of light transmission to compute can take great advantages in both computing efficiency and power cost. Though the potential ability of optical fiber was demonstrated via the establishing of fiber neural networks, it will be of great significance of combining both fiber transmission and computing functions so as to cater the needs of future beyond 5G intelligent communication signal processing. Thus, in this letter, the fiber neural networks and their related optical signal processing methods will be both developed. In this way, information derived from the transmitted signals can be directly processed in the optical domain rather than being converted to the electronic domain. As a result, both prominent gains in processing efficiency and power cost can be further obtained. The fidelity of the whole structure and related methods is demonstrated by the task of modulation format recognition which plays important role in fiber optical communications without losing the generality.         ",
    "url": "https://arxiv.org/abs/2408.12602",
    "authors": [
      "Yubin Zang",
      "Zuxing Zhang",
      "Simin Li",
      "Fangzheng Zhang",
      "Hongwei Chen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2408.12605",
    "title": "Convolutional Neural Networks for Predictive Modeling of Lung Disease",
    "abstract": "           In this paper, Pro-HRnet-CNN, an innovative model combining HRNet and void-convolution techniques, is proposed for disease prediction under lung imaging. Through the experimental comparison on the authoritative LIDC-IDRI dataset, we found that compared with the traditional ResNet-50, Pro-HRnet-CNN showed better performance in the feature extraction and recognition of small-size nodules, significantly improving the detection accuracy. Particularly within the domain of detecting smaller targets, the model has exhibited a remarkable enhancement in accuracy, thereby pioneering an innovative avenue for the early identification and prognostication of pulmonary conditions.         ",
    "url": "https://arxiv.org/abs/2408.12605",
    "authors": [
      "Yingbin Liang",
      "Xiqing Liu",
      "Haohao Xia",
      "Yiru Cang",
      "Zitao Zheng",
      "Yuanfang Yang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12615",
    "title": "Pediatric TSC-related eplipsy classification from multi-contrast images using quantum neural network",
    "abstract": "           Tuberous sclerosis complex (TSC) manifests as a multisystem disorder with significant neurological implications. This study addresses the critical need for robust classification models tailored to TSC in pediatric patients, introducing QResNet,a novel deep learning model seamlessly integrating conventional convolutional neural networks with quantum neural networks. The model incorporates a two-layer quantum layer (QL), comprising ZZFeatureMap and Ansatz layers, strategically designed for processing classical data within a quantum framework. A comprehensive evaluation, demonstrates the superior performance of QResNet in TSC MRI image classification compared to conventional 3D-ResNet models. These compelling findings underscore the potential of quantum computing to revolutionize medical imaging and diagnostics.Remarkably, this method surpasses conventional CNNs in accuracy and Area Under the Curve (AUC) metrics with the current dataset. Future research endeavors may focus on exploring the scalability and practical implementation of quantum algorithms in real-world medical imaging scenarios.         ",
    "url": "https://arxiv.org/abs/2408.12615",
    "authors": [
      "Ling Lin",
      "Yihang Zhou",
      "Zhanqi Hu",
      "Dian Jiang",
      "Congcong Liu",
      "Shuo Zhou",
      "Yanjie Zhu",
      "Jianxiang Liao",
      "Dong Liang",
      "Hairong Zheng",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12621",
    "title": "StringNET: Neural Network based Variational Method for Transition Pathways",
    "abstract": "           Rare transition events in meta-stable systems under noisy fluctuations are crucial for many non-equilibrium physical and chemical processes. In these processes, the primary contributions to reactive flux are predominantly near the transition pathways that connect two meta-stable states. Efficient computation of these paths is essential in computational chemistry. In this work, we examine the temperature-dependent maximum flux path, the minimum energy path, and the minimum action path at zero temperature. We propose the StringNET method for training these paths using variational formulations and deep learning techniques. Unlike traditional chain-of-state methods, StringNET directly parametrizes the paths through neural network functions, utilizing the arc-length parameter as the main input. The tasks of gradient descent and re-parametrization in the string method are unified into a single framework using loss functions to train deep neural networks. More importantly, the loss function for the maximum flux path is interpreted as a softmax approximation to the numerically challenging minimax problem of the minimum energy path. To compute the minimum energy path efficiently and robustly, we developed a pre-training strategy that includes the maximum flux path loss in the early training stage, significantly accelerating the computation of minimum energy and action paths. We demonstrate the superior performance of this method through various analytical and chemical examples, as well as the two- and four-dimensional Ginzburg-Landau functional energy.         ",
    "url": "https://arxiv.org/abs/2408.12621",
    "authors": [
      "Jiayue Han",
      "Shuting Gu",
      "Xiang Zhou"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.12681",
    "title": "On 2-complexes embeddable in 4-space, and the excluded minors of their underlying graphs",
    "abstract": "           We study the potentially undecidable problem of whether a given 2-dimensional CW complex can be embedded into $\\mathbb{R}^4$. We provide operations that preserve embeddability, including joining and cloning of 2-cells, as well as $\\Delta\\mathrm Y$-transformations. We also construct a CW complex for which $\\mathrm Y\\Delta$-transformations do not preserve embeddability. We use these results to study 4-flat graphs, i.e., graphs that embed in $\\mathbb{R}^4$ after attaching any number of 2-cells to their cycles; a graph class that naturally generalizes planarity and linklessness. We verify several conjectures of van der Holst; in particular, we prove that each of the 78 graphs of the Heawood family is an excluded minor for the class of 4-flat graphs.         ",
    "url": "https://arxiv.org/abs/2408.12681",
    "authors": [
      "Agelos Georgakopoulos",
      "Martin Winter"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Geometric Topology (math.GT)"
    ]
  },
  {
    "id": "arXiv:2408.12739",
    "title": "Quantum Convolutional Neural Networks are (Effectively) Classically Simulable",
    "abstract": "           Quantum Convolutional Neural Networks (QCNNs) are widely regarded as a promising model for Quantum Machine Learning (QML). In this work we tie their heuristic success to two facts. First, that when randomly initialized, they can only operate on the information encoded in low-bodyness measurements of their input states. And second, that they are commonly benchmarked on \"locally-easy'' datasets whose states are precisely classifiable by the information encoded in these low-bodyness observables subspace. We further show that the QCNN's action on this subspace can be efficiently classically simulated by a classical algorithm equipped with Pauli shadows on the dataset. Indeed, we present a shadow-based simulation of QCNNs on up-to $1024$ qubits for phases of matter classification. Our results can then be understood as highlighting a deeper symptom of QML: Models could only be showing heuristic success because they are benchmarked on simple problems, for which their action can be classically simulated. This insight points to the fact that non-trivial datasets are a truly necessary ingredient for moving forward with QML. To finish, we discuss how our results can be extrapolated to classically simulate other architectures.         ",
    "url": "https://arxiv.org/abs/2408.12739",
    "authors": [
      "Pablo Bermejo",
      "Paolo Braccia",
      "Manuel S. Rudolph",
      "Zo\u00eb Holmes",
      "Lukasz Cincio",
      "M. Cerezo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2408.12760",
    "title": "Hierarchical Attention and Parallel Filter Fusion Network for Multi-Source Data Classification",
    "abstract": "           Hyperspectral image (HSI) and synthetic aperture radar (SAR) data joint classification is a crucial and yet challenging task in the field of remote sensing image interpretation. However, feature modeling in existing methods is deficient to exploit the abundant global, spectral, and local features simultaneously, leading to sub-optimal classification performance. To solve the problem, we propose a hierarchical attention and parallel filter fusion network for multi-source data classification. Concretely, we design a hierarchical attention module for hyperspectral feature extraction. This module integrates global, spectral, and local features simultaneously to provide more comprehensive feature representation. In addition, we develop parallel filter fusion module which enhances cross-modal feature interactions among different spatial locations in the frequency domain. Extensive experiments on two multi-source remote sensing data classification datasets verify the superiority of our proposed method over current state-of-the-art classification approaches. Specifically, our proposed method achieves 91.44% and 80.51% of overall accuracy (OA) on the respective datasets, highlighting its superior performance.         ",
    "url": "https://arxiv.org/abs/2408.12760",
    "authors": [
      "Han Luo",
      "Feng Gao",
      "Junyu Dong",
      "Lin Qi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12804",
    "title": "Universal dimensions of visual representation",
    "abstract": "           Do neural network models of vision learn brain-aligned representations because they share architectural constraints and task objectives with biological vision or because they learn universal features of natural image processing? We characterized the universality of hundreds of thousands of representational dimensions from visual neural networks with varied construction. We found that networks with varied architectures and task objectives learn to represent natural images using a shared set of latent dimensions, despite appearing highly distinct at a surface level. Next, by comparing these networks with human brain representations measured with fMRI, we found that the most brain-aligned representations in neural networks are those that are universal and independent of a network's specific characteristics. Remarkably, each network can be reduced to fewer than ten of its most universal dimensions with little impact on its representational similarity to the human brain. These results suggest that the underlying similarities between artificial and biological vision are primarily governed by a core set of universal image representations that are convergently learned by diverse systems.         ",
    "url": "https://arxiv.org/abs/2408.12804",
    "authors": [
      "Zirui Chen",
      "Michael F. Bonner"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12951",
    "title": "On z-coloring and ${\\rm b}^{\\ast}$-coloring of graphs as improved variants of the b-coloring",
    "abstract": "           Let $G$ be a simple graph and $c$ a proper vertex coloring of $G$. A vertex $u$ is called b-vertex in $(G,c)$ if all colors except $c(u)$ appear in the neighborhood of $u$. By a ${\\rm b}^{\\ast}$-coloring of $G$ using colors $\\{1, \\ldots, k\\}$ we define a proper vertex coloring $c$ such that there is a b-vertex $u$ (called nice vertex) such that for each $j\\in \\{1, \\ldots, k\\}$ with $j\\not=c(u)$, $u$ is adjacent to a b-vertex of color $j$. The ${\\rm b}^{\\ast}$-chromatic number of $G$ (denoted by ${\\rm b}^{\\ast}(G)$) is the largest integer $k$ such that $G$ has a ${\\rm b}^{\\ast}$-coloring using $k$ colors. Every graph $G$ admits a ${\\rm b}^{\\ast}$-coloring which is an improvement over the famous b-coloring. A z-coloring of $G$ is a coloring $c$ using colors $\\{1, 2, \\ldots, k\\}$ containing a nice vertex of color $k$ such that for each two colors $i<j$, each vertex of color $j$ has a neighbor of color $i$ in the graph (i.e. $c$ is obtained from a greedy coloring of $G$). We obtain results for ${\\rm b}^{\\ast}$-coloring and z-coloring of block graphs, cacti, $P_4$-sparse graphs and graphs with girth greater than $4$. We prove that z-coloring and ${\\rm b}^{\\ast}$-coloring have a locality property. A linear 0-1 programming model is also presented for z-coloring of graphs. The positive results suggest that researches can be focused on ${\\rm b}^{\\ast}$-colorings (or z-colorings) instead of b-coloring of graphs.         ",
    "url": "https://arxiv.org/abs/2408.12951",
    "authors": [
      "Manouchehr Zaker"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2408.12984",
    "title": "Zeoformer: Coarse-Grained Periodic Graph Transformer for OSDA-Zeolite Affinity Prediction",
    "abstract": "           To date, the International Zeolite Association Structure Commission (IZA-SC) has cataloged merely 255 distinct zeolite structures, with millions of theoretically possible structures yet to be discovered. The synthesis of a specific zeolite typically necessitates the use of an organic structure-directing agent (OSDA), since the selectivity for a particular zeolite is largely determined by the affinity between the OSDA and the zeolite. Therefore, finding the best affinity OSDA-zeolite pair is the key to the synthesis of targeted zeolite. However, OSDA-zeolite pairs frequently exhibit complex geometric structures, i.e., a complex crystal structure formed by a large number of atoms. Although some existing machine learning methods can represent the periodicity of crystals, they cannot accurately represent crystal structures with local variability. To address this issue, we propose a novel approach called Zeoformer, which can effectively represent coarse-grained crystal periodicity and fine-grained local variability. Zeoformer reconstructs the unit cell centered around each atom and encodes the pairwise distances between this central atom and other atoms within the reconstructed unit cell. The introduction of pairwise distances within the reconstructed unit cell more effectively represents the overall structure of the unit cell and the differences between different unit cells, enabling the model to more accurately and efficiently predict the properties of OSDA-zeolite pairs and general crystal structures. Through comprehensive evaluation, our Zeoformer model demonstrates the best performance on OSDA-zeolite pair datasets and two types of crystal material datasets.         ",
    "url": "https://arxiv.org/abs/2408.12984",
    "authors": [
      "Xiangxiang Shen",
      "Zheng Wan",
      "Lingfeng Wen",
      "Licheng Sun",
      "Ou Yang Ming Jie",
      "Xuan Tang",
      "Xian Zeng",
      "Mingsong Chen",
      "Xiao He",
      "Xian Wei"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.12997",
    "title": "Encoding Semi-Directed Phylogenetic Networks with Quarnets",
    "abstract": "           Phylogenetic networks are graphs that are used to represent evolutionary relationships between different taxa. They generalize phylogenetic trees since for example, unlike trees, they permit lineages to combine. Recently, there has been rising interest in semi-directed phylogenetic networks, which are mixed graphs in which certain lineage combination events are represented by directed edges coming together, whereas the remaining edges are left undirected. One reason to consider such networks is that it can be difficult to root a network using real data. In this paper, we consider the problem of when a semi-directed phylogenetic network is defined or encoded by the smaller networks that it induces on the $4$-leaf subsets of its leaf set. These smaller networks are called quarnets. We prove that semi-directed binary level-$2$ phylogenetic networks are encoded by their quarnets, but that this is not the case for level-$3$. In addition, we prove that the so-called blob tree of a semi-directed binary network, a tree that gives the coarse-grained structure of the network, is always encoded by the quarnets of the network.         ",
    "url": "https://arxiv.org/abs/2408.12997",
    "authors": [
      "Katharina T. Huber",
      "Leo van Iersel",
      "Mark Jones",
      "Vincent Moulton",
      "Leonie Veenema - Nipius"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2408.13065",
    "title": "SIMPLE: Simultaneous Multi-Plane Self-Supervised Learning for Isotropic MRI Restoration from Anisotropic Data",
    "abstract": "           Magnetic resonance imaging (MRI) is crucial in diagnosing various abdominal conditions and anomalies. Traditional MRI scans often yield anisotropic data due to technical constraints, resulting in varying resolutions across spatial dimensions, which limits diagnostic accuracy and volumetric analysis. Super-resolution (SR) techniques aim to address these limitations by reconstructing isotropic high-resolution images from anisotropic data. However, current SR methods often rely on indirect mappings and limited training data, focusing mainly on two-dimensional improvements rather than achieving true three-dimensional isotropy. We introduce SIMPLE, a Simultaneous Multi-Plane Self-Supervised Learning approach for isotropic MRI restoration from anisotropic data. Our method leverages existing anisotropic clinical data acquired in different planes, bypassing the need for simulated downsampling processes. By considering the inherent three-dimensional nature of MRI data, SIMPLE ensures realistic isotropic data generation rather than solely improving through-plane slices. This approach flexibility allows it to be extended to multiple contrast types and acquisition methods commonly used in clinical settings. Our experiments show that SIMPLE outperforms state-of-the-art methods both quantitatively using the Kernel Inception Distance (KID) and semi-quantitatively through radiologist evaluations. The generated isotropic volume facilitates more accurate volumetric analysis and 3D reconstructions, promising significant improvements in clinical diagnostic capabilities.         ",
    "url": "https://arxiv.org/abs/2408.13065",
    "authors": [
      "Rotem Benisty",
      "Yevgenia Shteynman",
      "Moshe Porat",
      "Anat Illivitzki",
      "Moti Freiman"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.13070",
    "title": "Context-free graphs and their transition groups",
    "abstract": "           We define a new class of groups arising from context-free inverse graphs. We provide closure properties, prove that their co-word problems are context-free, study the torsion elements, and realize them as subgroups of the asynchronous rational group. Furthermore, we use a generalized version of the free product of graphs and prove that such a product is context-free inverse closed. We also exhibit an example of a group in our class that is not residually finite and one that is not poly-context-free. These properties make them interesting candidates to disprove both the Lehnert conjecture (which characterizes co-context-free groups as all subgroups of Thompson's group V) and the Brough conjecture (which characterizes finitely generated poly-context-free groups as virtual finitely generated subgroups of direct products of free groups).         ",
    "url": "https://arxiv.org/abs/2408.13070",
    "authors": [
      "Daniele D'Angeli",
      "Francesco Matucci",
      "Davide Perego",
      "Emanuele Rodaro"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2408.13089",
    "title": "On the good reliability of an interval-based metric to validate prediction uncertainty for machine learning regression tasks",
    "abstract": "           This short study presents an opportunistic approach to a (more) reliable validation method for prediction uncertainty average calibration. Considering that variance-based calibration metrics (ZMS, NLL, RCE...) are quite sensitive to the presence of heavy tails in the uncertainty and error distributions, a shift is proposed to an interval-based metric, the Prediction Interval Coverage Probability (PICP). It is shown on a large ensemble of molecular properties datasets that (1) sets of z-scores are well represented by Student's-$t(\\nu)$ distributions, $\\nu$ being the number of degrees of freedom; (2) accurate estimation of 95 $\\%$ prediction intervals can be obtained by the simple $2\\sigma$ rule for $\\nu>3$; and (3) the resulting PICPs are more quickly and reliably tested than variance-based calibration metrics. Overall, this method enables to test 20 $\\%$ more datasets than ZMS testing. Conditional calibration is also assessed using the PICP approach.         ",
    "url": "https://arxiv.org/abs/2408.13089",
    "authors": [
      "Pascal Pernot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.13146",
    "title": "Reproduction of scan B-statistic for kernel change-point detection algorithm",
    "abstract": "           Change-point detection has garnered significant attention due to its broad range of applications, including epidemic disease outbreaks, social network evolution, image analysis, and wireless communications. In an online setting, where new data samples arrive sequentially, it is crucial to continuously test whether these samples originate from a different distribution. Ideally, the detection algorithm should be distribution-free to ensure robustness in real-world applications. In this paper, we reproduce a recently proposed online change-point detection algorithm based on an efficient kernel-based scan B-statistic, and compare its performance with two commonly used parametric statistics. Our numerical experiments demonstrate that the scan B-statistic consistently delivers superior performance. In more challenging scenarios, parametric methods may fail to detect changes, whereas the scan B-statistic successfully identifies them in a timely manner. Additionally, the use of subsampling techniques offers a modest improvement to the original algorithm.         ",
    "url": "https://arxiv.org/abs/2408.13146",
    "authors": [
      "Zihan Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.13211",
    "title": "Optimal Quantum Circuit Design via Unitary Neural Networks",
    "abstract": "           The process of translating a quantum algorithm into a form suitable for implementation on a quantum computing platform is crucial but yet challenging. This entails specifying quantum operations with precision, a typically intricate task. In this paper, we present an alternative approach: an automated method for synthesizing the functionality of a quantum algorithm into a quantum circuit model representation. Our methodology involves training a neural network model using diverse input-output mappings of the quantum algorithm. We demonstrate that this trained model can effectively generate a quantum circuit model equivalent to the original algorithm. Remarkably, our observations indicate that the trained model achieves near-perfect mapping of unseen inputs to their respective outputs.         ",
    "url": "https://arxiv.org/abs/2408.13211",
    "authors": [
      "M. Zomorodi",
      "H. Amini",
      "M. Abbaszadeh",
      "J. Sohrabi",
      "V. Salari",
      "P. Plawiak"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:1909.02203",
    "title": "2FA Sketch: Two-Factor Armor Sketch for Accurate and Efficient Heavy Hitter Detection in Data Streams",
    "abstract": "           Detecting heavy hitters, which are flows exceeding a specified threshold, is crucial for network measurement, but it faces challenges due to increasing throughput and memory constraints. Existing sketch-based solutions, particularly those using Comparative Counter Voting, have limitations in efficiently identifying heavy hitters. This paper introduces the Two-Factor Armor (2FA) Sketch, a novel data structure designed to enhance heavy hitter detection in data streams. 2FA Sketch implements dual-layer protection through an improved $\\mathtt{Arbitration}$ strategy for in-bucket competition and a cross-bucket conflict $\\mathtt{Avoidance}$ hashing scheme. By theoretically deriving an optimal $\\lambda$ parameter and redesigning $vote^+_{new}$ as a conflict indicator, it optimizes the Comparative Counter Voting strategy. Experimental results show that 2FA Sketch outperforms the standard Elastic Sketch, reducing error rates by 2.5 to 19.7 times and increasing processing speed by 1.03 times.         ",
    "url": "https://arxiv.org/abs/1909.02203",
    "authors": [
      "Xilai Liu",
      "Xinyi Zhang",
      "Bingqing Liu",
      "Tao Li",
      "Tong Yang",
      "Gaogang Xie"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2010.08929",
    "title": "Self-stabilizing Graph Exploration by a Single Agent",
    "abstract": "           In this paper, we present two self-stabilizing algorithms that enable a single (mobile) agent to explore graphs. The agent visits all nodes starting from any configuration, \\ie regardless of the initial state of the agent, the initial states of all nodes, and the initial location of the agent. We evaluate the algorithms using two metrics: cover time, which is the number of moves required to visit all nodes, and memory usage, which includes the storage needed for the state of the agent and the state of each node. The first algorithm is randomized. Given an integer $c = \\Omega(n)$, the cover time of this algorithm is optimal, \\ie $O(m)$ in expectation, and the memory requirements for the agent and each node $v$ are $O(\\log c)$ and $O(\\log (c+\\delta_v))$ bits, respectively, where $n$ and $m$ are the numbers of nodes and edges, respectively, and $\\delta_v$ is the degree of $v$. The second algorithm is deterministic. It requires an input integer $k \\ge \\max(D,d_{\\mathrm{max}})$, where $D$ and $d_{\\mathrm{max}}$ are the diameter and the maximum degree of the graph, respectively. The cover time of this algorithm is $O(m + nD)$, and it uses $O(\\log k)$ bits both for agent memory and each node.         ",
    "url": "https://arxiv.org/abs/2010.08929",
    "authors": [
      "Yuichi Sudo",
      "Fukuhito Ooshita",
      "Sayaka Kamei"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2301.02458",
    "title": "Topics as Entity Clusters: Entity-based Topics from Large Language Models and Graph Neural Networks",
    "abstract": "           Topic models aim to reveal latent structures within a corpus of text, typically through the use of term-frequency statistics over bag-of-words representations from documents. In recent years, conceptual entities -- interpretable, language-independent features linked to external knowledge resources -- have been used in place of word-level tokens, as words typically require extensive language processing with a minimal assurance of interpretability. However, current literature is limited when it comes to exploring purely entity-driven neural topic modeling. For instance, despite the advantages of using entities for eliciting thematic structure, it is unclear whether current techniques are compatible with these sparsely organised, information-dense conceptual units. In this work, we explore entity-based neural topic modeling and propose a novel topic clustering approach using bimodal vector representations of entities. Concretely, we extract these latent representations from large language models and graph neural networks trained on a knowledge base of symbolic relations, in order to derive the most salient aspects of these conceptual units. Analysis of coherency metrics confirms that our approach is better suited to working with entities in comparison to state-of-the-art models, particularly when using graph-based embeddings trained on a knowledge base.         ",
    "url": "https://arxiv.org/abs/2301.02458",
    "authors": [
      "Manuel V. Loureiro",
      "Steven Derby",
      "Tri Kurniawan Wijaya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2303.14565",
    "title": "Saihu: A Common Interface of Worst-Case Delay Analysis Tools for Time-Sensitive Networks",
    "abstract": "           Time-sensitive networks, as in the context of IEEE-TSN and IETF-Detnet, require bounds on worst-case delays. Various network analysis tools compute such bounds; these tools are based on different methods and provide delay bounds that are all valid but may differ; furthermore, it is generally not known which tool will provide the best bound. To obtain the best possible bound, users need to implement multiple pieces of code with a different syntax for every tool, which is impractical and error-prone. To address this issue, we present Saihu, a Python interface that integrates the three most frequently used worst-case network analysis tools: xTFA, DiscoDNC, and Panco. They altogether implement six analysis methods. Saihu provides a general interface that enables defining a network in a single file and executing all tools simultaneously without any modification. Saihu further exports analysis results as formatted reports automatically and allows quick generation of certain types of networks. With its simplified steps of execution, Saihu reduces the burden on users and makes it accessible for anyone working with time-sensitive networks.         ",
    "url": "https://arxiv.org/abs/2303.14565",
    "authors": [
      "Chun-Tso Tsai",
      "Seyed Mohammadhossein Tabatabaee",
      "St\u00e9phan Plassart",
      "Jean-Yves Le Boudec"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2304.01484",
    "title": "Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision",
    "abstract": "           Training a convolutional neural network (CNN) to detect infrared small targets in a fully supervised manner has gained remarkable research interests in recent years, but is highly labor expensive since a large number of per-pixel annotations are required. To handle this problem, in this paper, we make the first attempt to achieve infrared small target detection with point-level supervision. Interestingly, during the training phase supervised by point labels, we discover that CNNs first learn to segment a cluster of pixels near the targets, and then gradually converge to predict groundtruth point labels. Motivated by this \"mapping degeneration\" phenomenon, we propose a label evolution framework named label evolution with single point supervision (LESPS) to progressively expand the point label by leveraging the intermediate predictions of CNNs. In this way, the network predictions can finally approximate the updated pseudo labels, and a pixel-level target mask can be obtained to train CNNs in an end-to-end manner. We conduct extensive experiments with insightful visualizations to validate the effectiveness of our method. Experimental results show that CNNs equipped with LESPS can well recover the target masks from corresponding point labels, {and can achieve over 70% and 95% of their fully supervised performance in terms of pixel-level intersection over union (IoU) and object-level probability of detection (Pd), respectively. Code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2304.01484",
    "authors": [
      "Xinyi Ying",
      "Li Liu",
      "Yingqian Wang",
      "Ruojing Li",
      "Nuo Chen",
      "Zaiping Lin",
      "Weidong Sheng",
      "Shilin Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2304.02156",
    "title": "Reconfigurable Heterogeneous Quorum Systems",
    "abstract": "           In contrast to proof-of-work replication, Byzantine quorum systems maintain consistency across replicas with higher throughput modest energy consumption, and deterministic liveness guarantees. If complemented with heterogeneous trust and open membership, they have the potential to serve as blockchains backbone. This paper presents a general model of heterogeneous quorum systems where each participant can declare its own quorums, and captures the consistency, availability and inclusion properties of these systems. In order to support open membership, it then presents reconfiguration protocols for heterogeneous quorum systems including joining and leaving of a process, and adding and removing of a quorum, and further, proves their correctness in the face of Byzantine attacks. The design of the protocols is informed by the trade-offs that the paper proves for the properties that reconfigurations can preserve. The paper further presents a graph characterization of heterogeneous quorum systems, and its application for reconfiguration optimization.         ",
    "url": "https://arxiv.org/abs/2304.02156",
    "authors": [
      "Xiao Li",
      "Mohsen Lesani"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2305.05448",
    "title": "Robust Implicit Regularization via Weight Normalization",
    "abstract": "           Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient flow (continuous-time version of gradient descent) with weight normalization, where the weight vector is reparameterized in terms of polar coordinates, and gradient flow is applied to the polar coordinates. By analyzing key invariants of the gradient flow and using Lojasiewicz Theorem, we show that weight normalization also has an implicit bias towards sparse solutions in the diagonal linear model, but that in contrast to plain gradient flow, weight normalization enables a robust bias that persists even if the weights are initialized at practically large scale. Experiments suggest that the gains in both convergence speed and robustness of the implicit bias are improved dramatically by using weight normalization in overparameterized diagonal linear network models.         ",
    "url": "https://arxiv.org/abs/2305.05448",
    "authors": [
      "Hung-Hsu Chou",
      "Holger Rauhut",
      "Rachel Ward"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2306.05567",
    "title": "Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems",
    "abstract": "           This study confronts the growing challenges of energy consumption and the depletion of energy resources, particularly in the context of smart buildings. As the demand for energy increases alongside the necessity for efficient building maintenance, it becomes imperative to explore innovative energy management solutions. We present a comprehensive review of Internet of Things (IoT)-based frameworks aimed at smart city energy management, highlighting the pivotal role of IoT devices in addressing these issues due to their compactness, sensing, measurement, and computing capabilities. Our review methodology encompasses a thorough analysis of existing literature on IoT architectures and frameworks for intelligent energy management applications. We focus on systems that not only collect and store data but also support intelligent analysis for monitoring, controlling, and enhancing system efficiency. Additionally, we examine the potential for these frameworks to serve as platforms for the development of third-party applications, thereby extending their utility and adaptability. The findings from our review indicate that IoT-based frameworks offer significant potential to reduce energy consumption and environmental impact in smart buildings. Through the adoption of intelligent mechanisms and solutions, these frameworks facilitate effective energy management, leading to improved system efficiency and sustainability. Considering these findings, we recommend further exploration and adoption of IoT-based wireless sensing systems in smart buildings as a strategic approach to energy management. Our review underscores the importance of incorporating intelligent analysis and enabling the development of third-party applications within the IoT framework to efficiently meet the evolving energy demands and maintenance challenges         ",
    "url": "https://arxiv.org/abs/2306.05567",
    "authors": [
      "Maryam Nikpour",
      "Parisa Behvand Yousefi",
      "Hadi Jafarzadeh",
      "Kasra Danesh",
      "Roya Shomali",
      "Ahmad Gholizadeh Lonbar",
      "Mohsen Ahmadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2307.00481",
    "title": "Seeing is not Believing: An Identity Hider for Human Vision Privacy Protection",
    "abstract": "           Massive captured face images are stored in the database for the identification of individuals. However, these images can be observed unintentionally by data managers, which is not at the will of individuals and may cause privacy violations. Existing protection schemes can maintain identifiability but slightly change the facial appearance, rendering it still susceptible to the visual perception of the original identity by data managers. In this paper, we propose an effective identity hider for human vision protection, which can significantly change appearance to visually hide identity while allowing identification for face recognizers. Concretely, the identity hider benefits from two specially designed modules: 1) The virtual face generation module generates a virtual face with a new appearance by manipulating the latent space of StyleGAN2. In particular, the virtual face has a similar parsing map to the original face, supporting other vision tasks such as head pose detection. 2) The appearance transfer module transfers the appearance of the virtual face into the original face via attribute replacement. Meanwhile, identity information can be preserved well with the help of the disentanglement networks. In addition, diversity and background preservation are supported to meet the various requirements. Extensive experiments demonstrate that the proposed identity hider achieves excellent performance on privacy protection and identifiability preservation.         ",
    "url": "https://arxiv.org/abs/2307.00481",
    "authors": [
      "Tao Wang",
      "Yushu Zhang",
      "Zixuan Yang",
      "Xiangli Xiao",
      "Hua Zhang",
      "Zhongyun Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.07960",
    "title": "Did the Roll-Out of Community Notes Reduce Engagement With Misinformation on X/Twitter?",
    "abstract": "           Developing interventions that successfully reduce engagement with misinformation on social media is challenging. One intervention that has recently gained great attention is X/Twitter's Community Notes (previously known as \"Birdwatch\"). Community Notes is a crowdsourced fact-checking approach that allows users to write textual notes to inform others about potentially misleading posts on X/Twitter. Yet, empirical evidence regarding its effectiveness in reducing engagement with misinformation on social media is missing. In this paper, we perform a large-scale empirical study to analyze whether the introduction of the Community Notes feature and its roll-out to users in the U.S. and around the world have reduced engagement with misinformation on X/Twitter in terms of retweet volume and likes. We employ Difference-in-Differences (DiD) models and Regression Discontinuity Design (RDD) to analyze a comprehensive dataset consisting of all fact-checking notes and corresponding source tweets since the launch of Community Notes in early 2021. Although we observe a significant increase in the volume of fact-checks carried out via Community Notes, particularly for tweets from verified users with many followers, we find no evidence that the introduction of Community Notes significantly reduced engagement with misleading tweets on X/Twitter. Rather, our findings suggest that Community Notes might be too slow to effectively reduce engagement with misinformation in the early (and most viral) stage of diffusion. Our work emphasizes the importance of evaluating fact-checking interventions in the field and offers important implications to enhance crowdsourced fact-checking strategies on social media.         ",
    "url": "https://arxiv.org/abs/2307.07960",
    "authors": [
      "Yuwei Chuai",
      "Haoye Tian",
      "Nicolas Pr\u00f6llochs",
      "Gabriele Lenzini"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2307.11672",
    "title": "Robust Feature Inference: A Test-time Defense Strategy using Spectral Projections",
    "abstract": "           Test-time defenses are used to improve the robustness of deep neural networks to adversarial examples during inference. However, existing methods either require an additional trained classifier to detect and correct the adversarial samples, or perform additional complex optimization on the model parameters or the input to adapt to the adversarial samples at test-time, resulting in a significant increase in the inference time compared to the base model. In this work, we propose a novel test-time defense strategy called Robust Feature Inference (RFI) that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically characterize the subspace of the eigenspectrum of the feature covariance that is the most robust for a generalized additive model. Our extensive experiments on CIFAR-10, CIFAR-100, tiny ImageNet and ImageNet datasets for several robustness benchmarks, including the state-of-the-art methods in RobustBench show that RFI improves robustness across adaptive and transfer attacks consistently. We also compare RFI with adaptive test-time defenses to demonstrate the effectiveness of our proposed approach.         ",
    "url": "https://arxiv.org/abs/2307.11672",
    "authors": [
      "Anurag Singh",
      "Mahalakshmi Sabanayagam",
      "Krikamol Muandet",
      "Debarghya Ghoshdastidar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2307.11892",
    "title": "On the Vulnerability of Fairness Constrained Learning to Malicious Noise",
    "abstract": "           We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\\Theta(\\alpha)$ loss in accuracy, where $\\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\\sqrt{\\alpha})$ loss, and give a matching $\\Omega(\\sqrt{\\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple \"tricks\" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\\alpha)$,$O(\\sqrt{\\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.         ",
    "url": "https://arxiv.org/abs/2307.11892",
    "authors": [
      "Avrim Blum",
      "Princewill Okoroafor",
      "Aadirupa Saha",
      "Kevin Stangl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2308.10421",
    "title": "UniM$^2$AE: Multi-modal Masked Autoencoders with Unified 3D Representation for 3D Perception in Autonomous Driving",
    "abstract": "           Masked Autoencoders (MAE) play a pivotal role in learning potent representations, delivering outstanding results across various 3D perception tasks essential for autonomous driving. In real-world driving scenarios, it's commonplace to deploy multiple sensors for comprehensive environment perception. Despite integrating multi-modal features from these sensors can produce rich and powerful features, there is a noticeable challenge in MAE methods addressing this integration due to the substantial disparity between the different modalities. This research delves into multi-modal Masked Autoencoders tailored for a unified representation space in autonomous driving, aiming to pioneer a more efficient fusion of two distinct modalities. To intricately marry the semantics inherent in images with the geometric intricacies of LiDAR point clouds, we propose UniM$^2$AE. This model stands as a potent yet straightforward, multi-modal self-supervised pre-training framework, mainly consisting of two designs. First, it projects the features from both modalities into a cohesive 3D volume space to intricately marry the bird's eye view (BEV) with the height dimension. The extension allows for a precise representation of objects and reduces information loss when aligning multi-modal features. Second, the Multi-modal 3D Interactive Module (MMIM) is invoked to facilitate the efficient inter-modal interaction during the interaction process. Extensive experiments conducted on the nuScenes Dataset attest to the efficacy of UniM$^2$AE, indicating enhancements in 3D object detection and BEV map segmentation by 1.2\\% NDS and 6.5\\% mIoU, respectively. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2308.10421",
    "authors": [
      "Jian Zou",
      "Tianyu Huang",
      "Guanglei Yang",
      "Zhenhua Guo",
      "Tao Luo",
      "Chun-Mei Feng",
      "Wangmeng Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.13080",
    "title": "SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels",
    "abstract": "           The proliferation of news media outlets has increased the demand for intelligent systems capable of detecting redundant information in news articles in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a novel dataset of similar news, SPICED, which includes seven topics: Crime & Law, Culture & Entertainment, Disasters & Accidents, Economy & Business, Politics & Conflicts, Science & Technology, and Sports. Futhermore, we present four different levels of complexity, specifically designed for news similarity detection task. We benchmarked the created datasets using MinHash, BERT, SBERT, and SimCSE models.         ",
    "url": "https://arxiv.org/abs/2309.13080",
    "authors": [
      "Elena Shushkevich",
      "Long Mai",
      "Manuel V. Loureiro",
      "Steven Derby",
      "Tri Kurniawan Wijaya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.08750",
    "title": "Search-Adaptor: Embedding Customization for Information Retrieval",
    "abstract": "           Embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data can further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the embeddings generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via prediction APIs. On multiple English, multilingual, and multimodal retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5% improvements for Google Embedding APIs in nDCG@10 averaged over 14 BEIR datasets.         ",
    "url": "https://arxiv.org/abs/2310.08750",
    "authors": [
      "Jinsung Yoon",
      "Sercan O Arik",
      "Yanfei Chen",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.02192",
    "title": "Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models",
    "abstract": "           Identifying contextual integrity (CI) and governing knowledge commons (GKC) parameters in privacy policy texts can facilitate normative privacy analysis. However, GKC-CI annotation has heretofore required manual or crowdsourced effort. This paper demonstrates that high-accuracy GKC-CI parameter annotation of privacy policies can be performed automatically using large language models. We fine-tune 50 open-source and proprietary models on 21,588 GKC-CI annotations from 16 ground truth privacy policies. Our best performing model has an accuracy of 90.65%, which is comparable to the accuracy of experts on the same task. We apply our best performing model to 456 privacy policies from a variety of online services, demonstrating the effectiveness of scaling GKC-CI annotation for privacy policy exploration and analysis. We publicly release our model training code, training and testing data, an annotation visualizer, and all annotated policies for future GKC-CI research.         ",
    "url": "https://arxiv.org/abs/2311.02192",
    "authors": [
      "Jake Chanenson",
      "Madison Pickering",
      "Noah Apthorpe"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2311.09077",
    "title": "Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation",
    "abstract": "           A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct a comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of the spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our method achieves SOTA performance. The source code and the supplementary material are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2311.09077",
    "authors": [
      "Zhanfeng Liao",
      "Qian Zheng",
      "Yan Liu",
      "Gang Pan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2401.16352",
    "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization",
    "abstract": "           The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline to acquire the robust purifier model, named Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks, resulting in the robustness generalization to unseen attacks, and FT is essential for the improvement of robustness. To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves optimal robustness and exhibits generalization ability against unseen attacks.         ",
    "url": "https://arxiv.org/abs/2401.16352",
    "authors": [
      "Guang Lin",
      "Chao Li",
      "Jianhai Zhang",
      "Toshihisa Tanaka",
      "Qibin Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.14202",
    "title": "Comparing Graph Transformers via Positional Encodings",
    "abstract": "           The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in terms of graph transformers. Based on our theoretical results, we provide a study on several APEs and RPEs (including the resistance distance and the recently introduced stable and expressive positional encoding (SPE)) and compare their distinguishing power in terms of transformers. We believe our work will help navigate the huge number of choices of positional encoding and will provide guidance on the future design of positional encodings for graph transformers.         ",
    "url": "https://arxiv.org/abs/2402.14202",
    "authors": [
      "Mitchell Black",
      "Zhengchao Wan",
      "Gal Mishne",
      "Amir Nayyeri",
      "Yusu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.14609",
    "title": "Federated Neural Graph Databases",
    "abstract": "           The increasing demand for large-scale language models (LLMs) has highlighted the importance of efficient data retrieval mechanisms. Neural graph databases (NGDBs) have emerged as a promising approach to storing and querying graph-structured data in neural space, enabling the retrieval of relevant information for LLMs. However, existing NGDBs are typically designed to operate on a single graph, limiting their ability to reason across multiple graphs. Furthermore, the lack of support for multi-source graph data in existing NGDBs hinders their ability to capture the complexity and diversity of real-world data. In many applications, data is distributed across multiple sources, and the ability to reason across these sources is crucial for making informed decisions. This limitation is particularly problematic when dealing with sensitive graph data, as directly sharing and aggregating such data poses significant privacy risks. As a result, many applications that rely on NGDBs are forced to choose between compromising data privacy or sacrificing the ability to reason across multiple graphs. To address these limitations, we propose Federated Neural Graph Database (FedNGDB), a novel framework that enables reasoning over multi-source graph-based data while preserving privacy. FedNGDB leverages federated learning to collaboratively learn graph representations across multiple sources, enriching relationships between entities and improving the overall quality of the graph data. Unlike existing methods, FedNGDB can handle complex graph structures and relationships, making it suitable for various downstream tasks.         ",
    "url": "https://arxiv.org/abs/2402.14609",
    "authors": [
      "Qi Hu",
      "Weifeng Jiang",
      "Haoran Li",
      "Zihao Wang",
      "Jiaxin Bai",
      "Qianren Mao",
      "Yangqiu Song",
      "Lixin Fan",
      "Jianxin Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2402.18093",
    "title": "ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection",
    "abstract": "           The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in malicious email filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as potentially fraudulent, risking the possibility of missing important communications or mistakenly trusting deceptive phishing emails. This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.         ",
    "url": "https://arxiv.org/abs/2402.18093",
    "authors": [
      "Takashi Koide",
      "Naoki Fukushi",
      "Hiroki Nakano",
      "Daiki Chiba"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2403.05379",
    "title": "Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification",
    "abstract": "           Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for supervised model training. Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level. Multiple Instance Learning (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data. In this study, we explore Self-Supervised Learning (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training. We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against supervised pre-training. Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL. This breakthrough offers a cost-effective and data-efficient solution, propelling the field of AI-based disease diagnosis.         ",
    "url": "https://arxiv.org/abs/2403.05379",
    "authors": [
      "Salome Kazeminia",
      "Max Joosten",
      "Dragan Bosnacki",
      "Carsten Marr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.11865",
    "title": "Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging",
    "abstract": "           Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: this https URL.         ",
    "url": "https://arxiv.org/abs/2403.11865",
    "authors": [
      "Mert \u00d6zer",
      "Maximilian Weiherer",
      "Martin Hundhausen",
      "Bernhard Egger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2403.16067",
    "title": "Robust Diffusion Models for Adversarial Purification",
    "abstract": "           Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also mitigate the accuracy-robustness trade-off of DMs for the first time, which also provides DM-based AP an efficient adaptive ability to new attacks. Extensive experiments are conducted on CIFAR-10, CIFAR-100 and ImageNet to demonstrate that our method achieves the state-of-the-art results and exhibits generalization against different attacks.         ",
    "url": "https://arxiv.org/abs/2403.16067",
    "authors": [
      "Guang Lin",
      "Zerui Tao",
      "Jianhai Zhang",
      "Toshihisa Tanaka",
      "Qibin Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.00204",
    "title": "AirPilot: A PPO-based DRL Auto-Tuned Nonlinear PID Drone Controller for Robust Autonomous Flights",
    "abstract": "           Navigation precision, speed and stability are crucial for safe Unmanned Aerial Vehicle (UAV) flight maneuvers and effective flight mission executions in dynamic environments. Different flight missions may have varying objectives, such as minimizing energy consumption, achieving precise positioning, or maximizing speed. A controller that can adapt to different objectives on the fly is highly valuable. Proportional Integral Derivative (PID) controllers are one of the most popular and widely used control algorithms for drones and other control systems, but their linear control algorithm fails to capture the nonlinear nature of the dynamic wind conditions and complex drone system. Manually tuning the PID gains for various missions can be time-consuming and requires significant expertise. This paper aims to revolutionize drone flight control by presenting the AirPilot, a nonlinear Deep Reinforcement Learning (DRL) - enhanced Proportional Integral Derivative (PID) drone controller using Proximal Policy Optimization (PPO). AirPilot controller combines the simplicity and effectiveness of traditional PID control with the adaptability, learning capability, and optimization potential of DRL. This makes it better suited for modern drone applications where the environment is dynamic, and mission-specific performance demands are high. We employed a COEX Clover autonomous drone for training the DRL agent within the simulator and implemented it in a real-world lab setting, which marks a significant milestone as one of the first attempts to apply a DRL-based flight controller on an actual drone. Airpilot is capable of reducing the navigation error of the default PX4 PID position controller by 90%, improving effective navigation speed of a fine-tuned PID controller by 21%, reducing settling time and overshoot by 17% and 16% respectively.         ",
    "url": "https://arxiv.org/abs/2404.00204",
    "authors": [
      "Junyang Zhang",
      "Cristian Emanuel Ocampo Rivera",
      "Kyle Tyni",
      "Steven Nguyen",
      "Ulices Santa Cruz Leal",
      "Yasser Shoukry"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2404.18645",
    "title": "Graph Search Trees and the Intermezzo Problem",
    "abstract": "           The last in-tree recognition problem asks whether a given spanning tree can be derived by connecting each vertex with its rightmost left neighbor of some search ordering. In this study, we demonstrate that the last-in-tree recognition problem for Generic Search is $\\mathsf{NP}$-complete. We utilize this finding to strengthen a complexity result from order theory. Given a partial order $\\pi$ and a set of triples, the $\\mathsf{NP}$-complete intermezzo problem asks for a linear extension of $\\pi$ where each first element of a triple is not between the other two. We show that this problem remains $\\mathsf{NP}$-complete even when the Hasse diagram of the partial order forms a tree of bounded height. In contrast, we give an $\\mathsf{XP}$-algorithm for the problem when parameterized by the width of the partial order. Furthermore, we show that $\\unicode{x2013}$ under the assumption of the Exponential Time Hypothesis $\\unicode{x2013}$ the running time of this algorithm is asymptotically optimal.         ",
    "url": "https://arxiv.org/abs/2404.18645",
    "authors": [
      "Jesse Beisegel",
      "Ekkehard K\u00f6hler",
      "Fabienne Ratajczak",
      "Robert Scheffler",
      "Martin Strehler"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2405.00129",
    "title": "Reconstructing networks from simple and complex contagions",
    "abstract": "           Network scientists often use complex dynamic processes to describe network contagions, but tools for fitting contagion models typically assume simple dynamics. Here, we address this gap by developing a nonparametric method to reconstruct a network and dynamics from a series of node states, using a model that breaks the dichotomy between simple pairwise and complex neighborhood-based contagions. We then show that a network is more easily reconstructed when observed through the lens of complex contagions if it is dense or the dynamic saturates, and that simple contagions are better otherwise.         ",
    "url": "https://arxiv.org/abs/2405.00129",
    "authors": [
      "Nicholas W. Landry",
      "William Thompson",
      "Laurent H\u00e9bert-Dufresne",
      "Jean-Gabriel Young"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Populations and Evolution (q-bio.PE)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2405.01190",
    "title": "On the Impact of Dynamic Beamforming on EMF Exposure and Network Coverage: A Stochastic Geometry Perspective",
    "abstract": "           This paper introduces a new mathematical framework for dynamic beamforming-based cellular networks, grounded in stochastic geometry. The framework is used to study the electromagnetic field exposure (EMFE) of active and idle users as a function of the distance between them. A novel multi-cosine antenna pattern is introduced, offering more accurate modeling by incorporating both main and side lobes. Results show that the cumulative distribution functions of EMFE and coverage obtained with the multi-cosine pattern align closely with theoretical models, reducing error to less than 2\\%, compared to a minimum of 8\\% for other models. The marginal distribution of EMFE for each user type is mathematically derived. A unique contribution is the introduction of the SCAIU (\\underline{S}patial \\underline{C}DF for \\underline{A}ctive and \\underline{I}dle \\underline{U}sers), a metric that ensures coverage for active users while limiting EMFE for idle users. Network performance is analyzed using these metrics across varying distances and antenna elements. The analysis reveals that, for the chosen network parameters, with 64 antenna elements, the impact on idle user EMFE becomes negligible beyond 60~m. However, to maintain active user SINR above 10 dB and idle user EMFE below -50~dBm at 2~m, more than 256 elements are required.         ",
    "url": "https://arxiv.org/abs/2405.01190",
    "authors": [
      "Quentin Gontier",
      "Charles Wiame",
      "Joe Wiart",
      "Fran\u00e7ois Horlin",
      "Christo Tsigros",
      "Claude Oestges",
      "Philippe De Doncker"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Applications (stat.AP)"
    ]
  },
  {
    "id": "arXiv:2405.15886",
    "title": "A Neurosymbolic Framework for Bias Correction in Convolutional Neural Networks",
    "abstract": "           Recent efforts in interpreting Convolutional Neural Networks (CNNs) focus on translating the activation of CNN filters into a stratified Answer Set Program (ASP) rule-sets. The CNN filters are known to capture high-level image concepts, thus the predicates in the rule-set are mapped to the concept that their corresponding filter represents. Hence, the rule-set exemplifies the decision-making process of the CNN w.r.t the concepts that it learns for any image classification task. These rule-sets help understand the biases in CNNs, although correcting the biases remains a challenge. We introduce a neurosymbolic framework called NeSyBiCor for bias correction in a trained CNN. Given symbolic concepts, as ASP constraints, that the CNN is biased towards, we convert the concepts to their corresponding vector representations. Then, the CNN is retrained using our novel semantic similarity loss that pushes the filters away from (or towards) learning the desired/undesired concepts. The final ASP rule-set obtained after retraining, satisfies the constraints to a high degree, thus showing the revision in the knowledge of the CNN. We demonstrate that our NeSyBiCor framework successfully corrects the biases of CNNs trained with subsets of classes from the \"Places\" dataset while sacrificing minimal accuracy and improving interpretability.         ",
    "url": "https://arxiv.org/abs/2405.15886",
    "authors": [
      "Parth Padalkar",
      "Natalia \u015alusarz",
      "Ekaterina Komendantskaya",
      "Gopal Gupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.07961",
    "title": "Accurate Explanation Model for Image Classifiers using Class Association Embedding",
    "abstract": "           Image classification is a primary task in data analysis where explainable models are crucially demanded in various applications. Although amounts of methods have been proposed to obtain explainable knowledge from the black-box classifiers, these approaches lack the efficiency of extracting global knowledge regarding the classification task, thus is vulnerable to local traps and often leads to poor accuracy. In this study, we propose a generative explanation model that combines the advantages of global and local knowledge for explaining image classifiers. We develop a representation learning method called class association embedding (CAE), which encodes each sample into a pair of separated class-associated and individual codes. Recombining the individual code of a given sample with altered class-associated code leads to a synthetic real-looking sample with preserved individual characters but modified class-associated features and possibly flipped class assignments. A building-block coherency feature extraction algorithm is proposed that efficiently separates class-associated features from individual ones. The extracted feature space forms a low-dimensional manifold that visualizes the classification decision patterns. Explanation on each individual sample can be then achieved in a counter-factual generation manner which continuously modifies the sample in one direction, by shifting its class-associated code along a guided path, until its classification outcome is changed. We compare our method with state-of-the-art ones on explaining image classification tasks in the form of saliency maps, demonstrating that our method achieves higher accuracies. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2406.07961",
    "authors": [
      "Ruitao Xie",
      "Jingbang Chen",
      "Limai Jiang",
      "Rui Xiao",
      "Yi Pan",
      "Yunpeng Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.15152",
    "title": "Generative Topological Networks",
    "abstract": "           Generative models have seen significant advancements in recent years, yet often remain challenging and costly to train and use. We introduce Generative Topological Networks (GTNs) -- a new class of generative models that addresses these shortcomings. GTNs are trained deterministically using a simple supervised learning approach grounded in topology theory. GTNs are fast to train, and require only a single forward pass in a standard feedforward neural network to generate samples. We demonstrate the strengths of GTNs on several datasets, including MNIST, CelebA and the Hands and Palm Images dataset. Finally, the theory behind GTNs offers insights into how to train generative models for improved performance. Code and weights are available at: this https URL ",
    "url": "https://arxiv.org/abs/2406.15152",
    "authors": [
      "Alona Levy-Jurgenson",
      "Zohar Yakhini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2407.07018",
    "title": "End-To-End Causal Effect Estimation from Unstructured Natural Language Data",
    "abstract": "           Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce NATURAL, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.         ",
    "url": "https://arxiv.org/abs/2407.07018",
    "authors": [
      "Nikita Dhawan",
      "Leonardo Cotta",
      "Karen Ullrich",
      "Rahul G. Krishnan",
      "Chris J. Maddison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2408.09072",
    "title": "Enhancing Community Detection in Networks: A Comparative Analysis of Local Metrics and Hierarchical Algorithms",
    "abstract": "           The analysis and detection of communities in network structures are becoming increasingly relevant for understanding social behavior. One of the principal challenges in this field is the complexity of existing algorithms. The Girvan-Newman algorithm, which uses the betweenness metric as a measure of node similarity, is one of the most representative algorithms in this area. This study employs the same method to evaluate the relevance of using local similarity metrics for community detection. A series of local metrics were tested on a set of networks constructed using the Girvan-Newman basic algorithm. The efficacy of these metrics was evaluated by applying the base algorithm to several real networks with varying community sizes, using modularity and NMI. The results indicate that approaches based on local similarity metrics have significant potential for community detection.         ",
    "url": "https://arxiv.org/abs/2408.09072",
    "authors": [
      "Julio-Omar Palacio-Ni\u00f1o",
      "Fernando Berzal"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.09266",
    "title": "Graph Classification with GNNs: Optimisation, Representation and Inductive Bias",
    "abstract": "           Theoretical studies on the representation power of GNNs have been centered around understanding the equivalence of GNNs, using WL-Tests for detecting graph isomorphism. In this paper, we argue that such equivalence ignores the accompanying optimization issues and does not provide a holistic view of the GNN learning process. We illustrate these gaps between representation and optimization with examples and experiments. We also explore the existence of an implicit inductive bias (e.g. fully connected networks prefer to learn low frequency functions in their input space) in GNNs, in the context of graph classification tasks. We further prove theoretically that the message-passing layers in the graph, have a tendency to search for either discriminative subgraphs, or a collection of discriminative nodes dispersed across the graph, depending on the different global pooling layers used. We empirically verify this bias through experiments over real-world and synthetic datasets. Finally, we show how our work can help in incorporating domain knowledge via attention based architectures, and can evince their capability to discriminate coherent subgraphs.         ",
    "url": "https://arxiv.org/abs/2408.09266",
    "authors": [
      "P. Krishna Kumar a",
      "Harish G. Ramaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.10437",
    "title": "Understanding Generative AI Content with Embedding Models",
    "abstract": "           The construction of high-quality numerical features is critical to any quantitative data analysis. Feature engineering has been historically addressed by carefully hand-crafting data representations based on domain expertise. This work views the internal representations of modern deep neural networks (DNNs), called embeddings, as an automated form of traditional feature engineering. For trained DNNs, we show that these embeddings can reveal interpretable, high-level concepts in unstructured sample data. We use these embeddings in natural language and computer vision tasks to uncover both inherent heterogeneity in the underlying data and human-understandable explanations for it. In particular, we find empirical evidence that there is inherent separability between real data and that generated from AI models.         ",
    "url": "https://arxiv.org/abs/2408.10437",
    "authors": [
      "Max Vargas",
      "Reilly Cannon",
      "Andrew Engel",
      "Anand D. Sarwate",
      "Tony Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.10878",
    "title": "DBHP: Trajectory Imputation in Multi-Agent Sports Using Derivative-Based Hybrid Prediction",
    "abstract": "           Many spatiotemporal domains handle multi-agent trajectory data, but in real-world scenarios, collected trajectory data are often partially missing due to various reasons. While existing approaches demonstrate good performance in trajectory imputation, they face challenges in capturing the complex dynamics and interactions between agents due to a lack of physical constraints that govern realistic trajectories, leading to suboptimal results. To address this issue, the paper proposes a Derivative-Based Hybrid Prediction (DBHP) framework that can effectively impute multiple agents' missing trajectories. First, a neural network equipped with Set Transformers produces a naive prediction of missing trajectories while satisfying the permutation-equivariance in terms of the order of input agents. Then, the framework makes alternative predictions leveraging velocity and acceleration information and combines all the predictions with properly determined weights to provide final imputed trajectories. In this way, our proposed framework not only accurately predicts position, velocity, and acceleration values but also enforces the physical relationship between them, eventually improving both the accuracy and naturalness of the predicted trajectories. Accordingly, the experiment results about imputing player trajectories in team sports show that our framework significantly outperforms existing imputation baselines.         ",
    "url": "https://arxiv.org/abs/2408.10878",
    "authors": [
      "Hanjun Choi",
      "Hyunsung Kim",
      "Minho Lee",
      "Chang-Jo Kim",
      "Jinsung Yoon",
      "Sang-Ki Ko"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2408.11557",
    "title": "A Quick, trustworthy spectral detection Q&A system based on the SDAAP Dataset and large language model",
    "abstract": "           Large Language Model (LLM) has demonstrated significant success in a range of natural language processing (NLP) tasks within general domain. The emergence of LLM has introduced innovative methodologies across diverse fields, including the natural sciences. Researchers aim to implement automated, concurrent process driven by LLM to supplant conventional manual, repetitive and labor-intensive work. In the domain of spectral analysis and detection, it is imperative for researchers to autonomously acquire pertinent knowledge across various research objects, which encompasses the spectroscopic techniques and the chemometric methods that are employed in experiments and analysis. Paradoxically, despite the recognition of spectroscopic detection as an effective analytical method, the fundamental process of knowledge retrieval remains both time-intensive and repetitive. In response to this challenge, we first introduced the Spectral Detection and Analysis Based Paper(SDAAP) dataset, which is the first open-source textual knowledge dataset for spectral analysis and detection and contains annotated literature data as well as corresponding knowledge instruction data. Subsequently, we also designed an automated Q\\&A framework based on the SDAAP dataset, which can retrieve relevant knowledge and generate high-quality responses by extracting entities in the input as retrieval parameters. It is worth noting that: within this framework, LLM is only used as a tool to provide generalizability, while RAG technique is used to accurately capture the source of the knowledge.This approach not only improves the quality of the generated responses, but also ensures the traceability of the knowledge. Experimental results show that our framework generates responses with more reliable expertise compared to the baseline.         ",
    "url": "https://arxiv.org/abs/2408.11557",
    "authors": [
      "Jiheng Liang",
      "Ziru Yu",
      "Zujie Xie",
      "Xiangyang Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2408.11611",
    "title": "DTN: Deep Multiple Task-specific Feature Interactions Network for Multi-Task Recommendation",
    "abstract": "           Neural-based multi-task learning (MTL) has been successfully applied to many recommendation applications. However, these MTL models (e.g., MMoE, PLE) did not consider feature interaction during the optimization, which is crucial for capturing complex high-order features and has been widely used in ranking models for real-world recommender systems. Moreover, through feature importance analysis across various tasks in MTL, we have observed an interesting divergence phenomenon that the same feature can have significantly different importance across different tasks in MTL. To address these issues, we propose Deep Multiple Task-specific Feature Interactions Network (DTN) with a novel model structure design. DTN introduces multiple diversified task-specific feature interaction methods and task-sensitive network in MTL networks, enabling the model to learn task-specific diversified feature interaction representations, which improves the efficiency of joint representation learning in a general setup. We applied DTN to our company's real-world E-commerce recommendation dataset, which consisted of over 6.3 billion samples, the results demonstrated that DTN significantly outperformed state-of-the-art MTL models. Moreover, during online evaluation of DTN in a large-scale E-commerce recommender system, we observed a 3.28% in clicks, a 3.10% increase in orders and a 2.70% increase in GMV (Gross Merchandise Value) compared to the state-of-the-art MTL models. Finally, extensive offline experiments conducted on public benchmark datasets demonstrate that DTN can be applied to various scenarios beyond recommendations, enhancing the performance of ranking models.         ",
    "url": "https://arxiv.org/abs/2408.11611",
    "authors": [
      "Yaowen Bi",
      "Yuteng Lian",
      "Jie Cui",
      "Jun Liu",
      "Peijian Wang",
      "Guanghui Li",
      "Xuejun Chen",
      "Jinglin Zhao",
      "Hao Wen",
      "Jing Zhang",
      "Zhaoqi Zhang",
      "Wenzhuo Song",
      "Yang Sun",
      "Weiwei Zhang",
      "Mingchen Cai",
      "Guanxing Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.11752",
    "title": "Consensus over Clustered Networks Using Intermittent and Asynchronous Output Feedback",
    "abstract": "           In recent years, multi-agent teaming has garnered considerable interest since complex objectives, such as intelligence, surveillance, and reconnaissance, can be divided into multiple cluster-level sub-tasks and assigned to a cluster of agents with the appropriate functionality. Yet, coordination and information dissemination between clusters may be necessary to accomplish a desired objective. Distributed consensus protocols provide a mechanism for spreading information within clustered networks, allowing agents and clusters to make decisions without requiring direct access to the state of the ensemble. Hence, we propose a strategy for achieving system-wide consensus in the states of identical linear time-invariant systems coupled by an undirected graph whose directed sub-graphs are available only at sporadic times. Within this work, the agents of the network are organized into pairwise disjoint clusters, which induce sub-graphs of the undirected parent graph. Some cluster sub-graph pairs are linked by an inter-cluster sub-graph, where the union of all cluster and inter-cluster sub-graphs yields the undirected parent graph. Each agent utilizes a distributed consensus protocol with components that are updated intermittently and asynchronously with respect to other agents. The closed-loop ensemble dynamics is modeled as a hybrid system, and a Lyapunov-based stability analysis yields sufficient conditions for rendering the agreement subspace (consensus set) globally exponentially stable. Furthermore, an input-to-state stability argument demonstrates the consensus set is robust to a class of perturbations. A numerical simulation considering both nominal and perturbed scenarios is provided for validation purposes.         ",
    "url": "https://arxiv.org/abs/2408.11752",
    "authors": [
      "Federico M. Zegers",
      "Sean Phillips"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.12380",
    "title": "UMERegRobust - Universal Manifold Embedding Compatible Features for Robust Point Cloud Registration",
    "abstract": "           In this paper, we adopt the Universal Manifold Embedding (UME) framework for the estimation of rigid transformations and extend it, so that it can accommodate scenarios involving partial overlap and differently sampled point clouds. UME is a methodology designed for mapping observations of the same object, related by rigid transformations, into a single low-dimensional linear subspace. This process yields a transformation-invariant representation of the observations, with its matrix form representation being covariant (i.e. equivariant) with the transformation. We extend the UME framework by introducing a UME-compatible feature extraction method augmented with a unique UME contrastive loss and a sampling equalizer. These components are integrated into a comprehensive and robust registration pipeline, named UMERegRobust. We propose the RotKITTI registration benchmark, specifically tailored to evaluate registration methods for scenarios involving large rotations. UMERegRobust achieves better than state-of-the-art performance on the KITTI benchmark, especially when strict precision of (1\u00b0, 10cm) is considered (with an average gain of +9%), and notably outperform SOTA methods on the RotKITTI benchmark (with +45% gain compared the most recent SOTA method).         ",
    "url": "https://arxiv.org/abs/2408.12380",
    "authors": [
      "Yuval Haitman",
      "Amit Efraim",
      "Joseph M. Francos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.12594",
    "title": "Non-Homophilic Graph Pre-Training and Prompt Learning",
    "abstract": "           Graphs are ubiquitous for modeling complex relationships between objects across various fields. Graph neural networks (GNNs) have become a mainstream technique for graph-based applications, but their performance heavily relies on abundant labeled data. To reduce labeling requirement, pre-training and prompt learning has become a popular alternative. However, most existing prompt methods do not differentiate homophilic and heterophilic characteristics of real-world graphs. In particular, many real-world graphs are non-homophilic, not strictly or uniformly homophilic with mixing homophilic and heterophilic patterns, exhibiting varying non-homophilic characteristics across graphs and nodes. In this paper, we propose ProNoG, a novel pre-training and prompt learning framework for such non-homophilic graphs. First, we analyze existing graph pre-training methods, providing theoretical insights into the choice of pre-training tasks. Second, recognizing that each node exhibits unique non-homophilic characteristics, we propose a conditional network to characterize the node-specific patterns in downstream tasks. Finally, we thoroughly evaluate and analyze ProNoG through extensive experiments on ten public datasets.         ",
    "url": "https://arxiv.org/abs/2408.12594",
    "authors": [
      "Xingtong Yu",
      "Jie Zhang",
      "Yuan Fang",
      "Renhe Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.14861",
    "title": "Leveraging Task Structures for Improved Identifiability in Neural Network Representations",
    "abstract": "           This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that linear identifiability is achievable in the general multi-task regression setting. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent factors reduces the equivalence class for identifiability to permutations and scaling of the true latent factors, a stronger and more useful result than linear identifiability. Crucially, when we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization, and suggests potential downstream applications to causal representation learning. Empirically, we find that this straightforward optimization procedure enables our model to outperform more general unsupervised models in recovering canonical representations for both synthetic data and real-world molecular data.         ",
    "url": "https://arxiv.org/abs/2306.14861",
    "authors": [
      "Wenlin Chen",
      "Julien Horwood",
      "Juyeon Heo",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.15704",
    "title": "A Heterogeneous Dynamic Convolutional Neural Network for Image Super-resolution",
    "abstract": "           Convolutional neural networks can automatically learn features via deep network architectures and given input samples. However, robustness of obtained models may have challenges in varying scenes. Bigger differences of a network architecture are beneficial to extract more complementary structural information to enhance robustness of an obtained super-resolution model. In this paper, we present a heterogeneous dynamic convolutional network in image super-resolution (HDSRNet). To capture more information, HDSRNet is implemented by a heterogeneous parallel network. The upper network can facilitate more contexture information via stacked heterogeneous blocks to improve effects of image super-resolution. Each heterogeneous block is composed of a combination of a dilated, dynamic, common convolutional layers, ReLU and residual learning operation. It can not only adaptively adjust parameters, according to different inputs, but also prevent long-term dependency problem. The lower network utilizes a symmetric architecture to enhance relations of different layers to mine more structural information, which is complementary with a upper network for image super-resolution. The relevant experimental results show that the proposed HDSRNet is effective to deal with image resolving. The code of HDSRNet can be obtained at this https URL.         ",
    "url": "https://arxiv.org/abs/2402.15704",
    "authors": [
      "Chunwei Tian",
      "Xuanyu Zhang",
      "Tao Wang",
      "Wangmeng Zuo",
      "Yanning Zhang",
      "Chia-Wen Lin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2404.08372",
    "title": "Opinion dynamics on signed graphs and graphons: Beyond the piece-wise constant case",
    "abstract": "           In this paper we make use of graphon theory to study opinion dynamics on large undirected networks. The opinion dynamics models that we take into consideration allow for negative interactions between the individuals, i.e. competing entities whose opinions can grow apart. We consider both the repelling model and the opposing model that are studied in the literature. We define the repelling and the opposing dynamics on graphons and we show that their initial value problem's solutions exist and are unique. We then show that the graphon dynamics well approximate the dynamics on large graphs that converge to a graphon. This result applies to large random graphs that are sampled according to a graphon. All these facts are illustrated in an extended numerical example.         ",
    "url": "https://arxiv.org/abs/2404.08372",
    "authors": [
      "Raoul Prisant",
      "Federica Garin",
      "Paolo Frasca"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2405.15812",
    "title": "Pseudo Channel: Time Embedding for Motor Imagery Decoding",
    "abstract": "           Motor imagery (MI) based EEG represents a frontier in enabling direct neural control of external devices and advancing neural rehabilitation. This study introduces a novel time embedding technique, termed traveling-wave based time embedding, utilized as a pseudo channel to enhance the decoding accuracy of MI-EEG signals across various neural network architectures. Unlike traditional neural network methods that fail to account for the temporal dynamics in MI-EEG in individual difference, our approach captures time-related changes for different participants based on a priori knowledge. Through extensive experimentation with multiple participants, we demonstrate that this method not only improves classification accuracy but also exhibits greater adaptability to individual differences compared to position encoding used in Transformer architecture. Significantly, our results reveal that traveling-wave based time embedding crucially enhances decoding accuracy, particularly for participants typically considered \"EEG-illiteracy\". As a novel direction in EEG research, the traveling-wave based time embedding not only offers fresh insights for neural network decoding strategies but also expands new avenues for research into attention mechanisms in neuroscience and a deeper understanding of EEG signals.         ",
    "url": "https://arxiv.org/abs/2405.15812",
    "authors": [
      "Zhengqing Miao",
      "Meirong Zhao"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2406.14044",
    "title": "Encoder-Decoder Neural Networks in Interpretation of X-ray Spectra",
    "abstract": "           Encoder--decoder neural networks (EDNN) condense information most relevant to the output of the feedforward network to activation values at a bottleneck layer. We study the use of this architecture in emulation and interpretation of simulated X-ray spectroscopic data with the aim to identify key structural characteristics for the spectra, previously studied using emulator-based component analysis (ECA). We find an EDNN to outperform ECA in covered target variable variance, but also discover complications in interpreting the latent variables in physical terms. As a compromise of the benefits of these two approaches, we develop a network where the linear projection of ECA is used, thus maintaining the beneficial characteristics of vector expansion from the latent variables for their interpretation. These results underline the necessity of information recovery after its condensation and identification of decisive structural degrees of freedom for the output spectra for a justified interpretation.         ",
    "url": "https://arxiv.org/abs/2406.14044",
    "authors": [
      "Jalmari Passilahti",
      "Anton Vladyka",
      "Johannes Niskanen"
    ],
    "subjectives": [
      "Atomic and Molecular Clusters (physics.atm-clus)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ]
  },
  {
    "id": "arXiv:2407.18126",
    "title": "Proof of a conjecture on isolation of graphs dominated by a vertex",
    "abstract": "           A copy of a graph $F$ is called an $F$-copy. For any graph $G$, the $F$-isolation number of $G$, denoted by $\\iota(G,F)$, is the size of a smallest subset $D$ of the vertex set of $G$ such that the closed neighbourhood $N[D]$ of $D$ in $G$ intersects the vertex sets of the $F$-copies contained by $G$ (equivalently, $G-N[D]$ contains no $F$-copy). Thus, $\\iota(G,K_1)$ is the domination number $\\gamma(G)$ of $G$, and $\\iota(G,K_2)$ is the vertex-edge domination number of $G$. We prove that if $F$ is a $k$-edge graph, $\\gamma(F) = 1$ (that is, $F$ has a vertex that is adjacent to all the other vertices of $F$), and $G$ is a connected $m$-edge graph, then $\\iota(G,F) \\leq \\big\\lfloor \\frac{m+1}{k+2} \\big\\rfloor$ unless $G$ is an $F$-copy or $F$ is a $3$-path and $G$ is a $6$-cycle. This was recently posed as a conjecture by Zhang and Wu, who settled the case where $F$ is a star. The result for the case where $F$ is a clique had been obtained by Fenech, Kaemawichanurat and the present author. The bound is attainable for any $m \\geq 0$ unless $1 \\leq m = k \\leq 2$. New ideas, including divisibility considerations, are introduced in the proof of the conjecture.         ",
    "url": "https://arxiv.org/abs/2407.18126",
    "authors": [
      "Peter Borg"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2408.05854",
    "title": "On the Robustness of Kernel Goodness-of-Fit Tests",
    "abstract": "           Goodness-of-fit testing is often criticized for its lack of practical relevance; since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected when the sample size is large enough. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is good enough for a specific task. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated by a distribution corresponding to our model up to some mild perturbation. In this paper, we show that existing kernel goodness-of-fit tests are not robust according to common notions of robustness including qualitative and quantitative robustness. We also show that robust techniques based on tilted kernels from the parameter estimation literature are not sufficient for ensuring both types of robustness in the context of goodness-of-fit testing. We therefore propose the first robust kernel goodness-of-fit test which resolves this open problem using kernel Stein discrepancy balls, which encompass perturbation models such as Huber contamination models and density uncertainty bands.         ",
    "url": "https://arxiv.org/abs/2408.05854",
    "authors": [
      "Xing Liu",
      "Fran\u00e7ois-Xavier Briol"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ]
  }
]