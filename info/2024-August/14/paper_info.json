[
  {
    "id": "arXiv:2408.06344",
    "title": "The Signatures of Ideal Flow Networks",
    "abstract": "           An Ideal Flow Network (IFN) is a strongly connected network where relative flows are preserved (irreducible premagic matrix). IFN can be decomposed into canonical cycles to form a string code called network signature. A network signature can be composed back into an IFN by assignment and merging operations. Using string manipulations on network signatures, we can derive total flow, link values, sum of rows and columns, and probability matrices and test for irreducibility.         ",
    "url": "https://arxiv.org/abs/2408.06344",
    "authors": [
      "Kardi Teknomo"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Social and Information Networks (cs.SI)",
      "Representation Theory (math.RT)"
    ]
  },
  {
    "id": "arXiv:2408.06347",
    "title": "Automated Schizophrenia Detection from Handwriting Samples via Transfer Learning Convolutional Neural Networks",
    "abstract": "           Schizophrenia is a globally prevalent psychiatric disorder that severely impairs daily life. Schizophrenia is caused by dopamine imbalances in the fronto-striatal pathways of the brain, which influences fine motor control in the cerebellum. This leads to abnormalities in handwriting. The goal of this study was to develop an accurate, objective, and accessible computational method to be able to distinguish schizophrenic handwriting samples from non-schizophrenic handwriting samples. To achieve this, data from Crespo et al. (2019) was used, which contains images of handwriting samples from schizophrenic and non-schizophrenic patients. The data was preprocessed and augmented to produce a more robust model that can recognize different types of handwriting. The data was used to train several different convolutional neural networks, and the model with the base architecture of InceptionV3 performed the best, differentiating between the two types of image with a 92% accuracy rate. To make this model accessible, a secure website was developed for medical professionals to use for their patients. Such a result suggests that handwriting analysis through computational models holds promise as a non-invasive and objective method for clinicians to diagnose and monitor schizophrenia.         ",
    "url": "https://arxiv.org/abs/2408.06347",
    "authors": [
      "Rafael Castro",
      "Ishaan Patel",
      "Tarun Patanjali",
      "Priya Iyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06357",
    "title": "Algorithm Research of ELMo Word Embedding and Deep Learning Multimodal Transformer in Image Description",
    "abstract": "           Zero sample learning is an effective method for data deficiency. The existing embedded zero sample learning methods only use the known classes to construct the embedded space, so there is an overfitting of the known classes in the testing process. This project uses category semantic similarity measures to classify multiple tags. This enables it to incorporate unknown classes that have the same meaning as currently known classes into the vector space when it is built. At the same time, most of the existing zero sample learning algorithms directly use the depth features of medical images as input, and the feature extraction process does not consider semantic information. This project intends to take ELMo-MCT as the main task and obtain multiple visual features related to the original image through self-attention mechanism. In this paper, a large number of experiments are carried out on three zero-shot learning reference datasets, and the best harmonic average accuracy is obtained compared with the most advanced algorithms.         ",
    "url": "https://arxiv.org/abs/2408.06357",
    "authors": [
      "Xiaohan Cheng",
      "Taiyuan Mei",
      "Yun Zi",
      "Qi Wang",
      "Zijun Gao",
      "Haowei Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06380",
    "title": "Runtime Verification Containers for Publish/Subscribe Networks",
    "abstract": "           Publish/subscribe (pub/sub) networks are a cornerstone of modern distributed systems, playing a crucial role in applications like the Internet of Things (IoT) and robotics. While runtime verification techniques seem ideal for ensuring the correctness of such highly dynamic and large-scale networks, integrating runtime monitors seamlessly into real-world industrial use cases presents significant challenges. This paper studies modern containerization technology to deploy runtime verification tools to monitor publish/subscribe networks with a performance focus. Runtime verification containers are lightweight and deployable alongside other containerized publisher and subscriber participants. Each runtime verification container monitors message flow, enabling runtime verification of network behavior. We comprehensively benchmark the container-based approach using several experiments and a real-world case study from the software-defined vehicle domain.         ",
    "url": "https://arxiv.org/abs/2408.06380",
    "authors": [
      "Ali Mehran",
      "Dogan Ulus"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2408.06385",
    "title": "ViC: Virtual Compiler Is All You Need For Assembly Code Search",
    "abstract": "           Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs. Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code of any language to assembly code. This approach allows for virtual compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.         ",
    "url": "https://arxiv.org/abs/2408.06385",
    "authors": [
      "Zeyu Gao",
      "Hao Wang",
      "Yuanda Wang",
      "Chao Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06393",
    "title": "Leading by the Nodes: A Survey of Film Industry Network Analysis and Datasets",
    "abstract": "           This paper presents a comprehensive survey of network analysis research on the film industry, aiming to evaluate its emergence as a field of study and identify potential areas for further research. Many foundational network studies made use of the abundant data from the Internet Movie Database (IMDb) to test network methodologies. This survey focuses more specifically on examining research that employs network analysis to evaluate the film industry itself, revealing the social and business relationships involved in film production, distribution, and consumption. The paper adopts a classification approach based on node type and conducts network analyses of the reviewed literature, mapping citation and co-authorship networks. The findings provide insights into the structure and interconnectedness of the field, offering potential clusters of debates and shedding light on the institutional, geographical, and demographic characteristics shaping this field. In addition, this survey contributes to understanding film industry network analysis and informs researchers interested in network methods within the film industry and related cultural sectors.         ",
    "url": "https://arxiv.org/abs/2408.06393",
    "authors": [
      "Aresh Dadlani",
      "Vi Vo",
      "Ayushi Khemka",
      "Sophie Harvey",
      "Aigul Kantoro Kyzy",
      "Pete Jones",
      "Deb Verhoeven"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.06395",
    "title": "Fast John Ellipsoid Computation with Differential Privacy Optimization",
    "abstract": "           Determining the John ellipsoid - the largest volume ellipsoid contained within a convex polytope - is a fundamental problem with applications in machine learning, optimization, and data analytics. Recent work has developed fast algorithms for approximating the John ellipsoid using sketching and leverage score sampling techniques. However, these algorithms do not provide privacy guarantees for sensitive input data. In this paper, we present the first differentially private algorithm for fast John ellipsoid computation. Our method integrates noise perturbation with sketching and leverage score sampling to achieve both efficiency and privacy. We prove that (1) our algorithm provides $(\\epsilon,\\delta)$-differential privacy, and the privacy guarantee holds for neighboring datasets that are $\\epsilon_0$-close, allowing flexibility in the privacy definition; (2) our algorithm still converges to a $(1+\\xi)$-approximation of the optimal John ellipsoid in $O(\\xi^{-2}(\\log(n/\\delta_0) + (L\\epsilon_0)^{-2}))$ iterations where $n$ is the number of data point, $L$ is the Lipschitz constant, $\\delta_0$ is the failure probability, and $\\epsilon_0$ is the closeness of neighboring input datasets. Our theoretical analysis demonstrates the algorithm's convergence and privacy properties, providing a robust approach for balancing utility and privacy in John ellipsoid computation. This is the first differentially private algorithm for fast John ellipsoid computation, opening avenues for future research in privacy-preserving optimization techniques.         ",
    "url": "https://arxiv.org/abs/2408.06395",
    "authors": [
      "Jiuxiang Gu",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Junwei Yu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06428",
    "title": "Large Language Models for Secure Code Assessment: A Multi-Language Empirical Study",
    "abstract": "           Most vulnerability detection studies focus on datasets of vulnerabilities in C/C++ code, offering limited language diversity. Thus, the effectiveness of deep learning methods, including large language models (LLMs), in detecting software vulnerabilities beyond these languages is still largely unexplored. In this paper, we evaluate the effectiveness of LLMs in detecting and classifying Common Weakness Enumerations (CWE) using different prompt and role strategies. Our experimental study targets six state-of-the-art pre-trained LLMs (GPT-3.5- Turbo, GPT-4 Turbo, GPT-4o, CodeLLama-7B, CodeLLama- 13B, and Gemini 1.5 Pro) and five programming languages: Python, C, C++, Java, and JavaScript. We compiled a multi-language vulnerability dataset from different sources, to ensure representativeness. Our results showed that GPT-4o achieves the highest vulnerability detection and CWE classification scores using a few-shot setting. Aside from the quantitative results of our study, we developed a library called CODEGUARDIAN integrated with VSCode which enables developers to perform LLM-assisted real-time vulnerability analysis in real-world security scenarios. We have evaluated CODEGUARDIAN with a user study involving 22 developers from the industry. Our study showed that, by using CODEGUARDIAN, developers are more accurate and faster at detecting vulnerabilities.         ",
    "url": "https://arxiv.org/abs/2408.06428",
    "authors": [
      "Kohei Dozono",
      "Tiago Espinha Gasiba",
      "Andrea Stocco"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.06429",
    "title": "Wavelet based inpainting detection",
    "abstract": "           With the advancement in image editing tools, manipulating digital images has become alarmingly easy. Inpainting, which is used to remove objects or fill in parts of an image, serves as a powerful tool for both image restoration and forgery. This paper introduces a novel approach for detecting image inpainting forgeries by combining DT-CWT with Hierarchical Feature segmentation and with noise inconsistency analysis. The DT-CWT offers several advantages for this task, including inherent shift-invariance, which makes it robust to minor manipulations during the inpainting process, and directional selectivity, which helps capture subtle artifacts introduced by inpainting in specific frequency bands and orientations. By first applying color image segmentation and then analyzing for each segment, noise inconsistency obtained via DT-CW we can identify patterns indicative of inpainting forgeries. The proposed method is evaluated on a benchmark dataset created for this purpose and is compared with existing forgery detection techniques. Our approach demonstrates superior results compared with SOTA in detecting inpainted images.         ",
    "url": "https://arxiv.org/abs/2408.06429",
    "authors": [
      "Barglazan Adrian-Alin",
      "Brad Remus Ovidiu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06445",
    "title": "Multi-View Neural Differential Equations for Continuous-Time Stream Data in Long-Term Traffic Forecasting",
    "abstract": "           Long-term traffic flow forecasting plays a crucial role in intelligent transportation as it allows traffic managers to adjust their decisions in advance. However, the problem is challenging due to spatio-temporal correlations and complex dynamic patterns in continuous-time stream data. Neural Differential Equations (NDEs) are among the state-of-the-art methods for learning continuous-time traffic dynamics. However, the traditional NDE models face issues in long-term traffic forecasting due to failures in capturing delayed traffic patterns, dynamic edge (location-to-location correlation) patterns, and abrupt trend patterns. To fill this gap, we propose a new NDE architecture called Multi-View Neural Differential Equations. Our model captures current states, delayed states, and trends in different state variables (views) by learning latent multiple representations within Neural Differential Equations. Extensive experiments conducted on several real-world traffic datasets demonstrate that our proposed method outperforms the state-of-the-art and achieves superior prediction accuracy for long-term forecasting and robustness with noisy or missing inputs.         ",
    "url": "https://arxiv.org/abs/2408.06445",
    "authors": [
      "Zibo Liu",
      "Zhe Jiang",
      "Shigang Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06450",
    "title": "Evaluating Language Models for Efficient Code Generation",
    "abstract": "           We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score. As a proof of concept, we use DPE to create EvalPerf, a benchmark with 121 performance-challenging coding tasks. Our comprehensive evaluation draws interesting findings on the efficiency impact of model sizes, instruction tuning, and prompting. For example, while the scaling law fails to account for code efficiency, general instruction tuning benefits both code correctness and efficiency. We also evaluate the evaluation by examining the effectiveness of DPE, showing that EvalPerf is reliable and convenient to use even across platforms.         ",
    "url": "https://arxiv.org/abs/2408.06450",
    "authors": [
      "Jiawei Liu",
      "Songrun Xie",
      "Junhao Wang",
      "Yuxiang Wei",
      "Yifeng Ding",
      "Lingming Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06452",
    "title": "Wireless Channel Aware Data Augmentation Methods for Deep Leaning-Based Indoor Localization",
    "abstract": "           Indoor localization is a challenging problem that - unlike outdoor localization - lacks a universal and robust solution. Machine Learning (ML), particularly Deep Learning (DL), methods have been investigated as a promising approach. Although such methods bring remarkable localization accuracy, they heavily depend on the training data collected from the environment. The data collection is usually a laborious and time-consuming task, but Data Augmentation (DA) can be used to alleviate this issue. In this paper, different from previously used DA, we propose methods that utilize the domain knowledge about wireless propagation channels and devices. The methods exploit the typical hardware component drift in the transceivers and/or the statistical behavior of the channel, in combination with the measured Power Delay Profile (PDP). We comprehensively evaluate the proposed methods to demonstrate their effectiveness. This investigation mainly focuses on the impact of factors such as the number of measurements, augmentation proportion, and the environment of interest impact the effectiveness of the different DA methods. We show that in the low-data regime (few actual measurements available), localization accuracy increases up to 50%, matching non-augmented results in the high-data regime. In addition, the proposed methods may outperform the measurement-only high-data performance by up to 33% using only 1/4 of the amount of measured data. We also exhibit the effect of different training data distribution and quality on the effectiveness of DA. Finally, we demonstrate the power of the proposed methods when employed along with Transfer Learning (TL) to address the data scarcity in target and/or source environments.         ",
    "url": "https://arxiv.org/abs/2408.06452",
    "authors": [
      "Omer Gokalp Serbetci",
      "Daoud Burghal",
      "Andreas F. Molisch"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06457",
    "title": "Advanced Vision Transformers and Open-Set Learning for Robust Mosquito Classification: A Novel Approach to Entomological Studies",
    "abstract": "           Mosquito-related diseases pose a significant threat to global public health, necessitating efficient and accurate mosquito classification for effective surveillance and control. This work presents an innovative approach to mosquito classification by leveraging state-of-the-art vision transformers and open-set learning techniques. A novel framework has been introduced that integrates Transformer-based deep learning models with comprehensive data augmentation and preprocessing methods, enabling robust and precise identification of ten mosquito species. The Swin Transformer model achieves the best performance for traditional closed-set learning with 99.80\\% accuracy and 0.998 F1 score. The lightweight MobileViT technique attains an almost similar accuracy of 98.90\\% with significantly reduced parameters and model complexities. Next, the applied deep learning models' adaptability and generalizability in a static environment have been enhanced by using new classes of data samples during the inference stage that have not been included in the training set. The proposed framework's ability to handle unseen classes like insects similar to mosquitoes, even humans, through open-set learning further enhances its practical applicability employing the OpenMax technique and Weibull distribution. The traditional CNN model, Xception, outperforms the latest transformer with higher accuracy and F1 score for open-set learning. The study's findings highlight the transformative potential of advanced deep-learning architectures in entomology, providing a strong groundwork for future research and development in mosquito surveillance and vector control. The implications of this work extend beyond mosquito classification, offering valuable insights for broader ecological and environmental monitoring applications.         ",
    "url": "https://arxiv.org/abs/2408.06457",
    "authors": [
      "Ahmed Akib Jawad Karim",
      "Muhammad Zawad Mahmud",
      "Riasat Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06460",
    "title": "Evaluating Privacy Measures for Load Hiding",
    "abstract": "           In smart grids, the use of smart meters to measure electricity consumption at a household level raises privacy concerns. To address them, researchers have designed various load hiding algorithms that manipulate the electricity consumption measured. To compare how well these algorithms preserve privacy, various privacy measures have been proposed. However, there currently is no consensus on which privacy measure is most appropriate to use. In this study, we aim to identify the most effective privacy measure(s) for load hiding algorithms. We have crafted a series of experiments to assess the effectiveness of these measures. found 20 of the 25 measures studied to be ineffective. Next, focused on the well-known \"appliance usage\" secret, we have designed synthetic data to find the measure that best deals with this secret. We observe that such a measure, a variant of mutual information, actually exists.         ",
    "url": "https://arxiv.org/abs/2408.06460",
    "authors": [
      "Vadim Arzamasov",
      "Klemens B\u00f6hm"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ]
  },
  {
    "id": "arXiv:2408.06467",
    "title": "Generalization Enhancement Strategies to Enable Cross-year Cropland Mapping with Convolutional Neural Networks Trained Using Historical Samples",
    "abstract": "           The accuracy of mapping agricultural fields across large areas is steadily improving with high-resolution satellite imagery and deep learning (DL) models, even in regions where fields are small and geometrically irregular. However, developing effective DL models often requires large, expensive label datasets, typically available only for specific years or locations. This limits the ability to create annual maps essential for agricultural monitoring, as domain shifts occur between years and regions due to changes in farming practices and environmental conditions. The challenge is to design a model flexible enough to account for these shifts without needing yearly labels. While domain adaptation techniques or semi-supervised training are common solutions, we explored enhancing the model's generalization power. Our results indicate that a holistic approach is essential, combining methods to improve generalization. Specifically, using an area-based loss function, such as Tversky-focal loss (TFL), significantly improved predictions across multiple years. The use of different augmentation techniques helped to encode different types of invariance, particularly photometric augmentations encoded invariance to brightness changes, though they increased false positives. The combination of photometric augmentation, TFL loss, and MC-dropout produced the best results, although dropout alone led to more false negatives in subsequent year predictions. Additionally, the choice of input normalization had a significant impact, with the best results obtained when statistics were calculated either locally or across the entire dataset over all bands (lab and gab). We developed a workflow that enabled a U-Net model to generate effective multi-year crop maps over large areas. Our code, available at: this https URL, will be regularly updated with improvements.         ",
    "url": "https://arxiv.org/abs/2408.06467",
    "authors": [
      "Sam Khallaghi",
      "Rahebe Abedi",
      "Hanan Abou Ali",
      "Mary Dziedzorm Asipunu",
      "Ismail Alatise",
      "Nguyen Ha",
      "Boka Luo",
      "Cat Mai",
      "Lei Song",
      "Amos Wussah",
      "Sitian Xiong",
      "Qi Zhang",
      "Lyndon D. Estes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06481",
    "title": "UniT: Unified Tactile Representation for Robot Learning",
    "abstract": "           UniT is a novel approach to tactile representation learning, using VQVAE to learn a compact latent space and serve as the tactile representation. It uses tactile images obtained from a single simple object to train the representation with transferability and generalizability. This tactile representation can be zero-shot transferred to various downstream tasks, including perception tasks and manipulation policy learning. Our benchmarking on an in-hand 3D pose estimation task shows that UniT outperforms existing visual and tactile representation learning methods. Additionally, UniT's effectiveness in policy learning is demonstrated across three real-world tasks involving diverse manipulated objects and complex robot-object-environment interactions. Through extensive experimentation, UniT is shown to be a simple-to-train, plug-and-play, yet widely effective method for tactile representation learning. For more details, please refer to our open-source repository this https URL and the project website this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06481",
    "authors": [
      "Zhengtong Xu",
      "Raghava Uppuluri",
      "Xinwei Zhang",
      "Cael Fitch",
      "Philip Glen Crandall",
      "Wan Shou",
      "Dongyi Wang",
      "Yu She"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.06486",
    "title": "Implicit Neural Representation For Accurate CFD Flow Field Prediction",
    "abstract": "           Despite the plethora of deep learning frameworks for flow field prediction, most of them deal with flow fields on regular domains, and although the best ones can cope with irregular domains, they mostly rely on graph networks, so that real industrial applications remain currently elusive. We present a deep learning framework for 3D flow field prediction applied to blades of aircraft engine turbines and compressors. Crucially, we view any 3D field as a function from coordinates that is modeled by a neural network we call the backbone-net. It inherits the property of coordinate-based MLPs, namely the discretization-agnostic representation of flow fields in domains of arbitrary topology at infinite resolution. First, we demonstrate the performance of the backbone-net solo in regressing 3D steady simulations of single blade rows in various flow regimes: it can accurately render important flow characteristics such as boundary layers, wakes and shock waves. Second, we introduce a hyper-net that maps the surface mesh of a blade to the parameters of the backbone-net. By doing so, the flow solution can be directly predicted from the blade geometry, irrespective of its parameterization. Together, backbone-net and hyper-net form a highly-accurate memory-efficient data-driven proxy to CFD solvers with good generalization on unseen geometries.         ",
    "url": "https://arxiv.org/abs/2408.06486",
    "authors": [
      "Laurent de Vito",
      "Nils Pinnau",
      "Simone Dey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2408.06489",
    "title": "Path Partitions of Phylogenetic Networks",
    "abstract": "           In phylogenetics, evolution is traditionally represented in a tree-like manner. However, phylogenetic networks can be more appropriate for representing evolutionary events such as hybridization, horizontal gene transfer, and others. In particular, the class of forest-based networks was recently introduced to represent introgression, in which genes are swapped between between species. A network is forest-based if it can be obtained by adding arcs to a collection of trees, so that the endpoints of the new arcs are in different trees. This contrasts with so-called tree-based networks, which are formed by adding arcs within a single tree. We are interested in the computational complexity of recognizing forest-based networks, which was recently left as an open problem by Huber et al. Forest-based networks coincide with directed acyclic graphs that can be partitioned into induced paths, each ending at a leaf of the original graph. Several types of path partitions have been studied in the graph theory literature, but to our knowledge this type of leaf induced path partition has not been considered before. The study of forest-based networks in terms of these partitions allows us to establish closer relationships between phylogenetics and algorithmic graph theory, and to provide answers to problems in both fields. We show that deciding whether a network is forest-based is NP-complete, even on input networks that are tree-based, binary, and have only three leaves. This shows that partitioning a directed acyclic graph into three induced paths is NP-complete, answering a recent question of Ferneau et al. We then show that the problem is polynomial-time solvable on binary networks with two leaves and on the class of orchards. Finally, for undirected graphs, we introduce unrooted forest-based networks and provide hardness results for this class as well.         ",
    "url": "https://arxiv.org/abs/2408.06489",
    "authors": [
      "Manuel Lafond",
      "Vincent Moulton"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2408.06503",
    "title": "Decentralized Cooperation in Heterogeneous Multi-Agent Reinforcement Learning via Graph Neural Network-Based Intrinsic Motivation",
    "abstract": "           Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for various sequential decision-making and control tasks. Unlike their single-agent counterparts, multi-agent systems necessitate successful cooperation among the agents. The deployment of these systems in real-world scenarios often requires decentralized training, a diverse set of agents, and learning from infrequent environmental reward signals. These challenges become more pronounced under partial observability and the lack of prior knowledge about agent heterogeneity. While notable studies use intrinsic motivation (IM) to address reward sparsity or cooperation in decentralized settings, those dealing with heterogeneity typically assume centralized training, parameter sharing, and agent indexing. To overcome these limitations, we propose the CoHet algorithm, which utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to facilitate the learning of heterogeneous agent policies in decentralized settings, under the challenges of partial observability and reward sparsity. Evaluation of CoHet in the Multi-agent Particle Environment (MPE) and Vectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior performance compared to the state-of-the-art in a range of cooperative multi-agent scenarios. Our research is supplemented by an analysis of the impact of the agent dynamics model on the intrinsic motivation module, insights into the performance of different CoHet variants, and its robustness to an increasing number of heterogeneous agents.         ",
    "url": "https://arxiv.org/abs/2408.06503",
    "authors": [
      "Jahir Sadik Monon",
      "Deeparghya Dutta Barua",
      "Md. Mosaddek Khan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.06509",
    "title": "Fooling SHAP with Output Shuffling Attacks",
    "abstract": "           Explainable AI~(XAI) methods such as SHAP can help discover feature attributions in black-box models. If the method reveals a significant attribution from a ``protected feature'' (e.g., gender, race) on the model output, the model is considered unfair. However, adversarial attacks can subvert the detection of XAI methods. Previous approaches to constructing such an adversarial model require access to underlying data distribution, which may not be possible in many practical scenarios. We relax this constraint and propose a novel family of attacks, called shuffling attacks, that are data-agnostic. The proposed attack strategies can adapt any trained machine learning model to fool Shapley value-based explanations. We prove that Shapley values cannot detect shuffling attacks. However, algorithms that estimate Shapley values, such as linear SHAP and SHAP, can detect these attacks with varying degrees of effectiveness. We demonstrate the efficacy of the attack strategies by comparing the performance of linear SHAP and SHAP using real-world datasets.         ",
    "url": "https://arxiv.org/abs/2408.06509",
    "authors": [
      "Jun Yuan",
      "Aritra Dasgupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.06516",
    "title": "Quantifying Phase Unbalance and Coordination Impacts on Distribution Network Flexibility",
    "abstract": "           The increasing integration of distributed energy resources (DER) provides distribution system operators (DSO) with new flexible resources to support more efficient operation and planning of distribution networks. To utilise these resources, various DER flexibility aggregation methods have been proposed in the literature, such as aggregated P-Q flexibility areas at the interface with other networks. However, whereas focusing on estimating the limits of flexibility services, existing studies make the critical assumption that all available flexible units are perfectly coordinated to jointly provide flexibility and manage network constraints. Moreover, due to the extensive use of single-phase power flow analysis, the impacts of phase unbalance on DER flexibility aggregation remain largely unexplored. To address these gaps in knowledge, this work proposes a framework for modelling flexibility services in low voltage (LV) distribution networks which enables explicitly imposing voltage unbalance and phase coordination constraints. The simulations, performed for an illustrative 5-bus system and a real 221-bus LV network in the UK, demonstrate that a significant share (over 30%) of total aggregated DER flexibility potential may be unavailable due to voltage unbalances and lack of coordination between DER connected to different phases.         ",
    "url": "https://arxiv.org/abs/2408.06516",
    "authors": [
      "Andrey Churkin",
      "Wangwei Kong",
      "Pierluigi Mancarella",
      "Eduardo A. Mart\u00ednez Cese\u00f1a"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.06552",
    "title": "HaptoFloater: Visuo-Haptic Augmented Reality by Embedding Imperceptible Color Vibration Signals for Tactile Display Control in a Mid-Air Image",
    "abstract": "           We propose HaptoFloater, a low-latency mid-air visuo-haptic augmented reality (VHAR) system that utilizes imperceptible color vibrations. When adding tactile stimuli to the visual information of a mid-air image, the user should not perceive the latency between the tactile and visual information. However, conventional tactile presentation methods for mid-air images, based on camera-detected fingertip positioning, introduce latency due to image processing and communication. To mitigate this latency, we use a color vibration technique; humans cannot perceive the vibration when the display alternates between two different color stimuli at a frequency of 25 Hz or higher. In our system, we embed this imperceptible color vibration into the mid-air image formed by a micromirror array plate, and a photodiode on the fingertip device directly detects this color vibration to provide tactile stimulation. Thus, our system allows for the tactile perception of multiple patterns on a mid-air image in 59.5 ms. In addition, we evaluate the visual-haptic delay tolerance on a mid-air display using our VHAR system and a tactile actuator with a single pattern and faster response time. The results of our user study indicate a visual-haptic delay tolerance of 110.6 ms, which is considerably larger than the latency associated with systems using multiple tactile patterns.         ",
    "url": "https://arxiv.org/abs/2408.06552",
    "authors": [
      "Rina Nagano",
      "Takahiro Kinoshita",
      "Shingo Hattori",
      "Yuichi Hiroi",
      "Yuta Itoh",
      "Takefumi Hiraki"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2408.06569",
    "title": "Social Debiasing for Fair Multi-modal LLMs",
    "abstract": "           Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender. This paper addresses the issue of social biases in MLLMs by i) Introducing a comprehensive Counterfactual dataset with Multiple Social Concepts (CMSC), which provides a more diverse and extensive training set compared to existing datasets. ii) Proposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by revisiting the MLLM training process, rescaling the autoregressive loss function, and improving data sampling methods to counteract biases. Through extensive experiments on various MLLMs, our CMSC dataset and ASD method demonstrate a significant reduction in social biases while maintaining the models' original performance.         ",
    "url": "https://arxiv.org/abs/2408.06569",
    "authors": [
      "Harry Cheng",
      "Yangyang Guo",
      "Qingpei Guo",
      "Ming Yang",
      "Tian Gan",
      "Liqiang Nie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06578",
    "title": "OpenEP: Open-Ended Future Event Prediction",
    "abstract": "           Future event prediction (FEP) is a long-standing and crucial task in the world, as understanding the evolution of events enables early risk identification, informed decision-making, and strategic planning. Existing work typically treats event prediction as classification tasks and confines the outcomes of future events to a fixed scope, such as yes/no questions, candidate set, and taxonomy, which is difficult to include all possible outcomes of future events. In this paper, we introduce OpenEP (an Open-Ended Future Event Prediction task), which generates flexible and diverse predictions aligned with real-world scenarios. This is mainly reflected in two aspects: firstly, the predictive questions are diverse, covering different stages of event development and perspectives; secondly, the outcomes are flexible, without constraints on scope or format. To facilitate the study of this task, we construct OpenEPBench, an open-ended future event prediction dataset. For question construction, we pose questions from seven perspectives, including location, time, event development, event outcome, event impact, event response, and other, to facilitate an in-depth analysis and understanding of the comprehensive evolution of events. For outcome construction, we collect free-form text containing the outcomes as ground truth to provide semantically complete and detail-enriched outcomes. Furthermore, we propose StkFEP, a stakeholder-enhanced future event prediction framework, that incorporates event characteristics for open-ended settings. Our method extracts stakeholders involved in events to extend questions to gather diverse information. We also collect historically events that are relevant and similar to the question to reveal potential evolutionary patterns. Experiment results indicate that accurately predicting future events in open-ended settings is challenging for existing LLMs.         ",
    "url": "https://arxiv.org/abs/2408.06578",
    "authors": [
      "Yong Guan",
      "Hao Peng",
      "Xiaozhi Wang",
      "Lei Hou",
      "Juanzi Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06579",
    "title": "Understanding Power Consumption Metric on Heterogeneous Memory Systems",
    "abstract": "           Contemporary memory systems contain a variety of memory types, each possessing distinct characteristics. This trend empowers applications to opt for memory types aligning with developer's desired behavior. As a result, developers gain flexibility to tailor their applications to specific needs, factoring in attributes like latency, bandwidth, and power consumption. Our research centers on the aspect of power consumption within memory systems. We introduce an approach that equips developers with comprehensive insights into the power consumption of individual memory types. Additionally, we propose an ordered hierarchy of memory types. Through this methodology, developers can make informed decisions for efficient memory usage aligned with their unique requirements.         ",
    "url": "https://arxiv.org/abs/2408.06579",
    "authors": [
      "Andr\u00e8s Rubio Proa\u00f1o",
      "Kento Sato"
    ],
    "subjectives": [
      "Performance (cs.PF)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2408.06585",
    "title": "SSAAM: Sentiment Signal-based Asset Allocation Method with Causality Information",
    "abstract": "           This study demonstrates whether financial text is useful for tactical asset allocation using stocks by using natural language processing to create polarity indexes in financial news. In this study, we performed clustering of the created polarity indexes using the change-point detection algorithm. In addition, we constructed a stock portfolio and rebalanced it at each change point utilizing an optimization algorithm. Consequently, the asset allocation method proposed in this study outperforms the comparative approach. This result suggests that the polarity index helps construct the equity asset allocation method.         ",
    "url": "https://arxiv.org/abs/2408.06585",
    "authors": [
      "Rei Taguchi",
      "Hiroki Sakaji",
      "Kiyoshi Izumi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2408.06603",
    "title": "Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion",
    "abstract": "           Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06603",
    "authors": [
      "Rui Ying",
      "Mengting Hu",
      "Jianfeng Wu",
      "Yalan Xie",
      "Xiaoyi Liu",
      "Zhunheng Wang",
      "Ming Jiang",
      "Hang Gao",
      "Linlin Zhang",
      "Renhong Cheng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06604",
    "title": "MV-DETR: Multi-modality indoor object detection by Multi-View DEtecton TRansformers",
    "abstract": "           We introduce a novel MV-DETR pipeline which is effective while efficient transformer based detection method. Given input RGBD data, we notice that there are super strong pretraining weights for RGB data while less effective works for depth related data. First and foremost , we argue that geometry and texture cues are both of vital importance while could be encoded separately. Secondly, we find that visual texture feature is relatively hard to extract compared with geometry feature in 3d space. Unfortunately, single RGBD dataset with thousands of data is not enough for training an discriminating filter for visual texture feature extraction. Last but certainly not the least, we designed a lightweight VG module consists of a visual textual encoder, a geometry encoder and a VG connector. Compared with previous state of the art works like V-DETR, gains from pretrained visual encoder could be seen. Extensive experiments on ScanNetV2 dataset shows the effectiveness of our method. It is worth mentioned that our method achieve 78\\% AP which create new state of the art on ScanNetv2 benchmark.         ",
    "url": "https://arxiv.org/abs/2408.06604",
    "authors": [
      "Zichao Dong",
      "Yilin Zhang",
      "Xufeng Huang",
      "Hang Ji",
      "Zhan Shi",
      "Xin Zhan",
      "Junbo Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06608",
    "title": "Potamoi: Accelerating Neural Rendering via a Unified Streaming Architecture",
    "abstract": "           Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today's resource-constrained devices remains challenging. In this paper, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi, designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW, which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speed-up and energy reduction of 53.1$\\times$ and 67.7$\\times$, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.         ",
    "url": "https://arxiv.org/abs/2408.06608",
    "authors": [
      "Yu Feng",
      "Weikai Lin",
      "Zihan Liu",
      "Jingwen Leng",
      "Minyi Guo",
      "Han Zhao",
      "Xiaofeng Hou",
      "Jieru Zhao",
      "Yuhao Zhu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2408.06620",
    "title": "Unveiling the Flaws: A Critical Analysis of Initialization Effect on Time Series Anomaly Detection",
    "abstract": "           Deep learning for time-series anomaly detection (TSAD) has gained significant attention over the past decade. Despite the reported improvements in several papers, the practical application of these models remains limited. Recent studies have cast doubt on these models, attributing their results to flawed evaluation techniques. However, the impact of initialization has largely been overlooked. This paper provides a critical analysis of the initialization effects on TSAD model performance. Our extensive experiments reveal that TSAD models are highly sensitive to hyperparameters such as window size, seed number, and normalization. This sensitivity often leads to significant variability in performance, which can be exploited to artificially inflate the reported efficacy of these models. We demonstrate that even minor changes in initialization parameters can result in performance variations that overshadow the claimed improvements from novel model architectures. Our findings highlight the need for rigorous evaluation protocols and transparent reporting of preprocessing steps to ensure the reliability and fairness of anomaly detection methods. This paper calls for a more cautious interpretation of TSAD advancements and encourages the development of more robust and transparent evaluation practices to advance the field and its practical applications.         ",
    "url": "https://arxiv.org/abs/2408.06620",
    "authors": [
      "Alex Koran",
      "Hadi Hojjati",
      "Narges Armanfard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06621",
    "title": "Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models",
    "abstract": "           Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, training LLMs on human-written text entails significant risk of privacy and copyright violations, which demands an efficient machine unlearning framework to remove knowledge of sensitive data without retraining the model from scratch. While Gradient Ascent (GA) is widely used for unlearning by reducing the likelihood of generating unwanted information, the unboundedness of increasing the cross-entropy loss causes not only unstable optimization, but also catastrophic forgetting of knowledge that needs to be retained. We also discover its joint application under low-rank adaptation results in significantly suboptimal computational cost vs. generative performance trade-offs. In light of this limitation, we propose two novel techniques for robust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge loss that suppresses unwanted tokens by increasing the probability of the next most likely token, thereby retaining fluency and structure in language generation. We also propose to initialize low-rank adapter weights based on Fisher-weighted low-rank approximation, which induces faster unlearning and better knowledge retention by allowing model updates to be focused on parameters that are important in generating textual data we wish to remove.         ",
    "url": "https://arxiv.org/abs/2408.06621",
    "authors": [
      "Sungmin Cha",
      "Sungjun Cho",
      "Dasol Hwang",
      "Moontae Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06625",
    "title": "DePatch: Towards Robust Adversarial Patch for Evading Person Detectors in the Real World",
    "abstract": "           Recent years have seen an increasing interest in physical adversarial attacks, which aim to craft deployable patterns for deceiving deep neural networks, especially for person detectors. However, the adversarial patterns of existing patch-based attacks heavily suffer from the self-coupling issue, where a degradation, caused by physical transformations, in any small patch segment can result in a complete adversarial dysfunction, leading to poor robustness in the complex real world. Upon this observation, we introduce the Decoupled adversarial Patch (DePatch) attack to address the self-coupling issue of adversarial patches. Specifically, we divide the adversarial patch into block-wise segments, and reduce the inter-dependency among these segments through randomly erasing out some segments during the optimization. We further introduce a border shifting operation and a progressive decoupling strategy to improve the overall attack capabilities. Extensive experiments demonstrate the superior performance of our method over other physical adversarial attacks, especially in the real world.         ",
    "url": "https://arxiv.org/abs/2408.06625",
    "authors": [
      "Jikang Cheng",
      "Ying Zhang",
      "Zhongyuan Wang",
      "Zou Qin",
      "Chen Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06629",
    "title": "Fast Information Streaming Handler (FisH): A Unified Seismic Neural Network for Single Station Real-Time Earthquake Early Warning",
    "abstract": "           Existing EEW approaches often treat phase picking, location estimation, and magnitude estimation as separate tasks, lacking a unified framework. Additionally, most deep learning models in seismology rely on full three-component waveforms and are not suitable for real-time streaming data. To address these limitations, we propose a novel unified seismic neural network called Fast Information Streaming Handler (FisH). FisH is designed to process real-time streaming seismic data and generate simultaneous results for phase picking, location estimation, and magnitude estimation in an end-to-end fashion. By integrating these tasks within a single model, FisH simplifies the overall process and leverages the nonlinear relationships between tasks for improved performance. The FisH model utilizes RetNet as its backbone, enabling parallel processing during training and recurrent handling during inference. This capability makes FisH suitable for real-time applications, reducing latency in EEW systems. Extensive experiments conducted on the STEAD benchmark dataset provide strong validation for the effectiveness of our proposed FisH model. The results demonstrate that FisH achieves impressive performance across multiple seismic event detection and characterization tasks. Specifically, it achieves an F1 score of 0.99/0.96. Also, FisH demonstrates precise earthquake location estimation, with location error of only 6.0km, a distance error of 2.6km, and a back-azimuth error of 19\u00b0. The model also exhibits accurate earthquake magnitude estimation, with a magnitude error of just 0.14. Additionally, FisH is capable of generating real-time estimations, providing location and magnitude estimations with a location error of 8.06km and a magnitude error of 0.18 within a mere 3 seconds after the P-wave arrives.         ",
    "url": "https://arxiv.org/abs/2408.06629",
    "authors": [
      "Tianning Zhang",
      "Feng Liu",
      "Yuming Yuan",
      "Rui Su",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06633",
    "title": "A lightweight YOLOv5-FFM model for occlusion pedestrian detection",
    "abstract": "           The development of autonomous driving technology must be inseparable from pedestrian detection. Because of the fast speed of the vehicle, the accuracy and real-time performance of the pedestrian detection algorithm are very important. YOLO, as an efficient and simple one-stage target detection method, is often used for pedestrian detection in various environments. However, this series of detectors face some challenges, such as excessive computation and undesirable detection rate when facing occluded pedestrians. In this paper, we propose an improved lightweight YOLOv5 model to deal with these problems. This model can achieve better pedestrian detection accuracy with fewer floating-point operations (FLOPs), especially for occluded targets. In order to achieve the above goals, we made improvements based on the YOLOv5 model framework and introduced Ghost module and SE block. Furthermore, we designed a local feature fusion module (FFM) to deal with occlusion in pedestrian detection. To verify the validity of our method, two datasets, Citypersons and CUHK Occlusion, were selected for the experiment. The experimental results show that, compared with the original yolov5s model, the average precision (AP) of our method is significantly improved, while the number of parameters is reduced by 27.9% and FLOPs are reduced by 19.0%.         ",
    "url": "https://arxiv.org/abs/2408.06633",
    "authors": [
      "Xiangjie Luo",
      "Bo Shao",
      "Zhihao Cai",
      "Yingxun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06636",
    "title": "Unified-IoU: For High-Quality Object Detection",
    "abstract": "           Object detection is an important part in the field of computer vision, and the effect of object detection is directly determined by the regression accuracy of the prediction box. As the key to model training, IoU (Intersection over Union) greatly shows the difference between the current prediction box and the Ground Truth box. Subsequent researchers have continuously added more considerations to IoU, such as center distance, aspect ratio, and so on. However, there is an upper limit to just refining the geometric differences; And there is a potential connection between the new consideration index and the IoU itself, and the direct addition or subtraction between the two may lead to the problem of \"over-consideration\". Based on this, we propose a new IoU loss function, called Unified-IoU (UIoU), which is more concerned with the weight assignment between different quality prediction boxes. Specifically, the loss function dynamically shifts the model's attention from low-quality prediction boxes to high-quality prediction boxes in a novel way to enhance the model's detection performance on high-precision or intensive datasets and achieve a balance in training speed. Our proposed method achieves better performance on multiple datasets, especially at a high IoU threshold, UIoU has a more significant improvement effect compared with other improved IoU losses. Our code is publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06636",
    "authors": [
      "Xiangjie Luo",
      "Zhihao Cai",
      "Bo Shao",
      "Yingxun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06638",
    "title": "COD: Learning Conditional Invariant Representation for Domain Adaptation Regression",
    "abstract": "           Aiming to generalize the label knowledge from a source domain with continuous outputs to an unlabeled target domain, Domain Adaptation Regression (DAR) is developed for complex practical learning problems. However, due to the continuity problem in regression, existing conditional distribution alignment theory and methods with discrete prior, which are proven to be effective in classification settings, are no longer applicable. In this work, focusing on the feasibility problems in DAR, we establish the sufficiency theory for the regression model, which shows the generalization error can be sufficiently dominated by the cross-domain conditional discrepancy. Further, to characterize conditional discrepancy with continuous conditioning variable, a novel Conditional Operator Discrepancy (COD) is proposed, which admits the metric property on conditional distributions via the kernel embedding theory. Finally, to minimize the discrepancy, a COD-based conditional invariant representation learning model is proposed, and the reformulation is derived to show that reasonable modifications on moment statistics can further improve the discriminability of the adaptation model. Extensive experiments on standard DAR datasets verify the validity of theoretical results and the superiority over SOTA DAR methods.         ",
    "url": "https://arxiv.org/abs/2408.06638",
    "authors": [
      "Hao-Ran Yang",
      "Chuan-Xian Ren",
      "You-Wei Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06650",
    "title": "Physics-Informed Kolmogorov-Arnold Networks for Power System Dynamics",
    "abstract": "           This paper presents, for the first time, a framework for Kolmogorov-Arnold Networks (KANs) in power system applications. Inspired by the recently proposed KAN architecture, this paper proposes physics-informed Kolmogorov-Arnold Networks (PIKANs), a novel KAN-based physics-informed neural network (PINN) tailored to efficiently and accurately learn dynamics within power systems. The PIKANs present a promising alternative to conventional Multi-Layer Perceptrons (MLPs) based PINNs, achieving superior accuracy in predicting power system dynamics while employing a smaller network size. Simulation results on a single-machine infinite bus system and a 4-bus 2- generator system underscore the accuracy of the PIKANs in predicting rotor angle and frequency with fewer learnable parameters than conventional PINNs. Furthermore, the simulation results demonstrate PIKANs capability to accurately identify uncertain inertia and damping coefficients. This work opens up a range of opportunities for the application of KANs in power systems, enabling efficient determination of grid dynamics and precise parameter identification.         ",
    "url": "https://arxiv.org/abs/2408.06650",
    "authors": [
      "Hang Shuai",
      "Fangxing Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.06653",
    "title": "Hierarchical Structured Neural Network for Retrieval",
    "abstract": "           Embedding Based Retrieval (EBR) is a crucial component of the retrieval stage in (Ads) Recommendation System that utilizes Two Tower or Siamese Networks to learn embeddings for both users and items (ads). It then employs an Approximate Nearest Neighbor Search (ANN) to efficiently retrieve the most relevant ads for a specific user. Despite the recent rise to popularity in the industry, they have a couple of limitations. Firstly, Two Tower model architecture uses a single dot product interaction which despite their efficiency fail to capture the data distribution in practice. Secondly, the centroid representation and cluster assignment, which are components of ANN, occur after the training process has been completed. As a result, they do not take into account the optimization criteria used for retrieval model. In this paper, we present Hierarchical Structured Neural Network (HSNN), a deployed jointly optimized hierarchical clustering and neural network model that can take advantage of sophisticated interactions and model architectures that are more common in the ranking stages while maintaining a sub-linear inference cost. We achieve 6.5% improvement in offline evaluation and also demonstrate 1.22% online gains through A/B experiments. HSNN has been successfully deployed into the Ads Recommendation system and is currently handling major portion of the traffic. The paper shares our experience in developing this system, dealing with challenges like freshness, volatility, cold start recommendations, cluster collapse and lessons deploying the model in a large scale retrieval production system.         ",
    "url": "https://arxiv.org/abs/2408.06653",
    "authors": [
      "Kaushik Rangadurai",
      "Siyang Yuan",
      "Minhui Huang",
      "Yiqun Liu",
      "Golnaz Ghasemiesfeh",
      "Yunchen Pu",
      "Xinfeng Xie",
      "Xingfeng He",
      "Fangzhou Xu",
      "Andrew Cui",
      "Vidhoon Viswanathan",
      "Yan Dong",
      "Liang Xiong",
      "Lin Yang",
      "Liang Wang",
      "Jiyan Yang",
      "Chonglin Sun"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06658",
    "title": "ComGPT: Detecting Local Community Structure with Large Language Models",
    "abstract": "           Large language models (LLMs), like GPT, have demonstrated the ability to understand graph structures and have achieved excellent performance in various graph reasoning tasks such as node classification. So far, how to leverage LLMs to better detect local communities remains underexplored. Local community detection algorithms based on seed expansion often face a seed-dependent problem, community diffusion, and free rider effect. Using LLMs to solve existing local community work problems faces the following challenges: existing graph encoding methods fail to provide LLMs with sufficient community-related graph information; LLMs lack domain knowledge in mining communities. To address these issues, we improve graph encoding by supplementing community knowledge to enhance the ability of graph encoding to express graph information. Additionally, we design the NSG (Node Selection Guide) prompt to enhance LLMs' understanding of community characteristics, aiming to alleviate the seed-dependent problem, community diffusion, and free rider effect. Based on the graph encoding and NSG prompt, we present a GPT-guided local community detection, called ComGPT. ComGPT iteratively selects potential nodes from the detected community's neighbors and subsequently employs GPT to choose the node that optimally integrates into the detected community from these selected potential nodes. Experimental results demonstrate that ComGPT outperforms the comparison algorithms, thereby confirming the effectiveness of the designed graph encoding and prompt.         ",
    "url": "https://arxiv.org/abs/2408.06658",
    "authors": [
      "Li Ni",
      "Haowen Shen",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.06659",
    "title": "A hybrid neural network for real-time OD demand calibration under disruptions",
    "abstract": "           Existing automated urban traffic management systems, designed to mitigate traffic congestion and reduce emissions in real time, face significant challenges in effectively adapting to rapidly evolving conditions. Predominantly reactive, these systems typically respond to incidents only after they have transpired. A promising solution lies in implementing real-time traffic simulation models capable of accurately modelling environmental changes. Central to these real-time traffic simulations are origin-destination (OD) demand matrices. However, the inherent variability, stochasticity, and unpredictability of traffic demand complicate the precise calibration of these matrices in the face of disruptions. This paper introduces a hybrid neural network (NN) architecture specifically designed for real-time OD demand calibration to enhance traffic simulations' accuracy and reliability under both recurrent and non-recurrent traffic conditions. The proposed hybrid NN predicts the OD demand to reconcile the discrepancies between actual and simulated traffic patterns. To facilitate real-time updating of the internal parameters of the NN, we develop a metamodel-based backpropagation method by integrating data from real-world traffic systems and simulated environments. This ensures precise predictions of the OD demand even in the case of abnormal or unpredictable traffic patterns. Furthermore, we incorporate offline pre-training of the NN using the metamodel to improve computational efficiency. Validation through a toy network and a Tokyo expressway corridor case study illustrates the model's ability to dynamically adjust to shifting traffic patterns across various disruption scenarios. Our findings underscore the potential of advanced machine learning techniques in developing proactive traffic management strategies, offering substantial improvements over traditional reactive systems.         ",
    "url": "https://arxiv.org/abs/2408.06659",
    "authors": [
      "Takao Dantsuji",
      "Dong Ngoduy",
      "Ziyuan Pu",
      "Seunghyeon Lee",
      "Hai L. Vu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.06665",
    "title": "RW-NSGCN: A Robust Approach to Structural Attacks via Negative Sampling",
    "abstract": "           Node classification using Graph Neural Networks (GNNs) has been widely applied in various practical scenarios, such as predicting user interests and detecting communities in social networks. However, recent studies have shown that graph-structured networks often contain potential noise and attacks, in the form of topological perturbations and weight disturbances, which can lead to decreased classification performance in GNNs. To improve the robustness of the model, we propose a novel method: Random Walk Negative Sampling Graph Convolutional Network (RW-NSGCN). Specifically, RW-NSGCN integrates the Random Walk with Restart (RWR) and PageRank (PGR) algorithms for negative sampling and employs a Determinantal Point Process (DPP)-based GCN for convolution operations. RWR leverages both global and local information to manage noise and local variations, while PGR assesses node importance to stabilize the topological structure. The DPP-based GCN ensures diversity among negative samples and aggregates their features to produce robust node embeddings, thereby improving classification performance. Experimental results demonstrate that the RW-NSGCN model effectively addresses network topology attacks and weight instability, increasing the accuracy of anomaly detection and overall stability. In terms of classification accuracy, RW-NSGCN significantly outperforms existing methods, showing greater resilience across various scenarios and effectively mitigating the impact of such vulnerabilities.         ",
    "url": "https://arxiv.org/abs/2408.06665",
    "authors": [
      "Shuqi He",
      "Jun Zhuang",
      "Ding Wang",
      "Jun Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06699",
    "title": "Information Geometry and Beta Link for Optimizing Sparse Variational Student-t Processes",
    "abstract": "           Recently, a sparse version of Student-t Processes, termed sparse variational Student-t Processes, has been proposed to enhance computational efficiency and flexibility for real-world datasets using stochastic gradient descent. However, traditional gradient descent methods like Adam may not fully exploit the parameter space geometry, potentially leading to slower convergence and suboptimal performance. To mitigate these issues, we adopt natural gradient methods from information geometry for variational parameter optimization of Student-t Processes. This approach leverages the curvature and structure of the parameter space, utilizing tools such as the Fisher information matrix which is linked to the Beta function in our model. This method provides robust mathematical support for the natural gradient algorithm when using Student's t-distribution as the variational distribution. Additionally, we present a mini-batch algorithm for efficiently computing natural gradients. Experimental results across four benchmark datasets demonstrate that our method consistently accelerates convergence speed.         ",
    "url": "https://arxiv.org/abs/2408.06699",
    "authors": [
      "Jian Xu",
      "Delu Zeng",
      "John Paisley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06701",
    "title": "DiffSG: A Generative Solver for Network Optimization with Diffusion Model",
    "abstract": "           Diffusion generative models, famous for their performance in image generation, are popular in various cross-domain applications. However, their use in the communication community has been mostly limited to auxiliary tasks like data modeling and feature extraction. These models hold greater promise for fundamental problems in network optimization compared to traditional machine learning methods. Discriminative deep learning often falls short due to its single-step input-output mapping and lack of global awareness of the solution space, especially given the complexity of network optimization's objective functions. In contrast, diffusion generative models can consider a broader range of solutions and exhibit stronger generalization by learning parameters that describe the distribution of the underlying solution space, with higher probabilities assigned to better solutions. We propose a new framework Diffusion Model-based Solution Generation (DiffSG), which leverages the intrinsic distribution learning capabilities of diffusion generative models to learn high-quality solution distributions based on given inputs. The optimal solution within this distribution is highly probable, allowing it to be effectively reached through repeated sampling. We validate the performance of DiffSG on several typical network optimization problems, including mixed-integer non-linear programming, convex optimization, and hierarchical non-convex optimization. Our results show that DiffSG outperforms existing baselines. In summary, we demonstrate the potential of diffusion generative models in tackling complex network optimization problems and outline a promising path for their broader application in the communication community.         ",
    "url": "https://arxiv.org/abs/2408.06701",
    "authors": [
      "Ruihuai Liang",
      "Bo Yang",
      "Zhiwen Yu",
      "Bin Guo",
      "Xuelin Cao",
      "M\u00e9rouane Debbah",
      "H. Vincent Poor",
      "Chau Yuen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06702",
    "title": "Optimizing RPL Routing Using Tabu Search to Improve Link Stability and Energy Consumption in IoT Networks",
    "abstract": "           In the Internet of Things (IoT) networks, the Routing Protocol forLow-power and Lossy Networks (RPL) is a widely adopted standard due toits efficiency in managing resource-constrained and energy-limited nodes.However, persistent challenges such as high energy consumption, unstablelinks, and suboptimal routing continue to hinder network performance,affecting both the longevity of the network and the reliability of datatransmission. This paper proposes an enhanced RPL routing mechanismby integrating the Tabu Search optimization algorithm to address theseissues. The proposed approach focuses on optimizing the parent and childselection process in the RPL protocol, leveraging a composite cost func-tion that incorporates key parameters including Residual Energy, Trans-mission Energy, Distance to Sink, Hop Count, Expected TransmissionCount (ETX), and Link Stability Rate. Through extensive simulations,we demonstrate that our method significantly improves link stability, re-duces energy consumption, and enhances the packet delivery ratio, leadingto a more efficient and longer-lasting IoT network. The findings suggestthat Tabu Search can effectively balance the trade-offs inherent in IoTrouting, providing a practical solution for improving the overall perfor-mance of RPL-based networks.         ",
    "url": "https://arxiv.org/abs/2408.06702",
    "authors": [
      "Mehran Tarif"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2408.06707",
    "title": "MAIR++: Improving Multi-view Attention Inverse Rendering with Implicit Lighting Representation",
    "abstract": "           In this paper, we propose a scene-level inverse rendering framework that uses multi-view images to decompose the scene into geometry, SVBRDF, and 3D spatially-varying lighting. While multi-view images have been widely used for object-level inverse rendering, scene-level inverse rendering has primarily been studied using single-view images due to the lack of a dataset containing high dynamic range multi-view images with ground-truth geometry, material, and spatially-varying lighting. To improve the quality of scene-level inverse rendering, a novel framework called Multi-view Attention Inverse Rendering (MAIR) was recently introduced. MAIR performs scene-level multi-view inverse rendering by expanding the OpenRooms dataset, designing efficient pipelines to handle multi-view images, and splitting spatially-varying lighting. Although MAIR showed impressive results, its lighting representation is fixed to spherical Gaussians, which limits its ability to render images realistically. Consequently, MAIR cannot be directly used in applications such as material editing. Moreover, its multi-view aggregation networks have difficulties extracting rich features because they only focus on the mean and variance between multi-view features. In this paper, we propose its extended version, called MAIR++. MAIR++ addresses the aforementioned limitations by introducing an implicit lighting representation that accurately captures the lighting conditions of an image while facilitating realistic rendering. Furthermore, we design a directional attention-based multi-view aggregation network to infer more intricate relationships between views. Experimental results show that MAIR++ not only achieves better performance than MAIR and single-view-based methods, but also displays robust performance on unseen real-world scenes.         ",
    "url": "https://arxiv.org/abs/2408.06707",
    "authors": [
      "JunYong Choi",
      "SeokYeong Lee",
      "Haesol Park",
      "Seung-Won Jung",
      "Ig-Jae Kim",
      "Junghyun Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06717",
    "title": "Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models",
    "abstract": "           Graph Neural Networks (GNNs), like other neural networks, have shown remarkable success but are hampered by the complexity of their architecture designs, which heavily depend on specific data and tasks. Traditionally, designing proper architectures involves trial and error, which requires intensive manual effort to optimize various components. To reduce human workload, researchers try to develop automated algorithms to design GNNs. However, both experts and automated algorithms suffer from two major issues in designing GNNs: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graphs, GNNs, and performance. To further enhance the automation of GNN architecture design, we propose a computation-friendly way to empower Large Language Models (LLMs) with specialized knowledge in designing GNNs, thereby drastically shortening the computational overhead and development cycle of designing GNN architectures. Our framework begins by establishing a knowledge retrieval pipeline that comprehends the intercorrelations between graphs, GNNs, and performance. This pipeline converts past model design experiences into structured knowledge for LLM reference, allowing it to quickly suggest initial model proposals. Subsequently, we introduce a knowledge-driven search strategy that emulates the exploration-exploitation process of human experts, enabling quick refinement of initial proposals within a promising scope. Extensive experiments demonstrate that our framework can efficiently deliver promising (e.g., Top-5.77%) initial model proposals for unseen datasets within seconds and without any prior training and achieve outstanding search performance in a few iterations.         ",
    "url": "https://arxiv.org/abs/2408.06717",
    "authors": [
      "Jialiang Wang",
      "Shimin Di",
      "Hanmo Liu",
      "Zhili Wang",
      "Jiachuan Wang",
      "Lei Chen",
      "Xiaofang Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06730",
    "title": "Designing Consensus-Based Distributed Filtering over Directed Graphs",
    "abstract": "           This paper proposes a novel consensus-on-only-measurement distributed filter over directed graphs under the collectively observability condition. First, the distributed filter structure is designed with an augmented leader-following measurement fusion strategy. Subsequently, two parameter design methods are presented, and the consensus gain parameter is devised utilizing local information exclusively rather than global information. Additionally, the lower bound of the fusion step is derived to guarantee a uniformly upper bound of the estimation error covariance. Moreover, the lower bounds of the convergence rates of the steady-state performance gap between the proposed algorithm and the centralized filter are provided with the fusion step approaching infinity. The analysis demonstrates that the convergence rate is, at a minimum, as rapid as exponential convergence under the spectral norm condition of the communication graph. The transient performance is also analyzed with the fusion step tending to infinity. The inherent trade-off between the communication cost and the filtering performance is revealed from the analysis of the steady-state performance and the transient performance. Finally, the theoretical results are substantiated through the validation of two simulation examples.         ",
    "url": "https://arxiv.org/abs/2408.06730",
    "authors": [
      "Xiaoxu Lyu",
      "Guanghui Wen",
      "Yuezu Lv",
      "Zhisheng Duan",
      "Ling Shi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.06732",
    "title": "Exploring the anatomy of articulation rate in spontaneous English speech: relationships between utterance length effects and social factors",
    "abstract": "           Speech rate has been shown to vary across social categories such as gender, age, and dialect, while also being conditioned by properties of speech planning. The effect of utterance length, where speech rate is faster and less variable for longer utterances, has also been shown to reduce the role of social factors once it has been accounted for, leaving unclear the relationship between social factors and speech production in conditioning speech rate. Through modelling of speech rate across 13 English speech corpora, it is found that utterance length has the largest effect on speech rate, though this effect itself varies little across corpora and speakers. While age and gender also modulate speech rate, their effects are much smaller in magnitude. These findings suggest utterance length effects may be conditioned by articulatory and perceptual constraints, and that social influences on speech rate should be interpreted in the broader context of how speech rate variation is structured.         ",
    "url": "https://arxiv.org/abs/2408.06732",
    "authors": [
      "James Tanner",
      "Morgan Sonderegger",
      "Jane Stuart-Smith",
      "Tyler Kendall",
      "Jeff Mielke",
      "Robin Dodsworth",
      "Erik Thomas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2408.06734",
    "title": "Grasping by Hanging: a Learning-Free Grasping Detection Method for Previously Unseen Objects",
    "abstract": "           This paper proposes a novel learning-free three-stage method that predicts grasping poses, enabling robots to pick up and transfer previously unseen objects. Our method first identifies potential structures that can afford the action of hanging by analyzing the hanging mechanics and geometric properties. Then 6D poses are detected for a parallel gripper retrofitted with an extending bar, which when closed forms loops to hook each hangable structure. Finally, an evaluation policy qualities and rank grasp candidates for execution attempts. Compared to the traditional physical model-based and deep learning-based methods, our approach is closer to the human natural action of grasping unknown objects. And it also eliminates the need for a vast amount of training data. To evaluate the effectiveness of the proposed method, we conducted experiments with a real robot. Experimental results indicate that the grasping accuracy and stability are significantly higher than the state-of-the-art learning-based method, especially for thin and flat objects.         ",
    "url": "https://arxiv.org/abs/2408.06734",
    "authors": [
      "Wanze Li",
      "Wan Su",
      "Gregory S. Chirikjian"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.06737",
    "title": "Multilingual Models for Check-Worthy Social Media Posts Detection",
    "abstract": "           This work presents an extensive study of transformer-based NLP models for detection of social media posts that contain verifiable factual claims and harmful claims. The study covers various activities, including dataset collection, dataset pre-processing, architecture selection, setup of settings, model training (fine-tuning), model testing, and implementation. The study includes a comprehensive analysis of different models, with a special focus on multilingual models where the same model is capable of processing social media posts in both English and in low-resource languages such as Arabic, Bulgarian, Dutch, Polish, Czech, Slovak. The results obtained from the study were validated against state-of-the-art models, and the comparison demonstrated the robustness of the proposed models. The novelty of this work lies in the development of multi-label multilingual classification models that can simultaneously detect harmful posts and posts that contain verifiable factual claims in an efficient way.         ",
    "url": "https://arxiv.org/abs/2408.06737",
    "authors": [
      "Sebastian Kula",
      "Michal Gregor"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06741",
    "title": "Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective",
    "abstract": "           With recent generative models facilitating photo-realistic image synthesis, the proliferation of synthetic images has also engendered certain negative impacts on social platforms, thereby raising an urgent imperative to develop effective detectors. Current synthetic image detection (SID) pipelines are primarily dedicated to crafting universal artifact features, accompanied by an oversight about SID training paradigm. In this paper, we re-examine the SID problem and identify two prevalent biases in current training paradigms, i.e., weakened artifact features and overfitted artifact features. Meanwhile, we discover that the imaging mechanism of synthetic images contributes to heightened local correlations among pixels, suggesting that detectors should be equipped with local awareness. In this light, we propose SAFE, a lightweight and effective detector with three simple image transformations. Firstly, for weakened artifact features, we substitute the down-sampling operator with the crop operator in image pre-processing to help circumvent artifact distortion. Secondly, for overfitted artifact features, we include ColorJitter and RandomRotation as additional data augmentations, to help alleviate irrelevant biases from color discrepancies and semantic differences in limited training samples. Thirdly, for local awareness, we propose a patch-based random masking strategy tailored for SID, forcing the detector to focus on local regions at training. Comparative experiments are conducted on an open-world dataset, comprising synthetic images generated by 26 distinct generative models. Our pipeline achieves a new state-of-the-art performance, with remarkable improvements of 4.5% in accuracy and 2.9% in average precision against existing methods.         ",
    "url": "https://arxiv.org/abs/2408.06741",
    "authors": [
      "Ouxiang Li",
      "Jiayin Cai",
      "Yanbin Hao",
      "Xiaolong Jiang",
      "Yao Hu",
      "Fuli Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06762",
    "title": "Graph Neural Network Approach to Predict the Effects of Road Capacity Reduction Policies: A Case Study for Paris, France",
    "abstract": "           Rapid urbanization and growing urban populations worldwide present significant challenges for cities, including increased traffic congestion and air pollution. Effective strategies are needed to manage traffic volumes and reduce emissions. In practice, traditional traffic flow simulations are used to test those strategies. However, high computational intensity usually limits their applicability in investigating a magnitude of different scenarios to evaluate best policies. This paper introduces an innovative approach to assess the effects of traffic policies using Graph Neural Networks (GNN). By incorporating complex transport network structures directly into the neural network, this approach could enable rapid testing of various policies without the delays associated with traditional simulations. We provide a proof of concept that GNNs can learn and predict changes in car volume resulting from capacity reduction policies. We train a GNN model based on a training set generated with a MATSim simulation for Paris, France. We analyze the model's performance across different road types and scenarios, finding that the GNN is generally able to learn the effects on edge-based traffic volume induced by policies. The model is especially successful in predicting changes on major streets. Nevertheless, the evaluation also showed that the current model has problems in predicting impacts of spatially small policies and changes in traffic volume in regions where no policy is applied due to spillovers and/or relocation of traffic.         ",
    "url": "https://arxiv.org/abs/2408.06762",
    "authors": [
      "Elena Natterer",
      "Roman Engelhardt",
      "Sebastian H\u00f6rl",
      "Klaus Bogenberger"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2408.06766",
    "title": "Robust Black-box Testing of Deep Neural Networks using Co-Domain Coverage",
    "abstract": "           Rigorous testing of machine learning models is necessary for trustworthy deployments. We present a novel black-box approach for generating test-suites for robust testing of deep neural networks (DNNs). Most existing methods create test inputs based on maximizing some \"coverage\" criterion/metric such as a fraction of neurons activated by the test inputs. Such approaches, however, can only analyze each neuron's behavior or each layer's output in isolation and are unable to capture their collective effect on the DNN's output, resulting in test suites that often do not capture the various failure modes of the DNN adequately. These approaches also require white-box access, i.e., access to the DNN's internals (node activations). We present a novel black-box coverage criterion called Co-Domain Coverage (CDC), which is defined as a function of the model's output and thus takes into account its end-to-end behavior. Subsequently, we develop a new fuzz testing procedure named CoDoFuzz, which uses CDC to guide the fuzzing process to generate a test suite for a DNN. We extensively compare the test suite generated by CoDoFuzz with those generated using several state-of-the-art coverage-based fuzz testing methods for the DNNs trained on six publicly available datasets. Experimental results establish the efficiency and efficacy of CoDoFuzz in generating the largest number of misclassified inputs and the inputs for which the model lacks confidence in its decision.         ",
    "url": "https://arxiv.org/abs/2408.06766",
    "authors": [
      "Aishwarya Gupta",
      "Indranil Saha",
      "Piyush Rai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06772",
    "title": "Exploring Domain Shift on Radar-Based 3D Object Detection Amidst Diverse Environmental Conditions",
    "abstract": "           The rapid evolution of deep learning and its integration with autonomous driving systems have led to substantial advancements in 3D perception using multimodal sensors. Notably, radar sensors show greater robustness compared to cameras and lidar under adverse weather and varying illumination conditions. This study delves into the often-overlooked yet crucial issue of domain shift in 4D radar-based object detection, examining how varying environmental conditions, such as different weather patterns and road types, impact 3D object detection performance. Our findings highlight distinct domain shifts across various weather scenarios, revealing unique dataset sensitivities that underscore the critical role of radar point cloud generation. Additionally, we demonstrate that transitioning between different road types, especially from highways to urban settings, introduces notable domain shifts, emphasizing the necessity for diverse data collection across varied road environments. To the best of our knowledge, this is the first comprehensive analysis of domain shift effects on 4D radar-based object detection. We believe this empirical study contributes to understanding the complex nature of domain shifts in radar data and suggests paths forward for data collection strategy in the face of environmental variability.         ",
    "url": "https://arxiv.org/abs/2408.06772",
    "authors": [
      "Miao Zhang",
      "Sherif Abdulatif",
      "Benedikt Loesch",
      "Marco Altmann",
      "Marius Schwarz",
      "Bin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.06776",
    "title": "Robust Deep Reinforcement Learning for Inverter-based Volt-Var Control in Partially Observable Distribution Networks",
    "abstract": "           Inverter-based volt-var control is studied in this paper. One key issue in DRL-based approaches is the limited measurement deployment in active distribution networks, which leads to problems of a partially observable state and unknown reward. To address those problems, this paper proposes a robust DRL approach with a conservative critic and a surrogate reward. The conservative critic utilizes the quantile regression technology to estimate conservative state-action value function based on the partially observable state, which helps to train a robust policy; the surrogate rewards of power loss and voltage violation are designed that can be calculated from the limited measurements. The proposed approach optimizes the power loss of the whole network and the voltage profile of buses with measurable voltages while indirectly improving the voltage profile of other buses. Extensive simulations verify the effectiveness of the robust DRL approach in different limited measurement conditions, even when only the active power injection of the root bus and less than 10% of bus voltages are measurable.         ",
    "url": "https://arxiv.org/abs/2408.06776",
    "authors": [
      "Qiong Liu",
      "Ye Guo",
      "Tong Xu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06778",
    "title": "Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors",
    "abstract": "           Link prediction models can benefit from incorporating textual descriptions of entities and relations, enabling fully inductive learning and flexibility in dynamic graphs. We address the challenge of also capturing rich structured information about the local neighbourhood of entities and their relations, by introducing a Transformer-based approach that effectively integrates textual descriptions with graph structure, reducing the reliance on resource-intensive text encoders. Our experiments on three challenging datasets show that our Fast-and-Frugal Text-Graph (FnF-TG) Transformers achieve superior performance compared to the previous state-of-the-art methods, while maintaining efficiency and scalability.         ",
    "url": "https://arxiv.org/abs/2408.06778",
    "authors": [
      "Andrei C. Coman",
      "Christos Theodoropoulos",
      "Marie-Francine Moens",
      "James Henderson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06779",
    "title": "ED$^4$: Explicit Data-level Debiasing for Deepfake Detection",
    "abstract": "           Learning intrinsic bias from limited data has been considered the main reason for the failure of deepfake detection with generalizability. Apart from the discovered content and specific-forgery bias, we reveal a novel spatial bias, where detectors inertly anticipate observing structural forgery clues appearing at the image center, also can lead to the poor generalization of existing methods. We present ED$^4$, a simple and effective strategy, to address aforementioned biases explicitly at the data level in a unified framework rather than implicit disentanglement via network design. In particular, we develop ClockMix to produce facial structure preserved mixtures with arbitrary samples, which allows the detector to learn from an exponentially extended data distribution with much more diverse identities, backgrounds, local manipulation traces, and the co-occurrence of multiple forgery artifacts. We further propose the Adversarial Spatial Consistency Module (AdvSCM) to prevent extracting features with spatial bias, which adversarially generates spatial-inconsistent images and constrains their extracted feature to be consistent. As a model-agnostic debiasing strategy, ED$^4$ is plug-and-play: it can be integrated with various deepfake detectors to obtain significant benefits. We conduct extensive experiments to demonstrate its effectiveness and superiority over existing deepfake detection approaches.         ",
    "url": "https://arxiv.org/abs/2408.06779",
    "authors": [
      "Jikang Cheng",
      "Ying Zhang",
      "Qin Zou",
      "Zhiyuan Yan",
      "Chao Liang",
      "Zhongyuan Wang",
      "Chen Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06781",
    "title": "Do Vision-Language Foundational models show Robust Visual Perception?",
    "abstract": "           Recent advances in vision-language foundational models have enabled development of systems that can perform visual understanding and reasoning tasks. However, it is unclear if these models are robust to distribution shifts, and how their performance and generalization capabilities vary under changes in data distribution. In this project we strive to answer the question \"Are vision-language foundational models robust to distribution shifts like human perception?\" Specifically, we consider a diverse range of vision-language models and compare how the performance of these systems is affected by corruption based distribution shifts (such as \\textit{motion blur, fog, snow, gaussian noise}) commonly found in practical real-world scenarios. We analyse the generalization capabilities qualitatively and quantitatively on zero-shot image classification task under aforementioned distribution shifts. Our code will be avaible at \\url{this https URL}         ",
    "url": "https://arxiv.org/abs/2408.06781",
    "authors": [
      "Shivam Chandhok",
      "Pranav Tandon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06787",
    "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion",
    "abstract": "           Classical knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, which is ideal for mitigating the limitations of previous methods. Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results. In this work, we aim to leverage LLMs for KGC effectively and efficiently. We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC. We also generate entity descriptions with subgraph sampling on KGs, reducing the ambiguity of triplets and enriching the knowledge representation. Extensive experiments on standard benchmarks showcase the efficiency and effectiveness of our approach. We outperform classical KGC methods on most datasets and match the performance of fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU memory efficiency by \\textbf{$188\\times$} and speed up training+inference by \\textbf{$13.48\\times$}.         ",
    "url": "https://arxiv.org/abs/2408.06787",
    "authors": [
      "Bo Xue",
      "Yi Xu",
      "Yunchong Song",
      "Yiming Pang",
      "Yuyang Ren",
      "Jiaxin Ding",
      "Luoyi Fu",
      "Xinbing Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06788",
    "title": "Visual Neural Decoding via Improved Visual-EEG Semantic Consistency",
    "abstract": "           Visual neural decoding refers to the process of extracting and interpreting original visual experiences from human brain activity. Recent advances in metric learning-based EEG visual decoding methods have delivered promising results and demonstrated the feasibility of decoding novel visual categories from brain activity. However, methods that directly map EEG features to the CLIP embedding space may introduce mapping bias and cause semantic inconsistency among features, thereby degrading alignment and impairing decoding performance. To further explore the semantic consistency between visual and neural signals. In this work, we construct a joint semantic space and propose a Visual-EEG Semantic Decouple Framework that explicitly extracts the semantic-related features of these two modalities to facilitate optimal alignment. Specifically, a cross-modal information decoupling module is introduced to guide the extraction of semantic-related information from modalities. Then, by quantifying the mutual information between visual image and EEG features, we observe a strong positive correlation between the decoding performance and the magnitude of mutual information. Furthermore, inspired by the mechanisms of visual object understanding from neuroscience, we propose an intra-class geometric consistency approach during the alignment process. This strategy maps visual samples within the same class to consistent neural patterns, which further enhances the robustness and the performance of EEG visual decoding. Experiments on a large Image-EEG dataset show that our method achieves state-of-the-art results in zero-shot neural decoding tasks.         ",
    "url": "https://arxiv.org/abs/2408.06788",
    "authors": [
      "Hongzhou Chen",
      "Lianghua He",
      "Yihang Liu",
      "Longzhen Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2408.06803",
    "title": "Integrating Saliency Ranking and Reinforcement Learning for Enhanced Object Detection",
    "abstract": "           With the ever-growing variety of object detection approaches, this study explores a series of experiments that combine reinforcement learning (RL)-based visual attention methods with saliency ranking techniques to investigate transparent and sustainable solutions. By integrating saliency ranking for initial bounding box prediction and subsequently applying RL techniques to refine these predictions through a finite set of actions over multiple time steps, this study aims to enhance RL object detection accuracy. Presented as a series of experiments, this research investigates the use of various image feature extraction methods and explores diverse Deep Q-Network (DQN) architectural variations for deep reinforcement learning-based localisation agent training. Additionally, we focus on optimising the detection pipeline at every step by prioritising lightweight and faster models, while also incorporating the capability to classify detected objects, a feature absent in previous RL approaches. We show that by evaluating the performance of these trained agents using the Pascal VOC 2007 dataset, faster and more optimised models were developed. Notably, the best mean Average Precision (mAP) achieved in this study was 51.4, surpassing benchmarks set by RL-based single object detectors in the literature.         ",
    "url": "https://arxiv.org/abs/2408.06803",
    "authors": [
      "Matthias Bartolo",
      "Dylan Seychell",
      "Josef Bajada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06819",
    "title": "Enhancing Multiview Synergy: Robust Learning by Exploiting the Wave Loss Function with Consensus and Complementarity Principles",
    "abstract": "           Multiview learning (MvL) is an advancing domain in machine learning, leveraging multiple data perspectives to enhance model performance through view-consistency and view-discrepancy. Despite numerous successful multiview-based SVM models, existing frameworks predominantly focus on the consensus principle, often overlooking the complementarity principle. Furthermore, they exhibit limited robustness against noisy, error-prone, and view-inconsistent samples, prevalent in multiview datasets. To tackle the aforementioned limitations, this paper introduces Wave-MvSVM, a novel multiview support vector machine framework leveraging the wave loss (W-loss) function, specifically designed to harness both consensus and complementarity principles. Unlike traditional approaches that often overlook the complementary information among different views, the proposed Wave-MvSVM ensures a more comprehensive and resilient learning process by integrating both principles effectively. The W-loss function, characterized by its smoothness, asymmetry, and bounded nature, is particularly effective in mitigating the adverse effects of noisy and outlier data, thereby enhancing model stability. Theoretically, the W-loss function also exhibits a crucial classification-calibrated property, further boosting its effectiveness. Wave-MvSVM employs a between-view co-regularization term to enforce view consistency and utilizes an adaptive combination weight strategy to maximize the discriminative power of each view. The optimization problem is efficiently solved using a combination of GD and the ADMM, ensuring reliable convergence to optimal solutions. Theoretical analyses, grounded in Rademacher complexity, validate the generalization capabilities of the Wave-MvSVM model. Extensive empirical evaluations across diverse datasets demonstrate the superior performance of Wave-MvSVM in comparison to existing benchmark models.         ",
    "url": "https://arxiv.org/abs/2408.06819",
    "authors": [
      "A. Quadir",
      "Mushir Akhtar",
      "M. Tanveer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06825",
    "title": "Membership Inference Attack Against Masked Image Modeling",
    "abstract": "           Masked Image Modeling (MIM) has achieved significant success in the realm of self-supervised learning (SSL) for visual recognition. The image encoder pre-trained through MIM, involving the masking and subsequent reconstruction of input images, attains state-of-the-art performance in various downstream vision tasks. However, most existing works focus on improving the performance of this http URL this work, we take a different angle by studying the pre-training data privacy of MIM. Specifically, we propose the first membership inference attack against image encoders pre-trained by MIM, which aims to determine whether an image is part of the MIM pre-training dataset. The key design is to simulate the pre-training paradigm of MIM, i.e., image masking and subsequent reconstruction, and then obtain reconstruction errors. These reconstruction errors can serve as membership signals for achieving attack goals, as the encoder is more capable of reconstructing the input image in its training set with lower errors. Extensive evaluations are conducted on three model architectures and three benchmark datasets. Empirical results show that our attack outperforms baseline methods. Additionally, we undertake intricate ablation studies to analyze multiple factors that could influence the performance of the attack.         ",
    "url": "https://arxiv.org/abs/2408.06825",
    "authors": [
      "Zheng Li",
      "Xinlei He",
      "Ning Yu",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06834",
    "title": "GLGait: A Global-Local Temporal Receptive Field Network for Gait Recognition in the Wild",
    "abstract": "           Gait recognition has attracted increasing attention from academia and industry as a human recognition technology from a distance in non-intrusive ways without requiring cooperation. Although advanced methods have achieved impressive success in lab scenarios, most of them perform poorly in the wild. Recently, some Convolution Neural Networks (ConvNets) based methods have been proposed to address the issue of gait recognition in the wild. However, the temporal receptive field obtained by convolution operations is limited for long gait sequences. If directly replacing convolution blocks with visual transformer blocks, the model may not enhance a local temporal receptive field, which is important for covering a complete gait cycle. To address this issue, we design a Global-Local Temporal Receptive Field Network (GLGait). GLGait employs a Global-Local Temporal Module (GLTM) to establish a global-local temporal receptive field, which mainly consists of a Pseudo Global Temporal Self-Attention (PGTA) and a temporal convolution operation. Specifically, PGTA is used to obtain a pseudo global temporal receptive field with less memory and computation complexity compared with a multi-head self-attention (MHSA). The temporal convolution operation is used to enhance the local temporal receptive field. Besides, it can also aggregate pseudo global temporal receptive field to a true holistic temporal receptive field. Furthermore, we also propose a Center-Augmented Triplet Loss (CTL) in GLGait to reduce the intra-class distance and expand the positive samples in the training stage. Extensive experiments show that our method obtains state-of-the-art results on in-the-wild datasets, $i.e.$, Gait3D and GREW. The code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06834",
    "authors": [
      "Guozhen Peng",
      "Yunhong Wang",
      "Yuwei Zhao",
      "Shaoxiong Zhang",
      "Annan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06849",
    "title": "Causal Agent based on Large Language Model",
    "abstract": "           Large language models (LLMs) have achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. This lack of causal reasoning capability limits the development of LLMs. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tools module, the causal agent applies causal methods to align tabular data with natural language. In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%. For further insights and implementation details, our code is accessible via the GitHub repository this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06849",
    "authors": [
      "Kairong Han",
      "Kun Kuang",
      "Ziyu Zhao",
      "Junjian Ye",
      "Fei Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2408.06880",
    "title": "Architecture Specific Generation of Large Scale Lattice Boltzmann Methods for Sparse Complex Geometries",
    "abstract": "           We implement and analyse a sparse / indirect-addressing data structure for the Lattice Boltzmann Method to support efficient compute kernels for fluid dynamics problems with a high number of non-fluid nodes in the domain, such as in porous media flows. The data structure is integrated into a code generation pipeline to enable sparse Lattice Boltzmann Methods with a variety of stencils and collision operators and to generate efficient code for kernels for CPU as well as for AMD and NVIDIA accelerator cards. We optimize these sparse kernels with an in-place streaming pattern to save memory accesses and memory consumption and we implement a communication hiding technique to prove scalability. We present single GPU performance results with up to 99% of maximal bandwidth utilization. We integrate the optimized generated kernels in the high performance framework WALBERLA and achieve a scaling efficiency of at least 82% on up to 1024 NVIDIA A100 GPUs and up to 4096 AMD MI250X GPUs on modern HPC systems. Further, we set up three different applications to test the sparse data structure for realistic demonstrator problems. We show performance results for flow through porous media, free flow over a particle bed, and blood flow in a coronary artery. We achieve a maximal performance speed-up of 2 and a significantly reduced memory consumption by up to 75% with the sparse / indirect-addressing data structure compared to the direct-addressing data structure for these applications.         ",
    "url": "https://arxiv.org/abs/2408.06880",
    "authors": [
      "Philipp Suffa",
      "Markus Holzer",
      "Harald K\u00f6stler",
      "Ulrich R\u00fcde"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2408.06900",
    "title": "Entendre, a Social Bot Detection Tool for Niche, Fringe, and Extreme Social Media",
    "abstract": "           Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation. This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable. Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation. To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework. Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection. We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features). By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy. To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler. We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior. Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms.         ",
    "url": "https://arxiv.org/abs/2408.06900",
    "authors": [
      "Pranav Venkatesh",
      "Kami Vinton",
      "Dhiraj Murthy",
      "Kellen Sharp",
      "Akaash Kolluri"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2408.06920",
    "title": "Multi-Agent Continuous Control with Generative Flow Networks",
    "abstract": "           Generative Flow Networks (GFlowNets) aim to generate diverse trajectories from a distribution in which the final states of the trajectories are proportional to the reward, serving as a powerful alternative to reinforcement learning for exploratory control tasks. However, the individual-flow matching constraint in GFlowNets limits their applications for multi-agent systems, especially continuous joint-control problems. In this paper, we propose a novel Multi-Agent generative Continuous Flow Networks (MACFN) method to enable multiple agents to perform cooperative exploration for various compositional continuous objects. Technically, MACFN trains decentralized individual-flow-based policies in a centralized global-flow-based matching fashion. During centralized training, MACFN introduces a continuous flow decomposition network to deduce the flow contributions of each agent in the presence of only global rewards. Then agents can deliver actions solely based on their assigned local flow in a decentralized way, forming a joint policy distribution proportional to the rewards. To guarantee the expressiveness of continuous flow decomposition, we theoretically derive a consistency condition on the decomposition network. Experimental results demonstrate that the proposed method yields results superior to the state-of-the-art counterparts and better exploration capability. Our code is available at this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06920",
    "authors": [
      "Shuang Luo",
      "Yinchuan Li",
      "Shunyu Liu",
      "Xu Zhang",
      "Yunfeng Shao",
      "Chao Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2408.06922",
    "title": "Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge",
    "abstract": "           ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest global audio security challenges. It aims to advance the development of countermeasure (CM) to discriminate bonafide and spoofed speech utterances. In this paper, we focus on addressing the problem of open-domain audio deepfake detection, which corresponds directly to the ASVspoof5 Track1 open condition. At first, we comprehensively investigate various CM on ASVspoof5, including data expansion, data augmentation, and self-supervised learning (SSL) features. Due to the high-frequency gaps characteristic of the ASVspoof5 dataset, we introduce Frequency Mask, a data augmentation method that masks specific frequency bands to improve CM robustness. Combining various scale of temporal information with multiple SSL features, our experiments achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.         ",
    "url": "https://arxiv.org/abs/2408.06922",
    "authors": [
      "Yuankun Xie",
      "Xiaopeng Wang",
      "Zhiyong Wang",
      "Ruibo Fu",
      "Zhengqi Wen",
      "Haonan Cheng",
      "Long Ye"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2408.06943",
    "title": "Towards Holistic Disease Risk Prediction using Small Language Models",
    "abstract": "           Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes. Medical practitioners integrate these diverse data types daily to make informed and accurate decisions. With recent advancements in language models capable of handling multimodal data, it is a logical progression to apply these models to the healthcare sector. In this work, we introduce a framework that connects small language models to multiple data sources, aiming to predict the risk of various diseases simultaneously. Our experiments encompass 12 different tasks within a multitask learning setup. Although our approach does not surpass state-of-the-art methods specialized for single tasks, it demonstrates competitive performance and underscores the potential of small language models for multimodal reasoning in healthcare.         ",
    "url": "https://arxiv.org/abs/2408.06943",
    "authors": [
      "Liv Bj\u00f6rkdahl",
      "Oskar Pauli",
      "Johan \u00d6stman",
      "Chiara Ceccobello",
      "Sara Lundell",
      "Magnus Kjellberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06954",
    "title": "Neural Speech and Audio Coding",
    "abstract": "           This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.         ",
    "url": "https://arxiv.org/abs/2408.06954",
    "authors": [
      "Minje Kim",
      "Jan Skoglund"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2408.06966",
    "title": "DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs",
    "abstract": "           Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification). Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning. Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties. Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization. Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency.         ",
    "url": "https://arxiv.org/abs/2408.06966",
    "authors": [
      "Dongyuan Li",
      "Shiyin Tan",
      "Ying Zhang",
      "Ming Jin",
      "Shirui Pan",
      "Manabu Okumura",
      "Renhe Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06983",
    "title": "Optimization-Based Model Checking and Trace Synthesis for Complex STL Specifications",
    "abstract": "           We present a bounded model checking algorithm for signal temporal logic (STL) that exploits mixed-integer linear programming (MILP). A key technical element is our novel MILP encoding of the STL semantics; it follows the idea of stable partitioning from the recent work on SMT-based STL model checking. Assuming that our (continuous-time) system models can be encoded to MILP -- typical examples are rectangular hybrid automata (precisely) and hybrid dynamics with closed-form solutions (approximately) -- our MILP encoding yields an optimization-based model checking algorithm that is scalable, is anytime/interruptible, and accommodates parameter mining. Experimental evaluation shows our algorithm's performance advantages especially for complex STL formulas, demonstrating its practical relevance e.g. in the automotive domain.         ",
    "url": "https://arxiv.org/abs/2408.06983",
    "authors": [
      "Sota Sato",
      "Jie An",
      "Zhenya Zhang",
      "Ichiro Hasuo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2408.06999",
    "title": "Robust Model Predictive Control for Aircraft Intent-Aware Collision Avoidance",
    "abstract": "           This paper presents the use of robust model predictive control for the design of an intent-aware collision avoidance system for multi-agent aircraft engaged in horizontal maneuvering scenarios. We assume that information from other agents is accessible in the form of waypoints or destinations. Consequently, we consider that other agents follow their optimal Dubin's path--a trajectory that connects their current state to their intended state--while accounting for potential uncertainties. We propose using scenario tree model predictive control as a robust approach that offers computational efficiency. We demonstrate that the proposed method can easily integrate intent information and offer a robust scheme that handles different uncertainties. The method is illustrated through simulation results.         ",
    "url": "https://arxiv.org/abs/2408.06999",
    "authors": [
      "Arash Bahari Kordabad",
      "Andrea Da Col",
      "Arabinda Ghosh",
      "Sybert Stroeve",
      "Sadegh Soudjani"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2408.07004",
    "title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models",
    "abstract": "           Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience. Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services. However, the privacy consequences associated with these services and their third-party plugins are not well understood. Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins. In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services. At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier. We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.         ",
    "url": "https://arxiv.org/abs/2408.07004",
    "authors": [
      "Chun Jie Chong",
      "Chenxi Hou",
      "Zhihao Yao",
      "Seyed Mohammadjavad Seyed Talebi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.07018",
    "title": "Efficient Human-Object-Interaction (EHOI) Detection via Interaction Label Coding and Conditional Decision",
    "abstract": "           Human-Object Interaction (HOI) detection is a fundamental task in image understanding. While deep-learning-based HOI methods provide high performance in terms of mean Average Precision (mAP), they are computationally expensive and opaque in training and inference processes. An Efficient HOI (EHOI) detector is proposed in this work to strike a good balance between detection performance, inference complexity, and mathematical transparency. EHOI is a two-stage method. In the first stage, it leverages a frozen object detector to localize the objects and extract various features as intermediate outputs. In the second stage, the first-stage outputs predict the interaction type using the XGBoost classifier. Our contributions include the application of error correction codes (ECCs) to encode rare interaction cases, which reduces the model size and the complexity of the XGBoost classifier in the second stage. Additionally, we provide a mathematical formulation of the relabeling and decision-making process. Apart from the architecture, we present qualitative results to explain the functionalities of the feedforward modules. Experimental results demonstrate the advantages of ECC-coded interaction labels and the excellent balance of detection performance and complexity of the proposed EHOI method.         ",
    "url": "https://arxiv.org/abs/2408.07018",
    "authors": [
      "Tsung-Shan Yang",
      "Yun-Cheng Wang",
      "Chengwei Wei",
      "Suya You",
      "C.-C. Jay Kuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.07021",
    "title": "Improved Counting under Continual Observation with Pure Differential Privacy",
    "abstract": "           Counting under continual observation is a well-studied problem in the area of differential privacy. Given a stream of updates $x_1,x_2,\\dots,x_T \\in \\{0,1\\}$ the problem is to continuously release estimates of the prefix sums $\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the stream with differential privacy. Recently, significant leaps have been made in our understanding of this problem under $\\textit{approximate}$ differential privacy, aka. $(\\varepsilon,\\delta)$$\\textit{-differential privacy}$. However, for the classical case of $\\varepsilon$-differential privacy, we are not aware of any improvement in mean squared error since the work of Honaker (TPDP 2015). In this paper we present such an improvement, reducing the mean squared error by a factor of about 4, asymptotically. The key technique is a new generalization of the binary tree mechanism that uses a $k$-ary number system with $\\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our mechanism improves the mean squared error over all 'optimal' $(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on Gaussian noise whenever $\\delta$ is sufficiently small. Specifically, using $k=19$ we get an asymptotic improvement over the bound given in the work by Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$.         ",
    "url": "https://arxiv.org/abs/2408.07021",
    "authors": [
      "Joel Daniel Andersson",
      "Rasmus Pagh",
      "Sahel Torkamani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2408.07054",
    "title": "Exploiting Leakage in Password Managers via Injection Attacks",
    "abstract": "           This work explores injection attacks against password managers. In this setting, the adversary (only) controls their own application client, which they use to \"inject\" chosen payloads to a victim's client via, for example, sharing credentials with them. The injections are interleaved with adversarial observations of some form of protected state (such as encrypted vault exports or the network traffic received by the application servers), from which the adversary backs out confidential information. We uncover a series of general design patterns in popular password managers that lead to vulnerabilities allowing an adversary to efficiently recover passwords, URLs, usernames, and attachments. We develop general attack templates to exploit these design patterns and experimentally showcase their practical efficacy via analysis of ten distinct password manager applications. We disclosed our findings to these vendors, many of which deployed mitigations.         ",
    "url": "https://arxiv.org/abs/2408.07054",
    "authors": [
      "Andr\u00e9s F\u00e1brega",
      "Armin Namavari",
      "Rachit Agarwal",
      "Ben Nassi",
      "Thomas Ristenpart"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.06377",
    "title": "Masked Graph Autoencoders with Contrastive Augmentation for Spatially Resolved Transcriptomics Data",
    "abstract": "           With the rapid advancement of Spatial Resolved Transcriptomics (SRT) technology, it is now possible to comprehensively measure gene transcription while preserving the spatial context of tissues. Spatial domain identification and gene denoising are key objectives in SRT data analysis. We propose a Contrastively Augmented Masked Graph Autoencoder (STMGAC) to learn low-dimensional latent representations for domain identification. In the latent space, persistent signals for representations are obtained through self-distillation to guide self-supervised matching. At the same time, positive and negative anchor pairs are constructed using triplet learning to augment the discriminative ability. We evaluated the performance of STMGAC on five datasets, achieving results superior to those of existing baseline methods. All code and public datasets used in this paper are available at this https URL and this https URL.         ",
    "url": "https://arxiv.org/abs/2408.06377",
    "authors": [
      "Donghai Fang",
      "Fangfang Zhu",
      "Dongting Xie",
      "Wenwen Min"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06391",
    "title": "Autoregressive Enzyme Function Prediction with Multi-scale Multi-modality Fusion",
    "abstract": "           Accurate prediction of enzyme function is crucial for elucidating biological mechanisms and driving innovation across various sectors. Existing deep learning methods tend to rely solely on either sequence data or structural data and predict the EC number as a whole, neglecting the intrinsic hierarchical structure of EC numbers. To address these limitations, we introduce MAPred, a novel multi-modality and multi-scale model designed to autoregressively predict the EC number of proteins. MAPred integrates both the primary amino acid sequence and the 3D tokens of proteins, employing a dual-pathway approach to capture comprehensive protein characteristics and essential local functional sites. Additionally, MAPred utilizes an autoregressive prediction network to sequentially predict the digits of the EC number, leveraging the hierarchical organization of EC classifications. Evaluations on benchmark datasets, including New-392, Price, and New-815, demonstrate that our method outperforms existing models, marking a significant advance in the reliability and granularity of protein function prediction within bioinformatics.         ",
    "url": "https://arxiv.org/abs/2408.06391",
    "authors": [
      "Dingyi Rong",
      "Wenzhuo Zheng",
      "Bozitao Zhong",
      "Zhouhan Lin",
      "Liang Hong",
      "Ning Liu"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06392",
    "title": "Invariants of almost embeddings of graphs in the plane: results and problems",
    "abstract": "           A graph drawing in the plane is called an almost embedding if images of any two non-adjacent simplices (i.e. vertices or edges) are disjoint. We introduce integer invariants of almost embeddings: winding number, cyclic and triodic Wu numbers. We construct almost embeddings realizing some values of these invariants. We prove some relations between the invariants. We study values realizable as invariants of some almost embedding, but not of any embedding. This paper is expository and is accessible to mathematicians not specialized in the area (and to students). However elementary, this paper is motivated by frontline of research.         ",
    "url": "https://arxiv.org/abs/2408.06392",
    "authors": [
      "E. Alkin",
      "E. Bordacheva",
      "A. Miroshnikov",
      "O. Nikitenko",
      "A. Skopenkov"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)",
      "History and Overview (math.HO)"
    ]
  },
  {
    "id": "arXiv:2408.06403",
    "title": "From Diagnostic CT to DTI Tractography labels: Using Deep Learning for Corticospinal Tract Injury Assessment and Outcome Prediction in Intracerebral Haemorrhage",
    "abstract": "           The preservation of the corticospinal tract (CST) is key to good motor recovery after stroke. The gold standard method of assessing the CST with imaging is diffusion tensor tractography. However, this is not available for most intracerebral haemorrhage (ICH) patients. Non-contrast CT scans are routinely available in most ICH diagnostic pipelines, but delineating white matter from a CT scan is challenging. We utilise nnU-Net, trained on paired diagnostic CT scans and high-directional diffusion tractography maps, to segment the CST from diagnostic CT scans alone, and we show our model reproduces diffusion based tractography maps of the CST with a Dice similarity coefficient of 57%. Surgical haematoma evacuation is sometimes performed after ICH, but published clinical trials to date show that whilst surgery reduces mortality, there is no evidence of improved functional recovery. Restricting surgery to patients with an intact CST may reveal a subset of patients for whom haematoma evacuation improves functional outcome. We investigated the clinical utility of our model in the MISTIE III clinical trial dataset. We found that our model's CST integrity measure significantly predicted outcome after ICH in the acute and chronic time frames, therefore providing a prognostic marker for patients to whom advanced diffusion tensor imaging is unavailable. This will allow for future probing of subgroups who may benefit from surgery.         ",
    "url": "https://arxiv.org/abs/2408.06403",
    "authors": [
      "Olivia N Murray",
      "Hamied Haroon",
      "Paul Ryu",
      "Hiren Patel",
      "George Harston",
      "Marieke Wermer",
      "Wilmar Jolink",
      "Daniel Hanley",
      "Catharina Klijn",
      "Ulrike Hammerbeck",
      "Adrian Parry-Jones",
      "Timothy Cootes"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ]
  },
  {
    "id": "arXiv:2408.06421",
    "title": "Neural Networks as Spin Models: From Glass to Hidden Order Through Training",
    "abstract": "           We explore a one-to-one correspondence between a neural network (NN) and a statistical mechanical spin model where neurons are mapped to Ising spins and weights to spin-spin couplings. The process of training an NN produces a family of spin Hamiltonians parameterized by training time. We study the magnetic phases and the melting transition temperature as training progresses. First, we prove analytically that the common initial state before training--an NN with independent random weights--maps to a layered version of the classical Sherrington-Kirkpatrick spin glass exhibiting a replica symmetry breaking. The spin-glass-to-paramagnet transition temperature is calculated. Further, we use the Thouless-Anderson-Palmer (TAP) equations--a theoretical technique to analyze the landscape of energy minima of random systems--to determine the evolution of the magnetic phases on two types of NNs (one with continuous and one with binarized activations) trained on the MNIST dataset. The two NN types give rise to similar results, showing a quick destruction of the spin glass and the appearance of a phase with a hidden order, whose melting transition temperature $T_c$ grows as a power law in training time. We also discuss the properties of the spectrum of the spin system's bond matrix in the context of rich vs. lazy learning. We suggest that this statistical mechanical view of NNs provides a useful unifying perspective on the training process, which can be viewed as selecting and strengthening a symmetry-broken state associated with the training task.         ",
    "url": "https://arxiv.org/abs/2408.06421",
    "authors": [
      "Richard Barney",
      "Michael Winer",
      "Victor Galitksi"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ]
  },
  {
    "id": "arXiv:2408.06459",
    "title": "InfLocNet: Enhanced Lung Infection Localization and Disease Detection from Chest X-Ray Images Using Lightweight Deep Learning",
    "abstract": "           In recent years, the integration of deep learning techniques into medical imaging has revolutionized the diagnosis and treatment of lung diseases, particularly in the context of COVID-19 and pneumonia. This paper presents a novel, lightweight deep learning based segmentation-classification network designed to enhance the detection and localization of lung infections using chest X-ray images. By leveraging the power of transfer learning with pre-trained VGG-16 weights, our model achieves robust performance even with limited training data. The architecture incorporates refined skip connections within the UNet++ framework, reducing semantic gaps and improving precision in segmentation tasks. Additionally, a classification module is integrated at the end of the encoder block, enabling simultaneous classification and segmentation. This dual functionality enhances the model's versatility, providing comprehensive diagnostic insights while optimizing computational efficiency. Experimental results demonstrate that our proposed lightweight network outperforms existing methods in terms of accuracy and computational requirements, making it a viable solution for real-time and resource constrained medical imaging applications. Furthermore, the streamlined design facilitates easier hyperparameter tuning and deployment on edge devices. This work underscores the potential of advanced deep learning architectures in improving clinical outcomes through precise and efficient medical image analysis. Our model achieved remarkable results with an Intersection over Union (IoU) of 93.59% and a Dice Similarity Coefficient (DSC) of 97.61% in lung area segmentation, and an IoU of 97.67% and a DSC of 87.61% for infection region localization. Additionally, it demonstrated high accuracy of 93.86% and sensitivity of 89.55% in detecting chest diseases, highlighting its efficacy and reliability.         ",
    "url": "https://arxiv.org/abs/2408.06459",
    "authors": [
      "Md. Asiful Islam Miah",
      "Shourin Paul",
      "Sunanda Das",
      "M. M. A. Hashem"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06524",
    "title": "From Graphs to Qubits: A Critical Review of Quantum Graph Neural Networks",
    "abstract": "           Quantum Graph Neural Networks (QGNNs) represent a novel fusion of quantum computing and Graph Neural Networks (GNNs), aimed at overcoming the computational and scalability challenges inherent in classical GNNs that are powerful tools for analyzing data with complex relational structures but suffer from limitations such as high computational complexity and over-smoothing in large-scale applications. Quantum computing, leveraging principles like superposition and entanglement, offers a pathway to enhanced computational capabilities. This paper critically reviews the state-of-the-art in QGNNs, exploring various architectures. We discuss their applications across diverse fields such as high-energy physics, molecular chemistry, finance and earth sciences, highlighting the potential for quantum advantage. Additionally, we address the significant challenges faced by QGNNs, including noise, decoherence, and scalability issues, proposing potential strategies to mitigate these problems. This comprehensive review aims to provide a foundational understanding of QGNNs, fostering further research and development in this promising interdisciplinary field.         ",
    "url": "https://arxiv.org/abs/2408.06524",
    "authors": [
      "Andrea Ceschini",
      "Francesco Mauro",
      "Francesca De Falco",
      "Alessandro Sebastianelli",
      "Alessio Verdone",
      "Antonello Rosato",
      "Bertrand Le Saux",
      "Massimo Panella",
      "Paolo Gamba",
      "Silvia L. Ullo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06600",
    "title": "Deep Inertia $L_p$ Half-Quadratic Splitting Unrolling Network for Sparse View CT Reconstruction",
    "abstract": "           Sparse view computed tomography (CT) reconstruction poses a challenging ill-posed inverse problem, necessitating effective regularization techniques. In this letter, we employ $L_p$-norm ($0<p<1$) regularization to induce sparsity and introduce inertial steps, leading to the development of the inertial $L_p$-norm half-quadratic splitting algorithm. We rigorously prove the convergence of this algorithm. Furthermore, we leverage deep learning to initialize the conjugate gradient method, resulting in a deep unrolling network with theoretical guarantees. Our extensive numerical experiments demonstrate that our proposed algorithm surpasses existing methods, particularly excelling in fewer scanned views and complex noise conditions.         ",
    "url": "https://arxiv.org/abs/2408.06600",
    "authors": [
      "Yu Guo",
      "Caiying Wu",
      "Yaxin Li",
      "Qiyu Jin",
      "Tieyong Zeng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06640",
    "title": "Attention Based Feature Fusion Network for Monkeypox Skin Lesion Detection",
    "abstract": "           The recent monkeypox outbreak has raised significant public health concerns due to its rapid spread across multiple countries. Monkeypox can be difficult to distinguish from chickenpox and measles in the early stages because the symptoms of all three diseases are similar. Modern deep learning algorithms can be used to identify diseases, including COVID-19, by analyzing images of the affected areas. In this study, we introduce a lightweight model that merges two pre-trained architectures, EfficientNetV2B3 and ResNet151V2, to classify human monkeypox disease. We have also incorporated the squeeze-and-excitation attention network module to focus on the important parts of the feature maps for classifying the monkeypox images. This attention module provides channels and spatial attention to highlight significant areas within feature maps. We evaluated the effectiveness of our model by extensively testing it on a publicly available Monkeypox Skin Lesions Dataset using a four-fold cross-validation approach. The evaluation metrics of our model were compared with the existing others. Our model achieves a mean validation accuracy of 96.52%, with precision, recall, and F1-score values of 96.58%, 96.52%, and 96.51%, respectively.         ",
    "url": "https://arxiv.org/abs/2408.06640",
    "authors": [
      "Niloy Kumar Kundu",
      "Mainul Karim",
      "Sarah Kobir",
      "Dewan Md. Farid"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06644",
    "title": "Specialized Change Detection using Segment Anything",
    "abstract": "           Change detection (CD) is a fundamental task in Earth observation. While most change detection methods detect all changes, there is a growing need for specialized methods targeting specific changes relevant to particular applications while discarding the other changes. For instance, urban management might prioritize detecting the disappearance of buildings due to natural disasters or other reasons. Furthermore, while most supervised change detection methods require large-scale training datasets, in many applications only one or two training examples might be available instead of large datasets. Addressing such needs, we propose a focused CD approach using the Segment Anything Model (SAM), a versatile vision foundation model. Our method leverages a binary mask of the object of interest in pre-change images to detect their disappearance in post-change images. By using SAM's robust segmentation capabilities, we create prompts from the pre-change mask, use those prompts to segment the post-change image, and identify missing objects. This unsupervised approach demonstrated for building disappearance detection, is adaptable to various domains requiring specialized CD. Our contributions include defining a novel CD problem, proposing a method using SAM, and demonstrating its effectiveness. The proposed method also has benefits related to privacy preservation.         ",
    "url": "https://arxiv.org/abs/2408.06644",
    "authors": [
      "Tahir Ahmad",
      "Sudipan Saha"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.06681",
    "title": "Coherence Awareness in Diffractive Neural Networks",
    "abstract": "           Diffractive neural networks hold great promise for applications requiring intensive computational processing. Considerable attention has focused on diffractive networks for either spatially coherent or spatially incoherent illumination. Here we illustrate that, as opposed to imaging systems, in diffractive networks the degree of spatial coherence has a dramatic effect. In particular, we show that when the spatial coherence length on the object is comparable to the minimal feature size preserved by the optical system, neither the incoherent nor the coherent extremes serve as acceptable approximations. Importantly, this situation is inherent to many settings involving active illumination, including reflected light microscopy, autonomous vehicles and smartphones. Following this observation, we propose a general framework for training diffractive networks for any specified degree of spatial and temporal coherence, supporting all types of linear and nonlinear layers. Using our method, we numerically optimize networks for image classification, and thoroughly investigate their performance dependence on the illumination coherence properties. We further introduce the concept of coherence-blind networks, which have enhanced resilience to changes in illumination conditions. Our findings serve as a steppingstone toward adopting all-optical neural networks in real-world applications, leveraging nothing but natural light.         ",
    "url": "https://arxiv.org/abs/2408.06681",
    "authors": [
      "Matan Kleiner",
      "Lior Michaeli",
      "Tomer Michaeli"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06782",
    "title": "Robustness of optimal quantum annealing protocols",
    "abstract": "           Noise in quantum computing devices poses a key challenge in their realization. In this paper, we study the robustness of optimal quantum annealing protocols against coherent control errors, which are multiplicative Hamlitonian errors causing detrimental effects on current quantum devices. We show that the norm of the Hamiltonian quantifies the robustness against these errors, motivating the introduction of an additional regularization term in the cost function. We analyze the optimality conditions of the resulting robust quantum optimal control problem based on Pontryagin's maximum principle, showing that robust protocols admit larger smooth annealing sections. This suggests that quantum annealing admits improved robustness in comparison to bang-bang solutions such as the quantum approximate optimization algorithm. Finally, we perform numerical simulations to verify our analytical results and demonstrate the improved robustness of the proposed approach.         ",
    "url": "https://arxiv.org/abs/2408.06782",
    "authors": [
      "Niklas Funcke",
      "Julian Berberich"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2408.06784",
    "title": "Enhancing Diabetic Retinopathy Diagnosis: A Lightweight CNN Architecture for Efficient Exudate Detection in Retinal Fundus Images",
    "abstract": "           Retinal fundus imaging plays an essential role in diagnosing various stages of diabetic retinopathy, where exudates are critical markers of early disease onset. Prompt detection of these exudates is pivotal for enabling optometrists to arrest or significantly decelerate the disease progression. This paper introduces a novel, lightweight convolutional neural network architecture tailored for automated exudate detection, designed to identify these markers efficiently and accurately. To address the challenge of limited training data, we have incorporated domain-specific data augmentations to enhance the model's generalizability. Furthermore, we applied a suite of regularization techniques within our custom architecture to boost diagnostic accuracy while optimizing computational efficiency. Remarkably, this streamlined model contains only 4.73 million parameters a reduction of nearly 60% compared to the standard ResNet-18 model, which has 11.69 million parameters. Despite its reduced complexity, our model achieves an impressive F1 score of 90%, demonstrating its efficacy in the early detection of diabetic retinopathy through fundus imaging.         ",
    "url": "https://arxiv.org/abs/2408.06784",
    "authors": [
      "Mujadded Al Rabbani Alif"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.06851",
    "title": "BSS-CFFMA: Cross-Domain Feature Fusion and Multi-Attention Speech Enhancement Network based on Self-Supervised Embedding",
    "abstract": "           Speech self-supervised learning (SSL) represents has achieved state-of-the-art (SOTA) performance in multiple downstream tasks. However, its application in speech enhancement (SE) tasks remains immature, offering opportunities for improvement. In this study, we introduce a novel cross-domain feature fusion and multi-attention speech enhancement network, termed BSS-CFFMA, which leverages self-supervised embeddings. BSS-CFFMA comprises a multi-scale cross-domain feature fusion (MSCFF) block and a residual hybrid multi-attention (RHMA) block. The MSCFF block effectively integrates cross-domain features, facilitating the extraction of rich acoustic information. The RHMA block, serving as the primary enhancement module, utilizes three distinct attention modules to capture diverse attention representations and estimate high-quality speech signals. We evaluate the performance of the BSS-CFFMA model through comparative and ablation studies on the VoiceBank-DEMAND dataset, achieving SOTA results. Furthermore, we select three types of data from the WHAMR! dataset, a collection specifically designed for speech enhancement tasks, to assess the capabilities of BSS-CFFMA in tasks such as denoising only, dereverberation only, and simultaneous denoising and dereverberation. This study marks the first attempt to explore the effectiveness of self-supervised embedding-based speech enhancement methods in complex tasks encompassing dereverberation and simultaneous denoising and dereverberation. The demo implementation of BSS-CFFMA is available online\\footnote[2]{this https URL. \\label{s1}}.         ",
    "url": "https://arxiv.org/abs/2408.06851",
    "authors": [
      "Alimjan Mattursun",
      "Liejun Wang",
      "Yinfeng Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06906",
    "title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis Vocoders",
    "abstract": "           Since the introduction of Generative Adversarial Networks (GANs) in speech synthesis, remarkable achievements have been attained. In a thorough exploration of vocoders, it has been discovered that audio waveforms can be generated at speeds exceeding real-time while maintaining high fidelity, achieved through the utilization of GAN-based models. Typically, the inputs to the vocoder consist of band-limited spectral information, which inevitably sacrifices high-frequency details. To address this, we adopt the full-band Mel spectrogram information as input, aiming to provide the vocoder with the most comprehensive information possible. However, previous studies have revealed that the use of full-band spectral information as input can result in the issue of over-smoothing, compromising the naturalness of the synthesized speech. To tackle this challenge, we propose VNet, a GAN-based neural vocoder network that incorporates full-band spectral information and introduces a Multi-Tier Discriminator (MTD) comprising multiple sub-discriminators to generate high-resolution signals. Additionally, we introduce an asymptotically constrained method that modifies the adversarial loss of the generator and discriminator, enhancing the stability of the training process. Through rigorous experiments, we demonstrate that the VNet model is capable of generating high-fidelity speech and significantly improving the performance of the vocoder.         ",
    "url": "https://arxiv.org/abs/2408.06906",
    "authors": [
      "Yubing Cao",
      "Yongming Li",
      "Liejun Wang",
      "Yinfeng Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06911",
    "title": "Heterogeneous Space Fusion and Dual-Dimension Attention: A New Paradigm for Speech Enhancement",
    "abstract": "           Self-supervised learning has demonstrated impressive performance in speech tasks, yet there remains ample opportunity for advancement in the realm of speech enhancement research. In addressing speech tasks, confining the attention mechanism solely to the temporal dimension poses limitations in effectively focusing on critical speech features. Considering the aforementioned issues, our study introduces a novel speech enhancement framework, HFSDA, which skillfully integrates heterogeneous spatial features and incorporates a dual-dimension attention mechanism to significantly enhance speech clarity and quality in noisy environments. By leveraging self-supervised learning embeddings in tandem with Short-Time Fourier Transform (STFT) spectrogram features, our model excels at capturing both high-level semantic information and detailed spectral data, enabling a more thorough analysis and refinement of speech signals. Furthermore, we employ the innovative Omni-dimensional Dynamic Convolution (ODConv) technology within the spectrogram input branch, enabling enhanced extraction and integration of crucial information across multiple dimensions. Additionally, we refine the Conformer model by enhancing its feature extraction capabilities not only in the temporal dimension but also across the spectral domain. Extensive experiments on the VCTK-DEMAND dataset show that HFSDA is comparable to existing state-of-the-art models, confirming the validity of our approach.         ",
    "url": "https://arxiv.org/abs/2408.06911",
    "authors": [
      "Tao Zheng",
      "Liejun Wang",
      "Yinfeng Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.06968",
    "title": "Event-Stream Super Resolution using Sigma-Delta Neural Network",
    "abstract": "           This study introduces a novel approach to enhance the spatial-temporal resolution of time-event pixels based on luminance changes captured by event cameras. These cameras present unique challenges due to their low resolution and the sparse, asynchronous nature of the data they collect. Current event super-resolution algorithms are not fully optimized for the distinct data structure produced by event cameras, resulting in inefficiencies in capturing the full dynamism and detail of visual scenes with improved computational complexity. To bridge this gap, our research proposes a method that integrates binary spikes with Sigma Delta Neural Networks (SDNNs), leveraging spatiotemporal constraint learning mechanism designed to simultaneously learn the spatial and temporal distributions of the event stream. The proposed network is evaluated using widely recognized benchmark datasets, including N-MNIST, CIFAR10-DVS, ASL-DVS, and Event-NFS. A comprehensive evaluation framework is employed, assessing both the accuracy, through root mean square error (RMSE), and the computational efficiency of our model. The findings demonstrate significant improvements over existing state-of-the-art methods, specifically, the proposed method outperforms state-of-the-art performance in computational efficiency, achieving a 17.04-fold improvement in event sparsity and a 32.28-fold increase in synaptic operation efficiency over traditional artificial neural networks, alongside a two-fold better performance over spiking neural networks.         ",
    "url": "https://arxiv.org/abs/2408.06968",
    "authors": [
      "Waseem Shariff",
      "Joe Lemley",
      "Peter Corcoran"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.07006",
    "title": "The Complexities of Differential Privacy for Survey Data",
    "abstract": "           The concept of differential privacy (DP) has gained substantial attention in recent years, most notably since the U.S. Census Bureau announced the adoption of the concept for its 2020 Decennial Census. However, despite its attractive theoretical properties, implementing DP in practice remains challenging, especially when it comes to survey data. In this paper we present some results from an ongoing project funded by the U.S. Census Bureau that is exploring the possibilities and limitations of DP for survey data. Specifically, we identify five aspects that need to be considered when adopting DP in the survey context: the multi-staged nature of data production; the limited privacy amplification from complex sampling designs; the implications of survey-weighted estimates; the weighting adjustments for nonresponse and other data deficiencies, and the imputation of missing values. We summarize the project's key findings with respect to each of these aspects and also discuss some of the challenges that still need to be addressed before DP could become the new data protection standard at statistical agencies.         ",
    "url": "https://arxiv.org/abs/2408.07006",
    "authors": [
      "J\u00f6rg Drechsler",
      "James Bailie"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2408.07032",
    "title": "QIris: Quantum Implementation of Rainbow Table Attacks",
    "abstract": "           This paper explores the use of Grover's Algorithm in the classical rainbow table, uncovering the potential of integrating quantum computing techniques with conventional cryptographic methods to develop a Quantum Rainbow Table Proof-of-Concept. This leverages on Quantum concepts and algorithms which includes the principle of qubit superposition, entanglement and teleportation, coupled with Grover's Algorithm to enable a more efficient search through the rainbow table. The paper also details on the hardware constraints and the work around to produce better results in the implementation stages. Through this work we develop a working prototype of quantum rainbow table and demonstrate how quantum computing could significantly improve the speed of cyber tools such as password crackers and thus impact the cyber security landscape.         ",
    "url": "https://arxiv.org/abs/2408.07032",
    "authors": [
      "Lee Jun Quan",
      "Tan Jia Ye",
      "Goh Geok Ling",
      "Vivek Balachandran"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2009.07525",
    "title": "Detectability of hierarchical communities in networks",
    "abstract": "           We study the problem of recovering a planted hierarchy of partitions in a network. The detectability of a single planted partition has previously been analysed in detail and a phase transition has been identified below which the partition cannot be detected. Here we show that, in the hierarchical setting, there exist additional phases in which the presence of multiple consistent partitions can either help or hinder detection. Accordingly, the detectability limit for non-hierarchical partitions typically provides insufficient information about the detectability of the complete hierarchical structure, as we highlight with several constructive examples.         ",
    "url": "https://arxiv.org/abs/2009.07525",
    "authors": [
      "Leto Peel",
      "Michael T. Schaub"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2305.01094",
    "title": "Performative Prediction with Bandit Feedback: Learning through Reparameterization",
    "abstract": "           Performative prediction, as introduced by Perdomo et al, is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work in this field usually hinges on three assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, that the mapping from the model to the data distribution is known to the model designer in advance, and the first-order information of the performative risk is available. In this paper, we initiate the study of performative prediction problems that do not require these assumptions. Specifically, we develop a reparameterization framework that reparametrizes the performative prediction objective as a function of the induced data distribution. We then develop a two-level zeroth-order optimization procedure, where the first level performs iterative optimization on the distribution parameter space, and the second level learns the model that induces a particular target distribution at each iteration. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and achieve provable regret guarantees. In particular, we provide a regret bound that is sublinear in the total number of performative samples taken and is only polynomial in the dimension of the model parameter.         ",
    "url": "https://arxiv.org/abs/2305.01094",
    "authors": [
      "Yatong Chen",
      "Wei Tang",
      "Chien-Ju Ho",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2307.00347",
    "title": "Spatial-Temporal Graph Enhanced DETR Towards Multi-Frame 3D Object Detection",
    "abstract": "           The Detection Transformer (DETR) has revolutionized the design of CNN-based object detection systems, showcasing impressive performance. However, its potential in the domain of multi-frame 3D object detection remains largely unexplored. In this paper, we present STEMD, a novel end-to-end framework that enhances the DETR-like paradigm for multi-frame 3D object detection by addressing three key aspects specifically tailored for this task. First, to model the inter-object spatial interaction and complex temporal dependencies, we introduce the spatial-temporal graph attention network, which represents queries as nodes in a graph and enables effective modeling of object interactions within a social context. To solve the problem of missing hard cases in the proposed output of the encoder in the current frame, we incorporate the output of the previous frame to initialize the query input of the decoder. Finally, it poses a challenge for the network to distinguish between the positive query and other highly similar queries that are not the best match. And similar queries are insufficiently suppressed and turn into redundant prediction boxes. To address this issue, our proposed IoU regularization term encourages similar queries to be distinct during the refinement. Through extensive experiments, we demonstrate the effectiveness of our approach in handling challenging scenarios, while incurring only a minor additional computational overhead. The code is publicly available at this https URL.         ",
    "url": "https://arxiv.org/abs/2307.00347",
    "authors": [
      "Yifan Zhang",
      "Zhiyu Zhu",
      "Junhui Hou",
      "Dapeng Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2307.05170",
    "title": "Neural Quantile Optimization for Edge-Cloud Networking",
    "abstract": "           We seek the best traffic allocation scheme for the edge-cloud computing network that satisfies constraints and minimizes the cost based on burstable billing. First, for a fixed network topology, we formulate a family of integer programming problems with random parameters describing the various traffic demands. Then, to overcome the difficulty caused by the discrete feature of the problem, we generalize the Gumbel-softmax reparameterization method to induce an unconstrained continuous optimization problem as a regularized continuation of the discrete problem. Finally, we introduce the Gumbel-softmax sampling network to solve the optimization problems via unsupervised learning. The network structure reflects the edge-cloud computing topology and is trained to minimize the expectation of the cost function for unconstrained continuous optimization problems. The trained network works as an efficient traffic allocation scheme sampler, remarkably outperforming the random strategy in feasibility and cost function value. Besides testing the quality of the output allocation scheme, we examine the generalization property of the network by increasing the time steps and the number of users. We also feed the solution to existing integer optimization solvers as initial conditions and verify the warm-starts can accelerate the short-time iteration process. The framework is general with solid performance, and the decoupled feature of the random neural networks is adequate for practical implementations.         ",
    "url": "https://arxiv.org/abs/2307.05170",
    "authors": [
      "Bin Du",
      "He Zhang",
      "Xiangle Cheng",
      "Lei Zhang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2308.01487",
    "title": "Data-Driven Nonlinear TDOA for Accurate Source Localization in Complex Signal Dynamics",
    "abstract": "           The complex and dynamic propagation of oscillations and waves is often triggered by sources at unknown locations. Accurate source localization enables the elimination of the rotor core in atrial fibrillation (AFib) as an effective treatment for such severe cardiac disorder; it also finds potential use in locating the spreading source in natural disasters such as forest fires and tsunamis. However, existing approaches such as time of arrival (TOA) and time difference of arrival (TDOA) do not yield accurate localization results since they tacitly assume a constant signal propagation speed whereas realistic propagation is often non-static and heterogeneous. In this paper, we develop a nonlinear TDOA (NTDOA) approach which utilizes observational data from various positions to jointly learn the propagation speed at different angles and distances as well as the location of the source itself. Through examples of simulating the complex dynamics of electrical signals along the surface of the heart and satellite imagery from forest fires and tsunamis, we show that with a small handful of measurements, NTDOA, as a data-driven approach, can successfully locate the spreading source, leading also to better forecasting of the speed and direction of subsequent propagation.         ",
    "url": "https://arxiv.org/abs/2308.01487",
    "authors": [
      "Chinmay Sahu",
      "Mahesh Banavar",
      "Jie Sun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2308.08945",
    "title": "Interpretable Graph Neural Networks for Tabular Data",
    "abstract": "           Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true Shapley values of the features without incurring any additional computational overhead.         ",
    "url": "https://arxiv.org/abs/2308.08945",
    "authors": [
      "Amr Alkhatib",
      "Sofiane Ennadir",
      "Henrik Bostr\u00f6m",
      "Michalis Vazirgiannis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2309.07640",
    "title": "Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid Representation and Normal Prior Enhancement",
    "abstract": "           The reconstruction of indoor scenes from multi-view RGB images is challenging due to the coexistence of flat and texture-less regions alongside delicate and fine-grained regions. Recent methods leverage neural radiance fields aided by predicted surface normal priors to recover the scene geometry. These methods excel in producing complete and smooth results for floor and wall areas. However, they struggle to capture complex surfaces with high-frequency structures due to the inadequate neural representation and the inaccurately predicted normal priors. This work aims to reconstruct high-fidelity surfaces with fine-grained details by addressing the above limitations. To improve the capacity of the implicit representation, we propose a hybrid architecture to represent low-frequency and high-frequency regions separately. To enhance the normal priors, we introduce a simple yet effective image sharpening and denoising technique, coupled with a network that estimates the pixel-wise uncertainty of the predicted surface normal vectors. Identifying such uncertainty can prevent our model from being misled by unreliable surface normal supervisions that hinder the accurate reconstruction of intricate geometries. Experiments on the benchmark datasets show that our method outperforms existing methods in terms of reconstruction quality. Furthermore, the proposed method also generalizes well to real-world indoor scenarios captured by our hand-held mobile phones. Our code is publicly available at: this https URL.         ",
    "url": "https://arxiv.org/abs/2309.07640",
    "authors": [
      "Sheng Ye",
      "Yubin Hu",
      "Matthieu Lin",
      "Yu-Hui Wen",
      "Wang Zhao",
      "Yong-Jin Liu",
      "Wenping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.13506",
    "title": "Evaluating the Usability of Differential Privacy Tools with Data Practitioners",
    "abstract": "           Differential privacy (DP) has become the gold standard in privacy-preserving data analytics, but implementing it in real-world datasets and systems remains challenging. Recently developed DP tools aim to make DP implementation easier, but limited research has investigated these DP tools' usability. Through a usability study with 24 US data practitioners with varying prior DP knowledge, we evaluated the usability of four Python-based open-source DP tools: DiffPrivLib, Tumult Analytics, PipelineDP, and OpenDP. Our results suggest that using DP tools in this study may help DP novices better understand DP; that Application Programming Interface (API) design and documentation are vital for successful DP implementation; and that user satisfaction correlates with how well participants completed study tasks with these DP tools. We provide evidence-based recommendations to improve DP tools' usability to broaden DP adoption.         ",
    "url": "https://arxiv.org/abs/2309.13506",
    "authors": [
      "Ivoline C. Ngong",
      "Brad Stenger",
      "Joseph P. Near",
      "Yuanyuan Feng"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2311.11798",
    "title": "Neural Dynamical Operator: Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods",
    "abstract": "           Data-driven modeling techniques have been explored in the spatial-temporal modeling of complex dynamical systems for many engineering applications. However, a systematic approach is still lacking to leverage the information from different types of data, e.g., with different spatial and temporal resolutions, and the combined use of short-term trajectories and long-term statistics. In this work, we build on the recent progress of neural operator and present a data-driven modeling framework called neural dynamical operator that is continuous in both space and time. A key feature of the neural dynamical operator is the resolution-invariance with respect to both spatial and temporal discretizations, without demanding abundant training data in different temporal resolutions. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the neural dynamical operator with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results confirm the resolution-invariance of the proposed modeling framework and also demonstrate stable long-term simulations with only short-term time series data. In addition, we show that the proposed model can better predict long-term statistics via the hybrid optimization scheme with a combined use of short-term and long-term data.         ",
    "url": "https://arxiv.org/abs/2311.11798",
    "authors": [
      "Chuanqi Chen",
      "Jin-Long Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2312.08879",
    "title": "Regularizing Self-supervised 3D Scene Flows with Surface Awareness and Cyclic Consistency",
    "abstract": "           Learning without supervision how to predict 3D scene flows from point clouds is essential to many perception systems. We propose a novel learning framework for this task which improves the necessary regularization. Relying on the assumption that scene elements are mostly rigid, current smoothness losses are built on the definition of \"rigid clusters\" in the input point clouds. The definition of these clusters is challenging and has a significant impact on the quality of predicted flows. We introduce two new consistency losses that enlarge clusters while preventing them from spreading over distinct objects. In particular, we enforce \\emph{temporal} consistency with a forward-backward cyclic loss and \\emph{spatial} consistency by considering surface orientation similarity in addition to spatial proximity. The proposed losses are model-independent and can thus be used in a plug-and-play fashion to significantly improve the performance of existing models, as demonstrated on two most widely used architectures. We also showcase the effectiveness and generalization capability of our framework on four standard sensor-unique driving datasets, achieving state-of-the-art performance in 3D scene flow estimation. Our codes are available on this https URL.         ",
    "url": "https://arxiv.org/abs/2312.08879",
    "authors": [
      "Patrik Vacek",
      "David Hurych",
      "Karel Zimmermann",
      "Patrick Perez",
      "Tomas Svoboda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2312.10257",
    "title": "The Physics-Informed Neural Network Gravity Model: Generation III",
    "abstract": "           Scientific machine learning and the advent of the Physics-Informed Neural Network (PINN) have shown high potential in their ability to solve complex differential equations. One example is the use of PINNs to solve the gravity field modeling problem -- learning convenient representations of the gravitational potential from position and acceleration data. These PINN gravity models, or PINN-GMs, have demonstrated advantages in model compactness, robustness to noise, and sample efficiency when compared to popular alternatives; however, further investigation has revealed various failure modes for these and other machine learning gravity models which this manuscript aims to address. Specifically, this paper introduces the third generation Physics-Informed Neural Network Gravity Model (PINN-GM-III) which includes design changes that solve the problems of feature divergence, bias towards low-altitude samples, numerical instability, and extrapolation error. Six evaluation metrics are proposed to expose these past pitfalls and illustrate the PINN-GM-III's robustness to them. This study concludes by evaluating the PINN-GM-III modeling accuracy on a heterogeneous density asteroid, and comparing its performance to other analytic and machine learning gravity models.         ",
    "url": "https://arxiv.org/abs/2312.10257",
    "authors": [
      "John Martin",
      "Hanspeter Schaub"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ]
  },
  {
    "id": "arXiv:2401.06855",
    "title": "Fine-grained Hallucination Detection and Editing for Language Models",
    "abstract": "           Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.         ",
    "url": "https://arxiv.org/abs/2401.06855",
    "authors": [
      "Abhika Mishra",
      "Akari Asai",
      "Vidhisha Balachandran",
      "Yizhong Wang",
      "Graham Neubig",
      "Yulia Tsvetkov",
      "Hannaneh Hajishirzi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2401.09543",
    "title": "Token Jumping in Planar Graphs has Linear Sized Kernels",
    "abstract": "           Let $G$ be a planar graph and $I_s$ and $I_t$ be two independent sets in $G$, each of size $k$. We begin with a \"token\" on each vertex of $I_s$ and seek to move all tokens to $I_t$, by repeated \"token jumping\", removing a single token from one vertex and placing it on another vertex. We require that each intermediate arrangement of tokens again specifies an independent set of size $k$. Given $G$, $I_s$, and $I_t$, we ask whether there exists a sequence of token jumps that transforms $I_s$ to $I_t$. When $k$ is part of the input, this problem is known to be PSPACE-complete. However, it was shown by Ito, Kami\u0144ski, and Ono to be fixed-parameter tractable. That is, when $k$ is fixed, the problem can be solved in time polynomial in the order of $G$. Here we strengthen the upper bound on the running time in terms of $k$ by showing that the problem has a kernel of size linear in $k$. More precisely, we transform an arbitrary input problem on a planar graph into an equivalent problem on a (planar) graph with order $O(k)$.         ",
    "url": "https://arxiv.org/abs/2401.09543",
    "authors": [
      "Daniel W. Cranston"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Complexity (cs.CC)"
    ]
  },
  {
    "id": "arXiv:2401.12920",
    "title": "Truck Parking Usage Prediction with Decomposed Graph Neural Networks",
    "abstract": "           Truck parking on freight corridors faces the major challenge of insufficient parking spaces. This is exacerbated by the Hour-of-Service (HOS) regulations, which often result in unauthorized parking practices, causing safety concerns. It has been shown that providing accurate parking usage prediction can be a cost-effective solution to reduce unsafe parking practices. In light of this, existing studies have developed various methods to predict the usage of a truck parking site and have demonstrated satisfactory accuracy. However, these studies focus on a single parking site, and few approaches have been proposed to predict the usage of multiple truck parking sites considering spatio-temporal dependencies, due to the lack of data. This paper aims to fill this gap and presents the Regional Temporal Graph Neural Network (RegT-GCN) to predict parking usage across the entire state to provide more comprehensive truck parking information. The framework leverages the topological structures of truck parking site locations and historical parking data to predict the occupancy rate considering spatio-temporal dependencies across a state. To achieve this, we introduce a Regional Decomposition approach, which effectively captures the geographical characteristics of the truck parking locations and their spatial correlations. Evaluation results demonstrate that the proposed model outperforms other baseline models, improving performance by more than 20%.         ",
    "url": "https://arxiv.org/abs/2401.12920",
    "authors": [
      "Rei Tamaru",
      "Yang Cheng",
      "Steven Parker",
      "Ernie Perry",
      "Bin Ran",
      "Soyoung Ahn"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.07867",
    "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
    "abstract": "           Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.         ",
    "url": "https://arxiv.org/abs/2402.07867",
    "authors": [
      "Wei Zou",
      "Runpeng Geng",
      "Binghui Wang",
      "Jinyuan Jia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2402.11196",
    "title": "Maintaining Adversarial Robustness in Continuous Learning",
    "abstract": "           Adversarial robustness is essential for security and reliability of machine learning systems. However, adversarial robustness enhanced by defense algorithms is easily erased as the neural network's weights update to learn new tasks. To address this vulnerability, it is essential to improve the capability of neural networks in terms of robust continual learning. Specially, we propose a novel gradient projection technique that effectively stabilizes sample gradients from previous data by orthogonally projecting back-propagation gradients onto a crucial subspace before using them for weight updates. This technique can maintaining robustness by collaborating with a class of defense algorithms through sample gradient smoothing. The experimental results on four benchmarks including Split-CIFAR100 and Split-miniImageNet, demonstrate that the superiority of the proposed approach in mitigating rapidly degradation of robustness during continual learning even when facing strong adversarial attacks.         ",
    "url": "https://arxiv.org/abs/2402.11196",
    "authors": [
      "Xiaolei Ru",
      "Xiaowei Cao",
      "Zijia Liu",
      "Jack Murdoch Moore",
      "Xin-Ya Zhang",
      "Xia Zhu",
      "Wenjia Wei",
      "Gang Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2402.11432",
    "title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",
    "abstract": "           Deception detection has attracted increasing attention due to its importance in real-world scenarios. Its main goal is to detect deceptive behaviors from multimodal clues such as gestures, facial expressions, prosody, etc. However, these bases are usually subjective and related to personal habits. Therefore, we extend deception detection to deception reasoning, further providing objective evidence to support subjective judgment. Specifically, we provide potential lies and basic facts and then analyze why this sentence may be a lie by combining factual inconsistencies and intent behind them. Compared with deception detection, this task is more applicable to real-world scenarios. For example, in interrogation, the police should judge whether a person is lying based on solid evidence. This paper presents our initial attempts at this task, including constructing a dataset and defining evaluation metrics. Meanwhile, this task can serve as a benchmark for evaluating the complex reasoning capability of large language models. Our code and data are provided in the supplementary material.         ",
    "url": "https://arxiv.org/abs/2402.11432",
    "authors": [
      "Kang Chen",
      "Zheng Lian",
      "Haiyang Sun",
      "Rui Liu",
      "Jiangyan Yi",
      "Bin Liu",
      "Jianhua Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.12261",
    "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms",
    "abstract": "           The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments.         ",
    "url": "https://arxiv.org/abs/2402.12261",
    "authors": [
      "Jonathan Zheng",
      "Alan Ritter",
      "Wei Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2402.13033",
    "title": "Enhancing Node Representations for Real-World Complex Networks with Topological Augmentation",
    "abstract": "           Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures, and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks, which often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Topological Augmentation (TopoAug), a novel graph augmentation method that builds a combinatorial complex from the original graph by constructing virtual hyperedges directly from the raw data. TopoAug then produces auxiliary node features by extracting information from the combinatorial complex, which are used for enhancing GNN performances on downstream tasks. We design three diverse virtual hyperedge construction strategies to accompany the construction of combinatorial complexes: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality. Furthermore, to facilitate TopoAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce. Our empirical study shows that TopoAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into the graph augmentation for real-world complex networks.         ",
    "url": "https://arxiv.org/abs/2402.13033",
    "authors": [
      "Xiangyu Zhao",
      "Zehui Li",
      "Mingzhu Shen",
      "Guy-Bart Stan",
      "Pietro Li\u00f2",
      "Yiren Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2402.14735",
    "title": "How Transformers Learn Causal Structure with Gradient Descent",
    "abstract": "           The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.         ",
    "url": "https://arxiv.org/abs/2402.14735",
    "authors": [
      "Eshaan Nichani",
      "Alex Damian",
      "Jason D. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2402.17232",
    "title": "Two-scale Neural Networks for Partial Differential Equations with Small Parameters",
    "abstract": "           We propose a two-scale neural network method for solving partial differential equations (PDEs) with small parameters using physics-informed neural networks (PINNs). We directly incorporate the small parameters into the architecture of neural networks. The proposed method enables solving PDEs with small parameters in a simple fashion, without adding Fourier features or other computationally taxing searches of truncation parameters. Various numerical examples demonstrate reasonable accuracy in capturing features of large derivatives in the solutions caused by small parameters.         ",
    "url": "https://arxiv.org/abs/2402.17232",
    "authors": [
      "Qiao Zhuang",
      "Chris Ziyi Yao",
      "Zhongqiang Zhang",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2403.06095",
    "title": "RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completionn",
    "abstract": "           Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present \\tool, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHYPER is the {\\em Repo-level Semantic Graph} (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that \\tool markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines. Our implementation of RepoHYPER can be found at this https URL.         ",
    "url": "https://arxiv.org/abs/2403.06095",
    "authors": [
      "Huy N. Phan",
      "Hoang N. Phan",
      "Tien N. Nguyen",
      "Nghi D. Q. Bui"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2403.09070",
    "title": "Analytical Heterogeneous Die-to-Die 3D Placement with Macros",
    "abstract": "           This paper presents an innovative approach to 3D mixed-size placement in heterogeneous face-to-face (F2F) bonded 3D ICs. We propose an analytical framework that utilizes a dedicated density model and a bistratal wirelength model, effectively handling macros and standard cells in a 3D solution space. A novel 3D preconditioner is developed to resolve the topological and physical gap between macros and standard cells. Additionally, we propose a mixed-integer linear programming (MILP) formulation for macro rotation to optimize wirelength. Our framework is implemented with full-scale GPU acceleration, leveraging an adaptive 3D density accumulation algorithm and an incremental wirelength gradient algorithm. Experimental results on ICCAD 2023 contest benchmarks demonstrate that our framework can achieve 5.9% quality score improvement compared to the first-place winner with 4.0x runtime speedup. Additional experiments on modern RISC-V designs further validate the generalizability and superiority of our framework.         ",
    "url": "https://arxiv.org/abs/2403.09070",
    "authors": [
      "Yuxuan Zhao",
      "Peiyu Liao",
      "Siting Liu",
      "Jiaxi Jiang",
      "Yibo Lin",
      "Bei Yu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ]
  },
  {
    "id": "arXiv:2403.16130",
    "title": "AKBR: Learning Adaptive Kernel-based Representations for Graph Classification",
    "abstract": "           In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel matrix can be theoretically seen as the embedding vector of a sample graph, the proposed AKBR model is able to directly employ the resulting kernel matrix as the graph feature matrix and input it into the classifier for classification (i.e., the SoftMax layer), naturally providing an end-to-end learning architecture between the kernel computation as well as the classifier. Experimental results show that the proposed AKBR model outperforms existing state-of-the-art graph kernels and deep learning methods on standard graph benchmarks.         ",
    "url": "https://arxiv.org/abs/2403.16130",
    "authors": [
      "Feifei Qian",
      "Lixin Cui",
      "Ming Li",
      "Yue Wang",
      "Hangyuan Du",
      "Lixiang Xu",
      "Lu Bai",
      "Philip S. Yu",
      "Edwin R. Hancock"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2404.04662",
    "title": "Learning Minimal Neural Specifications",
    "abstract": "           Formal verification is only as good as the specification of a system, which is also true for neural network verification. Existing specifications follow the paradigm of data as specification, where the local neighborhood around a reference data point is considered correct or robust. While these specifications provide a fair testbed for assessing model robustness, they are too restrictive for verifying unseen test data-a challenging task with significant real-world implications. Recent work shows great promise through a new paradigm, neural representation as specification, which uses neural activation patterns (NAPs) for this purpose. However, it computes the most refined NAPs, which include many redundant neurons. In this paper, we study the following problem: Given a neural network, find a minimal (general) NAP specification that is sufficient for formal verification of the network's robustness. Finding the minimal NAP specification not only expands verifiable bounds but also provides insights into which neurons contribute to the model's robustness. To address this problem, we propose several exact and approximate approaches. Our exact approaches leverage the verification tool to find minimal NAP specifications in either a deterministic or statistical manner. Whereas the approximate methods efficiently estimate minimal NAPs using adversarial examples and local gradients, without making calls to the verification tool. This allows us to inspect potential causal links between neurons and the robustness of state-of-the art neural networks, a task for which existing verification frameworks fail to scale. Our experimental results suggest that minimal NAP specifications require much smaller fractions of neurons compared to the most refined NAP specifications computed by previous work, yet they can significantly expand the verifiable boundaries to several orders of magnitude larger.         ",
    "url": "https://arxiv.org/abs/2404.04662",
    "authors": [
      "Chuqin Geng",
      "Zhaoyue Wang",
      "Haolin Ye",
      "Saifei Liao",
      "Xujie Si"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2404.09848",
    "title": "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge Representation",
    "abstract": "           In a hyper-relational knowledge graph (HKG), each fact is composed of a main triple associated with attribute-value qualifiers, which express additional factual knowledge. The hyper-relational knowledge graph completion (HKGC) task aims at inferring plausible missing links in a HKG. Most existing approaches to HKGC focus on enhancing the communication between qualifier pairs and main triples, while overlooking two important properties that emerge from the monotonicity of the hyper-relational graphs representation regime. Stage Reasoning allows for a two-step reasoning process, facilitating the integration of coarse-grained inference results derived solely from main triples and fine-grained inference results obtained from hyper-relational facts with qualifiers. In the initial stage, coarse-grained results provide an upper bound for correct predictions, which are subsequently refined in the fine-grained step. More generally, Qualifier Monotonicity implies that by attaching more qualifier pairs to a main triple, we may only narrow down the answer set, but never enlarge it. This paper proposes the HyperMono model for hyper-relational knowledge graph completion, which realizes stage reasoning and qualifier monotonicity. To implement qualifier monotonicity HyperMono resorts to cone embeddings. Experiments on three real-world datasets with three different scenario conditions demonstrate the strong performance of HyperMono when compared to the SoTA.         ",
    "url": "https://arxiv.org/abs/2404.09848",
    "authors": [
      "Zhiwei Hu",
      "V\u00edctor Guti\u00e9rrez-Basulto",
      "Zhiliang Xiang",
      "Ru Li",
      "Jeff Z. Pan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.16551",
    "title": "Surprisingly Strong Performance Prediction with Neural Graph Features",
    "abstract": "           Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training. Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks. Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training. However, they are still poorly understood, exhibit biases with network properties, and their performance is limited. Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs. GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings. In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost.         ",
    "url": "https://arxiv.org/abs/2404.16551",
    "authors": [
      "Gabriela Kadlecov\u00e1",
      "Jovita Lukasik",
      "Martin Pil\u00e1t",
      "Petra Vidnerov\u00e1",
      "Mahmoud Safari",
      "Roman Neruda",
      "Frank Hutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2404.16828",
    "title": "Made to Order: Discovering monotonic temporal changes via self-supervised video ordering",
    "abstract": "           Our objective is to discover and localize monotonic temporal changes in a sequence of images. To achieve this, we exploit a simple proxy task of ordering a shuffled image sequence, with `time' serving as a supervisory signal, since only changes that are monotonic with time can give rise to the correct ordering. We also introduce a transformer-based model for ordering of image sequences of arbitrary length with built-in attribution maps. After training, the model successfully discovers and localizes monotonic changes while ignoring cyclic and stochastic ones. We demonstrate applications of the model in multiple domains covering different scene and object types, discovering both object-level and environmental changes in unseen sequences. We also demonstrate that the attention-based attribution maps function as effective prompts for segmenting the changing regions, and that the learned representations can be used for downstream applications. Finally, we show that the model achieves the state-of-the-art on standard benchmarks for image ordering.         ",
    "url": "https://arxiv.org/abs/2404.16828",
    "authors": [
      "Charig Yang",
      "Weidi Xie",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2405.02022",
    "title": "STX-Vote: Improving Reliability with Bit Voting in Synchronous Transmission-based IoT Networks",
    "abstract": "           Industrial Internet of Things (IIoT) networks must meet strict reliability, latency, and low energy consumption requirements. However, traditional low-power wireless protocols are ineffective in finding a sweet spot for balancing these performance metrics. Recently, network flooding protocols based on Synchronous Transmissions (STX) have been proposed for better performance in reliability-critical IIoT, where simultaneous transmissions are possible without packet collisions. STX-based protocols can offer a competitive edge over routing-based protocols, particularly in dependability. However, they notably suffer from the beating effect, a physical layer phenomenon that results in sinusoidal interference across a packet and, consequently, packet loss. Thus, we introduce STX-Vote, an error correction scheme that can handle errors caused by beating effects. Importantly, we utilize transmission redundancy already inherent within STX protocols so do not incur additional on-air overhead. Through simulation, we demonstrate STX-Vote can provide a 40% increase in reliability. We subsequently implement STX-Vote on nRF52840-DK devices and perform extensive experiments. The results confirm that STX-Vote improves reliability by 25-28% for BLE 5 PHYs and 8% for IEEE 802.15.4; thus, it can complement existing error correction schemes.         ",
    "url": "https://arxiv.org/abs/2405.02022",
    "authors": [
      "Burhanuddin Rangwala",
      "Ava Powelson",
      "Michael Baddeley",
      "Israat Haque"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2405.08596",
    "title": "Towards Robust Audio Deepfake Detection: A Evolving Benchmark for Continual Learning",
    "abstract": "           The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging. Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types. Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework. To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection. EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o. It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM). Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods         ",
    "url": "https://arxiv.org/abs/2405.08596",
    "authors": [
      "Xiaohui Zhang",
      "Jiangyan Yi",
      "Jianhua Tao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2405.13349",
    "title": "Building a Verifiable Logical Clock for P2P Networks",
    "abstract": "           Logical clocks are a fundamental tool to establish causal ordering of events in a distributed system. They have been applied in weakly consistent storage systems, causally ordered broadcast, distributed snapshots, deadlock detection, and distributed system debugging. However, prior logical clock constructs fail to work in an open network with Byzantine participants. In this work, we present Chrono, a novel logical clock system that targets such challenging environment. We first redefine causality properties among distributed processes under the Byzantine failure model. To enforce these properties, Chrono defines a new validator abstraction for building fault-tolerant logical clocks. Furthermore, our validator abstraction is customizable: Chrono includes multiple backend implementations for the abstraction, each with different security-performance trade-offs. We have applied Chrono to build two decentralized applications, a mutual exclusive service and a weakly consistent key-value store. Chrono adds only marginal overhead compared to systems that tolerate no Byzantine faults. It also out-performs state-of-the-art BFT total order protocols by significant margins.         ",
    "url": "https://arxiv.org/abs/2405.13349",
    "authors": [
      "Guangda Sun",
      "Tianyang Tao",
      "Yanpei Guo",
      "Michael Yiqing Hu",
      "Jialin Li"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2405.17034",
    "title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",
    "abstract": "           Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at this https URL.         ",
    "url": "https://arxiv.org/abs/2405.17034",
    "authors": [
      "Renqiang Luo",
      "Huafei Huang",
      "Shuo Yu",
      "Zhuoyang Han",
      "Estrid He",
      "Xiuzhen Zhang",
      "Feng Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2405.17571",
    "title": "Bluesky: Network Topology, Polarization, and Algorithmic Curation",
    "abstract": "           Bluesky is a nascent Twitter-like and decentralized social media network with novel features and unprecedented data access. This paper provides a characterization of its interaction network, studying the political leaning, polarization, network structure, and algorithmic curation mechanisms of five million users. The dataset spans from the website's first release in February of 2023 to May of 2024. We investigate the replies, likes, reposts, and follows layers of the Bluesky network. We find that all networks are characterized by heavy-tailed distributions, high clustering, and short connection paths, similar to other larger social networks. Bluesky introduced feeds - algorithmic content recommenders created for and by users. We analyze all feeds and find that while a large number of custom feeds have been created, users' uptake of them appears to be limited. We analyze the hyperlinks shared by Bluesky's users and find no evidence of polarization in terms of the political leaning of the news sources they share. They share predominantly left-center news sources and little to no links associated with questionable news sources. In contrast to the homogeneous political ideology, we find significant issues-based divergence by studying opinions related to the Israel-Palestine conflict. Two clear homophilic clusters emerge: Pro-Palestinian voices outnumber pro-Israeli users, and the proportion has increased. We conclude by claiming that Bluesky-for all its novel features - is very similar in its network structure to existing and larger social media sites and provides unprecedented research opportunities for social scientists, network scientists, and political scientists alike.         ",
    "url": "https://arxiv.org/abs/2405.17571",
    "authors": [
      "Dorian Quelle",
      "Alexandre Bovet"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2406.02304",
    "title": "Phenotype control and elimination of variables in Boolean networks",
    "abstract": "           We investigate how elimination of variables can affect the asymptotic dynamics and phenotype control of Boolean networks. In particular, we look at the impact on minimal trap spaces, and identify a structural condition that guarantees their preservation. We examine the possible effects of variable elimination under three of the most popular approaches to control (attractor-based control, value propagation and control of minimal trap spaces), and under different update schemes (synchronous, asynchronous, generalized asynchronous). We provide some insights on the application of reduction, and an ample inventory of examples and counterexamples.         ",
    "url": "https://arxiv.org/abs/2406.02304",
    "authors": [
      "Elisa Tonello",
      "Lo\u00efc Paulev\u00e9"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ]
  },
  {
    "id": "arXiv:2406.16464",
    "title": "InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection",
    "abstract": "           The prevalence of sarcasm in social media, conveyed through text-image combinations, presents significant challenges for sentiment analysis and intention mining. Existing multi-modal sarcasm detection methods have been proven to overestimate performance, as they struggle to effectively capture the intricate sarcastic cues that arise from the interaction between an image and text. To address these issues, we propose InterCLIP-MEP, a novel framework for multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP (InterCLIP) as the backbone to extract text-image representations, enhancing them by embedding cross-modality information directly within each encoder, thereby improving the representations to capture text-image interactions better. Furthermore, an efficient training strategy is designed to adapt InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic, fixed-length dual-channel memory to store historical knowledge of valuable test samples during inference. It then leverages this memory as a non-parametric classifier to derive the final prediction, offering a more robust recognition of multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves state-of-the-art performance on the MMSD2.0 benchmark, with an accuracy improvement of 1.08% and an F1 score improvement of 1.51% over the previous best method.         ",
    "url": "https://arxiv.org/abs/2406.16464",
    "authors": [
      "Junjie Chen",
      "Hang Yu",
      "Weidong Liu",
      "Subin Huang",
      "Sanmin Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2406.16633",
    "title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap Augmented Auxiliary Network",
    "abstract": "           Deep neural networks (DNNs) typically employ an end-to-end (E2E) training paradigm which presents several challenges, including high GPU memory consumption, inefficiency, and difficulties in model parallelization during training. Recent research has sought to address these issues, with one promising approach being local learning. This method involves partitioning the backbone network into gradient-isolated modules and manually designing auxiliary networks to train these local modules. Existing methods often neglect the interaction of information between local modules, leading to myopic issues and a performance gap compared to E2E training. To address these limitations, we propose the Multilaminar Leap Augmented Auxiliary Network (MLAAN). Specifically, MLAAN comprises Multilaminar Local Modules (MLM) and Leap Augmented Modules (LAM). MLM captures both local and global features through independent and cascaded auxiliary networks, alleviating performance issues caused by insufficient global features. However, overly simplistic auxiliary networks can impede MLM's ability to capture global information. To address this, we further design LAM, an enhanced auxiliary network that uses the Exponential Moving Average (EMA) method to facilitate information exchange between local modules, thereby mitigating the shortsightedness resulting from inadequate interaction. The synergy between MLM and LAM has demonstrated excellent performance. Our experiments on the CIFAR-10, STL-10, SVHN, and ImageNet datasets show that MLAAN can be seamlessly integrated into existing local learning frameworks, significantly enhancing their performance and even surpassing end-to-end (E2E) training methods, while also reducing GPU memory consumption.         ",
    "url": "https://arxiv.org/abs/2406.16633",
    "authors": [
      "Yuming Zhang",
      "Shouxin Zhang",
      "Peizhe Wang",
      "Feiyu Zhu",
      "Dongzhi Guan",
      "Junhao Su",
      "Jiabin Liu",
      "Changpeng Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2407.02461",
    "title": "Decentralized Intelligence Network (DIN)",
    "abstract": "           Decentralized Intelligence Network (DIN) is a theoretical framework addressing data fragmentation and siloing challenges, enabling scalable AI through data sovereignty. It facilitates effective AI utilization within sovereign networks by overcoming barriers to accessing diverse data sources, leveraging: 1) personal data stores to ensure data sovereignty, where data remains securely within Participants' control; 2) a scalable federated learning protocol implemented on a public blockchain for decentralized AI training, where only model parameter updates are shared, keeping data within the personal data stores; and 3) a scalable, trustless cryptographic rewards mechanism on a public blockchain to incentivize participation and ensure fair reward distribution through a decentralized auditing protocol. This approach guarantees that no entity can prevent or control access to training data or influence financial benefits, as coordination and reward distribution are managed on the public blockchain with an immutable record. The framework supports effective AI training by allowing Participants to maintain control over their data, benefit financially, and contribute to a decentralized, scalable ecosystem that leverages collective AI to develop beneficial algorithms.         ",
    "url": "https://arxiv.org/abs/2407.02461",
    "authors": [
      "Abraham Nash"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2407.11038",
    "title": "Fuzzy Recurrent Stochastic Configuration Networks for Industrial Data Analytics",
    "abstract": "           This paper presents a novel neuro-fuzzy model, termed fuzzy recurrent stochastic configuration networks (F-RSCNs), for industrial data analytics. Unlike the original recurrent stochastic configuration network (RSCN), the proposed F-RSCN is constructed by multiple sub-reservoirs, and each sub-reservoir is associated with a Takagi-Sugeno-Kang (TSK) fuzzy rule. Through this hybrid framework, first, the interpretability of the model is enhanced by incorporating fuzzy reasoning to embed the prior knowledge into the network. Then, the parameters of the neuro-fuzzy model are determined by the recurrent stochastic configuration (RSC) algorithm. This scheme not only ensures the universal approximation property and fast learning speed of the built model but also overcomes uncertain problems, such as unknown dynamic orders, arbitrary structure determination, and the sensitivity of learning parameters in modelling nonlinear dynamics. Finally, an online update of the output weights is performed using the projection algorithm, and the convergence analysis of the learning parameters is given. By integrating TSK fuzzy inference systems into RSCNs, F-RSCNs have strong fuzzy inference capability and can achieve sound performance for both learning and generalization. Comprehensive experiments show that the proposed F-RSCNs outperform other classical neuro-fuzzy and non-fuzzy models, demonstrating great potential for modelling complex industrial systems.         ",
    "url": "https://arxiv.org/abs/2407.11038",
    "authors": [
      "Dianhui Wang",
      "Gang Dang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ]
  },
  {
    "id": "arXiv:2407.11253",
    "title": "Separable Operator Networks",
    "abstract": "           Operator learning has become a powerful tool in machine learning for modeling complex physical systems governed by partial differential equations (PDEs). Although Deep Operator Networks (DeepONet) show promise, they require extensive data acquisition. Physics-informed DeepONets (PI-DeepONet) mitigate data scarcity but suffer from inefficient training processes. We introduce Separable Operator Networks (SepONet), a novel framework that significantly enhances the efficiency of physics-informed operator learning. SepONet uses independent trunk networks to learn basis functions separately for different coordinate axes, enabling faster and more memory-efficient training via forward-mode automatic differentiation. We provide a universal approximation theorem for SepONet proving that it generalizes to arbitrary operator learning problems, and then validate its performance through comprehensive benchmarking against PI-DeepONet. Our results demonstrate SepONet's superior performance across various nonlinear and inseparable PDEs, with SepONet's advantages increasing with problem complexity, dimension, and scale. For 1D time-dependent PDEs, SepONet achieves up to $112\\times$ faster training and $82\\times$ reduction in GPU memory usage compared to PI-DeepONet, while maintaining comparable accuracy. For the 2D time-dependent nonlinear diffusion equation, SepONet efficiently handles the complexity, achieving a 6.44\\% mean relative $\\ell_{2}$ test error, while PI-DeepONet fails due to memory constraints. This work paves the way for extreme-scale learning of continuous mappings between infinite-dimensional function spaces. Open source code is available at \\url{this https URL}.         ",
    "url": "https://arxiv.org/abs/2407.11253",
    "authors": [
      "Xinling Yu",
      "Sean Hooten",
      "Ziyue Liu",
      "Yequan Zhao",
      "Marco Fiorentino",
      "Thomas Van Vaerenbergh",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2407.14274",
    "title": "Mixed-precision Neural Networks on RISC-V Cores: ISA extensions for Multi-Pumped Soft SIMD Operations",
    "abstract": "           Recent advancements in quantization and mixed-precision approaches offers substantial opportunities to improve the speed and energy efficiency of Neural Networks (NN). Research has shown that individual parameters with varying low precision, can attain accuracies comparable to full-precision counterparts. However, modern embedded microprocessors provide very limited support for mixed-precision NNs regarding both Instruction Set Architecture (ISA) extensions and their hardware design for efficient execution of mixed-precision operations, i.e., introducing several performance bottlenecks due to numerous instructions for data packing and unpacking, arithmetic unit under-utilizations etc. In this work, we bring together, for the first time, ISA extensions tailored to mixed-precision hardware optimizations, targeting energy-efficient DNN inference on leading RISC-V CPU architectures. To this end, we introduce a hardware-software co-design framework that enables cooperative hardware design, mixed-precision quantization, ISA extensions and inference in cycle-accurate emulations. At hardware level, we firstly expand the ALU unit within our proof-of-concept micro-architecture to support configurable fine grained mixed-precision arithmetic operations. Subsequently, we implement multi-pumping to minimize execution latency, with an additional soft SIMD optimization applied for 2-bit operations. At the ISA level, three distinct MAC instructions are encoded extending the RISC-V ISA, and exposed up to the compiler level, each corresponding to a different mixed-precision operational mode. Our extensive experimental evaluation over widely used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our framework can achieve, on average, 15x energy reduction for less than 1% accuracy loss and outperforms the ISA-agnostic state-of-the-art RISC-V cores.         ",
    "url": "https://arxiv.org/abs/2407.14274",
    "authors": [
      "Giorgos Armeniakos",
      "Alexis Maras",
      "Sotirios Xydis",
      "Dimitrios Soudris"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2407.16205",
    "title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models",
    "abstract": "           The rapid development of Large Language Models (LLMs) has brought remarkable generative capabilities across diverse tasks. However, despite the impressive achievements, these LLMs still have numerous inherent vulnerabilities, particularly when faced with jailbreak attacks. By investigating jailbreak attacks, we can uncover hidden weaknesses in LLMs and inform the development of more robust defense mechanisms to fortify their security. In this paper, we further explore the boundary of jailbreak attacks on LLMs and propose Analyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes advantage of LLMs' growing analyzing and reasoning capability and reveals their underlying vulnerabilities when facing analyzing-based tasks. We conduct a detailed evaluation of ABJ across various open-source and closed-source LLMs, which achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE) on GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and efficiency. Our research highlights the importance of prioritizing and enhancing the safety of LLMs to mitigate the risks of misuse. The code is publicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This paper contains examples of LLMs that might be offensive or harmful.         ",
    "url": "https://arxiv.org/abs/2407.16205",
    "authors": [
      "Shi Lin",
      "Rongchang Li",
      "Xun Wang",
      "Changting Lin",
      "Wenpeng Xing",
      "Meng Han"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2408.01233",
    "title": "CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset Augmentation using Diffusion Models",
    "abstract": "           Forensic sketch-to-mugshot matching is a challenging task in face recognition, primarily hindered by the scarcity of annotated forensic sketches and the modality gap between sketches and photographs. To address this, we propose CLIP4Sketch, a novel approach that leverages diffusion models to generate a large and diverse set of sketch images, which helps in enhancing the performance of face recognition systems in sketch-to-mugshot matching. Our method utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate sketches with explicit control over identity and style. We combine CLIP and Adaface embeddings of a reference mugshot, along with textual descriptions of style, as the conditions to the diffusion model. We demonstrate the efficacy of our approach by generating a comprehensive dataset of sketches corresponding to mugshots and training a face recognition model on our synthetic data. Our results show significant improvements in sketch-to-mugshot matching accuracy over training on an existing, limited amount of real face sketch data, validating the potential of diffusion models in enhancing the performance of face recognition systems across modalities. We also compare our dataset with datasets generated using GAN-based methods to show its superiority.         ",
    "url": "https://arxiv.org/abs/2408.01233",
    "authors": [
      "Kushal Kumar Jain",
      "Steve Grosz",
      "Anoop M. Namboodiri",
      "Anil K. Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.04043",
    "title": "Ownership in low-level intermediate representation",
    "abstract": "           The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.         ",
    "url": "https://arxiv.org/abs/2408.04043",
    "authors": [
      "Siddharth Priya",
      "Arie Gurfinkel"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2408.05498",
    "title": "A Laplacian-based Quantum Graph Neural Network for Semi-Supervised Learning",
    "abstract": "           Laplacian learning method is a well-established technique in classical graph-based semi-supervised learning, but its potential in the quantum domain remains largely unexplored. This study investigates the performance of the Laplacian-based Quantum Semi-Supervised Learning (QSSL) method across four benchmark datasets -- Iris, Wine, Breast Cancer Wisconsin, and Heart Disease. Further analysis explores the impact of increasing Qubit counts, revealing that adding more Qubits to a quantum system doesn't always improve performance. The effectiveness of additional Qubits depends on the quantum algorithm and how well it matches the dataset. Additionally, we examine the effects of varying entangling layers on entanglement entropy and test accuracy. The performance of Laplacian learning is highly dependent on the number of entangling layers, with optimal configurations varying across different datasets. Typically, moderate levels of entanglement offer the best balance between model complexity and generalization capabilities. These observations highlight the crucial need for precise hyperparameter tuning tailored to each dataset to achieve optimal performance in Laplacian learning methods.         ",
    "url": "https://arxiv.org/abs/2408.05498",
    "authors": [
      "Hamed Gholipour",
      "Farid Bozorgnia",
      "Kailash Hambarde",
      "Hamzeh Mohammadigheymasi",
      "Javier Mancilla",
      "Andre Sequeira",
      "Joao Neves",
      "Hugo Proen\u00e7a"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ]
  },
  {
    "id": "arXiv:2408.05518",
    "title": "Long working distance portable smartphone microscopy for metallic mesh defect detection",
    "abstract": "           Metallic mesh is a transparent electromagnetic shielding film with a fine metal line structure. However, it can develop defects that affect the optoelectronic performance whether in the production preparation or in actual use. The development of in-situ non-destructive testing (NDT) devices for metallic mesh requires long working distances, reflective optical path design, and miniaturization. To address the limitations of existing smartphone microscopes, which feature short working distances and inadequate transmission imaging for industrial in-situ inspection, we propose a novel long-working distance reflective smartphone microscopy system (LD-RSM). LD-RSM builds a 4f optical imaging system with external optical components and a smartphone, utilizing a beam splitter to achieve reflective imaging with the illumination system and imaging system on the same side of the sample. It achieves an optical resolution of 4.92$\\mu$m and a working distance of up to 22.23 mm. Additionally, we introduce a dual prior weighted Robust Principal Component Analysis (DW-RPCA) for defect detection. This approach leverages spectral filter fusion and Hough transform to model different defect types, enhancing the accuracy and efficiency of defect identification. Coupled with an optimized threshold segmentation algorithm, DW-RPCA method achieves a pixel-level accuracy of 84.8%. Our work showcases strong potential for growth in the field of in-situ on-line inspection of industrial products.         ",
    "url": "https://arxiv.org/abs/2408.05518",
    "authors": [
      "Zhengang Lu",
      "Hongsheng Qin",
      "Jing Li",
      "Ming Sun",
      "Jiubin Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.05625",
    "title": "Memento Filter: A Fast, Dynamic, and Robust Range Filter",
    "abstract": "           Range filters are probabilistic data structures that answer approximate range emptiness queries. They aid in avoiding processing empty range queries and have use cases in many application domains such as key-value stores and social web analytics. However, current range filter designs do not support dynamically changing and growing datasets. Moreover, several of these designs also exhibit impractically high false positive rates under correlated workloads, which are common in practice. These impediments restrict the applicability of range filters across a wide range of use cases. We introduce Memento filter, the first range filter to offer dynamicity, fast operations, and a robust false positive rate guarantee for any workload. Memento filter partitions the key universe and clusters its keys according to this partitioning. For each cluster, it stores a fingerprint and a list of key suffixes contiguously. The encoding of these lists makes them amenable to existing dynamic filter structures. Due to the well-defined one-to-one mapping from keys to suffixes, Memento filter supports inserts and deletes and can even expand to accommodate a growing dataset. We implement Memento filter on top of a Rank-and-Select Quotient filter and InfiniFilter and demonstrate that it achieves competitive false positive rates and performance with the state-of-the-art while also providing dynamicity. Due to its dynamicity, Memento filter is the first range filter applicable to B-Trees. We showcase this by integrating Memento filter into WiredTiger, a B-Tree-based key-value store. Memento filter doubles WiredTiger's range query throughput when 50\\% of the queries are empty while keeping all other cost metrics unharmed.         ",
    "url": "https://arxiv.org/abs/2408.05625",
    "authors": [
      "Navid Eslami",
      "Niv Dayan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2408.05743",
    "title": "Neural Architecture Search based Global-local Vision Mamba for Palm-Vein Recognition",
    "abstract": "           Due to the advantages such as high security, high privacy, and liveness recognition, vein recognition has been received more and more attention in past years. Recently, deep learning models, e.g., Mamba has shown robust feature representation with linear computational complexity and successfully applied for visual tasks. However, vision Manba can capture long-distance feature dependencies but unfortunately deteriorate local feature details. Besides, manually designing a Mamba architecture based on human priori knowledge is very time-consuming and error-prone. In this paper, first, we propose a hybrid network structure named Global-local Vision Mamba (GLVM), to learn the local correlations in images explicitly and global dependencies among tokens for vein feature representation. Secondly, we design a Multi-head Mamba to learn the dependencies along different directions, so as to improve the feature representation ability of vision Mamba. Thirdly, to learn the complementary features, we propose a ConvMamba block consisting of three branches, named Multi-head Mamba branch (MHMamba), Feature Iteration Unit branch (FIU), and Convolutional Neural Network (CNN) branch, where the Feature Iteration Unit branch aims to fuse convolutional local features with Mamba-based global representations. Finally, a Globallocal Alternate Neural Architecture Search (GLNAS) method is proposed to search the optimal architecture of GLVM alternately with the evolutionary algorithm, thereby improving the recognition performance for vein recognition tasks. We conduct rigorous experiments on three public palm-vein databases to estimate the performance. The experimental results demonstrate that the proposed method outperforms the representative approaches and achieves state-of-the-art recognition accuracy.         ",
    "url": "https://arxiv.org/abs/2408.05743",
    "authors": [
      "Huafeng Qin",
      "Yuming Fu",
      "Jing Chen",
      "Mounim A. El-Yacoubi",
      "Xinbo Gao",
      "Jun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2408.05846",
    "title": "A Universal Flexible Near-sensor Neuromorphic Tactile System with Multi-threshold strategy for Pressure Characteristic Detection",
    "abstract": "           Constructing the new generation information processing system by mimicking biological nervous system is a feasible way for implement of high-efficient intelligent sensing device and bionic robot. However, most biological nervous system, especially the tactile system, have various powerful functions. This is a big challenge for bionic system design. Here we report a universal fully flexible neuromorphic tactile perception system with strong compatibility and a multithreshold signal processing strategy. Like nervous system, signal in our system is transmitted as pulses and processed as threshold information. For feasibility verification, recognition of three different type pressure signals (continuous changing signal, Morse code signal and symbol pattern) is tested respectively. Our system can output trend of these signals accurately and have a high accuracy in the recognition of symbol pattern and Morse code. Comparing to conventional system, consumption of our system significantly decreases in a same recognition task. Meanwhile, we give the detail introduction and demonstration of our system universality.         ",
    "url": "https://arxiv.org/abs/2408.05846",
    "authors": [
      "Jialin Liu",
      "Diansheng Liao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.05905",
    "title": "Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts",
    "abstract": "           Current weakly supervised video anomaly detection (WSVAD) task aims to achieve frame-level anomalous event detection with only coarse video-level annotations available. Existing works typically involve extracting global features from full-resolution video frames and training frame-level classifiers to detect anomalies in the temporal dimension. However, most anomalous events tend to occur in localized spatial regions rather than the entire video frames, which implies existing frame-level feature based works may be misled by the dominant background information and lack the interpretation of the detected anomalies. To address this dilemma, this paper introduces a novel method called STPrompt that learns spatio-temporal prompt embeddings for weakly supervised video anomaly detection and localization (WSVADL) based on pre-trained vision-language models (VLMs). Our proposed method employs a two-stream network structure, with one stream focusing on the temporal dimension and the other primarily on the spatial dimension. By leveraging the learned knowledge from pre-trained VLMs and incorporating natural motion priors from raw videos, our model learns prompt embeddings that are aligned with spatio-temporal regions of videos (e.g., patches of individual frames) for identify specific local regions of anomalies, enabling accurate video anomaly detection while mitigating the influence of background information. Without relying on detailed spatio-temporal annotations or auxiliary object detection/tracking, our method achieves state-of-the-art performance on three public benchmarks for the WSVADL task.         ",
    "url": "https://arxiv.org/abs/2408.05905",
    "authors": [
      "Peng Wu",
      "Xuerong Zhou",
      "Guansong Pang",
      "Zhiwei Yang",
      "Qingsen Yan",
      "Peng Wang",
      "Yanning Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2408.05940",
    "title": "Spb3DTracker: A Robust LiDAR-Based Person Tracker for Noisy Environment",
    "abstract": "           Person detection and tracking (PDT) has seen significant advancements with 2D camera-based systems in the autonomous vehicle field, leading to widespread adoption of these algorithms. However, growing privacy concerns have recently emerged as a major issue, prompting a shift towards LiDAR-based PDT as a viable alternative. Within this domain, \"Tracking-by-Detection\" (TBD) has become a prominent methodology. Despite its effectiveness, LiDAR-based PDT has not yet achieved the same level of performance as camera-based PDT. This paper examines key components of the LiDAR-based PDT framework, including detection post-processing, data association, motion modeling, and lifecycle management. Building upon these insights, we introduce SpbTrack, a robust person tracker designed for diverse environments. Our method achieves superior performance on noisy datasets and state-of-the-art results on KITTI Dataset benchmarks and custom office indoor dataset among LiDAR-based trackers.         ",
    "url": "https://arxiv.org/abs/2408.05940",
    "authors": [
      "Eunsoo Im",
      "Changhyun Jee",
      "Jung Kwon Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.06240",
    "title": "Decentralized Intelligence Health Network (DIHN)",
    "abstract": "           Decentralized Health Intelligence Network (DHIN) is a theoretical framework addressing significant challenges of health data sovereignty and AI utilization in healthcare caused by data fragmentation across providers and institutions. It establishes a sovereign architecture for healthcare provision as a prerequisite to a sovereign health network, then facilitates effective AI utilization by overcoming barriers to accessing diverse medical data sources. This comprehensive framework leverages: 1) self-sovereign identity architecture coupled with a personal health record (PHR) as a prerequisite for health data sovereignty; 2) a scalable federated learning (FL) protocol implemented on a public blockchain for decentralized AI training in healthcare, where health data remains with participants and only model parameter updates are shared; and 3) a scalable, trustless rewards mechanism to incentivize participation and ensure fair reward distribution. This framework ensures that no entity can prevent or control access to training on health data offered by participants or determine financial benefits, as these processes operate on a public blockchain with an immutable record and without a third party. It supports effective AI training in healthcare, allowing patients to maintain control over their health data, benefit financially, and contribute to a decentralized, scalable ecosystem that leverages collective AI to develop beneficial healthcare algorithms. Patients receive rewards into their digital wallets as an incentive to opt-in to the FL protocol, with a long-term roadmap to funding decentralized insurance solutions. This approach introduces a novel, self-financed healthcare model that adapts to individual needs, complements existing systems, and redefines universal coverage. It highlights the potential to transform healthcare data management and AI utilization while empowering patients.         ",
    "url": "https://arxiv.org/abs/2408.06240",
    "authors": [
      "Abraham Nash"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2304.11140",
    "title": "Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs",
    "abstract": "           We study the convergence of message passing graph neural networks on random graph models to their continuous counterpart as the number of nodes tends to infinity. Until now, this convergence was only known for architectures with aggregation functions in the form of normalized means, or, equivalently, of an application of classical operators like the adjacency matrix or the graph Laplacian. We extend such results to a large class of aggregation functions, that encompasses all classically used message passing graph neural networks, such as attention-based message passing, max convolutional message passing, (degree-normalized) convolutional message passing, or moment-based aggregation message passing. Under mild assumptions, we give non-asymptotic bounds with high probability to quantify this convergence. Our main result is based on the McDiarmid inequality. Interestingly, this result does not apply to the case where the aggregation is a coordinate-wise maximum. We treat this case separately and obtain a different convergence rate.         ",
    "url": "https://arxiv.org/abs/2304.11140",
    "authors": [
      "Matthieu Cordonnier",
      "Nicolas Keriven",
      "Nicolas Tremblay",
      "Samuel Vaiter"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.16422",
    "title": "Neural networks can detect model-free static arbitrage strategies",
    "abstract": "           In this paper we demonstrate both theoretically as well as numerically that neural networks can detect model-free static arbitrage opportunities whenever the market admits some. Due to the use of neural networks, our method can be applied to financial markets with a high number of traded securities and ensures almost immediate execution of the corresponding trading strategies. To demonstrate its tractability, effectiveness, and robustness we provide examples using real financial data. From a technical point of view, we prove that a single neural network can approximately solve a class of convex semi-infinite programs, which is the key result in order to derive our theoretical results that neural networks can detect model-free static arbitrage strategies whenever the financial market admits such opportunities.         ",
    "url": "https://arxiv.org/abs/2306.16422",
    "authors": [
      "Ariel Neufeld",
      "Julian Sester"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Mathematical Finance (q-fin.MF)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2401.03951",
    "title": "The Robust Bilevel Selection Problem",
    "abstract": "           In bilevel optimization problems, a leader and a follower make their decisions in a hierarchy, and both decisions may influence each other. Usually one assumes that both players have full knowledge also of the other player's data. In a more realistic model, uncertainty can be quantified, e.g., using the robust optimization approach: We assume that the leader does not know the follower's objective precisely, but only up to some uncertainty set, and her aim is to optimize the worst case of the corresponding scenarios. Now the question arises how the complexity of bilevel optimization changes under the additional complications of this uncertainty. We make a further step towards answering this question by examining an easy bilevel problem. In the Bilevel Selection Problem (BSP), the leader and the follower each select some items from their own item set, while a common number of items to select in total is given, and each player minimizes the total costs of the selected items, according to different sets of item costs. We show that the BSP can be solved in polynomial time and then investigate its robust version. If the two players' item sets are disjoint, it can still be solved in polynomial time for several types of uncertainty sets. Otherwise, we show that the Robust BSP is NP-hard and present a 2-approximation algorithm and exact exponential-time approaches. Furthermore, we investigate variants of the BSP where one or both of the two players take a continuous decision. One variant leads to an example of a bilevel optimization problem whose optimal value may not be attained. For the Robust Continuous BSP, where all variables are continuous, we also develop a new approach for the setting of discrete uncorrelated uncertainty, which gives a polynomial-time algorithm for the Robust Continuous BSP and a pseudopolynomial-time algorithm for the Robust Bilevel Continuous Knapsack Problem.         ",
    "url": "https://arxiv.org/abs/2401.03951",
    "authors": [
      "Dorothee Henke"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2404.18560",
    "title": "Non-convex Pose Graph Optimization in SLAM via Proximal Linearized Riemannian ADMM",
    "abstract": "           Pose graph optimization (PGO) is a well-known technique for solving the pose-based simultaneous localization and mapping (SLAM) problem. In this paper, we represent the rotation and translation by a unit quaternion and a three-dimensional vector, and propose a new PGO model based on the von Mises-Fisher distribution. The constraints derived from the unit quaternions are spherical manifolds, and the projection onto the constraints can be calculated by normalization. Then a proximal linearized Riemannian alternating direction method of multipliers (PieADMM) is developed to solve the proposed model, which not only has low memory requirements, but also can update the poses in parallel. Furthermore, we establish the iteration complexity of $O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of our model. The efficiency of our proposed algorithm is demonstrated by numerical experiments on two synthetic and four 3D SLAM benchmark datasets.         ",
    "url": "https://arxiv.org/abs/2404.18560",
    "authors": [
      "Xin Chen",
      "Chunfeng Cui",
      "Deren Han",
      "Liqun Qi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2408.05623",
    "title": "Behavioral and Topological Heterogeneities in Network Versions of Schelling's Segregation Model",
    "abstract": "           Agent-based models of residential segregation have been of persistent interest to various research communities since their origin with James Sakoda and popularization by Thomas Schelling. Frequently, these models have sought to elucidate the extent to which the collective dynamics of individual preferences may cause segregation to emerge. This open question has sustained relevance in U.S. jurisprudence. Previous investigation of heterogeneity of behaviors (preferences) by Xie & Zhou has shown reductions in segregation. Meanwhile, previous investigation of heterogeneity of social network topologies by Gandica, Gargiulo, and Carletti has shown no significant impact to observed segregation levels. In the present study, we examined effects of the concurrent presence of both behavioral and topological heterogeneities in network segregation models. Simulations were conducted using both Schelling's and Xie & Zhou's preference models on 2D lattices with varied levels of densification to create topological heterogeneities (i.e., clusters, hubs). Results show a richer variety of outcomes, including novel differences in resultant segregation levels and hub composition. Notably, with concurrent increased representations of heterogeneous preferences and heterogenous topologies, reduced levels of segregation emerge. Simultaneously, we observe a novel dynamic of segregation between tolerance levels as highly tolerant nodes take residence in dense areas and push intolerant nodes to sparse areas mimicking the urban-rural divide.         ",
    "url": "https://arxiv.org/abs/2408.05623",
    "authors": [
      "Will Deter",
      "Hiroki Sayama"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ]
  }
]