[
  {
    "id": "arXiv:2310.02269",
    "title": "ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph  Convolution",
    "abstract": "In the realm of modern service-oriented architecture, ensuring Quality of Service (QoS) is of paramount importance. The ability to predict QoS values in advance empowers users to make informed decisions. However, achieving accurate QoS predictions in the presence of various issues and anomalies, including outliers, data sparsity, grey-sheep instances, and cold-start scenarios, remains a challenge. Current state-of-the-art methods often fall short when addressing these issues simultaneously, resulting in performance degradation. In this paper, we introduce a real-time QoS prediction framework (called ARRQP) with a specific emphasis on improving resilience to anomalies in the data. ARRQP utilizes the power of graph convolution techniques to capture intricate relationships and dependencies among users and services, even when the data is limited or sparse. ARRQP integrates both contextual information and collaborative insights, enabling a comprehensive understanding of user-service interactions. By utilizing robust loss functions, ARRQP effectively reduces the impact of outliers during the model training. Additionally, we introduce a sparsity-resilient grey-sheep detection method, which is subsequently treated separately for QoS prediction. Furthermore, we address the cold-start problem by emphasizing contextual features over collaborative features. Experimental results on the benchmark WS-DREAM dataset demonstrate the framework's effectiveness in achieving accurate and timely QoS predictions. ",
    "url": "https://arxiv.org/abs/2310.02269",
    "authors": [
      "Suraj Kumar",
      "Soumi Chattopadhyay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02275",
    "title": "MuSe-GNN: Learning Unified Gene Representation From Multimodal  Biological Graph Data",
    "abstract": "Discovering genes with similar functions across diverse biomedical contexts poses a significant challenge in gene representation learning due to data heterogeneity. In this study, we resolve this problem by introducing a novel model called Multimodal Similarity Learning Graph Neural Network, which combines Multimodal Machine Learning and Deep Graph Neural Networks to learn gene representations from single-cell sequencing and spatial transcriptomic data. Leveraging 82 training datasets from 10 tissues, three sequencing techniques, and three species, we create informative graph structures for model training and gene representations generation, while incorporating regularization with weighted similarity learning and contrastive learning to learn cross-data gene-gene relationships. This novel design ensures that we can offer gene representations containing functional similarity across different contexts in a joint space. Comprehensive benchmarking analysis shows our model's capacity to effectively capture gene function similarity across multiple modalities, outperforming state-of-the-art methods in gene representation learning by up to 97.5%. Moreover, we employ bioinformatics tools in conjunction with gene representations to uncover pathway enrichment, regulation causal networks, and functions of disease-associated or dosage-sensitive genes. Therefore, our model efficiently produces unified gene representations for the analysis of gene functions, tissue functions, diseases, and species evolution. ",
    "url": "https://arxiv.org/abs/2310.02275",
    "authors": [
      "Tianyu Liu",
      "Yuge Wang",
      "Rex Ying",
      "Hongyu Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)"
    ]
  },
  {
    "id": "arXiv:2310.02280",
    "title": "Expert enhanced dynamic time warping based anomaly detection",
    "abstract": "Dynamic time warping (DTW) is a well-known algorithm for time series elastic dissimilarity measure. Its ability to deal with non-linear time distortions makes it helpful in variety of data mining tasks. Such a task is also anomaly detection which attempts to reveal unexpected behaviour without false detection alarms. In this paper, we propose a novel anomaly detection method named Expert enhanced dynamic time warping anomaly detection (E-DTWA). It is based on DTW with additional enhancements involving human-in-the-loop concept. The main benefits of our approach comprise efficient detection, flexible retraining based on strong consideration of the expert's detection feedback while retaining low computational and space complexity. ",
    "url": "https://arxiv.org/abs/2310.02280",
    "authors": [
      "Matej Kloska",
      "Gabriela Grmanova",
      "Viera Rozinajova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02282",
    "title": "SWMLP: Shared Weight Multilayer Perceptron for Car Trajectory Speed  Prediction using Road Topographical Features",
    "abstract": "Although traffic is one of the massively collected data, it is often only available for specific regions. One concern is that, although there are studies that give good results for these data, the data from these regions may not be sufficiently representative to describe all the traffic patterns in the rest of the world. In quest of addressing this concern, we propose a speed prediction method that is independent of large historical speed data. To predict a vehicle's speed, we use the trajectory road topographical features to fit a Shared Weight Multilayer Perceptron learning model. Our results show significant improvement, both qualitative and quantitative, over standard regression analysis. Moreover, the proposed framework sheds new light on the way to design new approaches for traffic analysis. ",
    "url": "https://arxiv.org/abs/2310.02282",
    "authors": [
      "Sarah Almeida Carneiro",
      "Giovanni Chierchia",
      "Jean Charl\u00e9ty",
      "Aur\u00e9lie Chataignon",
      "Laurent Najman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02284",
    "title": "PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation  gating for fine-grained crowd flow prediction",
    "abstract": "Understanding the movement patterns of objects (e.g., humans and vehicles) in a city is essential for many applications, including city planning and management. This paper proposes a method for predicting future city-wide crowd flows by modeling the spatio-temporal patterns of historical crowd flows in fine-grained city-wide maps. We introduce a novel neural network named PArallel Spatio-Temporal Attention with spatial auto-correlation gating (PASTA) that effectively captures the irregular spatio-temporal patterns of fine-grained maps. The novel components in our approach include spatial auto-correlation gating, multi-scale residual block, and temporal attention gating module. The spatial auto-correlation gating employs the concept of spatial statistics to identify irregular spatial regions. The multi-scale residual block is responsible for handling multiple range spatial dependencies in the fine-grained map, and the temporal attention gating filters out irrelevant temporal information for the prediction. The experimental results demonstrate that our model outperforms other competing baselines, especially under challenging conditions that contain irregular spatial regions. We also provide a qualitative analysis to derive the critical time information where our model assigns high attention scores in prediction. ",
    "url": "https://arxiv.org/abs/2310.02284",
    "authors": [
      "Chung Park",
      "Junui Hong",
      "Cheonbok Park",
      "Taesan Kim",
      "Minsung Choi",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02294",
    "title": "Beyond-Accuracy: A Review on Diversity, Serendipity and Fairness in  Recommender Systems Based on Graph Neural Networks",
    "abstract": "By providing personalized suggestions to users, recommender systems have become essential to numerous online platforms. Collaborative filtering, particularly graph-based approaches using Graph Neural Networks (GNNs), have demonstrated great results in terms of recommendation accuracy. However, accuracy may not always be the most important criterion for evaluating recommender systems' performance, since beyond-accuracy aspects such as recommendation diversity, serendipity, and fairness can strongly influence user engagement and satisfaction. This review paper focuses on addressing these dimensions in GNN-based recommender systems, going beyond the conventional accuracy-centric perspective. We begin by reviewing recent developments in approaches that improve not only the accuracy-diversity trade-off but also promote serendipity and fairness in GNN-based recommender systems. We discuss different stages of model development including data preprocessing, graph construction, embedding initialization, propagation layers, embedding fusion, score computation, and training methodologies. Furthermore, we present a look into the practical difficulties encountered in assuring diversity, serendipity, and fairness, while retaining high accuracy. Finally, we discuss potential future research directions for developing more robust GNN-based recommender systems that go beyond the unidimensional perspective of focusing solely on accuracy. This review aims to provide researchers and practitioners with an in-depth understanding of the multifaceted issues that arise when designing GNN-based recommender systems, setting our work apart by offering a comprehensive exploration of beyond-accuracy dimensions. ",
    "url": "https://arxiv.org/abs/2310.02294",
    "authors": [
      "Tomislav Duricic",
      "Dominik Kowald",
      "Emanuel Lacic",
      "Elisabeth Lex"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ]
  },
  {
    "id": "arXiv:2310.02295",
    "title": "Unsupervised Complex Semi-Binary Matrix Factorization for Activation  Sequence Recovery of Quasi-Stationary Sources",
    "abstract": "Advocating for a sustainable, resilient and human-centric industry, the three pillars of Industry 5.0 call for an increased understanding of industrial processes and manufacturing systems, as well as their energy sustainability. One of the most fundamental elements of comprehension is knowing when the systems are operated, as this is key to locating energy intensive subsystems and operations. Such knowledge is often lacking in practice. Activation statuses can be recovered from sensor data though. Some non-intrusive sensors (accelerometers, current sensors, etc.) acquire mixed signals containing information about multiple actuators at once. Despite their low cost as regards the fleet of systems they monitor, additional signal processing is required to extract the individual activation sequences. To that end, sparse regression techniques can extract leading dynamics in sequential data. Notorious dictionary learning algorithms have proven effective in this regard. This paper considers different industrial settings in which the identification of binary subsystem activation sequences is sought. In this context, it is assumed that each sensor measures an extensive physical property, source signals are periodic, quasi-stationary and independent, albeit these signals may be correlated and their noise distribution is arbitrary. Existing methods either restrict these assumptions, e.g., by imposing orthogonality or noise characteristics, or lift them using additional assumptions, typically using nonlinear transforms. ",
    "url": "https://arxiv.org/abs/2310.02295",
    "authors": [
      "Romain Delabeye",
      "Martin Ghienne",
      "Olivia Penas",
      "Jean-Luc Dion"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2310.02298",
    "title": "Prompting Audios Using Acoustic Properties For Emotion Representation",
    "abstract": "Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset. ",
    "url": "https://arxiv.org/abs/2310.02298",
    "authors": [
      "Hira Dhamyal",
      "Benjamin Elizalde",
      "Soham Deshmukh",
      "Huaming Wang",
      "Bhiksha Raj",
      "Rita Singh"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2310.02304",
    "title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation",
    "abstract": "Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a \"scaffolding\" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed \"improver\" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our proof-of-concept experiments, is capable of writing code that can call itself to improve itself. We critically consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox. ",
    "url": "https://arxiv.org/abs/2310.02304",
    "authors": [
      "Eric Zelikman",
      "Eliana Lorch",
      "Lester Mackey",
      "Adam Tauman Kalai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2310.02339",
    "title": "Safe and Robust Robot Behavior Planning via Constraint Programming",
    "abstract": "The safe operation of an autonomous system is a complex endeavor, one pivotal element being its decision-making. Decision-making logic can formally be analyzed using model checking or other formal verification approaches. Yet, the non-deterministic nature of realistic environments makes these approaches rather troublesome and often impractical. Constraint-based planning approaches such as Tumato have been shown to be capable of generating policies for a system to reach a stated goal and abiding safety constraints, with guarantees of soundness and completeness by construction. However, uncertain outcomes of actions in the environment are not explicitly modeled or accounted for, severely limiting the expressiveness of Tumato. In this work, we extend Tumato with support for non-deterministic outcomes of actions. Actions have a specific intended result yet can be modeled to have alternative outcomes that may realistically occur. The adapted solver generates a policy that enables reaching the goals in a safe manner, even when alternative outcomes of actions occur. Furthermore, we introduce a purely declarative way of defining safety in Tumato, increasing its expressiveness. Finally, the addition of cost or duration values to actions enables the solver to restore safety when necessary, in the most preferred way. ",
    "url": "https://arxiv.org/abs/2310.02339",
    "authors": [
      "Jan Vermaelen",
      "Tom Holvoet"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2310.02350",
    "title": "Neuromimetic Dynamic Networks with Hebbian Learning",
    "abstract": "Leveraging recent advances in neuroscience and control theory, this paper presents a neuromimetic network model with dynamic symmetric connections governed by Hebbian learning rules. Formal analysis grounded in graph theory and classical control establishes that this biologically plausible model exhibits boundedness, stability, and structural controllability given a generalized sym-cactus structure with multiple control nodes. We prove the necessity of this topology when there are distributed control inputs. Simulations using a 14-node generalized sym-cactus network with two input types validate the model's effectiveness in capturing key neural dynamics. ",
    "url": "https://arxiv.org/abs/2310.02350",
    "authors": [
      "Zexin Sun",
      "John Baillieul"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2310.02361",
    "title": "Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle  Avoidance",
    "abstract": "Autonomous obstacle avoidance is of vital importance for an intelligent agent such as a mobile robot to navigate in its environment. Existing state-of-the-art methods train a spiking neural network (SNN) with deep reinforcement learning (DRL) to achieve energy-efficient and fast inference speed in complex/unknown scenes. These methods typically assume that the environment is static while the obstacles in real-world scenes are often dynamic. The movement of obstacles increases the complexity of the environment and poses a great challenge to the existing methods. In this work, we approach robust dynamic obstacle avoidance twofold. First, we introduce the neuromorphic vision sensor (i.e., event camera) to provide motion cues complementary to the traditional Laser depth data for handling dynamic obstacles. Second, we develop an DRL-based event-enhanced multimodal spiking actor network (EEM-SAN) that extracts information from motion events data via unsupervised representation learning and fuses Laser and event camera data with learnable thresholding. Experiments demonstrate that our EEM-SAN outperforms state-of-the-art obstacle avoidance methods by a significant margin, especially for dynamic obstacle avoidance. ",
    "url": "https://arxiv.org/abs/2310.02361",
    "authors": [
      "Yang Wang",
      "Bo Dong",
      "Yuji Zhang",
      "Yunduo Zhou",
      "Haiyang Mei",
      "Ziqi Wei",
      "Xin Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2310.02372",
    "title": "ProtoNER: Few shot Incremental Learning for Named Entity Recognition  using Prototypical Networks",
    "abstract": "Key value pair (KVP) extraction or Named Entity Recognition(NER) from visually rich documents has been an active area of research in document understanding and data extraction domain. Several transformer based models such as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art results. However, addition of even a single new class to the existing model requires (a) re-annotation of entire training dataset to include this new class and (b) retraining the model again. Both of these issues really slow down the deployment of updated model. \\\\ We present \\textbf{ProtoNER}: Prototypical Network based end-to-end KVP extraction model that allows addition of new classes to an existing model while requiring minimal number of newly annotated training samples. The key contributions of our model are: (1) No dependency on dataset used for initial training of the model, which alleviates the need to retain original training dataset for longer duration as well as data re-annotation which is very time consuming task, (2) No intermediate synthetic data generation which tends to add noise and results in model's performance degradation, and (3) Hybrid loss function which allows model to retain knowledge about older classes as well as learn about newly added classes.\\\\ Experimental results show that ProtoNER finetuned with just 30 samples is able to achieve similar results for the newly added classes as that of regular model finetuned with 2600 samples. ",
    "url": "https://arxiv.org/abs/2310.02372",
    "authors": [
      "Ritesh Kumar",
      "Saurabh Goyal",
      "Ashish Verma",
      "Vatche Isahagian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02380",
    "title": "Non-blocking Dynamic Unbounded Graphs with Wait-Free Snapshot",
    "abstract": "Graphs are arguably one of the most fundamental data-structure used in many domains such as block-chain, networks etc. Theoretically and practically, improving Graph performance is one of the most studied and omnipresent research problems. In this paper, we have implemented a dynamic unbounded concurrent graph which can perform the add, delete or lookup operations on vertices and edges concurrently. All these methods are lock-free and linearizable. On top of this, we have also implemented the wait-free graph snapshot algorithm. To the best of knowledge this is first wait-free implementation of snapshot on concurrent graphs. We have used the snapshot of the algorithm to calculate the diameter and between centrality. We have compared our implementation with its counterparts and outperformed them by a good margin. This illustrates the efficiency of our snapshot method which is a generic method and can be used to perform other useful graph analytics operations. ",
    "url": "https://arxiv.org/abs/2310.02380",
    "authors": [
      "Gaurav Bhardwaj",
      "Sathya Peri",
      "Pratik Shetty"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2310.02386",
    "title": "ScaleNet: An Unsupervised Representation Learning Method for Limited  Information",
    "abstract": "Although large-scale labeled data are essential for deep convolutional neural networks (ConvNets) to learn high-level semantic visual representations, it is time-consuming and impractical to collect and annotate large-scale datasets. A simple and efficient unsupervised representation learning method named ScaleNet based on multi-scale images is proposed in this study to enhance the performance of ConvNets when limited information is available. The input images are first resized to a smaller size and fed to the ConvNet to recognize the rotation degree. Next, the ConvNet learns the rotation-prediction task for the original size images based on the parameters transferred from the previous model. The CIFAR-10 and ImageNet datasets are examined on different architectures such as AlexNet and ResNet50 in this study. The current study demonstrates that specific image features, such as Harris corner information, play a critical role in the efficiency of the rotation-prediction task. The ScaleNet supersedes the RotNet by ~7% in the limited CIFAR-10 dataset. The transferred parameters from a ScaleNet model with limited data improve the ImageNet Classification task by about 6% compared to the RotNet model. This study shows the capability of the ScaleNet method to improve other cutting-edge models such as SimCLR by learning effective features for classification tasks. ",
    "url": "https://arxiv.org/abs/2310.02386",
    "authors": [
      "Huili Huang",
      "M. Mahdi Roozbahani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02394",
    "title": "Estimating systemic importance with missing data in input-output graphs",
    "abstract": "In the context of the Cobb-Douglas productivity model we consider the $N \\times N$ input-output linkage matrix $W$ for a network of $N$ firms $f_1, f_2, \\cdots, f_N$. The associated influence vector $v_w$ of $W$ is defined in terms of the Leontief inverse $L_W$ of $W$ as $v_W = \\frac{\\alpha}{N} L_W \\vec{\\mathbf{1}}$ where $L_W = (I - (1-\\alpha) W')^{-1}$, $W'$ denotes the transpose of $W$ and $I$ is the identity matrix. Here $\\vec{\\mathbf{1}}$ is the $N \\times 1$ vector whose entries are all one. The influence vector is a metric of the importance for the firms in the production network. Under the realistic assumption that the data to compute the influence vector is incomplete, we prove bounds on the worst-case error for the influence vector that are sharp up to a constant factor. We also consider the situation where the missing data is binomially distributed and contextualize the bound on the influence vector accordingly. We also investigate how far off the influence vector can be when we only have data on nodes and connections that are within distance $k$ of some source node. A comparison of our results is juxtaposed against PageRank analogues. We close with a discussion on a possible extension beyond Cobb-Douglas to the Constant Elasticity of Substitution model, as well as the possibility of considering other probability distributions for missing data. ",
    "url": "https://arxiv.org/abs/2310.02394",
    "authors": [
      "Jesse Geneson",
      "Alvin Moon",
      "Nicolas Robles",
      "Aaron Strong",
      "Jonathan Welburn"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ]
  },
  {
    "id": "arXiv:2310.02396",
    "title": "Implicit regularization of multi-task learning and finetuning in  overparameterized neural networks",
    "abstract": "It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or \"lazy\") regime and the feature learning (\"rich\") regime identified in prior work. Moreover, PT+FT can exhibit a novel \"nested feature learning\" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned during pretraining. In ReLU networks, we reproduce all of these qualitative behaviors. We also observe that PT+FT (but not MTL) is biased to learn features that are correlated with (but distinct from) those needed for the auxiliary task, while MTL is biased toward using identical features for both tasks. As a result, we find that in realistic settings, MTL generalizes better when comparatively little data is available for the task of interest, while PT+FT outperforms it with more data available. We show that our findings hold qualitatively for a deep architecture trained on image classification tasks. Our characterization of the nested feature learning regime also motivates a modification to PT+FT that we find empirically improves performance. Overall, our results shed light on the impact of auxiliary task learning and suggest ways to leverage it more effectively. ",
    "url": "https://arxiv.org/abs/2310.02396",
    "authors": [
      "Jack W. Lindsey",
      "Samuel Lippl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02408",
    "title": "MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis  of Ethereum-based Decentralised Applications",
    "abstract": "This paper presents MindTheDApp, a toolchain designed specifically for the structural analysis of Ethereum-based Decentralized Applications (DApps), with a distinct focus on a complex network-driven approach. Unlike existing tools, our toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST) traversal techniques to transform the architecture and interactions within smart contracts into a specialized bipartite graph. This enables advanced network analytics to highlight operational efficiencies within the DApp's architecture. The bipartite graph generated by the proposed tool comprises two sets of nodes: one representing smart contracts, interfaces, and libraries, and the other including functions, events, and modifiers. Edges in the graph connect functions to smart contracts they interact with, offering a granular view of interdependencies and execution flow within the DApp. This network-centric approach allows researchers and practitioners to apply complex network theory in understanding the robustness, adaptability, and intricacies of decentralized systems. Our work contributes to the enhancement of security in smart contracts by allowing the visualisation of the network, and it provides a deep understanding of the architecture and operational logic within DApps. Given the growing importance of smart contracts in the blockchain ecosystem and the emerging application of complex network theory in technology, our toolchain offers a timely contribution to both academic research and practical applications in the field of blockchain technology. ",
    "url": "https://arxiv.org/abs/2310.02408",
    "authors": [
      "Giacomo Ibba",
      "Sabrina Aufiero",
      "Silvia Bartolucci",
      "Rumyana Neykova",
      "Marco Ortu",
      "Roberto Tonelli",
      "Giuseppe Destefanis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.02410",
    "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit  Quantization and Robustness",
    "abstract": "Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit expert weights can deliver better model performance than the dense model trained on the same dataset. As a result of low-bit quantization, we show the model size can be reduced by 79.6% of the original half precision floating point (fp16) MoE model. Combined with an optimized GPU runtime implementation, it also achieves 1.24X speed-up on A100 GPUs. ",
    "url": "https://arxiv.org/abs/2310.02410",
    "authors": [
      "Young Jin Kim",
      "Raffy Fahim",
      "Hany Hassan Awadalla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.02417",
    "title": "Jailbreaker in Jail: Moving Target Defense for Large Language Models",
    "abstract": "Large language models (LLMs), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks. Researchers have found that current commercial LLMs either fail to be \"harmless\" by presenting unethical answers, or fail to be \"helpful\" by refusing to offer meaningful answers when faced with adversarial queries. To strike a balance between being helpful and harmless, we design a moving target defense (MTD) enhanced LLM system. The system aims to deliver non-toxic answers that align with outputs from multiple model candidates, making them more robust against adversarial attacks. We design a query and output analysis model to filter out unsafe or non-responsive answers. %to achieve the two objectives of randomly selecting outputs from different LLMs. We evaluate over 8 most recent chatbot models with state-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the attack success rate from 37.5\\% to 0\\%. Meanwhile, it decreases the response refusal rate from 50\\% to 0\\%. ",
    "url": "https://arxiv.org/abs/2310.02417",
    "authors": [
      "Bocheng Chen",
      "Advait Paliwal",
      "Qiben Yan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2310.02428",
    "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields  for Atomistic Simulations",
    "abstract": "Equivariant graph neural networks force fields (EGraFFs) have shown great promise in modelling complex interactions in atomic systems by exploiting the graphs' inherent symmetries. Recent works have led to a surge in the development of novel architectures that incorporate equivariance-based inductive biases alongside architectural innovations like graph transformers and message passing to model atomic interactions. However, thorough evaluations of these deploying EGraFFs for the downstream task of real-world atomistic simulations, is lacking. To this end, here we perform a systematic benchmarking of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet), with the aim of understanding their capabilities and limitations for realistic atomistic simulations. In addition to our thorough evaluation and analysis on eight existing datasets based on the benchmarking literature, we release two new benchmark datasets, propose four new metrics, and three new challenging tasks. The new datasets and tasks evaluate the performance of EGraFF to out-of-distribution data, in terms of different crystal structures, temperatures, and new molecules. Interestingly, evaluation of the EGraFF models based on dynamic simulations reveals that having a lower error on energy or force does not guarantee stable or reliable simulation or faithful replication of the atomic structures. Moreover, we find that no model clearly outperforms other models on all datasets and tasks. Importantly, we show that the performance of all the models on out-of-distribution datasets is unreliable, pointing to the need for the development of a foundation model for force fields that can be used in real-world simulations. In summary, this work establishes a rigorous framework for evaluating machine learning force fields in the context of atomic simulations and points to open research challenges within this domain. ",
    "url": "https://arxiv.org/abs/2310.02428",
    "authors": [
      "Vaibhav Bihani",
      "Utkarsh Pratiush",
      "Sajid Mannan",
      "Tao Du",
      "Zhimin Chen",
      "Santiago Miret",
      "Matthieu Micoulaut",
      "Morten M Smedskjaer",
      "Sayan Ranu",
      "N M Anoop Krishnan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ]
  },
  {
    "id": "arXiv:2310.02430",
    "title": "Episodic Memory Theory for the Mechanistic Interpretation of Recurrent  Neural Networks",
    "abstract": "Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. We show that the privileged basis enhances the interpretability of the learned parameters and hidden states of RNNs. Our work represents a step toward demystifying the internal mechanisms of RNNs and, for computational neuroscience, serves to bridge the gap between artificial neural networks and neural memory models. ",
    "url": "https://arxiv.org/abs/2310.02430",
    "authors": [
      "Arjun Karuvally",
      "Peter Delmastro",
      "Hava T. Siegelmann"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02431",
    "title": "Can Large Language Models Provide Security & Privacy Advice? Measuring  the Ability of LLMs to Refute Misconceptions",
    "abstract": "Users seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT). ",
    "url": "https://arxiv.org/abs/2310.02431",
    "authors": [
      "Yufan Chen",
      "Arjun Arunasalam",
      "Z. Berkay Celik"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.02437",
    "title": "EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields",
    "abstract": "We present EvDNeRF, a pipeline for generating event data and training an event-based dynamic NeRF, for the purpose of faithfully reconstructing eventstreams on scenes with rigid and non-rigid deformations that may be too fast to capture with a standard camera. Event cameras register asynchronous per-pixel brightness changes at MHz rates with high dynamic range, making them ideal for observing fast motion with almost no motion blur. Neural radiance fields (NeRFs) offer visual-quality geometric-based learnable rendering, but prior work with events has only considered reconstruction of static scenes. Our EvDNeRF can predict eventstreams of dynamic scenes from a static or moving viewpoint between any desired timestamps, thereby allowing it to be used as an event-based simulator for a given scene. We show that by training on varied batch sizes of events, we can improve test-time predictions of events at fine time resolutions, outperforming baselines that pair standard dynamic NeRFs with event simulators. We release our simulated and real datasets, as well as code for both event-based data generation and the training of event-based dynamic NeRF models (https://github.com/anish-bhattacharya/EvDNeRF). ",
    "url": "https://arxiv.org/abs/2310.02437",
    "authors": [
      "Anish Bhattacharya",
      "Ratnesh Madaan",
      "Fernando Cladera",
      "Sai Vemprala",
      "Rogerio Bonatti",
      "Kostas Daniilidis",
      "Ashish Kapoor",
      "Vijay Kumar",
      "Nikolai Matni",
      "Jayesh K. Gupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02451",
    "title": "Backdoor Adjustment of Confounding by Provenance for Robust Text  Classification of Multi-institutional Clinical Notes",
    "abstract": "Natural Language Processing (NLP) methods have been broadly applied to clinical tasks. Machine learning and deep learning approaches have been used to improve the performance of clinical NLP. However, these approaches require sufficiently large datasets for training, and trained models have been shown to transfer poorly across sites. These issues have led to the promotion of data collection and integration across different institutions for accurate and portable models. However, this can introduce a form of bias called confounding by provenance. When source-specific data distributions differ at deployment, this may harm model performance. To address this issue, we evaluate the utility of backdoor adjustment for text classification in a multi-site dataset of clinical notes annotated for mentions of substance abuse. Using an evaluation framework devised to measure robustness to distributional shifts, we assess the utility of backdoor adjustment. Our results indicate that backdoor adjustment can effectively mitigate for confounding shift. ",
    "url": "https://arxiv.org/abs/2310.02451",
    "authors": [
      "Xiruo Ding",
      "Zhecheng Sheng",
      "Meliha Yeti\u015fgen",
      "Serguei Pakhomov",
      "Trevor Cohen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.02466",
    "title": "Parameterized Model-checking of Discrete-Timed Networks and  Symmetric-Broadcast Systems",
    "abstract": "We study the complexity of the model-checking problem for discrete-timed systems with arbitrarily many anonymous and identical contributors, with and without a distinguished \"controller\" process, communicating via synchronous rendezvous. Our work extends the seminal work on untimed systems by German and Sistla adding discrete-time clocks, thus allowing one to model more realistic protocols. For the case without a controller, we show that the systems can be efficiently simulated -- and vice versa -- by systems of untimed processes communicating via rendezvous and symmetric broadcast, which we call \"RB-systems\". Symmetric broadcast is a novel communication primitive that, like ordinary asymmetric broadcast, allows all processes to synchronize without distinction between sender/receiver processes. We show that the complexity of the parameterized model-checking problem for safety specifications is pspace-complete, and for liveness specifications it is decidable in exptime. The latter result required automata theory, rational linear programming, and geometric reasoning for solving certain reachability questions in a new variant of vector addition systems called \"vector rendezvous systems\". We believe such proof techniques are of independent interest and will be useful in solving related problems. For the case with a controller, we show that the parameterized model-checking problems for RB-systems and systems with asymmetric broadcast are inter-reducible. This implies that for discrete timed-networks with a controller the parameterized model-checking problem is undecidable for liveness specifications. Our work exploits the intimate connection between discrete-timed systems and systems of processes communicating via broadcast. This allows us to prove decidability results for liveness properties of parameterized timed-systems, as well as extend work from untimed systems to timed systems. ",
    "url": "https://arxiv.org/abs/2310.02466",
    "authors": [
      "Benjamin Aminof",
      "Sasha Rubin",
      "Francesco Spegni",
      "Florian Zuleger"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2310.02469",
    "title": "Large Language Models Can Be Good Privacy Protection Learners",
    "abstract": "The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust privacy protection learners. ",
    "url": "https://arxiv.org/abs/2310.02469",
    "authors": [
      "Yijia Xiao",
      "Yiqiao Jin",
      "Yushi Bai",
      "Yue Wu",
      "Xianjun Yang",
      "Xiao Luo",
      "Wenchao Yu",
      "Xujiang Zhao",
      "Yanchi Liu",
      "Haifeng Chen",
      "Wei Wang",
      "Wei Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.02480",
    "title": "Splitting the Difference on Adversarial Training",
    "abstract": "The existence of adversarial examples points to a basic weakness of deep neural networks. One of the most effective defenses against such examples, adversarial training, entails training models with some degree of robustness, usually at the expense of a degraded natural accuracy. Most adversarial training methods aim to learn a model that finds, for each class, a common decision boundary encompassing both the clean and perturbed examples. In this work, we take a fundamentally different approach by treating the perturbed examples of each class as a separate class to be learned, effectively splitting each class into two classes: \"clean\" and \"adversarial.\" This split doubles the number of classes to be learned, but at the same time considerably simplifies the decision boundaries. We provide a theoretical plausibility argument that sheds some light on the conditions under which our approach can be expected to be beneficial. Likewise, we empirically demonstrate that our method learns robust models while attaining optimal or near-optimal natural accuracy, e.g., on CIFAR-10 we obtain near-optimal natural accuracy of $95.01\\%$ alongside significant robustness across multiple tasks. The ability to achieve such near-optimal natural accuracy, while maintaining a significant level of robustness, makes our method applicable to real-world applications where natural accuracy is at a premium. As a whole, our main contribution is a general method that confers a significant level of robustness upon classifiers with only minor or negligible degradation of their natural accuracy. ",
    "url": "https://arxiv.org/abs/2310.02480",
    "authors": [
      "Matan Levi",
      "Aryeh Kontorovich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02491",
    "title": "DON-LSTM: Multi-Resolution Learning with DeepONets and Long Short-Term  Memory Neural Networks",
    "abstract": "Deep operator networks (DeepONets, DONs) offer a distinct advantage over traditional neural networks in their ability to be trained on multi-resolution data. This property becomes especially relevant in real-world scenarios where high-resolution measurements are difficult to obtain, while low-resolution data is more readily available. Nevertheless, DeepONets alone often struggle to capture and maintain dependencies over long sequences compared to other state-of-the-art algorithms. We propose a novel architecture, named DON-LSTM, which extends the DeepONet with a long short-term memory network (LSTM). Combining these two architectures, we equip the network with explicit mechanisms to leverage multi-resolution data, as well as capture temporal dependencies in long sequences. We test our method on long-time-evolution modeling of multiple non-linear systems and show that the proposed multi-resolution DON-LSTM achieves significantly lower generalization error and requires fewer high-resolution samples compared to its vanilla counterparts. ",
    "url": "https://arxiv.org/abs/2310.02491",
    "authors": [
      "Katarzyna Micha\u0142owska",
      "Somdatta Goswami",
      "George Em Karniadakis",
      "Signe Riemer-S\u00f8rensen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02497",
    "title": "Towards an Interpretable Representation of Speaker Identity via  Perceptual Voice Qualities",
    "abstract": "Unlike other data modalities such as text and vision, speech does not lend itself to easy interpretation. While lay people can understand how to describe an image or sentence via perception, non-expert descriptions of speech often end at high-level demographic information, such as gender or age. In this paper, we propose a possible interpretable representation of speaker identity based on perceptual voice qualities (PQs). By adding gendered PQs to the pathology-focused Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) protocol, our PQ-based approach provides a perceptual latent space of the character of adult voices that is an intermediary of abstraction between high-level demographics and low-level acoustic, physical, or learned representations. Contrary to prior belief, we demonstrate that these PQs are hearable by ensembles of non-experts, and further demonstrate that the information encoded in a PQ-based representation is predictable by various speech representations. ",
    "url": "https://arxiv.org/abs/2310.02497",
    "authors": [
      "Robin Netzorg",
      "Bohan Yu",
      "Andrea Guzman",
      "Peter Wu",
      "Luna McNulty",
      "Gopala Anumanchipalli"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2310.02515",
    "title": "Community Archetypes: An Empirical Framework for Guiding Research  Methodologies to Reflect User Experiences of Sense of Virtual Community",
    "abstract": "Humans need a sense of community (SOC), and social media platforms afford opportunities to address this need by providing users with a sense of virtual community (SOVC). This paper explores SOVC on Reddit and is motivated by two goals: (1) providing researchers with an excellent resource for methodological decisions in studies of Reddit communities; and (2) creating the foundation for a new class of research methods and community support tools that reflect users' experiences of SOVC. To ensure that methods are respectfully and ethically designed in service and accountability to impacted communities, our work takes a qualitative, community-centered approach by engaging with two key stakeholder groups. First, we interviewed 21 researchers to understand how they study \"community\" on Reddit. Second, we surveyed 12 subreddits to gain insight into user experiences of SOVC. Results show that some research methods can broadly reflect users' SOVC regardless of the topic or type of subreddit. However, user responses also evidenced the existence of five distinct Community Archetypes: Topical Q&A, Learning & Perspective Broadening, Social Support, Content Generation, and Affiliation with an Entity. We offer the Community Archetypes framework to support future work in designing methods that align more closely with user experiences of SOVC and to create community support tools that can meaningfully nourish the human need for SOC/SOVC in our modern world. ",
    "url": "https://arxiv.org/abs/2310.02515",
    "authors": [
      "Gale H. Prinster",
      "C. Estelle Smith",
      "Chenhao Tan",
      "Brian C. Keegan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2310.02518",
    "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics  of Uncertainty and Prediction Error in Musical Improvisation",
    "abstract": "Musical improvisation, much like spontaneous speech, reveals intricate facets of the improviser's state of mind and emotional character. However, the specific musical components that reveal such individuality remain largely unexplored. Within the framework of brain's statistical learning and predictive processing, this study examined the temporal dynamics of uncertainty and surprise (prediction error) in a piece of musical improvisation. This study employed the HBSL model to analyze a corpus of 456 Jazz improvisations, spanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated distinctive temporal patterns of surprise and uncertainty, especially in pitch and pitch-rhythm sequences, revealing era-specific features from the early 20th to the 21st centuries. Conversely, rhythm sequences exhibited a consistent degree of uncertainty across eras. Further, the acoustic properties remain unchanged across different periods. These findings highlight the importance of how temporal dynamics of surprise and uncertainty in improvisational music change over periods, profoundly influencing the distinctive methodologies artists adopt for improvisation in each era. Further, it is suggested that the development of improvisational music can be attributed to the brain's adaptive statistical learning mechanisms, which constantly refine internal models to mirror the cultural and emotional nuances of their respective epochs. This study unravels the evolutionary trajectory of improvisational music and highlights the nuanced shifts artists employ to resonate with the cultural and emotional landscapes of their times. ",
    "url": "https://arxiv.org/abs/2310.02518",
    "authors": [
      "Tatsuya Daikoku"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Information Retrieval (cs.IR)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2310.02520",
    "title": "MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data  Augmentation",
    "abstract": "Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk prediction performance by creating synthetic patient data during training to enlarge sample space. Furthermore, MedDiffusion discerns hidden relationships between patient visits using a step-wise attention mechanism, enabling the model to automatically retain the most vital information for generating high-quality data. Experimental evaluation on four real-world medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge baselines in terms of PR-AUC, F1, and Cohen's Kappa. We also conduct ablation studies and benchmark our model against GAN-based alternatives to further validate the rationality and adaptability of our model design. Additionally, we analyze generated data to offer fresh insights into the model's interpretability. ",
    "url": "https://arxiv.org/abs/2310.02520",
    "authors": [
      "Yuan Zhong",
      "Suhan Cui",
      "Jiaqi Wang",
      "Xiaochen Wang",
      "Ziyi Yin",
      "Yaqing Wang",
      "Houping Xiao",
      "Mengdi Huai",
      "Ting Wang",
      "Fenglong Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02530",
    "title": "Identifying Vulnerability Patches by Comprehending Code Commits with  Comprehensive Change Contexts",
    "abstract": "To help application developers apply vulnerability patches timely, security researchers maintain vulnerability databases such as National Vulnerability Database (NVD). By directly monitoring NVD with the name of each used library, application developers can be aware of vulnerabilities and their patches. Given that the monitoring results of vulnerability patches are unreliable due to patch incompleteness of NVD, existing approaches employ deep-learning (DL) models to identify additional vulnerability patches by determining whether a code commit fixes a vulnerability. However, these approaches suffer from low accuracy due to not considering code commits' comprehensive contexts such as control/data-flow contexts or method-invocation contexts. To improve accuracy, we design CompVPD, the first approach to identify vulnerability patches by fine-tuning a large language model (LLM) named StarCoder to comprehend code commits with comprehensive contexts. Considering that including comprehensive contexts needs to balance the context size and the training costs of LLM, CompVPD includes our two novel algorithms to generate comprehensive contexts within the given window size by removing irrelevant components (i.e., files, methods, and statements) and adaptively expanding each context. We empirically compare CompVPD with four state-of-the-art/practice (SOTA) approaches that identify vulnerability patches. The results show that CompVPD improves the AUC score by 11% and the F1 score by 30% when compared with the best scores of the SOTA approaches. Additionally, CompVPD provides high value to security practice by helping identify 20 vulnerability patches and 18 fixes of high-risk bugs from 2,500 recent code commits of five highly popular open-source projects. ",
    "url": "https://arxiv.org/abs/2310.02530",
    "authors": [
      "Tianyu Chen",
      "Lin Li",
      "Taotao Qian",
      "Zeyu Wang",
      "Guangtai Liang",
      "Ding Li",
      "Qianxiang Wang",
      "Tao Xie"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2310.02541",
    "title": "Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data",
    "abstract": "Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting -- achieving a perfect fit to training data with near-random performance on test data -- before transitioning (\"grokking\") to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labels in the training data, exhibiting a \"grokking\" phenomenon. This provides the first theoretical result of benign overfitting in neural network classification when the data distribution is not linearly separable. Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps. ",
    "url": "https://arxiv.org/abs/2310.02541",
    "authors": [
      "Zhiwei Xu",
      "Yutong Wang",
      "Spencer Frei",
      "Gal Vardi",
      "Wei Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2310.02542",
    "title": "Tightly Joining Positioning and Control for Trustworthy Unmanned Aerial  Vehicles Based on Factor Graph Optimization in Urban Transportation",
    "abstract": "Unmanned aerial vehicles (UAV) showed great potential in improving the efficiency of parcel delivery applications in the coming smart cities era. Unfortunately, the trustworthy positioning and control algorithms of the UAV are significantly challenged in complex urban areas. For example, the ubiquitous global navigation satellite system (GNSS) positioning can be degraded by the signal reflections from surrounding high-rising buildings, leading to significantly increased positioning uncertainty. An additional challenge is introduced to the control algorithm due to the complex wind disturbances in urban canyons. Given the fact that the system positioning and control are highly correlated with each other, for example, the system dynamics of the control can largely help with the positioning, this paper proposed a joint positioning and control method (JPCM) based on factor graph optimization (FGO), which combines sensors' measurements and control intention. In particular, the positioning measurements are formulated as the factors in the factor graph model, such as the positioning from the GNSS. The model predictive control (MPC) is also formulated as the additional factors in the factor graph model. By solving the factor graph contributed by both the positioning factor and the MPC-based factors, the complementariness of positioning and control can be fully explored. To guarantee reliable system dynamic parameters, we validate the effectiveness of the proposed method using a simulated quadrotor system which showed significantly improved trajectory following performance. To benefit the research community, we open-source our code and make it available at https://github.com/RoboticsPolyu/IPN_MPC. ",
    "url": "https://arxiv.org/abs/2310.02542",
    "authors": [
      "Peiwen Yang",
      "Weisong Wen"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ]
  },
  {
    "id": "arXiv:2310.02543",
    "title": "Provable Tensor Completion with Graph Information",
    "abstract": "Graphs, depicting the interrelations between variables, has been widely used as effective side information for accurate data recovery in various matrix/tensor recovery related applications. In this paper, we study the tensor completion problem with graph information. Current research on graph-regularized tensor completion tends to be task-specific, lacking generality and systematic approaches. Furthermore, a recovery theory to ensure performance remains absent. Moreover, these approaches overlook the dynamic aspects of graphs, treating them as static akin to matrices, even though graphs could exhibit dynamism in tensor-related scenarios. To confront these challenges, we introduce a pioneering framework in this paper that systematically formulates a novel model, theory, and algorithm for solving the dynamic graph regularized tensor completion problem. For the model, we establish a rigorous mathematical representation of the dynamic graph, based on which we derive a new tensor-oriented graph smoothness regularization. By integrating this regularization into a tensor decomposition model based on transformed t-SVD, we develop a comprehensive model simultaneously capturing the low-rank and similarity structure of the tensor. In terms of theory, we showcase the alignment between the proposed graph smoothness regularization and a weighted tensor nuclear norm. Subsequently, we establish assurances of statistical consistency for our model, effectively bridging a gap in the theoretical examination of the problem involving tensor recovery with graph information. In terms of the algorithm, we develop a solution of high effectiveness, accompanied by a guaranteed convergence, to address the resulting model. To showcase the prowess of our proposed model in contrast to established ones, we provide in-depth numerical experiments encompassing synthetic data as well as real-world datasets. ",
    "url": "https://arxiv.org/abs/2310.02543",
    "authors": [
      "Kaidong Wang",
      "Yao Wang",
      "Xiuwu Liao",
      "Shaojie Tang",
      "Can Yang",
      "Deyu Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02544",
    "title": "SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy  Efficiency of Inference Efficient Vision Transformers",
    "abstract": "Recently, there has been a lot of progress in reducing the computation of deep models at inference time. These methods can reduce both the computational needs and power usage of deep models. Some of these approaches adaptively scale the compute based on the input instance. We show that such models can be vulnerable to a universal adversarial patch attack, where the attacker optimizes for a patch that when pasted on any image, can increase the compute and power consumption of the model. We run experiments with three different efficient vision transformer methods showing that in some cases, the attacker can increase the computation to the maximum possible level by simply pasting a patch that occupies only 8\\% of the image area. We also show that a standard adversarial training defense method can reduce some of the attack's success. We believe adaptive efficient methods will be necessary for the future to lower the power usage of deep models, so we hope our paper encourages the community to study the robustness of these methods and develop better defense methods for the proposed attack. ",
    "url": "https://arxiv.org/abs/2310.02544",
    "authors": [
      "KL Navaneet",
      "Soroush Abbasi Koohpayegani",
      "Essam Sleiman",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02548",
    "title": "Exact and soft boundary conditions in Physics-Informed Neural Networks  for the Variable Coefficient Poisson equation",
    "abstract": "Boundary conditions (BCs) are a key component in every Physics-Informed Neural Network (PINN). By defining the solution to partial differential equations (PDEs) along domain boundaries, BCs constrain the underlying boundary value problem (BVP) that a PINN tries to approximate. Without them, unique PDE solutions may not exist and finding approximations with PINNs would be a challenging, if not impossible task. This study examines how soft loss-based and exact distance function-based BC imposition approaches differ when applied in PINNs. The well known variable coefficient Poisson equation serves as the target PDE for all PINN models trained in this work. Besides comparing BC imposition approaches, the goal of this work is to also provide resources on how to implement these PINNs in practice. To this end, Keras models with Tensorflow backend as well as a Python notebook with code examples and step-by-step explanations on how to build soft/exact BC PINNs are published alongside this review. ",
    "url": "https://arxiv.org/abs/2310.02548",
    "authors": [
      "Sebastian Barschkis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02549",
    "title": "Heterogeneous Federated Learning Using Knowledge Codistillation",
    "abstract": "Federated Averaging, and many federated learning algorithm variants which build upon it, have a limitation: all clients must share the same model architecture. This results in unused modeling capacity on many clients, which limits model performance. To address this issue, we propose a method that involves training a small model on the entire pool and a larger model on a subset of clients with higher capacity. The models exchange information bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a server without sharing parameters. We present two variants of our method, which improve upon federated averaging on image classification and language modeling tasks. We show this technique can be useful even if only out-of-domain or limited in-domain distillation data is available. Additionally, the bi-directional knowledge distillation allows for domain transfer between the models when different pool populations introduce domain shift. ",
    "url": "https://arxiv.org/abs/2310.02549",
    "authors": [
      "Jared Lichtarge",
      "Ehsan Amid",
      "Shankar Kumar",
      "Tien-Ju Yang",
      "Rohan Anil",
      "Rajiv Mathews"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02550",
    "title": "Convergence Analysis and Latency Minimization for Semi-Federated  Learning in Massive IoT Networks",
    "abstract": "As the number of sensors becomes massive in Internet of Things (IoT) networks, the amount of data is humongous. To process data in real-time while protecting user privacy, federated learning (FL) has been regarded as an enabling technique to push edge intelligence into IoT networks with massive devices. However, FL latency increases dramatically due to the increase of the number of parameters in deep neural network and the limited computation and communication capabilities of IoT devices. To address this issue, we propose a semi-federated learning (SemiFL) paradigm in which network pruning and over-the-air computation are efficiently applied. To be specific, each small base station collects the raw data from its served sensors and trains its local pruned model. After that, the global aggregation of local gradients is achieved through over-the-air computation. We first analyze the performance of the proposed SemiFL by deriving its convergence upper bound. To reduce latency, a convergence-constrained SemiFL latency minimization problem is formulated. By decoupling the original problem into several sub-problems, iterative algorithms are designed to solve them efficiently. Finally, numerical simulations are conducted to verify the effectiveness of our proposed scheme in reducing latency and guaranteeing the identification accuracy. ",
    "url": "https://arxiv.org/abs/2310.02550",
    "authors": [
      "Jianyang Ren",
      "Wanli Ni",
      "Hui Tian",
      "Gaofeng Nie"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:2310.02556",
    "title": "NOLA: Networks as Linear Combination of Low Rank Random Basis",
    "abstract": "Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we introduce NOLA, which overcomes the rank one lower bound present in LoRA. It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only. This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture. We present adaptation results using GPT-2 and ViT in natural language and computer vision tasks. NOLA performs as well as, or better than models with equivalent parameter counts. Furthermore, we demonstrate that we can halve the parameters in larger models compared to LoRA with rank one, without sacrificing performance. ",
    "url": "https://arxiv.org/abs/2310.02556",
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "KL Navaneet",
      "Parsa Nooralinejad",
      "Soheil Kolouri",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02557",
    "title": "Generalization in diffusion models arises from geometry-adaptive  harmonic representation",
    "abstract": "High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the \"true\" continuous density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates an alignment of powerful inductive biases in the DNN architecture and/or training algorithm with properties of the data distribution. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image regions. We show that trained denoisers are inductively biased towards these geometry-adaptive harmonic representations by demonstrating that they arise even when the network is trained on image classes such as low-dimensional manifolds, for which the harmonic basis is suboptimal. Additionally, we show that the denoising performance of the networks is near-optimal when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic. ",
    "url": "https://arxiv.org/abs/2310.02557",
    "authors": [
      "Zahra Kadkhodaie",
      "Florentin Guth",
      "Eero P. Simoncelli",
      "St\u00e9phane Mallat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02565",
    "title": "Improving Drumming Robot Via Attention Transformer Network",
    "abstract": "Robotic technology has been widely used in nowadays society, which has made great progress in various fields such as agriculture, manufacturing and entertainment. In this paper, we focus on the topic of drumming robots in entertainment. To this end, we introduce an improving drumming robot that can automatically complete music transcription based on the popular vision transformer network based on the attention mechanism. Equipped with the attention transformer network, our method can efficiently handle the sequential audio embedding input and model their global long-range dependencies. Massive experimental results demonstrate that the improving algorithm can help the drumming robot promote drum classification performance, which can also help the robot to enjoy a variety of smart applications and services. ",
    "url": "https://arxiv.org/abs/2310.02565",
    "authors": [
      "Yang Yi",
      "Zonghan Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02568",
    "title": "Stand for Something or Fall for Everything: Predict Misinformation  Spread with Stance-Aware Graph Neural Networks",
    "abstract": "Although pervasive spread of misinformation on social media platforms has become a pressing challenge, existing platform interventions have shown limited success in curbing its dissemination. In this study, we propose a stance-aware graph neural network (stance-aware GNN) that leverages users' stances to proactively predict misinformation spread. As different user stances can form unique echo chambers, we customize four information passing paths in stance-aware GNN, while the trainable attention weights provide explainability by highlighting each structure's importance. Evaluated on a real-world dataset, stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs without user stance by over 4.69%. Furthermore, the attention weights indicate that users' opposition stances have a higher impact on their neighbors' behaviors than supportive ones, which function as social correction to halt misinformation propagation. Overall, our study provides an effective predictive model for platforms to combat misinformation, and highlights the impact of user stances in the misinformation propagation. ",
    "url": "https://arxiv.org/abs/2310.02568",
    "authors": [
      "Zihan Chen",
      "Jingyi Sun",
      "Rong Liu",
      "Feng Mai"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02571",
    "title": "The role of local bounds on neighborhoods in the network for scale-free  state synchronization of multi-agent systems",
    "abstract": "This paper provides necessary and sufficient conditions for the existence of solutions to the state synchronization problem of homogeneous multi-agent systems (MAS) via scale-free linear dynamic non-collaborative protocol for both continuous- and discrete-time. We investigate protocol design with and without utilizing local bounds on neighborhood. The results show that the availability of local bounds on neighborhoods plays a key role. ",
    "url": "https://arxiv.org/abs/2310.02571",
    "authors": [
      "Anton A. Stoorvogel",
      "Ali Saberi",
      "Zhenwei Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2310.02573",
    "title": "Robust Collision Detection for Robots with Variable Stiffness Actuation  by Using MAD-CNN: Modularized-Attention-Dilated Convolutional Neural Network",
    "abstract": "Ensuring safety is paramount in the field of collaborative robotics to mitigate the risks of human injury and environmental damage. Apart from collision avoidance, it is crucial for robots to rapidly detect and respond to unexpected collisions. While several learning-based collision detection methods have been introduced as alternatives to purely model-based detection techniques, there is currently a lack of such methods designed for collaborative robots equipped with variable stiffness actuators. Moreover, there is potential for further enhancing the network's robustness and improving the efficiency of data training. In this paper, we propose a new network, the Modularized Attention-Dilated Convolutional Neural Network (MAD-CNN), for collision detection in robots equipped with variable stiffness actuators. Our model incorporates a dual inductive bias mechanism and an attention module to enhance data efficiency and improve robustness. In particular, MAD-CNN is trained using only a four-minute collision dataset focusing on the highest level of joint stiffness. Despite limited training data, MAD-CNN robustly detects all collisions with minimal detection delay across various stiffness conditions. Moreover, it exhibits a higher level of collision sensitivity, which is beneficial for effectively handling false positives, which is a common issue in learning-based methods. Experimental results demonstrate that the proposed MAD-CNN model outperforms existing state-of-the-art models in terms of collision sensitivity and robustness. ",
    "url": "https://arxiv.org/abs/2310.02573",
    "authors": [
      "Zhenwei Niu",
      "Lyes Saad Saoud",
      "Irfan Hussain"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ]
  },
  {
    "id": "arXiv:2310.02576",
    "title": "A Prototype-Based Neural Network for Image Anomaly Detection and  Localization",
    "abstract": "Image anomaly detection and localization perform not only image-level anomaly classification but also locate pixel-level anomaly regions. Recently, it has received much research attention due to its wide application in various fields. This paper proposes ProtoAD, a prototype-based neural network for image anomaly detection and localization. First, the patch features of normal images are extracted by a deep network pre-trained on nature images. Then, the prototypes of the normal patch features are learned by non-parametric clustering. Finally, we construct an image anomaly localization network (ProtoAD) by appending the feature extraction network with $L2$ feature normalization, a $1\\times1$ convolutional layer, a channel max-pooling, and a subtraction operation. We use the prototypes as the kernels of the $1\\times1$ convolutional layer; therefore, our neural network does not need a training phase and can conduct anomaly detection and localization in an end-to-end manner. Extensive experiments on two challenging industrial anomaly detection datasets, MVTec AD and BTAD, demonstrate that ProtoAD achieves competitive performance compared to the state-of-the-art methods with a higher inference speed. The source code is available at: https://github.com/98chao/ProtoAD. ",
    "url": "https://arxiv.org/abs/2310.02576",
    "authors": [
      "Chao Huang",
      "Zhao Kang",
      "Hong Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02579",
    "title": "On the Stability of Expressive Positional Encodings for Graph Neural  Networks",
    "abstract": "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \\emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \\emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a \"hard partition\" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to \"softly partition\" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods. ",
    "url": "https://arxiv.org/abs/2310.02579",
    "authors": [
      "Yinan Huang",
      "William Lu",
      "Joshua Robinson",
      "Yu Yang",
      "Muhan Zhang",
      "Stefanie Jegelka",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02606",
    "title": "Learning adjacency matrix for dynamic graph neural network",
    "abstract": "In recent work, [1] introduced the concept of using a Block Adjacency Matrix (BA) for the representation of spatio-temporal data. While their method successfully concatenated adjacency matrices to encapsulate spatio-temporal relationships in a single graph, it formed a disconnected graph. This limitation hampered the ability of Graph Convolutional Networks (GCNs) to perform message passing across nodes belonging to different time steps, as no temporal links were present. To overcome this challenge, we introduce an encoder block specifically designed to learn these missing temporal links. The encoder block processes the BA and predicts connections between previously unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix (STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to capture the complex spatio-temporal topology of the network. Our evaluations on benchmark datasets, surgVisDom and C2D2, demonstrate that our method, with slightly higher complexity, achieves superior results compared to state-of-the-art results. Our approach's computational overhead remains significantly lower than conventional non-graph-based methodologies for spatio-temporal data. ",
    "url": "https://arxiv.org/abs/2310.02606",
    "authors": [
      "Osama Ahmad",
      "Omer Abdul Jalil",
      "Usman Nazir",
      "Murtaza Taj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2310.02611",
    "title": "Analyzing and Improving OT-based Adversarial Networks",
    "abstract": "Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and generated distributions. Recently, OT transport map between data and prior distributions has been utilized as a generative model. These OT-based generative models share a similar adversarial training objective. In this paper, we begin by unifying these OT-based adversarial methods within a single framework. Then, we elucidate the role of each component in training dynamics through a comprehensive analysis of this unified framework. Moreover, we suggest a simple but novel method that improves the previously best-performing OT-based model. Intuitively, our approach conducts a gradual refinement of the generated distribution, progressively aligning it with the data distribution. Our approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified OT-based adversarial approaches. ",
    "url": "https://arxiv.org/abs/2310.02611",
    "authors": [
      "Jaemoo Choi",
      "Jaewoong Choi",
      "Myungjoo Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02612",
    "title": "Top-k contrast order-preserving pattern mining for time series  classification",
    "abstract": "Recently, order-preserving pattern (OPP) mining, a new sequential pattern mining method, has been proposed to mine frequent relative orders in a time series. Although frequent relative orders can be used as features to classify a time series, the mined patterns do not reflect the differences between two classes of time series well. To effectively discover the differences between time series, this paper addresses the top-k contrast OPP (COPP) mining and proposes a COPP-Miner algorithm to discover the top-k contrast patterns as features for time series classification, avoiding the problem of improper parameter setting. COPP-Miner is composed of three parts: extreme point extraction to reduce the length of the original time series, forward mining, and reverse mining to discover COPPs. Forward mining contains three steps: group pattern fusion strategy to generate candidate patterns, the support rate calculation method to efficiently calculate the support of a pattern, and two pruning strategies to further prune candidate patterns. Reverse mining uses one pruning strategy to prune candidate patterns and consists of applying the same process as forward mining. Experimental results validate the efficiency of the proposed algorithm and show that top-k COPPs can be used as features to obtain a better classification performance. ",
    "url": "https://arxiv.org/abs/2310.02612",
    "authors": [
      "Youxi Wu",
      "Yufei Meng",
      "Yan Li",
      "Lei Guo",
      "Xingquan Zhu",
      "Philippe Fournier-Viger",
      "Xindong Wu"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ]
  },
  {
    "id": "arXiv:2310.02633",
    "title": "Multi-rules mining algorithm for combinatorially exploded decision trees  with modified Aitchison-Aitken function-based Bayesian optimization",
    "abstract": "Decision trees offer the benefit of easy interpretation because they allow the classification of input data based on if--then rules. However, as decision trees are constructed by an algorithm that achieves clear classification with minimum necessary rules, the trees possess the drawback of extracting only minimum rules, even when various latent rules exist in data. Approaches that construct multiple trees using randomly selected feature subsets do exist. However, the number of trees that can be constructed remains at the same scale because the number of feature subsets is a combinatorial explosion. Additionally, when multiple trees are constructed, numerous rules are generated, of which several are untrustworthy and/or highly similar. Therefore, we propose \"MAABO-MT\" and \"GS-MRM\" algorithms that strategically construct trees with high estimation performance among all possible trees with small computational complexity and extract only reliable and non-similar rules, respectively. Experiments are conducted using several open datasets to analyze the effectiveness of the proposed method. The results confirm that MAABO-MT can discover reliable rules at a lower computational cost than other methods that rely on randomness. Furthermore, the proposed method is confirmed to provide deeper insights than single decision trees commonly used in previous studies. Therefore, MAABO-MT and GS-MRM can efficiently extract rules from combinatorially exploded decision trees. ",
    "url": "https://arxiv.org/abs/2310.02633",
    "authors": [
      "Yuto Omae",
      "Masaya Mori",
      "Yohei Kakimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02638",
    "title": "P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD  Model from Point Clouds",
    "abstract": "Computer Aided Design (CAD), especially the feature-based parametric CAD, plays an important role in modern industry and society. However, the reconstruction of featured CAD model is more challenging than the reconstruction of other CAD models. To this end, this paper proposes an end-to-end network to reconstruct featured CAD model from point cloud (P2CADNet). Initially, the proposed P2CADNet architecture combines a point cloud feature extractor, a CAD sequence reconstructor and a parameter optimizer. Subsequently, in order to reconstruct the featured CAD model in an autoregressive way, the CAD sequence reconstructor applies two transformer decoders, one with target mask and the other without mask. Finally, for predicting parameters more precisely, we design a parameter optimizer with cross-attention mechanism to further refine the CAD feature parameters. We evaluate P2CADNet on the public dataset, and the experimental results show that P2CADNet has excellent reconstruction quality and accuracy. To our best knowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD model from point cloud, and can be regarded as baseline for future works. Therefore, we open the source code at https://github.com/Blice0415/P2CADNet. ",
    "url": "https://arxiv.org/abs/2310.02638",
    "authors": [
      "Zhihao Zong",
      "Fazhi He",
      "Rubin Fan",
      "Yuxin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02641",
    "title": "Deformation-Invariant Neural Network and Its Applications in Distorted  Image Restoration and Analysis",
    "abstract": "Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconformality of the output deformation map. By controlling the Beltrami coefficient, the local geometric distortion under the quasiconformal mapping can be controlled. The QCTN is lightweight and simple, which can be readily integrated into other existing deep neural networks to enhance their performance. Leveraging our framework, we have developed an image classification network that achieves accurate classification of distorted images. Our proposed framework has been applied to restore geometrically distorted images by atmospheric turbulence and water turbulence. DINN outperforms existing GAN-based restoration methods under these scenarios, demonstrating the effectiveness of the proposed framework. Additionally, we apply our proposed framework to the 1-1 verification of human face images under atmospheric turbulence and achieve satisfactory performance, further demonstrating the efficacy of our approach. ",
    "url": "https://arxiv.org/abs/2310.02641",
    "authors": [
      "Han Zhang",
      "Qiguang Chen",
      "Lok Ming Lui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2310.02674",
    "title": "Land-cover change detection using paired OpenStreetMap data and optical  high-resolution imagery via object-guided Transformer",
    "abstract": "Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative features of different levels from OSM data and optical images; a decoder consisting of object-guided cross-attention modules can progressively recover the land-cover changes from the extracted heterogeneous features. In addition to the basic supervised binary change detection task, this paper raises a new semi-supervised semantic change detection task that does not require any manually annotated land-cover labels of optical images to train semantic change detectors. Two lightweight semantic decoders are added to ObjFormer to accomplish this task efficiently. A converse cross-entropy loss is designed to fully utilize the negative samples, thereby contributing to the great performance improvement in this task. The first large-scale benchmark dataset containing 1,287 map-image pairs (1024$\\times$ 1024 pixels for each sample) covering 40 regions on six continents ...(see the manuscript for the full abstract) ",
    "url": "https://arxiv.org/abs/2310.02674",
    "authors": [
      "Hongruixuan Chen",
      "Cuiling Lan",
      "Jian Song",
      "Clifford Broni-Bediako",
      "Junshi Xia",
      "Naoto Yokoya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Multimedia (cs.MM)"
    ]
  },
  {
    "id": "arXiv:2310.02687",
    "title": "USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields",
    "abstract": "Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image synthesis as well as camera motion estimation. Furthermore, our algorithm can also be used to recover high-fidelity high frame-rate global shutter video from a sequence of RS images. ",
    "url": "https://arxiv.org/abs/2310.02687",
    "authors": [
      "Moyang Li",
      "Peng Wang",
      "Lingzhe Zhao",
      "Bangyan Liao",
      "Peidong Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02691",
    "title": "Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural  Operators",
    "abstract": "In climate simulations, small-scale processes shape ocean dynamics but remain computationally expensive to resolve directly. For this reason, their contributions are commonly approximated using empirical parameterizations, which lead to significant errors in long-term projections. In this work, we develop parameterizations based on Fourier Neural Operators, showcasing their accuracy and generalizability in comparison to other approaches. Finally, we discuss the potential and limitations of neural networks operating in the frequency domain, paving the way for future investigation. ",
    "url": "https://arxiv.org/abs/2310.02691",
    "authors": [
      "Victor Mangeleer",
      "Gilles Louppe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ]
  },
  {
    "id": "arXiv:2310.02692",
    "title": "Bridging the Domain Gap by Clustering-based Image-Text Graph Matching",
    "abstract": "Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication. ",
    "url": "https://arxiv.org/abs/2310.02692",
    "authors": [
      "Nokyung Park",
      "Daewon Chae",
      "Jeongyong Shim",
      "Sangpil Kim",
      "Eun-Sol Kim",
      "Jinkyu Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02700",
    "title": "Robust Tracking for a 3D Diffusion Equation: Controlling Seismicity Rate  in Geothermal Reservoirs",
    "abstract": "Deep Geothermal Energy has significant potential to meet the large-scale needs of the energy sector. However, the injection of fluids into the earth's crust, upon which it relies, can lead to the formation of new seismogenic faults or the reactivation of existing ones, thereby causing earthquakes. To date, no effective method exists for mitigating these human-induced earthquakes. In this study, we propose a novel approach based on control theory to address this issue. First, we model induced seismicity resulting from fluid injections in a geothermal reservoir using a diffusion equation in three dimensions. Then, we design a robust tracking control approach to force the seismicity rate to follow the desired references. In this way, the induced seismicity is minimized while ensuring fluid circulation for the needs of energy production. The designed control guarantees the stabilization of the error variable even in the presence of system uncertainties and unknown dynamics. Finally, we present simulations of a geothermal reservoir under different scenarios of intermittent energy demand to show the reliability and performance of the control approach, opening new perspectives for field experiments based on real-time regulators for the first time. ",
    "url": "https://arxiv.org/abs/2310.02700",
    "authors": [
      "Diego Gutierrez-Oribio",
      "Ioannis Stefanou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ]
  },
  {
    "id": "arXiv:2310.02704",
    "title": "Extending Isabelle/HOL's Code Generator with support for the Go  programming language",
    "abstract": "The Isabelle proof assistant includes a small functional language, which allows users to write and reason about programs. So far, these programs could be extracted into a number of functional languages: Standard ML, OCaml, Scala, and Haskell. This work adds support for Go as a fifth target language for the Code Generator. Unlike the previous targets, Go is not a functional language and encourages code in an imperative style, thus many of the features of Isabelle's language (particularly data types, pattern matching, and type classes) have to be emulated using imperative language constructs in Go. The developed Code Generation is provided as an add-on library that can be simply imported into existing theories. ",
    "url": "https://arxiv.org/abs/2310.02704",
    "authors": [
      "Terru St\u00fcbinger",
      "Lars Hupel"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ]
  },
  {
    "id": "arXiv:2310.02720",
    "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised  Learning with Masked Unit Prediction",
    "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB). ",
    "url": "https://arxiv.org/abs/2310.02720",
    "authors": [
      "Jiatong Shi",
      "Hirofumi Inaguma",
      "Xutai Ma",
      "Ilia Kulikov",
      "Anna Sun"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2310.02721",
    "title": "Leveraging Temporal Graph Networks Using Module Decoupling",
    "abstract": "Modern approaches for learning on dynamic graphs have adopted the use of batches instead of applying updates one by one. The use of batches allows these techniques to become helpful in streaming scenarios where updates to graphs are received at extreme speeds. Using batches, however, forces the models to update infrequently, which results in the degradation of their performance. In this work, we suggest a decoupling strategy that enables the models to update frequently while using batches. By decoupling the core modules of temporal graph networks and implementing them using a minimal number of learnable parameters, we have developed the Lightweight Decoupled Temporal Graph Network (LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG was validated on various dynamic graph benchmarks, providing comparable or state-of-the-art results with significantly higher throughput than previous art. Notably, our method outperforms previous approaches by more than 20\\% on benchmarks that require rapid model update rates, such as USLegis or UNTrade. The code to reproduce our experiments is available at \\href{https://orfeld415.github.io/module-decoupling}{this http url}. ",
    "url": "https://arxiv.org/abs/2310.02721",
    "authors": [
      "Or Feldman",
      "Chaim Baskin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02724",
    "title": "End-to-End Training of a Neural HMM with Label and Transition  Probabilities",
    "abstract": "We investigate a novel modeling approach for end-to-end neural network training using hidden Markov models (HMM) where the transition probabilities between hidden states are modeled and learned explicitly. Most contemporary sequence-to-sequence models allow for from-scratch training by summing over all possible label segmentations in a given topology. In our approach there are explicit, learnable probabilities for transitions between segments as opposed to a blank label that implicitly encodes duration statistics. We implement a GPU-based forward-backward algorithm that enables the simultaneous training of label and transition probabilities. We investigate recognition results and additionally Viterbi alignments of our models. We find that while the transition model training does not improve recognition performance, it has a positive impact on the alignment quality. The generated alignments are shown to be viable targets in state-of-the-art Viterbi trainings. ",
    "url": "https://arxiv.org/abs/2310.02724",
    "authors": [
      "Daniel Mann",
      "Tina Raissi",
      "Wilfried Michel",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2310.02729",
    "title": "Efficient Quantification and Representation of Aggregate Flexibility in  Electric Vehicles",
    "abstract": "Aggregation is crucial to the effective use of flexibility, especially in the case of electric vehicles (EVs) because of their limited individual battery sizes and large aggregate impact. This research proposes a novel method of quantifying and representing the aggregate flexibility of EV fleets within a fixed flexibility request window. These windows can be chosen based on relevant network operator needs, such as evening congestion periods. The proposed UL-flexibility is independent of the number of assets but scales only with the number of discrete time steps in the chosen window. The representation involves $2T$ parameters, with T being the number of time steps in the window. Feasibility of signals can be checked using $2T$ constraints and optimization using $2(2^T-1)$ constraints, both exactly capturing the flexibility region. Using a request window eliminates uncertainty related to EV arrival and departure times outside the window. We present the necessary theoretical framework for our proposed methods and outline steps for transitioning between representations. Additionally, we compare the computational efficiency of the proposed method with the common direct aggregation method. ",
    "url": "https://arxiv.org/abs/2310.02729",
    "authors": [
      "Nanda Kishor Panda",
      "Simon H. Tindemans"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2310.02731",
    "title": "State Feedback Control Design for Input-output Decoupling of Boolean  Control Networks",
    "abstract": "A state feedback control strategy is proposed for input-output (IO) decoupling of a class of fully output controllable Boolean control networks (BCNs). Some necessary and sufficient conditions for BCN IO-decoupling are presented. As an instrumental tool in our design, we introduce a canonical form for IO-decoupled BCNs along with some conditions guaranteeing its existence. Finally, two numerical examples are provided to illustrate the effectiveness of the proposed approach. ",
    "url": "https://arxiv.org/abs/2310.02731",
    "authors": [
      "Yiliang Li",
      "Hongli Lyu",
      "Jun-e Feng",
      "Abdelhamid Tayebi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ]
  },
  {
    "id": "arXiv:2310.02772",
    "title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural  Networks",
    "abstract": "In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the above contents through experiments and showed that it is possible to reduce memory and training time while maintaining accuracy. ",
    "url": "https://arxiv.org/abs/2310.02772",
    "authors": [
      "Ryuji Saiin",
      "Tomoya Shirakawa",
      "Sota Yoshihara",
      "Yoshihide Sawada",
      "Hiroyuki Kusumoto"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02774",
    "title": "Graph Neural Networks and Time Series as Directed Graphs for Quality  Recognition",
    "abstract": "Graph Neural Networks (GNNs) are becoming central in the study of time series, coupled with existing algorithms as Temporal Convolutional Networks and Recurrent Neural Networks. In this paper, we see time series themselves as directed graphs, so that their topology encodes time dependencies and we start to explore the effectiveness of GNNs architectures on them. We develop two distinct Geometric Deep Learning models, a supervised classifier and an autoencoder-like model for signal reconstruction. We apply these models on a quality recognition problem. ",
    "url": "https://arxiv.org/abs/2310.02774",
    "authors": [
      "Angelica Simonetti",
      "Ferdinando Zanchetta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02779",
    "title": "Expected flow networks in stochastic environments and two-player  zero-sum games",
    "abstract": "Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments. ",
    "url": "https://arxiv.org/abs/2310.02779",
    "authors": [
      "Marco Jiralerspong",
      "Bilun Sun",
      "Danilo Vucetic",
      "Tianyu Zhang",
      "Yoshua Bengio",
      "Gauthier Gidel",
      "Nikolay Malkin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ]
  },
  {
    "id": "arXiv:2310.02782",
    "title": "Discovering General Reinforcement Learning Algorithms with Adversarial  Environment Design",
    "abstract": "The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment Design (GROOVE). In a series of experiments, we show that GROOVE achieves superior generalization to LPG, and evaluate AR against baseline metrics from UED, identifying it as a critical component of environment design in this setting. We believe this approach is a step towards the discovery of truly general RL algorithms, capable of solving a wide range of real-world environments. ",
    "url": "https://arxiv.org/abs/2310.02782",
    "authors": [
      "Matthew Thomas Jackson",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Risto Vuorio",
      "Chris Lu",
      "Gregory Farquhar",
      "Shimon Whiteson",
      "Jakob Nicolaus Foerster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02800",
    "title": "Everest: GPU-Accelerated System For Mining Temporal Motifs",
    "abstract": "Temporal motif mining is the task of finding the occurrences of subgraph patterns within a large input temporal graph that obey the specified structural and temporal constraints. Despite its utility in several critical application domains that demand high performance (e.g., detecting fraud in financial transaction graphs), the performance of existing software is limited on commercial hardware platforms, in that it runs for tens of hours. This paper presents Everest - a system that efficiently maps the workload of mining (supports both enumeration and counting) temporal motifs to the highly parallel GPU architecture. In particular, using an input temporal graph and a more expressive user-defined temporal motif query definition compared to prior works, Everest generates an execution plan and runtime primitives that optimize the workload execution by exploiting the high compute throughput of a GPU. Everest generates motif-specific mining code to reduce long-latency memory accesses and frequent thread divergence operations. Everest incorporates novel low-cost runtime mechanisms to enable load balancing to improve GPU hardware utilization. To support large graphs that do not fit on GPU memory, Everest also supports multi-GPU execution by intelligently partitioning the edge list that prevents inter-GPU communication. Everest hides the implementation complexity of presented optimizations away from the targeted system user for better usability. Our evaluation shows that, using proposed optimizations, Everest improves the performance of a baseline GPU implementation by 19x, on average. ",
    "url": "https://arxiv.org/abs/2310.02800",
    "authors": [
      "Yichao Yuan",
      "Haojie Ye",
      "Sanketh Vedula Wynn Kaza",
      "Nishil Talati"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2310.02815",
    "title": "CoBEV: Elevating Roadside 3D Object Detection with Depth and Height  Complementarity",
    "abstract": "Roadside camera-driven 3D object detection is a crucial task in intelligent transportation systems, which extends the perception range beyond the limitations of vision-centric vehicles and enhances road safety. While previous studies have limitations in using only depth or height information, we find both depth and height matter and they are in fact complementary. The depth feature encompasses precise geometric cues, whereas the height feature is primarily focused on distinguishing between various categories of height intervals, essentially providing semantic context. This insight motivates the development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D object detection framework that integrates depth and height to construct robust BEV representations. In essence, CoBEV estimates each pixel's depth and height distribution and lifts the camera features into 3D space for lateral fusion using the newly proposed two-stage complementary feature selection (CFS) module. A BEV feature distillation framework is also seamlessly integrated to further enhance the detection accuracy from the prior knowledge of the fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as the private Supremind-Road dataset, demonstrating that CoBEV not only achieves the accuracy of the new state-of-the-art, but also significantly advances the robustness of previous methods in challenging long-distance scenarios and noisy camera disturbance, and enhances generalization by a large margin in heterologous settings with drastic changes in scene and camera parameters. For the first time, the vehicle AP score of a camera model reaches 80% on DAIR-V2X-I in terms of easy mode. The source code will be made publicly available at https://github.com/MasterHow/CoBEV. ",
    "url": "https://arxiv.org/abs/2310.02815",
    "authors": [
      "Hao Shi",
      "Chengshan Pang",
      "Jiaming Zhang",
      "Kailun Yang",
      "Yuhao Wu",
      "Huajian Ni",
      "Yining Lin",
      "Rainer Stiefelhagen",
      "Kaiwei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ]
  },
  {
    "id": "arXiv:2310.02821",
    "title": "Improving Vision Anomaly Detection with the Guidance of Language  Modality",
    "abstract": "Recent years have seen a surge of interest in anomaly detection for tackling industrial defect detection, event detection, etc. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. Conversely, the language modality performs well due to its relatively single data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. CMER masks parts of the raw image and computes the matching score with the text. Then, CMER discards irrelevant pixels to make the detector focus on critical contents. To learn a more compact latent space for the vision anomaly detector, CMLE learns a correlation structure matrix from the language modality, and then the latent space of vision modality will be learned with the guidance of the matrix. Thereafter, the vision latent space will get semantically similar images closer. Extensive experiments demonstrate the effectiveness of the proposed methods. Particularly, CMG outperforms the baseline that only uses images by 16.81%. Ablation experiments further confirm the synergy among the proposed methods, as each component depends on the other to achieve optimal performance. ",
    "url": "https://arxiv.org/abs/2310.02821",
    "authors": [
      "Dong Chen",
      "Kaihang Pan",
      "Guoming Wang",
      "Yueting Zhuang",
      "Siliang Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02832",
    "title": "Out-of-Distribution Detection by Leveraging Between-Layer Transformation  Smoothness",
    "abstract": "Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases with more complex tasks. ",
    "url": "https://arxiv.org/abs/2310.02832",
    "authors": [
      "Fran Jeleni\u0107",
      "Josip Juki\u0107",
      "Martin Tutek",
      "Mate Puljiz",
      "Jan \u0160najder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02840",
    "title": "Mosaic benchmark networks: Modular link streams for testing dynamic  community detection algorithms",
    "abstract": "Community structure is a critical feature of real networks, providing insights into nodes' internal organization. Nowadays, with the availability of highly detailed temporal networks such as link streams, studying community structures becomes more complex due to increased data precision and time sensitivity. Despite numerous algorithms developed in the past decade for dynamic community discovery, assessing their performance on link streams remains a challenge. Synthetic benchmark graphs are a well-accepted approach for evaluating static community detection algorithms. Additionally, there have been some proposals for slowly evolving communities in low-resolution temporal networks like snapshots. Nevertheless, this approach is not yet suitable for link streams. To bridge this gap, we introduce a novel framework that generates synthetic modular link streams with predefined communities. Subsequently, we evaluate established dynamic community detection methods to uncover limitations that may not be evident in snapshots with slowly evolving communities. While no method emerges as a clear winner, we observe notable differences among them. ",
    "url": "https://arxiv.org/abs/2310.02840",
    "authors": [
      "Yasaman Asgari",
      "Remy Cazabet",
      "Pierre Borgnat"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2310.02854",
    "title": "Multi-Domain Causal Representation Learning via Weak Distributional  Invariances",
    "abstract": "Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set of latents from the rest across different settings. ",
    "url": "https://arxiv.org/abs/2310.02854",
    "authors": [
      "Kartik Ahuja",
      "Amin Mansouri",
      "Yixin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2310.02859",
    "title": "Tight Sampling in Unbounded Networks",
    "abstract": "The default approach to deal with the enormous size and limited accessibility of many Web and social media networks is to sample one or more subnetworks from a conceptually unbounded unknown network. Clearly, the extracted subnetworks will crucially depend on the sampling scheme. Motivated by studies of homophily and opinion formation, we propose a variant of snowball sampling designed to prioritize inclusion of entire cohesive communities rather than any kind of representativeness, breadth, or depth of coverage. The method is illustrated on a concrete example, and experiments on synthetic networks suggest that it behaves as desired. ",
    "url": "https://arxiv.org/abs/2310.02859",
    "authors": [
      "Kshitijaa Jaglan",
      "Meher Chaitanya",
      "Triansh Sharma",
      "Abhijeeth Singam",
      "Nidhi Goyal",
      "Ponnurangam Kumaraguru",
      "Ulrik Brandes"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ]
  },
  {
    "id": "arXiv:2310.02861",
    "title": "Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly  Detection",
    "abstract": "Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graphs. Specifically, we introduce a novel framework that consists of two components: the Rayleigh Quotient learning component (RQL) and Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space of graphs. Extensive experiments on 10 real-world datasets show that RQGNN outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC, demonstrating the effectiveness of our framework. ",
    "url": "https://arxiv.org/abs/2310.02861",
    "authors": [
      "Xiangyu Dong",
      "Xingyi Zhang",
      "Sibo Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02875",
    "title": "Approximating Robot Configuration Spaces with few Convex Sets using  Clique Covers of Visibility Graphs",
    "abstract": "Many computations in robotics can be dramatically accelerated if the robot configuration space is described as a collection of simple sets. For example, recently developed motion planners rely on a convex decomposition of the free space to design collision-free trajectories using fast convex optimization. In this work, we present an efficient method for approximately covering complex configuration spaces with a small number of polytopes. The approach constructs a visibility graph using sampling and generates a clique cover of this graph to find clusters of samples that have mutual line of sight. These clusters are then inflated into large, full-dimensional, polytopes. We evaluate our method on a variety of robotic systems and show that it consistently covers larger portions of free configuration space, with fewer polytopes, and in a fraction of the time compared to previous methods. ",
    "url": "https://arxiv.org/abs/2310.02875",
    "authors": [
      "Peter Werner",
      "Alexandre Amice",
      "Tobia Marcucci",
      "Daniela Rus",
      "Russ Tedrake"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computational Geometry (cs.CG)"
    ]
  },
  {
    "id": "arXiv:2310.02876",
    "title": "Hate Speech Detection in Limited Data Contexts using Synthetic Data  Generation",
    "abstract": "A growing body of work has focused on text classification methods for detecting the increasing amount of hate speech posted online. This progress has been limited to only a select number of highly-resourced languages causing detection systems to either under-perform or not exist in limited data contexts. This is majorly caused by a lack of training data which is expensive to collect and curate in these settings. In this work, we propose a data augmentation approach that addresses the problem of lack of data for online hate speech detection in limited data contexts using synthetic data generation techniques. Given a handful of hate speech examples in a high-resource language such as English, we present three methods to synthesize new examples of hate speech data in a target language that retains the hate sentiment in the original examples but transfers the hate targets. We apply our approach to generate training data for hate speech classification tasks in Hindi and Vietnamese. Our findings show that a model trained on synthetic data performs comparably to, and in some cases outperforms, a model trained only on the samples available in the target domain. This method can be adopted to bootstrap hate speech detection models from scratch in limited data contexts. As the growth of social media within these contexts continues to outstrip response efforts, this work furthers our capacities for detection, understanding, and response to hate speech. ",
    "url": "https://arxiv.org/abs/2310.02876",
    "authors": [
      "Aman Khullar",
      "Daniel Nkemelu",
      "Cuong V. Nguyen",
      "Michael L. Best"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2310.02887",
    "title": "A Grammatical Compositional Model for Video Action Detection",
    "abstract": "Analysis of human actions in videos demands understanding complex human dynamics, as well as the interaction between actors and context. However, these interaction relationships usually exhibit large intra-class variations from diverse human poses or object manipulations, and fine-grained inter-class differences between similar actions. Thus the performance of existing methods is severely limited. Motivated by the observation that interactive actions can be decomposed into actor dynamics and participating objects or humans, we propose to investigate the composite property of them. In this paper, we present a novel Grammatical Compositional Model (GCM) for action detection based on typical And-Or graphs. Our model exploits the intrinsic structures and latent relationships of actions in a hierarchical manner to harness both the compositionality of grammar models and the capability of expressing rich features of DNNs. The proposed model can be readily embodied into a neural network module for efficient optimization in an end-to-end manner. Extensive experiments are conducted on the AVA dataset and the Something-Else task to demonstrate the superiority of our model, meanwhile the interpretability is enhanced through an inference parsing procedure. ",
    "url": "https://arxiv.org/abs/2310.02887",
    "authors": [
      "Zhijun Zhang",
      "Xu Zou",
      "Jiahuan Zhou",
      "Sheng Zhong",
      "Ying Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02901",
    "title": "Computationally Efficient Quadratic Neural Networks",
    "abstract": "Higher order artificial neurons whose outputs are computed by applying an activation function to a higher order multinomial function of the inputs have been considered in the past, but did not gain acceptance due to the extra parameters and computational cost. However, higher order neurons have significantly greater learning capabilities since the decision boundaries of higher order neurons can be complex surfaces instead of just hyperplanes. The boundary of a single quadratic neuron can be a general hyper-quadric surface allowing it to learn many nonlinearly separable datasets. Since quadratic forms can be represented by symmetric matrices, only $\\frac{n(n+1)}{2}$ additional parameters are needed instead of $n^2$. A quadratic Logistic regression model is first presented. Solutions to the XOR problem with a single quadratic neuron are considered. The complete vectorized equations for both forward and backward propagation in feedforward networks composed of quadratic neurons are derived. A reduced parameter quadratic neural network model with just $ n $ additional parameters per neuron that provides a compromise between learning ability and computational cost is presented. Comparison on benchmark classification datasets are used to demonstrate that a final layer of quadratic neurons enables networks to achieve higher accuracy with significantly fewer hidden layer neurons. In particular this paper shows that any dataset composed of $C$ bounded clusters can be separated with only a single layer of $C$ quadratic neurons. ",
    "url": "https://arxiv.org/abs/2310.02901",
    "authors": [
      "Mathew Mithra Noel",
      "Venkataraman Muthiah-Nakarajan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02903",
    "title": "FroSSL: Frobenius Norm Minimization for Self-Supervised Learning",
    "abstract": "Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. We also show that FroSSL learns competitive representations on linear probe evaluation when used to train a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets. ",
    "url": "https://arxiv.org/abs/2310.02903",
    "authors": [
      "Oscar Skean",
      "Aayush Dhakal",
      "Nathan Jacobs",
      "Luis Gonzalo Sanchez Giraldo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02905",
    "title": "Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled  with Transformers",
    "abstract": "Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers} (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that our INSTINCT consistently outperforms the existing methods in different tasks, such as in various instruction induction tasks and the task of improving the zero-shot chain-of-thought instruction. ",
    "url": "https://arxiv.org/abs/2310.02905",
    "authors": [
      "Xiaoqiang Lin",
      "Zhaoxuan Wu",
      "Zhongxiang Dai",
      "Wenyang Hu",
      "Yao Shu",
      "See-Kiong Ng",
      "Patrick Jaillet",
      "Bryan Kian Hsiang Low"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.02919",
    "title": "Attention-based Multi-task Learning for Base Editor Outcome Prediction",
    "abstract": "Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing designs. ",
    "url": "https://arxiv.org/abs/2310.02919",
    "authors": [
      "Amina Mollaysa",
      "Ahmed Allam",
      "Michael Krauthammer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02926",
    "title": "Extensions to the SENSEI In situ Framework for Heterogeneous  Architectures",
    "abstract": "The proliferation of GPUs and accelerators in recent supercomputing systems, so called heterogeneous architectures, has led to increased complexity in execution environments and programming models as well as to deeper memory hierarchies on these systems. In this work, we discuss challenges that arise in in situ code coupling on these heterogeneous architectures. In particular, we present data and execution model extensions to the SENSEI in situ framework that are targeted at the effective use of systems with heterogeneous architectures. We then use these new data and execution model extensions to investigate several in situ placement and execution configurations and to analyze the impact these choices have on overall performance. ",
    "url": "https://arxiv.org/abs/2310.02926",
    "authors": [
      "Burlen Loring",
      "E. Wes Bethel",
      "Gunther H. Weber",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2310.02927",
    "title": "Joint Network Lifetime Maximization and Relay Selection Design in  Underwater Acoustic Sensor Networks",
    "abstract": "The paper proposes a new approach to minimize the number of relays while maximizing the lifetime of underwater acoustic sensor networks (UASNs). This involves formulating the relay node placement (RNP) problem as a multi-objective optimization problem and employing the multi-objective lexico-graphic method (MOLM) to solve it. To achieve the optimal solution, the MOLM consists of two steps. First, the problem of lifetime maximization is tackled to find RNP solutions. This transforms the RNP into a non-convex optimization problem which is then converted into a convex programming equivalent. The proposed method has the same computational complexity as previous relay-node adjustment (RA) and difference convex algorithm (DCA) methods. The second step introduces a novel relay node selection to reach the optimal number of relays. Simulation results demonstrate that it has superior network lifetime and efficiency compared to RA and DCA. ",
    "url": "https://arxiv.org/abs/2310.02927",
    "authors": [
      "Z. Mohammadi",
      "M. Soleimanpour-Moghadam",
      "S. Talebi",
      "H. Ahmadi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ]
  },
  {
    "id": "arXiv:2310.02931",
    "title": "Graph data modelling for outcome prediction in oropharyngeal cancer  patients",
    "abstract": "Graph neural networks (GNNs) are becoming increasingly popular in the medical domain for the tasks of disease classification and outcome prediction. Since patient data is not readily available as a graph, most existing methods either manually define a patient graph, or learn a latent graph based on pairwise similarities between the patients. There are also hypergraph neural network (HGNN)-based methods that were introduced recently to exploit potential higher order associations between the patients by representing them as a hypergraph. In this work, we propose a patient hypergraph network (PHGN), which has been investigated in an inductive learning setup for binary outcome prediction in oropharyngeal cancer (OPC) patients using computed tomography (CT)-based radiomic features for the first time. Additionally, the proposed model was extended to perform time-to-event analyses, and compared with GNN and baseline linear models. ",
    "url": "https://arxiv.org/abs/2310.02931",
    "authors": [
      "Nithya Bhasker",
      "Stefan Leger",
      "Alexander Zwanenburg",
      "Chethan Babu Reddy",
      "Sebastian Bodenstedt",
      "Steffen L\u00f6ck",
      "Stefanie Speidel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02956",
    "title": "Credit card score prediction using machine learning models: A new  dataset",
    "abstract": "The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving an impressive area under the curve (AUC) of 86.7% and an accuracy rate of 91.6%, with a recall rate exceeding 80%. These results indicate the superiority of MLP in predicting the default customers and assessing the potential risks. Furthermore, they help banks and other financial institutions in predicting loan defaults at an earlier stage. ",
    "url": "https://arxiv.org/abs/2310.02956",
    "authors": [
      "Anas Arram",
      "Masri Ayob",
      "Musatafa Abbas Abbood Albadr",
      "Alaa Sulaiman",
      "Dheeb Albashish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02960",
    "title": "CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for  Open-vocabulary 3D Object Detection",
    "abstract": "Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from an arbitrary list of categories within a 3D scene, which remains seldom explored in the literature. There are primarily two fundamental problems in OV-3DDet, i.e., localizing and classifying novel objects. This paper aims at addressing the two problems simultaneously via a unified framework, under the condition of limited base categories. To localize novel 3D objects, we propose an effective 3D Novel Object Discovery strategy, which utilizes both the 3D box geometry priors and 2D semantic open-vocabulary priors to generate pseudo box labels of the novel objects. To classify novel object boxes, we further develop a cross-modal alignment module based on discovered novel boxes, to align feature spaces between 3D point cloud and image/text modalities. Specifically, the alignment process contains a class-agnostic and a class-discriminative alignment, incorporating not only the base objects with annotations but also the increasingly discovered novel objects, resulting in an iteratively enhanced alignment. The novel box discovery and crossmodal alignment are jointly learned to collaboratively benefit each other. The novel object discovery can directly impact the cross-modal alignment, while a better feature alignment can, in turn, boost the localization capability, leading to a unified OV-3DDet framework, named CoDA, for simultaneous novel object localization and classification. Extensive experiments on two challenging datasets (i.e., SUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also show a significant mAP improvement upon the best-performing alternative method by 80%. Codes and pre-trained models are released on the project page. ",
    "url": "https://arxiv.org/abs/2310.02960",
    "authors": [
      "Yang Cao",
      "Yihan Zeng",
      "Hang Xu",
      "Dan Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02970",
    "title": "Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in  Position-Orientation Space",
    "abstract": "Based on the theory of homogeneous spaces we derive \\textit{geometrically optimal edge attributes} to be used within the flexible message passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\\mathbb{R}^3$, position and orientations $\\mathbb{R}^3 {\\times} S^2$, and the group SE$(3)$ itself. Among these, $\\mathbb{R}^3 {\\times} S^2$ is an optimal choice due to the ability to represent directional information, which $\\mathbb{R}^3$ methods cannot, and it significantly enhances computational efficiency compared to indexing features on the full SE$(3)$ group. We empirically support this claim by reaching state-of-the-art results -- in accuracy and speed -- on three different benchmarks: interatomic potential energy prediction, trajectory forecasting in N-body systems, and generating molecules via equivariant diffusion models. ",
    "url": "https://arxiv.org/abs/2310.02970",
    "authors": [
      "Erik J Bekkers",
      "Sharvaree Vadgama",
      "Rob D Hesselink",
      "Putri A van der Linden",
      "David W Romero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Group Theory (math.GR)"
    ]
  },
  {
    "id": "arXiv:2310.02973",
    "title": "UniverSLU: Universal Spoken Language Understanding for Diverse  Classification and Sequence Generation Tasks with a Single Network",
    "abstract": "Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model \"UniverSLU\" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases. ",
    "url": "https://arxiv.org/abs/2310.02973",
    "authors": [
      "Siddhant Arora",
      "Hayato Futami",
      "Jee-weon Jung",
      "Yifan Peng",
      "Roshan Sharma",
      "Yosuke Kashiwagi",
      "Emiru Tsunoo",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2310.02993",
    "title": "Finding coherent node groups in directed graphs",
    "abstract": "Summarizing a large graph by grouping the nodes into clusters is a standard technique for studying the given network. Traditionally, the order of the discovered groups does not matter. However, there are applications where, for example, given a directed graph, we would like to find coherent groups while minimizing the backward cross edges. More formally, in this paper, we study a problem where we are given a directed network and are asked to partition the graph into a sequence of coherent groups while attempting to conform to the cross edges. We assume that nodes in the network have features, and we measure the group coherence by comparing these features. Furthermore, we incorporate the cross edges by penalizing the forward cross edges and backward cross edges with different weights. If the weights are set to 0, then the problem is equivalent to clustering. However, if we penalize the backward edges significantly more, then the order of discovered groups matters, and we can view our problem as a generalization of a classic segmentation problem. To solve the algorithm we consider a common iterative approach where we solve the groups given the centroids, and then find the centroids given the groups. We show that - unlike in clustering - the first subproblem is NP-hard. However, we show that if the underlying graph is a tree we can solve the subproblem with dynamic programming. In addition, if the number of groups is 2, we can solve the subproblem with a minimum cut. For the more general case, we propose a heuristic where we optimize each pair of groups separately while keeping the remaining groups intact. We also propose a greedy search where nodes are moved between the groups while optimizing the overall loss. We demonstrate with our experiments that the algorithms are practical and yield interpretable results. ",
    "url": "https://arxiv.org/abs/2310.02993",
    "authors": [
      "Iiro Kumpulainen",
      "Nikolaj Tatti"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2310.03001",
    "title": "Learning characteristic parameters and dynamics of centrifugal pumps  under multi-phase flow using physics-informed neural networks",
    "abstract": "Electrical submersible pumps (ESP) are the second most used artificial lifting equipment in the oil and gas industry due to their high flow rates and boost pressures. They often have to handle multiphase flows, which usually contain a mixture of hydrocarbons, water, and/or sediments. Given these circumstances, emulsions are commonly formed. It is a liquid-liquid flow composed of two immiscible fluids whose effective viscosity and density differ from the single phase separately. In this context, accurate modeling of ESP systems is crucial for optimizing oil production and implementing control strategies. However, real-time and direct measurement of fluid and system characteristics is often impractical due to time constraints and economy. Hence, indirect methods are generally considered to estimate the system parameters. In this paper, we formulate a machine learning model based on Physics-Informed Neural Networks (PINNs) to estimate crucial system parameters. In order to study the efficacy of the proposed PINN model, we conduct computational studies using not only simulated but also experimental data for different water-oil ratios. We evaluate the state variable's dynamics and unknown parameters for various combinations when only intake and discharge pressure measurements are available. We also study structural and practical identifiability analyses based on commonly available pressure measurements. The PINN model could reduce the requirement of expensive field laboratory tests used to estimate fluid properties. ",
    "url": "https://arxiv.org/abs/2310.03001",
    "authors": [
      "Felipe de Castro Teixeira Carvalho",
      "Kamaljyoti Nath",
      "Alberto Luiz Serpa",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.03002",
    "title": "No Forking Way: Detecting Cloning Attacks on Intel SGX Applications",
    "abstract": "Forking attacks against TEEs like Intel SGX can be carried out either by rolling back the application to a previous state, or by cloning the application and by partitioning its inputs across the cloned instances. Current solutions to forking attacks require Trusted Third Parties (TTP) that are hard to find in real-world deployments. In the absence of a TTP, many TEE applications rely on monotonic counters to mitigate forking attacks based on rollbacks; however, they have no protection mechanism against forking attack based on cloning. In this paper, we analyze 72 SGX applications and show that approximately 20% of those are vulnerable to forking attacks based on cloning - including those that rely on monotonic counters. To address this problem, we present CloneBuster, the first practical clone-detection mechanism for Intel SGX that does not rely on a TTP and, as such, can be used directly to protect existing applications. CloneBuster allows enclaves to (self-) detect whether another enclave with the same binary is running on the same platform. To do so, CloneBuster relies on a cache-based covert channel for enclaves to signal their presence to (and detect the presence of) clones on the same machine. We show that CloneBuster is robust despite a malicious OS, only incurs a marginal impact on the application performance, and adds approximately 800 LoC to the TCB. When used in conjunction with monotonic counters, CloneBuster allows applications to benefit from a comprehensive protection against forking attacks. ",
    "url": "https://arxiv.org/abs/2310.03002",
    "authors": [
      "Samira Briongos",
      "Ghassan Karame",
      "Claudio Soriente",
      "Annika Wilde"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2310.03005",
    "title": "Reversing Deep Face Embeddings with Probable Privacy Protection",
    "abstract": "Generally, privacy-enhancing face recognition systems are designed to offer permanent protection of face embeddings. Recently, so-called soft-biometric privacy-enhancement approaches have been introduced with the aim of canceling soft-biometric attributes. These methods limit the amount of soft-biometric information (gender or skin-colour) that can be inferred from face embeddings. Previous work has underlined the need for research into rigorous evaluations and standardised evaluation protocols when assessing privacy protection capabilities. Motivated by this fact, this paper explores to what extent the non-invertibility requirement can be met by methods that claim to provide soft-biometric privacy protection. Additionally, a detailed vulnerability assessment of state-of-the-art face embedding extractors is analysed in terms of the transformation complexity used for privacy protection. In this context, a well-known state-of-the-art face image reconstruction approach has been evaluated on protected face embeddings to break soft biometric privacy protection. Experimental results show that biometric privacy-enhanced face embeddings can be reconstructed with an accuracy of up to approximately 98%, depending on the complexity of the protection algorithm. ",
    "url": "https://arxiv.org/abs/2310.03005",
    "authors": [
      "Daile Osorio-Roig",
      "Paul A. Gerlitz",
      "Christian Rathgeb",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.03023",
    "title": "Human-oriented Representation Learning for Robotic Manipulation",
    "abstract": "Humans inherently possess generalizable visual representations that empower them to efficiently explore and interact with the environments in manipulation tasks. We advocate that such a representation automatically arises from simultaneously learning about multiple simple perceptual skills that are critical for everyday scenarios (e.g., hand detection, state estimate, etc.) and is better suited for learning robot manipulation policies compared to current state-of-the-art visual representations purely based on self-supervised objectives. We formalize this idea through the lens of human-oriented multi-task fine-tuning on top of pre-trained visual encoders, where each task is a perceptual skill tied to human-environment interactions. We introduce Task Fusion Decoder as a plug-and-play embedding translator that utilizes the underlying relationships among these perceptual skills to guide the representation learning towards encoding meaningful structure for what's important for all perceptual skills, ultimately empowering learning of downstream robotic manipulation tasks. Extensive experiments across a range of robotic tasks and embodiments, in both simulations and real-world environments, show that our Task Fusion Decoder consistently improves the representation of three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for downstream manipulation policy-learning. Project page: https://sites.google.com/view/human-oriented-robot-learning ",
    "url": "https://arxiv.org/abs/2310.03023",
    "authors": [
      "Mingxiao Huo",
      "Mingyu Ding",
      "Chenfeng Xu",
      "Thomas Tian",
      "Xinghao Zhu",
      "Yao Mu",
      "Lingfeng Sun",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02276",
    "title": "Deep learning soliton dynamics and complex potentials recognition for 1D  and 2D PT-symmetric saturable nonlinear Schr\u00f6dinger equations",
    "abstract": "In this paper, we firstly extend the physics-informed neural networks (PINNs) to learn data-driven stationary and non-stationary solitons of 1D and 2D saturable nonlinear Schr\\\"odinger equations (SNLSEs) with two fundamental PT-symmetric Scarf-II and periodic potentials in optical fibers. Secondly, the data-driven inverse problems are studied for PT-symmetric potential functions discovery rather than just potential parameters in the 1D and 2D SNLSEs. Particularly, we propose a modified PINNs (mPINNs) scheme to identify directly the PT potential functions of the 1D and 2D SNLSEs by the solution data. And the inverse problems about 1D and 2D PT -symmetric potentials depending on propagation distance z are also investigated using mPINNs method. We also identify the potential functions by the PINNs applied to the stationary equation of the SNLSE. Furthermore, two network structures are compared under different parameter conditions such that the predicted PT potentials can achieve the similar high accuracy. These results illustrate that the established deep neural networks can be successfully used in 1D and 2D SNLSEs with high accuracies. Moreover, some main factors affecting neural networks performance are discussed in 1D and 2D PT Scarf-II and periodic potentials, including activation functions, structures of the networks, and sizes of the training data. In particular, twelve different nonlinear activation functions are in detail analyzed containing the periodic and non-periodic functions such that it is concluded that selecting activation functions according to the form of solution and equation usually can achieve better effect. ",
    "url": "https://arxiv.org/abs/2310.02276",
    "authors": [
      "Jin Song",
      "Zhenya Yan"
    ],
    "subjectives": [
      "Pattern Formation and Solitons (nlin.PS)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2310.02323",
    "title": "Approximately Equivariant Quantum Neural Network for $p4m$ Group  Symmetries in Images",
    "abstract": "Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms which can be efficiently simulated with a low depth on near-term quantum hardware in the presence of noises. However, their performance highly relies on choosing the most suitable architecture of Variational Quantum Algorithms (VQAs), and the problem-agnostic models often suffer issues regarding trainability and generalization power. As a solution, the most recent works explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with respect to the underlying symmetry of the dataset. GQML adds an inductive bias to the model by incorporating the prior knowledge on the given dataset and leads to enhancing the optimization performance while constraining the search space. This work proposes equivariant Quantum Convolutional Neural Networks (EquivQCNNs) for image classification under planar $p4m$ symmetry, including reflectional and $90^\\circ$ rotational symmetry. We present the results tested in different use cases, such as phase detection of the 2D Ising model and classification of the extended MNIST dataset, and compare them with those obtained with the non-equivariant model, proving that the equivariance fosters better generalization of the model. ",
    "url": "https://arxiv.org/abs/2310.02323",
    "authors": [
      "Su Yeon Chang",
      "Michele Grossi",
      "Bertrand Le Saux",
      "Sofia Vallecorsa"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02340",
    "title": "Learning Interpretable Deep Disentangled Neural Networks for  Hyperspectral Unmixing",
    "abstract": "Although considerable effort has been dedicated to improving the solution to the hyperspectral unmixing problem, non-idealities such as complex radiation scattering and endmember variability negatively impact the performance of most existing algorithms and can be very challenging to address. Recently, deep learning-based frameworks have been explored for hyperspectral umixing due to their flexibility and powerful representation capabilities. However, such techniques either do not address the non-idealities of the unmixing problem, or rely on black-box models which are not interpretable. In this paper, we propose a new interpretable deep learning method for hyperspectral unmixing that accounts for nonlinearity and endmember variability. The proposed method leverages a probabilistic variational deep-learning framework, where disentanglement learning is employed to properly separate the abundances and endmembers. The model is learned end-to-end using stochastic backpropagation, and trained using a self-supervised strategy which leverages benefits from semi-supervised learning techniques. Furthermore, the model is carefully designed to provide a high degree of interpretability. This includes modeling the abundances as a Dirichlet distribution, the endmembers using low-dimensional deep latent variable representations, and using two-stream neural networks composed of additive piecewise-linear/nonlinear components. Experimental results on synthetic and real datasets illustrate the performance of the proposed method compared to state-of-the-art algorithms. ",
    "url": "https://arxiv.org/abs/2310.02340",
    "authors": [
      "Ricardo Augusto Borsoi",
      "Deniz Erdo\u011fmu\u015f",
      "Tales Imbiriba"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02429",
    "title": "The directional flow generated by peristalsis in perivascular networks  -- theoretical and numerical reduced-order descriptions",
    "abstract": "Directional fluid flow in perivascular spaces surrounding cerebral arteries is hypothesized to play a key role in brain solute transport and clearance. While various drivers for pulsatile flow, such as cardiac or respiratory pulsations, are well quantified, the question remains as to which mechanisms could induce directional flow within physiological regimes. To address this question, we develop theoretical and numerical reduced-order models to quantify the directional (net) flow induceable by peristaltic pumping in periarterial networks. Each periarterial element is modeled as a slender annular space bounded internally by a circular tube supporting a periodic traveling (peristaltic) wave. Under the reasonable assumptions of small Reynolds number flow, small radii, and small-amplitude peristaltic waves, we use lubrication theory and regular perturbation methods to derive theoretical expressions for the directional net flow and pressure distribution in the perivascular network. The reduced model is used to derive closed-form analytical expressions for the net flow for simple network configurations of interest, including single elements, two elements in tandem, and a three element bifurcation, with results compared with numerical predictions. In particular, we provide a computable theoretical estimate of the net flow induced by peristaltic motion in perivascular networks as a function of physiological parameters, notably wave length, frequency, amplitude and perivascular dimensions. Quantifying the maximal net flow for specific physiological regimes, we find that vasomotion may induce net pial periarterial flow velocities on the order of a few to tens of mum/s and that sleep-related changes in vasomotion pulsatility may drive a threefold flow increase. ",
    "url": "https://arxiv.org/abs/2310.02429",
    "authors": [
      "Ingeborg G. Gjerde",
      "Marie E. Rognes",
      "Antonio L. Sanchez"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ]
  },
  {
    "id": "arXiv:2310.02581",
    "title": "Online Estimation and Inference for Robust Policy Evaluation in  Reinforcement Learning",
    "abstract": "Recently, reinforcement learning has gained prominence in modern statistics, with policy evaluation being a key component. Unlike traditional machine learning literature on this topic, our work places emphasis on statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards to follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework. In this paper, we develop an online robust policy evaluation procedure, and establish the limiting distribution of our estimator, based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinforcement learning, offering a more versatile and reliable approach to policy evaluation. Finally, we validate the efficacy of our algorithm through numerical experiments conducted in real-world reinforcement learning experiments. ",
    "url": "https://arxiv.org/abs/2310.02581",
    "authors": [
      "Weidong Liu",
      "Jiyuan Tu",
      "Yichen Zhang",
      "Xi Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02855",
    "title": "Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark  Detection",
    "abstract": "Cephalometric landmark detection on lateral skull X-ray images plays a crucial role in the diagnosis of certain dental diseases. Accurate and effective identification of these landmarks presents a significant challenge. Based on extensive data observations and quantitative analyses, we discovered that visual features from different receptive fields affect the detection accuracy of various landmarks differently. As a result, we employed an image pyramid structure, integrating multiple resolutions as input to train a series of models with different receptive fields, aiming to achieve the optimal feature combination for each landmark. Moreover, we applied several data augmentation techniques during training to enhance the model's robustness across various devices and measurement alternatives. We implemented this method in the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge and achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate (SDR) 2.0mm of 74.18% in the final testing phase. ",
    "url": "https://arxiv.org/abs/2310.02855",
    "authors": [
      "Dongqian Guo",
      "Wencheng Han"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.02904",
    "title": "Spline-based neural network interatomic potentials: blending classical  and machine learning models",
    "abstract": "While machine learning (ML) interatomic potentials (IPs) are able to achieve accuracies nearing the level of noise inherent in the first-principles data to which they are trained, it remains to be shown if their increased complexities are strictly necessary for constructing high-quality IPs. In this work, we introduce a new MLIP framework which blends the simplicity of spline-based MEAM (s-MEAM) potentials with the flexibility of a neural network (NN) architecture. The proposed framework, which we call the spline-based neural network potential (s-NNP), is a simplified version of the traditional NNP that can be used to describe complex datasets in a computationally efficient manner. We demonstrate how this framework can be used to probe the boundary between classical and ML IPs, highlighting the benefits of key architectural changes. Furthermore, we show that using spline filters for encoding atomic environments results in a readily interpreted embedding layer which can be coupled with modifications to the NN to incorporate expected physical behaviors and improve overall interpretability. Finally, we test the flexibility of the spline filters, observing that they can be shared across multiple chemical systems in order to provide a convenient reference point from which to begin performing cross-system analyses. ",
    "url": "https://arxiv.org/abs/2310.02904",
    "authors": [
      "Joshua A. Vita",
      "Dallas R. Trinkle"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.02924",
    "title": "Quantum forgery attacks against OTR structures based on Simon's  algorithm",
    "abstract": "Classical forgery attacks against Offset Two-round (OTR) structures require some harsh conditions, such as some plaintext and ciphertext pairs need to be known, and the success probability is not too high. To solve these problems, a quantum forgery attack on OTR structure using Simon's algorithm is proposed. The attacker intercept the ciphertext-tag pair $(C,T)$ between the sender and receiver, while Simon's algorithm is used to find the period of the tag generation function in OTR, then we can successfully forge new ciphertext $C'$ ($C'\\ne C$) for intercepted tag $T$. For a variant of OTR structure (Pr{/o}st-OTR-Even-Mansour structure), a universal forgery attack, in which it is easy to generate the correct tag of any given message if the attacker is allowed to change a single block in it, is proposed. It first obtains the secret parameter L using Simon's algorithm, then the secret parameter L is used to find the keys $k_1$ and $k_2$, so that an attacker can forge the changed messages. It only needs several plaintext blocks to help obtain the keys to forge any messages. Performance analysis shows that the query complexity of our attack is $O(n)$, and its success probability is very close to 1. ",
    "url": "https://arxiv.org/abs/2310.02924",
    "authors": [
      "Wenjie Liu",
      "Mengting Wang",
      "Zixian Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Emerging Technologies (cs.ET)"
    ]
  },
  {
    "id": "arXiv:2310.02971",
    "title": "Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech  Model",
    "abstract": "Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning. ",
    "url": "https://arxiv.org/abs/2310.02971",
    "authors": [
      "Kai-Wei Chang",
      "Ming-Hsin Chen",
      "Yun-Ping Lin",
      "Jing Neng Hsu",
      "Paul Kuo-Ming Huang",
      "Chien-yu Huang",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Signal Processing (eess.SP)"
    ]
  },
  {
    "id": "arXiv:1911.02903",
    "title": "How Implicit Regularization of ReLU Neural Networks Characterizes the  Learned Function -- Part I: the 1-D Case of Two Layers with Random First  Layer",
    "abstract": " Comments: adding Appendix C for more intuition, fixing typos, improving formulations, (moving end of Section 3.1 into Appendix B) ",
    "url": "https://arxiv.org/abs/1911.02903",
    "authors": [
      "Jakob Heiss",
      "Josef Teichmann",
      "Hanna Wutte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2106.05455",
    "title": "AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange",
    "abstract": " Title: AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange ",
    "url": "https://arxiv.org/abs/2106.05455",
    "authors": [
      "Liang Zeng",
      "Jin Xu",
      "Zijun Yao",
      "Yanqiao Zhu",
      "Jian Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2108.05641",
    "title": "SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural  Network",
    "abstract": " Title: SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural  Network ",
    "url": "https://arxiv.org/abs/2108.05641",
    "authors": [
      "Jinpeng Chen",
      "Haiyang Li",
      "Xudong Zhang",
      "Fan Zhang",
      "Senzhang Wang",
      "Kaimin Wei",
      "Jiaqi Ji"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2110.14053",
    "title": "NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks",
    "abstract": " Title: NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks ",
    "url": "https://arxiv.org/abs/2110.14053",
    "authors": [
      "Wenxi Wang",
      "Yang Hu",
      "Mohit Tiwari",
      "Sarfraz Khurshid",
      "Kenneth McMillan",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2112.00510",
    "title": "Trimap-guided Feature Mining and Fusion Network for Natural Image  Matting",
    "abstract": " Comments: Accepted to Computer Vision and Image Understanding ",
    "url": "https://arxiv.org/abs/2112.00510",
    "authors": [
      "Weihao Jiang",
      "Dongdong Yu",
      "Zhaozhi Xie",
      "Yaoyi Li",
      "Zehuan Yuan",
      "Hongtao Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2205.10159",
    "title": "Getting a-Round Guarantees: Floating-Point Attacks on Certified  Robustness",
    "abstract": " Title: Getting a-Round Guarantees: Floating-Point Attacks on Certified  Robustness ",
    "url": "https://arxiv.org/abs/2205.10159",
    "authors": [
      "Jiankai Jin",
      "Olga Ohrimenko",
      "Benjamin I. P. Rubinstein"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2206.03776",
    "title": "High-Throughput Secure Multiparty Computation with an Honest Majority in  Various Network Settings",
    "abstract": " Title: High-Throughput Secure Multiparty Computation with an Honest Majority in  Various Network Settings ",
    "url": "https://arxiv.org/abs/2206.03776",
    "authors": [
      "Christopher Harth-Kitzerow",
      "Georg Carcle"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ]
  },
  {
    "id": "arXiv:2208.05950",
    "title": "Interactive Code Generation via Test-Driven User-Intent Formalization",
    "abstract": " Comments: 18 pages ",
    "url": "https://arxiv.org/abs/2208.05950",
    "authors": [
      "Shuvendu K. Lahiri",
      "Sarah Fakhoury",
      "Aaditya Naik",
      "Georgios Sakkas",
      "Saikat Chakraborty",
      "Madanlal Musuvathi",
      "Piali Choudhury",
      "Curtis von Veh",
      "Jeevana Priya Inala",
      "Chenglong Wang",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ]
  },
  {
    "id": "arXiv:2208.10317",
    "title": "Latent Neural Stochastic Differential Equations for Change Point  Detection",
    "abstract": " Title: Latent Neural Stochastic Differential Equations for Change Point  Detection ",
    "url": "https://arxiv.org/abs/2208.10317",
    "authors": [
      "Artem Ryzhikov",
      "Mikhail Hushchyn",
      "Denis Derkach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2209.00517",
    "title": "The Neural Process Family: Survey, Applications and Perspectives",
    "abstract": " Comments: Work under review ",
    "url": "https://arxiv.org/abs/2209.00517",
    "authors": [
      "Saurav Jha",
      "Dong Gong",
      "Xuesong Wang",
      "Richard E. Turner",
      "Lina Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2211.14118",
    "title": "MS-PS: A Multi-Scale Network for Photometric Stereo With a New  Comprehensive Training Dataset",
    "abstract": " Title: MS-PS: A Multi-Scale Network for Photometric Stereo With a New  Comprehensive Training Dataset ",
    "url": "https://arxiv.org/abs/2211.14118",
    "authors": [
      "Cl\u00e9ment Hardy",
      "Yvain Qu\u00e9au",
      "David Tschumperl\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2302.02173",
    "title": "Neural Time Series Analysis with Fourier Transform: A Survey",
    "abstract": " Title: Neural Time Series Analysis with Fourier Transform: A Survey ",
    "url": "https://arxiv.org/abs/2302.02173",
    "authors": [
      "Kun Yi",
      "Qi Zhang",
      "Shoujin Wang",
      "Guodong Long",
      "Hui He",
      "Zhendong Niu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2302.04519",
    "title": "RayNet: A Simulation Platform for Developing Reinforcement  Learning-Driven Network Protocols",
    "abstract": " Title: RayNet: A Simulation Platform for Developing Reinforcement  Learning-Driven Network Protocols ",
    "url": "https://arxiv.org/abs/2302.04519",
    "authors": [
      "Luca Giacomoni",
      "Basil Benny",
      "George Parisis"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ]
  },
  {
    "id": "arXiv:2302.07426",
    "title": "Computational Complexity of Learning Neural Networks: Smoothness and  Degeneracy",
    "abstract": " Comments: Changed the title, and made some other minor modifications. arXiv admin note: text overlap with arXiv:2101.08303 ",
    "url": "https://arxiv.org/abs/2302.07426",
    "authors": [
      "Amit Daniely",
      "Nathan Srebro",
      "Gal Vardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2302.11351",
    "title": "Regularised neural networks mimic human insight",
    "abstract": " Comments: 17 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2302.11351",
    "authors": [
      "Anika T. L\u00f6we",
      "L\u00e9o Touzo",
      "Paul S. Muhle-Karbe",
      "Andrew M. Saxe",
      "Christopher Summerfield",
      "Nicolas W. Schuck"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ]
  },
  {
    "id": "arXiv:2303.01590",
    "title": "Technical report: Graph Neural Networks go Grammatical",
    "abstract": " Comments: 24 pages, 11 figures ",
    "url": "https://arxiv.org/abs/2303.01590",
    "authors": [
      "Jason Piquenot",
      "Aldo Moscatelli",
      "Maxime B\u00e9rar",
      "Pierre H\u00e9roux",
      "Romain raveaux",
      "Jean-Yves Ramel",
      "S\u00e9bastien Adam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2303.06138",
    "title": "Learning Object-Centric Neural Scattering Functions for Free-Viewpoint  Relighting and Scene Composition",
    "abstract": " Comments: Journal extension of arXiv:2012.08503 (TMLR 2023). The first two authors contributed equally to this work. Project page: this https URL ",
    "url": "https://arxiv.org/abs/2303.06138",
    "authors": [
      "Hong-Xing Yu",
      "Michelle Guo",
      "Alireza Fathi",
      "Yen-Yu Chang",
      "Eric Ryan Chan",
      "Ruohan Gao",
      "Thomas Funkhouser",
      "Jiajun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2303.10358",
    "title": "Neural Frailty Machine: Beyond proportional hazard assumption in neural  survival regressions",
    "abstract": " Title: Neural Frailty Machine: Beyond proportional hazard assumption in neural  survival regressions ",
    "url": "https://arxiv.org/abs/2303.10358",
    "authors": [
      "Ruofan Wu",
      "Jiawei Qiao",
      "Mingzhe Wu",
      "Wen Yu",
      "Ming Zheng",
      "Tengfei Liu",
      "Tianyi Zhang",
      "Weiqiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ]
  },
  {
    "id": "arXiv:2304.05727",
    "title": "Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks",
    "abstract": " Comments: 18 pages + supplement ",
    "url": "https://arxiv.org/abs/2304.05727",
    "authors": [
      "Lorenz Linhardt",
      "Klaus-Robert M\u00fcller",
      "Gr\u00e9goire Montavon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2305.08099",
    "title": "Self-supervised Neural Factor Analysis for Disentangling Utterance-level  Speech Representations",
    "abstract": " Comments: accepted by ICML 2023 ",
    "url": "https://arxiv.org/abs/2305.08099",
    "authors": [
      "Weiwei Lin",
      "Chenhang He",
      "Man-Wai Mak",
      "Youzhi Tu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ]
  },
  {
    "id": "arXiv:2305.11857",
    "title": "Computing high-dimensional optimal transport by flow neural networks",
    "abstract": " Title: Computing high-dimensional optimal transport by flow neural networks ",
    "url": "https://arxiv.org/abs/2305.11857",
    "authors": [
      "Chen Xu",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ]
  },
  {
    "id": "arXiv:2305.13650",
    "title": "Robust Model-Based Optimization for Challenging Fitness Landscapes",
    "abstract": " Title: Robust Model-Based Optimization for Challenging Fitness Landscapes ",
    "url": "https://arxiv.org/abs/2305.13650",
    "authors": [
      "Saba Ghaffari",
      "Ehsan Saleh",
      "Alexander G. Schwing",
      "Yu-Xiong Wang",
      "Martin D. Burke",
      "Saurabh Sinha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2305.14585",
    "title": "Faithful and Efficient Explanations for Neural Networks via Neural  Tangent Kernel Surrogate Models",
    "abstract": " Comments: Updated 10/4/2023: significant changes for ICLR2023 submission. Github repository will be live soon. 9 pages, 2 figures, 3 tables ",
    "url": "https://arxiv.org/abs/2305.14585",
    "authors": [
      "Andrew Engel",
      "Zhichao Wang",
      "Natalie S. Frank",
      "Ioana Dumitriu",
      "Sutanay Choudhury",
      "Anand Sarwate",
      "Tony Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.15508",
    "title": "How to fix a broken confidence estimator: Evaluating post-hoc methods  for selective classification with deep neural networks",
    "abstract": " Title: How to fix a broken confidence estimator: Evaluating post-hoc methods  for selective classification with deep neural networks ",
    "url": "https://arxiv.org/abs/2305.15508",
    "authors": [
      "Lu\u00eds Felipe P. Cattelan",
      "Danilo Silva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.17212",
    "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural  Networks",
    "abstract": " Comments: Code available at this https URL ",
    "url": "https://arxiv.org/abs/2305.17212",
    "authors": [
      "Atli Kosson",
      "Bettina Messmer",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2305.17359",
    "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of  GPT-Generated Text",
    "abstract": " Comments: Updates ",
    "url": "https://arxiv.org/abs/2305.17359",
    "authors": [
      "Xianjun Yang",
      "Wei Cheng",
      "Yue Wu",
      "Linda Petzold",
      "William Yang Wang",
      "Haifeng Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.01095",
    "title": "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization",
    "abstract": " Title: Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization ",
    "url": "https://arxiv.org/abs/2306.01095",
    "authors": [
      "Navid Ansari",
      "Hans-Peter Seidel",
      "Vahid Babaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ]
  },
  {
    "id": "arXiv:2306.03091",
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
    "abstract": " Title: RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems ",
    "url": "https://arxiv.org/abs/2306.03091",
    "authors": [
      "Tianyang Liu",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ]
  },
  {
    "id": "arXiv:2306.05880",
    "title": "Time Series Continuous Modeling for Imputation and Forecasting with  Implicit Neural Representations",
    "abstract": " Title: Time Series Continuous Modeling for Imputation and Forecasting with  Implicit Neural Representations ",
    "url": "https://arxiv.org/abs/2306.05880",
    "authors": [
      "Etienne Le Naour",
      "Louis Serrano",
      "L\u00e9on Migus",
      "Yuan Yin",
      "Ghislain Agoua",
      "Nicolas Baskiotis",
      "Patrick Gallinari",
      "Vincent Guigue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.09222",
    "title": "Stochastic Re-weighted Gradient Descent via Distributionally Robust  Optimization",
    "abstract": " Title: Stochastic Re-weighted Gradient Descent via Distributionally Robust  Optimization ",
    "url": "https://arxiv.org/abs/2306.09222",
    "authors": [
      "Ramnath Kumar",
      "Kushal Majmundar",
      "Dheeraj Nagaraj",
      "Arun Sai Suggala"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2306.11268",
    "title": "LightRidge: An End-to-end Agile Design Framework for Diffractive Optical  Neural Networks",
    "abstract": " Comments: 16 pages, 13 figures, Architectural Support for Programming Languages and Operating Systems (ASPLOS'24) ",
    "url": "https://arxiv.org/abs/2306.11268",
    "authors": [
      "Yingjie Li",
      "Ruiyang Chen",
      "Minhan Lou",
      "Berardi Sensale-Rodriguez",
      "Weilu Gao",
      "Cunxi Yu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Optics (physics.optics)"
    ]
  },
  {
    "id": "arXiv:2306.11313",
    "title": "Deep graph kernel point processes",
    "abstract": " Title: Deep graph kernel point processes ",
    "url": "https://arxiv.org/abs/2306.11313",
    "authors": [
      "Zheng Dong",
      "Matthew Repasky",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2306.15410",
    "title": "AutoGraph: Predicting Lane Graphs from Traffic Observations",
    "abstract": " Comments: 8 pages, 6 figures ",
    "url": "https://arxiv.org/abs/2306.15410",
    "authors": [
      "Jannik Z\u00fcrn",
      "Ingmar Posner",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2306.15963",
    "title": "Fused Gromov-Wasserstein Graph Mixup for Graph-level Classifications",
    "abstract": " Comments: Accepted in NeurIPS 2023 ",
    "url": "https://arxiv.org/abs/2306.15963",
    "authors": [
      "Xinyu Ma",
      "Xu Chu",
      "Yasha Wang",
      "Yang Lin",
      "Junfeng Zhao",
      "Liantao Ma",
      "Wenwu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2307.01918",
    "title": "Computational Reproducibility in Computational Social Science",
    "abstract": " Comments: v1: Working Paper; v2: fixed missing citation in text; v3: fixed some minor errors and formatting; v4: shortened paper ",
    "url": "https://arxiv.org/abs/2307.01918",
    "authors": [
      "David Schoch",
      "Chung-hong Chan",
      "Claudia Wagner",
      "Arnim Bleier"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ]
  },
  {
    "id": "arXiv:2307.02863",
    "title": "ValiTex -- a unified validation framework for computational text-based  measures of social science constructs",
    "abstract": " Title: ValiTex -- a unified validation framework for computational text-based  measures of social science constructs ",
    "url": "https://arxiv.org/abs/2307.02863",
    "authors": [
      "Lukas Birkenmaier",
      "Claudia Wagner",
      "Clemens Lechner"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2307.04139",
    "title": "A Randomized Algorithm for Single-Source Shortest Path on Undirected  Real-Weighted Graphs",
    "abstract": " Comments: 17 pages ",
    "url": "https://arxiv.org/abs/2307.04139",
    "authors": [
      "Ran Duan",
      "Jiayi Mao",
      "Xinkai Shu",
      "Longhui Yin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2307.11833",
    "title": "PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural  Networks",
    "abstract": " Comments: 16 pages (including 9 pages of main text, 3 pages of references, and 4 pages of appendix), 9 figures, 5 tables ",
    "url": "https://arxiv.org/abs/2307.11833",
    "authors": [
      "Zhiyuan Zhao",
      "Xueying Ding",
      "B. Aditya Prakash"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.05613",
    "title": "Learning the Geodesic Embedding with Graph Neural Networks",
    "abstract": " Comments: SIGGRAPH Asia 2023, Journal Track ",
    "url": "https://arxiv.org/abs/2309.05613",
    "authors": [
      "Bo Pang",
      "Zhongtian Zheng",
      "Guoping Wang",
      "Peng-Shuai Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ]
  },
  {
    "id": "arXiv:2309.10650",
    "title": "MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning  Pipeline for Histopathology Whole Slide Images",
    "abstract": " Comments: Accepted for publication at BMVC 2023 ",
    "url": "https://arxiv.org/abs/2309.10650",
    "authors": [
      "Amaya Gallagher-Syed",
      "Luca Rossi",
      "Felice Rivellese",
      "Costantino Pitzalis",
      "Myles Lewis",
      "Michael Barnes",
      "Gregory Slabaugh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ]
  },
  {
    "id": "arXiv:2309.13570",
    "title": "Towards Robust Mobile Digital-Twin Tracking via An RGBD-based  Transformer Model and A Comprehensive Mobile Dataset",
    "abstract": " Title: Towards Robust Mobile Digital-Twin Tracking via An RGBD-based  Transformer Model and A Comprehensive Mobile Dataset ",
    "url": "https://arxiv.org/abs/2309.13570",
    "authors": [
      "Zixun Huang",
      "Keling Yao",
      "Seth Z. Zhao",
      "Chuanyu Pan",
      "Tianjian Xu",
      "Weiyu Feng",
      "Allen Y. Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.13575",
    "title": "Probabilistic Weight Fixing: Large-scale training of neural network  weight uncertainties for quantization",
    "abstract": " Title: Probabilistic Weight Fixing: Large-scale training of neural network  weight uncertainties for quantization ",
    "url": "https://arxiv.org/abs/2309.13575",
    "authors": [
      "Christopher Subia-Waud",
      "Srinandan Dasmahapatra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2309.15669",
    "title": "On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning",
    "abstract": " Title: On Computational Entanglement and Its Interpretation in Adversarial  Machine Learning ",
    "url": "https://arxiv.org/abs/2309.15669",
    "authors": [
      "YenLung Lai",
      "Xingbo Dong",
      "Zhe Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Computational Physics (physics.comp-ph)"
    ]
  },
  {
    "id": "arXiv:2309.16364",
    "title": "FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for  Independence-Assumption-Free Uncertainty Estimation",
    "abstract": " Title: FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for  Independence-Assumption-Free Uncertainty Estimation ",
    "url": "https://arxiv.org/abs/2309.16364",
    "authors": [
      "Songlin Wei",
      "Jiazhao Zhang",
      "Yang Wang",
      "Fanbo Xiang",
      "Hao Su",
      "He Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2309.17348",
    "title": "Efficient Biologically Plausible Adversarial Training",
    "abstract": " Title: Efficient Biologically Plausible Adversarial Training ",
    "url": "https://arxiv.org/abs/2309.17348",
    "authors": [
      "Matilde Tristany Farinha",
      "Thomas Ortner",
      "Giorgia Dellaferrera",
      "Benjamin Grewe",
      "Angeliki Pantazi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2309.17357",
    "title": "Module-wise Training of Neural Networks via the Minimizing Movement  Scheme",
    "abstract": " Comments: NeurIPS 2023. arXiv admin note: text overlap with arXiv:2210.00949 ",
    "url": "https://arxiv.org/abs/2309.17357",
    "authors": [
      "Skander Karkar",
      "Ibrahim Ayed",
      "Emmanuel de B\u00e9zenac",
      "Patrick Gallinari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.00357",
    "title": "Structural Adversarial Objectives for Self-Supervised Representation  Learning",
    "abstract": " Title: Structural Adversarial Objectives for Self-Supervised Representation  Learning ",
    "url": "https://arxiv.org/abs/2310.00357",
    "authors": [
      "Xiao Zhang",
      "Michael Maire"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ]
  },
  {
    "id": "arXiv:2310.00526",
    "title": "Are Graph Neural Networks Optimal Approximation Algorithms?",
    "abstract": " Comments: Figure 1 pg 2. is inaccurate ",
    "url": "https://arxiv.org/abs/2310.00526",
    "authors": [
      "Morris Yau",
      "Eric Lu",
      "Nikolaos Karalias",
      "Jessica Xu",
      "Stefanie Jegelka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2310.00527",
    "title": "Self-supervised Learning of Contextualized Local Visual Embeddings",
    "abstract": " Comments: Pre-print. 4th Visual Inductive Priors for Data-Efficient Deep Learning Workshop ICCV 2023. Code at this https URL ",
    "url": "https://arxiv.org/abs/2310.00527",
    "authors": [
      "Thalles Santos Silva",
      "Helio Pedrini",
      "Ad\u00edn Ram\u00edrez Rivera"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ]
  },
  {
    "id": "arXiv:2310.00574",
    "title": "SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences  on CPUs",
    "abstract": " Title: SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences  on CPUs ",
    "url": "https://arxiv.org/abs/2310.00574",
    "authors": [
      "Cyrus Zhou",
      "Zack Hassman",
      "Ruize Xu",
      "Dhirpal Shah",
      "Vaugnn Richard",
      "Yanjing Li"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ]
  },
  {
    "id": "arXiv:2310.00648",
    "title": "Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning",
    "abstract": " Comments: 16 pages, 5 figures ",
    "url": "https://arxiv.org/abs/2310.00648",
    "authors": [
      "Lauren Hong",
      "Ting Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ]
  },
  {
    "id": "arXiv:2310.01469",
    "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial  Examples",
    "abstract": " Title: LLM Lies: Hallucinations are not Bugs, but Features as Adversarial  Examples ",
    "url": "https://arxiv.org/abs/2310.01469",
    "authors": [
      "Jia-Yu Yao",
      "Kun-Peng Ning",
      "Zhen-Hui Liu",
      "Mu-Nan Ning",
      "Li Yuan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.01737",
    "title": "Blending Imitation and Reinforcement Learning for Robust Policy  Improvement",
    "abstract": " Title: Blending Imitation and Reinforcement Learning for Robust Policy  Improvement ",
    "url": "https://arxiv.org/abs/2310.01737",
    "authors": [
      "Xuefeng Liu",
      "Takuma Yoneda",
      "Rick L. Stevens",
      "Matthew R. Walter",
      "Yuxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ]
  },
  {
    "id": "arXiv:2310.01866",
    "title": "Route Design in Sheepdog System--Traveling Salesman Problem Formulation  and Evolutionary Computation Solution--",
    "abstract": " Title: Route Design in Sheepdog System--Traveling Salesman Problem Formulation  and Evolutionary Computation Solution-- ",
    "url": "https://arxiv.org/abs/2310.01866",
    "authors": [
      "Wataru Imahayashi",
      "Yusuke Tsunoda",
      "Masaki Ogura"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ]
  },
  {
    "id": "arXiv:2310.02094",
    "title": "CoNO: Complex Neural Operator for Continuous Dynamical Systems",
    "abstract": " Title: CoNO: Complex Neural Operator for Continuous Dynamical Systems ",
    "url": "https://arxiv.org/abs/2310.02094",
    "authors": [
      "Karn Tiwari",
      "N M Anoop Krishnan",
      "Prathosh A P"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ]
  },
  {
    "id": "arXiv:2310.02130",
    "title": "Clustering Graphs of Bounded Treewidth to Minimize the Sum of  Radius-Dependent Costs",
    "abstract": " Comments: updated funding information ",
    "url": "https://arxiv.org/abs/2310.02130",
    "authors": [
      "Lukas Drexler",
      "Jan H\u00f6ckendorff",
      "Joshua K\u00f6nen",
      "Kevin Schewior"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ]
  },
  {
    "id": "arXiv:2310.02244",
    "title": "Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks",
    "abstract": " Title: Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks ",
    "url": "https://arxiv.org/abs/2310.02244",
    "authors": [
      "Greg Yang",
      "Dingli Yu",
      "Chen Zhu",
      "Soufiane Hayou"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Probability (math.PR)"
    ]
  }
]